{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c205d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msamwelmollel\\anaconda3\\envs\\python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import together\n",
    "from datasets import load_dataset\n",
    "\n",
    "WANDB_API_KEY= \"50a4ae3d3476d2fa831569e4d22bfec32200130f\"\n",
    "\n",
    "\n",
    "# WANDB_API_KEY = None # replace None with your weights and Biases API key (optional)\n",
    "\n",
    "\n",
    "# lets use our base model to see how it works before we finetune it\n",
    "\n",
    "# base_model_name = \"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
    "base_model_name = \"togethercomputer/LLaMA-2-7B-32K\"\n",
    "#base_model_name = \"togethercomputer/llama-2-7b-chat\"\n",
    "\n",
    "# Specify path to your JSON file\n",
    "json_path = os.path.join(os.getcwd(), 'google_translated - Copy.json')\n",
    "\n",
    "# Open the JSON file and load it into a dict\n",
    "with open(json_path, 'r') as json_file:\n",
    "    ourdataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168c9d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>> You are a good robot <</SYS>> hi robot [/INST] hello human </s> are you good? [/INST] yes im good </s> are you bad? [/INST] no, im good </s>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_to_llama2_chat(system_prompt, user_model_chat_list):\n",
    "\n",
    "    \"\"\" this function follows from\n",
    "    https://docs.together.ai/docs/fine-tuning-task-specific-sequences\n",
    "\n",
    "    It converts this ourdataset into the Llama-2 prompting structure\n",
    "\n",
    "    Args:\n",
    "      system_prompt (str): instructions from you the developer to the AI\n",
    "      user_model_chat_list (List[Tuple[str,str]]): a list of tuples,\n",
    "        where each tuple is a pair or exchange of string utterances, the first by the user,\n",
    "        the second by the AI. The earlier exchanges are on the left, meaning time\n",
    "        runs left to right.\n",
    "    Returns:\n",
    "      growing_prompt (str): the concatenated sequence starting with system_prompt and\n",
    "        alternating utterances between the user and AI with the last AI utternance on the right.\n",
    "    \"\"\"\n",
    "\n",
    "    growing_prompt = f\"\"\"<s>[INST] <<SYS>> {system_prompt} <</SYS>>\"\"\"\n",
    "\n",
    "    for user_msg, model_answer in user_model_chat_list:\n",
    "        growing_prompt += f\"\"\" {user_msg} [/INST] {model_answer} </s>\"\"\"\n",
    "\n",
    "    return growing_prompt\n",
    "\n",
    "format_to_llama2_chat(\n",
    "    \"You are a good robot\",\n",
    "    [(\"hi robot\", \"hello human\"),(\"are you good?\", \"yes im good\"),(\"are you bad?\", \"no, im good\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b7eb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>> you are a helpful  assistant <</SYS>> Je tatu jumlisha tatu ni ngapi? [/INST]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chat_prompt = \"<s>[INST] <<SYS>> you are a helpful  assistant <</SYS>> Je tatu jumlisha tatu ni ngapi? [/INST]\"\n",
    "test_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a3b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for sample in ourdataset[0:10000]:\n",
    "\n",
    "    instruction_input_separator = random.choice([\":\", \": \", \"\\n\", \"\\n\\n\", \" \"])\n",
    "    # instruction_input_separator = random.choice([\":\" ])\n",
    "    input = sample['input'] if sample['input'] is not None else \"\"\n",
    "    instruction = sample['instruction'] if sample['instruction'] is not None else \"\"\n",
    "\n",
    "    training_sequence = format_to_llama2_chat(\n",
    "        \"you are a helpful assistant\",\n",
    "        [(instruction+instruction_input_separator+input,sample['output'])]\n",
    "    )\n",
    "\n",
    "    data_list.append({\n",
    "        \"text\":training_sequence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacb8f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10000 records to Alpaca_Swahili_Dataset.jsonl\n",
      "{'is_check_passed': True, 'model_special_tokens': 'we are not yet checking end of sentence tokens for this model', 'file_present': 'File found', 'file_size': 'File size 0.004 GB', 'num_samples': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Alpaca_Swahili_Dataset.jsonl: 100%|██████████| 4.15M/4.15M [00:17<00:00, 251kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "{'filename': 'Alpaca_Swahili_Dataset.jsonl', 'id': 'file-9ff39f85-8b9c-46b7-89f9-1c3ea15db883', 'object': 'file', 'report_dict': {'is_check_passed': True, 'model_special_tokens': 'we are not yet checking end of sentence tokens for this model', 'file_present': 'File found', 'file_size': 'File size 0.004 GB', 'num_samples': 10000}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# save the reformatted dataset locally\n",
    "together.Files.save_jsonl(data_list, \"Alpaca_Swahili_Dataset.jsonl\")\n",
    "\n",
    "\n",
    "# check your data with your base model prompting type before uploading\n",
    "resp = together.Files.check(file=\"Alpaca_Swahili_Dataset.jsonl\")\n",
    "print(resp)\n",
    "\n",
    "\n",
    "# upload your dataset file to together and save the file-id, youll need it to start your finetuning run\n",
    "file_resp = together.Files.upload(file=\"Alpaca_Swahili_Dataset.jsonl\")\n",
    "file_id = file_resp[\"id\"]\n",
    "print(\"-\"*50)\n",
    "print(file_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceebddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_file': 'file-9ff39f85-8b9c-46b7-89f9-1c3ea15db883', 'validation_file': '', 'model_output_name': 'msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14', 'model_output_path': 's3://together-dev/finetune/65a91a45525ee5acd30edf0f/msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14/ft-7e2226aa-901c-4f47-b152-5152f4a58a74', 'Suffix': 'swahili', 'model': 'togethercomputer/LLaMA-2-7B-32K', 'n_epochs': 2, 'n_checkpoints': 1, 'batch_size': 4, 'learning_rate': 5e-05, 'user_id': '65a91a45525ee5acd30edf0f', 'lora': False, 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0, 'staring_epoch': 0, 'training_offset': 0, 'checkspoint_path': '', 'random_seed': '', 'created_at': '2024-01-29T10:15:14.756Z', 'updated_at': '2024-01-29T10:15:14.756Z', 'status': 'pending', 'owner_address': '0x2db181983c47b0d9923e38a09f76d50d3b4596a7', 'id': 'ft-7e2226aa-901c-4f47-b152-5152f4a58a74', 'job_id': '', 'token_count': 0, 'param_count': 0, 'total_price': 0, 'epochs_completed': 0, 'events': [{'object': 'fine-tune-event', 'created_at': '2024-01-29T10:15:14.756Z', 'level': '', 'message': 'Fine tune request created', 'type': 'JOB_PENDING', 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': ''}], 'queue_depth': 0, 'wandb_key': '50a4ae3d3476d2fa831569e4d22bfec32200130f', 'wandb_project_name': '', 'wandb_url': '', 'enable_checkpoints': False, 'internal_flags': '', 'UsedModelName': '', 'TrainingFileNumLines': 0, 'TrainingFileSize': 4351447, 'job_stats': {'FtUserTime': '', 'FtSysTime': '', 'FtMaxRss': 0, 'FtMinPgFlt': 0, 'FtMajPgFlt': 0, 'FtInBlock': 0, 'FtOutBlock': 0, 'FtNvCsw': 0, 'FtNivCsw': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Submit your finetune job\n",
    "ft_resp = together.Finetune.create(\n",
    "  training_file = file_id ,\n",
    "  model = base_model_name,\n",
    "  n_epochs = 2,\n",
    "  batch_size = 4,\n",
    "  n_checkpoints = 1,\n",
    "  learning_rate = 5e-5,\n",
    "  wandb_api_key = WANDB_API_KEY,\n",
    "  #estimate_price = True,\n",
    "  suffix = 'swahili',\n",
    ")\n",
    "\n",
    "fine_tune_id = ft_resp['id']\n",
    "print(ft_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31bab22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_file': 'file-9ff39f85-8b9c-46b7-89f9-1c3ea15db883', 'validation_file': '', 'model_output_name': 'msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14', 'model_output_path': 's3://together-dev/finetune/65a91a45525ee5acd30edf0f/msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14/ft-7e2226aa-901c-4f47-b152-5152f4a58a74', 'Suffix': 'swahili', 'model': 'togethercomputer/LLaMA-2-7B-32K', 'n_epochs': 2, 'n_checkpoints': 1, 'batch_size': 4, 'learning_rate': 5e-05, 'user_id': '65a91a45525ee5acd30edf0f', 'lora': False, 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0, 'staring_epoch': 0, 'training_offset': 0, 'checkspoint_path': '', 'random_seed': '', 'created_at': '2024-01-29T10:15:14.756Z', 'updated_at': '2024-01-29T10:17:27.287Z', 'status': 'running', 'owner_address': '0x2db181983c47b0d9923e38a09f76d50d3b4596a7', 'id': 'ft-7e2226aa-901c-4f47-b152-5152f4a58a74', 'job_id': '11977', 'token_count': 1740606, 'param_count': 6738415616, 'total_price': 5000000000, 'epochs_completed': 0, 'events': [{'object': 'fine-tune-event', 'created_at': '2024-01-29T10:15:14.756Z', 'level': '', 'message': 'Fine tune request created', 'type': 'JOB_PENDING', 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': ''}, {'object': 'fine-tune-event', 'created_at': '2024-01-29T10:15:22Z', 'level': 'Info', 'message': 'Job started at Mon Jan 29 02:15:21 PST 2024', 'type': 'JOB_START', 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': '1025714996004269300'}, {'object': 'fine-tune-event', 'created_at': '2024-01-29T10:16:54Z', 'level': 'Info', 'message': 'Model data downloaded for togethercomputer/LLaMA-2-7B-32K at Mon Jan 29 02:16:54 PST 2024', 'type': 'MODEL_DOWNLOAD_COMPLETE', 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': '2442668662235908290'}, {'object': 'fine-tune-event', 'created_at': '2024-01-29T10:16:56Z', 'level': 'Info', 'message': 'Training data downloaded for togethercomputer/LLaMA-2-7B-32K at Mon Jan 29 02:16:56 PST 2024', 'type': 'TRAINING_DATA_DOWNLOAD_COMPLETE', 'param_count': 0, 'token_count': 0, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': '-5105284255542130136'}, {'object': 'fine-tune-event', 'created_at': '2024-01-29T10:17:26Z', 'level': 'Info', 'message': 'WandB run initialized.', 'type': 'WANDB_INIT', 'param_count': 0, 'token_count': 0, 'wandb_url': 'https://wandb.ai/msamwelmollel/together/runs/hdh2z9cq', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': '-9130584895734206971'}, {'object': 'fine-tune-event', 'created_at': '2024-01-29T10:17:26Z', 'level': 'Info', 'message': 'Training started for model /work/job-ft-7e2226aa-901c-4f47-b152-5152f4a58a74/model', 'type': 'TRAINING_START', 'param_count': 6738415616, 'token_count': 1740606, 'wandb_url': '', 'checkpoint_path': '', 'model_path': '', 'training_offset': 0, 'hash': '-8540000122665415897'}], 'queue_depth': 0, 'wandb_key': '50a4ae3d3476d2fa831569e4d22bfec32200130f', 'wandb_project_name': '', 'wandb_url': 'https://wandb.ai/msamwelmollel/together/runs/hdh2z9cq', 'enable_checkpoints': False, 'internal_flags': '', 'UsedModelName': '', 'TrainingFileNumLines': 0, 'TrainingFileSize': 4351447, 'job_stats': {'FtUserTime': '', 'FtSysTime': '', 'FtMaxRss': 0, 'FtMinPgFlt': 0, 'FtMajPgFlt': 0, 'FtInBlock': 0, 'FtOutBlock': 0, 'FtNvCsw': 0, 'FtNivCsw': 0}}\n",
      "--------------------------------------------------\n",
      "running\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# run this from time to time to check on the status of your job\n",
    "print(together.Finetune.retrieve(fine_tune_id=fine_tune_id)) # retrieves information on finetune event\n",
    "print(\"-\"*50)\n",
    "print(together.Finetune.get_job_status(fine_tune_id=fine_tune_id)) # pending, running, completed\n",
    "print(together.Finetune.is_final_model_available(fine_tune_id=fine_tune_id)) # True, False\n",
    "print(together.Finetune.get_checkpoints(fine_tune_id=fine_tune_id)) # list of checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1c3b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 models available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace this name with the name of your newly finetuned model\n",
    "new_model_name = 'msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14'\n",
    "# replace this name with the name of your newly finetuned model\n",
    "# new_model_name = 'togethercomputer/llama-2-7b-chat'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<s> [INST] <<SYS>> you are a helpful assistant <</SYS>> Pretend you are a farmer. [/INST]\n",
    "model_list = together.Models.list()\n",
    "\n",
    "print(f\"{len(model_list)} models available\")\n",
    "\n",
    "available_model_names = [model_dict['name'] for model_dict in model_list]\n",
    "\n",
    "new_model_name in available_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce163278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'value': '163d4b015ade343d869897c454bf68b9faae962ddc10bc6d88ed998281031bb7-ae0d202637b02617e660c3565b39d2e145465efa352c37238da32e0361922540'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deploy your newly finetuned model\n",
    "together.Models.start(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1559ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelInstanceConfig': {'appearsIn': [], 'order': 0},\n",
       "  '_id': '65b77f5313e58adb27374e47',\n",
       "  'name': 'msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14',\n",
       "  'display_type': 'language',\n",
       "  'description': \"Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations.\",\n",
       "  'creator_organization': 'msamwelmollel@gmail.com',\n",
       "  'hardware_label': 'L40',\n",
       "  'num_parameters': '6738415616',\n",
       "  'release_date': '2024-01-29T10:34:59.193Z',\n",
       "  'show_in_playground': True,\n",
       "  'owner': 'msamwelmollel@gmail.com',\n",
       "  'owner_address': '0x2db181983c47b0d9923e38a09f76d50d3b4596a7',\n",
       "  'owner_userid': '65a91a45525ee5acd30edf0f',\n",
       "  'parent': 'togethercomputer/LLaMA-2-7B-32K',\n",
       "  'base': 'togethercomputer/LLaMA-2-7B-32K',\n",
       "  'path': 'r2://together-dev/finetune/65a91a45525ee5acd30edf0f/msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14/ft-7e2226aa-901c-4f47-b152-5152f4a58a74-2024-01-29-02-29-58',\n",
       "  'files': [{'filename': 'config.json',\n",
       "    'size': 822,\n",
       "    'hash': '3bed7f276f785bd0efb7d47a270abb8',\n",
       "    'modified': '2024-01-29T10:30:02.242Z'},\n",
       "   {'filename': 'generation_config.json',\n",
       "    'size': 132,\n",
       "    'hash': 'b655e018b2680db71f35cbdae6c0802',\n",
       "    'modified': '2024-01-29T10:30:02.256Z'},\n",
       "   {'filename': 'model-00001-of-00003.safetensors',\n",
       "    'size': 4938985248,\n",
       "    'hash': '9cb16ddaa87448188567d871da42c69f-58',\n",
       "    'modified': '2024-01-29T10:32:32.373Z'},\n",
       "   {'filename': 'model-00002-of-00003.safetensors',\n",
       "    'size': 4947390768,\n",
       "    'hash': '5671a14712cce4310127012b3e864654-59',\n",
       "    'modified': '2024-01-29T10:32:30.838Z'},\n",
       "   {'filename': 'model-00003-of-00003.safetensors',\n",
       "    'size': 3590488736,\n",
       "    'hash': 'c15539b7ca01c55346b84f4cf38536b8-42',\n",
       "    'modified': '2024-01-29T10:32:01.062Z'},\n",
       "   {'filename': 'model.safetensors.index.json',\n",
       "    'size': 23950,\n",
       "    'hash': 'ec4e8026d001d56b437cc9bbcaa1f45',\n",
       "    'modified': '2024-01-29T10:30:02.164Z'},\n",
       "   {'filename': 'special_tokens_map.json',\n",
       "    'size': 548,\n",
       "    'hash': '8204410f9f56109c279f870fc16d9da',\n",
       "    'modified': '2024-01-29T10:30:02.184Z'},\n",
       "   {'filename': 'tokenizer.model',\n",
       "    'size': 499723,\n",
       "    'hash': 'eeec4125e9c7560836b4873b6f8e302',\n",
       "    'modified': '2024-01-29T10:30:02.166Z'},\n",
       "   {'filename': 'tokenizer_config.json',\n",
       "    'size': 942,\n",
       "    'hash': '6b8ef2de3b29015c1ea6af512016f26',\n",
       "    'modified': '2024-01-29T10:30:02.741Z'}],\n",
       "  'config': {'stop': ['\\n\\n\\n\\n', '<|endoftext|>'],\n",
       "   'chat_template_name': 'llama'},\n",
       "  'worker_config': None,\n",
       "  'worker_template': None,\n",
       "  'pricing': {'hourly': 1400000000,\n",
       "   'input': 0,\n",
       "   'output': 0,\n",
       "   'finetune': 0,\n",
       "   'base': 0},\n",
       "  'created_at': '2024-01-29T10:34:59.556Z',\n",
       "  'update_at': '2024-01-29T10:34:59.556Z',\n",
       "  'autopilot_pool': None,\n",
       "  'has_wandb_telemetry': True,\n",
       "  'wandb_url': 'https://wandb.ai/msamwelmollel/together/runs/hdh2z9cq',\n",
       "  'ready': False}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if your model is finished deploying, if this returns {\"ready\": true}, you model is ready for inference\n",
    "together.Models.ready(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e0b10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>> you are a helpful  assistant <</SYS>> Toa vidokezo vitatu vya kudumisha afya. [/INST]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chat_prompt = \"<s>[INST] <<SYS>> you are a helpful  assistant <</SYS>> Toa vidokezo vitatu vya kudumisha afya. [/INST]\"\n",
    "test_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e754d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = \"28404b202e87f83fa2534b925cd9382f02a6d3780b5fba632524351ff98d851f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14\n"
     ]
    }
   ],
   "source": [
    "# use the inference API to generate text / create completion / chat\n",
    "print(new_model_name)\n",
    "\n",
    "output = together.Complete.create(\n",
    "  prompt = test_chat_prompt,\n",
    "  model = new_model_name,\n",
    "  max_tokens = 256,\n",
    "  temperature = 0.6,\n",
    "  top_k = 90,\n",
    "  top_p = 0.8,\n",
    "  repetition_penalty = 1.1,\n",
    "  stop = ['</s>']\n",
    ")\n",
    "\n",
    "# print generated text\n",
    "print(output['prompt'][0]+\" -> \"+output['output']['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c974fc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msamwelmollel@gmail.com/LLaMA-2-7B-32K-swahili-2024-01-29-10-15-14\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api.together.xyz/api/inference",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# use the inference API to generate text / create completion / chat\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_model_name)\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtogether\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mComplete\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_chat_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnew_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m</s>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print generated text\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39moutput[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python310\\lib\\site-packages\\together\\complete.py:48\u001b[0m, in \u001b[0;36mComplete.create\u001b[1;34m(cls, prompt, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, logprobs, api_key, cast, safety_model)\u001b[0m\n\u001b[0;32m     34\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_model,\n\u001b[0;32m     45\u001b[0m }\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# send request\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_post_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtogether\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base_complete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python310\\lib\\site-packages\\together\\utils.py:119\u001b[0m, in \u001b[0;36mcreate_post_request\u001b[1;34m(url, headers, json, stream, check_auth, api_key)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m together\u001b[38;5;241m.\u001b[39mResponseError(e)\n\u001b[1;32m--> 119\u001b[0m \u001b[43mresponse_status_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python310\\lib\\site-packages\\together\\utils.py:87\u001b[0m, in \u001b[0;36mresponse_status_exception\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid authentication credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python310\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api.together.xyz/api/inference"
     ]
    }
   ],
   "source": [
    "# use the inference API to generate text / create completion / chat\n",
    "print(new_model_name)\n",
    "\n",
    "output = together.Complete.create(\n",
    "  prompt = test_chat_prompt,\n",
    "  model = new_model_name,\n",
    "  max_tokens = 256,\n",
    "  temperature = 0.6,\n",
    "  top_k = 90,\n",
    "  top_p = 0.8,\n",
    "  repetition_penalty = 1.1,\n",
    "  stop = ['</s>']\n",
    ")\n",
    "\n",
    "# print generated text\n",
    "print(output['prompt'][0]+\" -> \"+output['output']['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efdce512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop your model and you will no longer be paying for it\n",
    "together.Models.stop(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e789b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import sseclient\n",
    "\n",
    "url = \"https://api.together.xyz/inference\"\n",
    "model = \"togethercomputer/RedPajama-INCITE-7B-Chat\"\n",
    "prompt = \"Tell me a story\\n\\n\"\n",
    "\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Prompt: {repr(prompt)}\")\n",
    "print(\"Repsonse:\")\n",
    "print()\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"stream_tokens\": True,\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ['TOGETHER_API_KEY']}\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "response.raise_for_status()\n",
    "\n",
    "client = sseclient.SSEClient(response)\n",
    "for event in client.events():\n",
    "    if event.data == \"[DONE]\":\n",
    "        break\n",
    "\n",
    "    partial_result = json.loads(event.data)\n",
    "    token = partial_result[\"choices\"][0][\"text\"]\n",
    "    print(token, end=\"\", flush=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90749de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
