{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695cdee2-64cb-4cc3-9fc2-221145ccf349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gguf\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\msamwelmollel\\anaconda3\\envs\\python310\\lib\\site-packages (from gguf) (1.26.3)\n",
      "Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: gguf\n",
      "Successfully installed gguf-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft==0.3.0\n",
    "# !pip install gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00f08ed4-3d55-4e49-b68e-d6b11a8605a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\llama_output\n",
      "LoRA model(s) ['C:\\\\\\\\Users\\\\\\\\msamwelmollel\\\\\\\\Documents\\\\\\\\checkpoint-95600\\\\\\\\pt_lora_model']:\n",
      "Loading C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\checkpoint-95600\\\\pt_lora_model\n",
      "Loading ckpt pytorch_model-00001-of-00003.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00001-of-00003.bin to C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama in HF format...\n",
      "Loading ckpt pytorch_model-00002-of-00003.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00002-of-00003.bin to C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama in HF format...\n",
      "Loading ckpt pytorch_model-00003-of-00003.bin\n",
      "Merging...\n",
      "Saving ckpt pytorch_model-00003-of-00003.bin to C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama in HF format...\n",
      "Saving tokenizer\n",
      "Saving config.json\n",
      "Saving generation_config.json\n",
      "Saving pytorch_model.bin.index.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python merge_llama_with_chinese_lora_low_mem.py \\\n",
    "    --base_model \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\llama_output\" \\\n",
    "    --lora_model \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\checkpoint-95600\\\\pt_lora_model\" \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0573e876-8fc5-46ee-8129-cf5e70e33985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\output_llama\\pytorch_model-00001-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\output_llama\\pytorch_model-00001-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\output_llama\\pytorch_model-00002-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\output_llama\\pytorch_model-00003-of-00003.bin\n",
      "params = Params(n_vocab=49053, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=WindowsPath('C:/Users/msamwelmollel/Documents/GitHub/swahili_llama/output_llama'))\n",
      "Found vocab files: {'tokenizer.model': WindowsPath('C:/Users/msamwelmollel/Documents/GitHub/swahili_llama/output_llama/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}\n",
      "Loading vocab file 'C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\output_llama\\tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 49053 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [49053, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [49053, 4096]\n",
      "Writing swahili_llama-7b-v0.1.gguf, format 7\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  49053 x   4096  | type Q8_0 | T+  10\n",
      "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  11\n",
      "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  11\n",
      "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  11\n",
      "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  11\n",
      "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  12\n",
      "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  14\n",
      "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  15\n",
      "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  15\n",
      "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  15\n",
      "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  15\n",
      "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  18\n",
      "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  18\n",
      "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  18\n",
      "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  18\n",
      "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  21\n",
      "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  21\n",
      "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  22\n",
      "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  22\n",
      "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  22\n",
      "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  24\n",
      "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  25\n",
      "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  25\n",
      "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  25\n",
      "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n",
      "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25\n",
      "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  27\n",
      "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  28\n",
      "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  28\n",
      "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  29\n",
      "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  30\n",
      "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  31\n",
      "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  32\n",
      "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  32\n",
      "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  32\n",
      "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  34\n",
      "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  35\n",
      "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  35\n",
      "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  35\n",
      "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  35\n",
      "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  35\n",
      "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  38\n",
      "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  38\n",
      "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  39\n",
      "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  39\n",
      "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  39\n",
      "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  39\n",
      "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  39\n",
      "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  39\n",
      "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  39\n",
      "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  42\n",
      "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  43\n",
      "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  43\n",
      "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  44\n",
      "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  44\n",
      "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  44\n",
      "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  46\n",
      "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  46\n",
      "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  47\n",
      "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  50\n",
      "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  50\n",
      "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  50\n",
      "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  50\n",
      "[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  51\n",
      "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  51\n",
      "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  53\n",
      "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  53\n",
      "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  54\n",
      "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  54\n",
      "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n",
      "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  54\n",
      "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  57\n",
      "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  57\n",
      "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  57\n",
      "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  58\n",
      "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  58\n",
      "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  60\n",
      "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  61\n",
      "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  61\n",
      "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
      "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
      "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  61\n",
      "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  64\n",
      "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  64\n",
      "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  65\n",
      "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
      "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
      "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  65\n",
      "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  67\n",
      "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  68\n",
      "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  68\n",
      "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  68\n",
      "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  68\n",
      "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  68\n",
      "[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  68\n",
      "[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  68\n",
      "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  68\n",
      "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  71\n",
      "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  71\n",
      "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  71\n",
      "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  72\n",
      "[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  72\n",
      "[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  72\n",
      "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  72\n",
      "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  74\n",
      "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  75\n",
      "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  75\n",
      "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  75\n",
      "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
      "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  75\n",
      "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  77\n",
      "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  78\n",
      "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  78\n",
      "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  78\n",
      "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  78\n",
      "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  78\n",
      "[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  78\n",
      "[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  78\n",
      "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  78\n",
      "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  81\n",
      "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  82\n",
      "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  82\n",
      "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  82\n",
      "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  82\n",
      "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  82\n",
      "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  85\n",
      "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  85\n",
      "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  86\n",
      "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  86\n",
      "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  86\n",
      "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  86\n",
      "[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  86\n",
      "[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  86\n",
      "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  86\n",
      "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  89\n",
      "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  89\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  89\n",
      "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  90\n",
      "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  90\n",
      "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  90\n",
      "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  92\n",
      "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  92\n",
      "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  93\n",
      "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  93\n",
      "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  93\n",
      "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  93\n",
      "[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  93\n",
      "[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  93\n",
      "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  93\n",
      "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  95\n",
      "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  96\n",
      "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  96\n",
      "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  96\n",
      "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  96\n",
      "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  96\n",
      "[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  96\n",
      "[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  96\n",
      "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  96\n",
      "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  99\n",
      "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  99\n",
      "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 100\n",
      "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 100\n",
      "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
      "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 102\n",
      "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 103\n",
      "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 103\n",
      "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 103\n",
      "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 103\n",
      "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 103\n",
      "[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 103\n",
      "[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 103\n",
      "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 103\n",
      "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 107\n",
      "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 107\n",
      "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 107\n",
      "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 107\n",
      "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 107\n",
      "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 107\n",
      "[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 107\n",
      "[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 108\n",
      "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 108\n",
      "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 110\n",
      "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 111\n",
      "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 111\n",
      "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 112\n",
      "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 112\n",
      "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 112\n",
      "[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 112\n",
      "[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 112\n",
      "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 112\n",
      "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 114\n",
      "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 115\n",
      "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 115\n",
      "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 115\n",
      "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 115\n",
      "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 116\n",
      "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 118\n",
      "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 119\n",
      "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 119\n",
      "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 119\n",
      "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 119\n",
      "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 119\n",
      "[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 119\n",
      "[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 119\n",
      "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 119\n",
      "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 122\n",
      "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 122\n",
      "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 122\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
      "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
      "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 122\n",
      "[291/291] Writing tensor output.weight                          | size  49053 x   4096  | type Q8_0 | T+ 128\n",
      "Wrote swahili_llama-7b-v0.1.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama\" \\\n",
    "  --outfile swahili_llama-7b-v0.1.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f977ad9f-fa6b-49ca-8c80-74dfd486314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\llama_output\\pytorch_model-00001-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\llama_output\\pytorch_model-00001-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\llama_output\\pytorch_model-00002-of-00003.bin\n",
      "Loading model file C:\\Users\\msamwelmollel\\Documents\\llama_output\\pytorch_model-00003-of-00003.bin\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=WindowsPath('C:/Users/msamwelmollel/Documents/llama_output'))\n",
      "Found vocab files: {'tokenizer.model': WindowsPath('C:/Users/msamwelmollel/Documents/llama_output/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': WindowsPath('C:/Users/msamwelmollel/Documents/llama_output/tokenizer.json')}\n",
      "Loading vocab file 'C:\\Users\\msamwelmollel\\Documents\\llama_output\\tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [11008, 4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 11008]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [11008, 4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 11008]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
      "Writing english_llama-7b-v0.1.gguf, format 7\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+  17\n",
      "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  17\n",
      "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  17\n",
      "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  17\n",
      "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  17\n",
      "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  17\n",
      "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  17\n",
      "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  18\n",
      "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  19\n",
      "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  20\n",
      "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  20\n",
      "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  23\n",
      "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  23\n",
      "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  24\n",
      "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  24\n",
      "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  24\n",
      "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  24\n",
      "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  24\n",
      "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  24\n",
      "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  24\n",
      "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  29\n",
      "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  30\n",
      "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  31\n",
      "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  31\n",
      "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  31\n",
      "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  31\n",
      "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  32\n",
      "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  35\n",
      "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  36\n",
      "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  36\n",
      "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  37\n",
      "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  37\n",
      "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  37\n",
      "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  40\n",
      "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  41\n",
      "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  41\n",
      "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  41\n",
      "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  41\n",
      "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  41\n",
      "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  42\n",
      "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  45\n",
      "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  46\n",
      "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  46\n",
      "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  47\n",
      "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  47\n",
      "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  47\n",
      "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  50\n",
      "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  51\n",
      "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  51\n",
      "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  52\n",
      "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  52\n",
      "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  52\n",
      "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  56\n",
      "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  56\n",
      "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  57\n",
      "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  58\n",
      "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  58\n",
      "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  58\n",
      "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  62\n",
      "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  63\n",
      "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  63\n",
      "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  64\n",
      "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  64\n",
      "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  64\n",
      "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  67\n",
      "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  68\n",
      "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  68\n",
      "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  69\n",
      "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  69\n",
      "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  69\n",
      "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  72\n",
      "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  73\n",
      "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  73\n",
      "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  73\n",
      "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n",
      "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  73\n",
      "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  74\n",
      "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  78\n",
      "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  78\n",
      "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  79\n",
      "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  79\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  79\n",
      "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  79\n",
      "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  83\n",
      "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  83\n",
      "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  84\n",
      "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  84\n",
      "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  84\n",
      "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  84\n",
      "[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  84\n",
      "[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  84\n",
      "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  84\n",
      "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  87\n",
      "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  88\n",
      "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  88\n",
      "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  88\n",
      "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  88\n",
      "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  88\n",
      "[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  88\n",
      "[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  89\n",
      "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  93\n",
      "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  94\n",
      "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  95\n",
      "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  95\n",
      "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  95\n",
      "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  95\n",
      "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  99\n",
      "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  99\n",
      "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 100\n",
      "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 100\n",
      "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
      "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 100\n",
      "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 103\n",
      "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 104\n",
      "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 104\n",
      "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 105\n",
      "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 105\n",
      "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 105\n",
      "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 108\n",
      "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 109\n",
      "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 110\n",
      "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 110\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 110\n",
      "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 110\n",
      "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 114\n",
      "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 114\n",
      "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 115\n",
      "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 115\n",
      "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 115\n",
      "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 115\n",
      "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 118\n",
      "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 119\n",
      "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 119\n",
      "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 120\n",
      "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 120\n",
      "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 120\n",
      "[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 120\n",
      "[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 120\n",
      "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 120\n",
      "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 124\n",
      "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 125\n",
      "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 125\n",
      "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 125\n",
      "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 125\n",
      "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 125\n",
      "[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 125\n",
      "[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 125\n",
      "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 126\n",
      "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 129\n",
      "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 129\n",
      "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 129\n",
      "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 130\n",
      "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 130\n",
      "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 130\n",
      "[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 130\n",
      "[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 130\n",
      "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 130\n",
      "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 133\n",
      "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 134\n",
      "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 135\n",
      "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 135\n",
      "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 135\n",
      "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 135\n",
      "[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 136\n",
      "[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 136\n",
      "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 136\n",
      "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 139\n",
      "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 140\n",
      "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 140\n",
      "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 141\n",
      "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 141\n",
      "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 141\n",
      "[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 141\n",
      "[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 141\n",
      "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 141\n",
      "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 145\n",
      "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 146\n",
      "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 146\n",
      "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 146\n",
      "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 146\n",
      "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 146\n",
      "[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 146\n",
      "[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 146\n",
      "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 146\n",
      "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 150\n",
      "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 151\n",
      "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 151\n",
      "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 152\n",
      "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 152\n",
      "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 152\n",
      "[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 152\n",
      "[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 152\n",
      "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 152\n",
      "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 156\n",
      "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 157\n",
      "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 157\n",
      "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 158\n",
      "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 158\n",
      "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 158\n",
      "[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 158\n",
      "[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 158\n",
      "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 158\n",
      "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 163\n",
      "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 163\n",
      "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 164\n",
      "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 164\n",
      "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 164\n",
      "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 164\n",
      "[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 164\n",
      "[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 164\n",
      "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 164\n",
      "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 168\n",
      "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 169\n",
      "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 169\n",
      "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 169\n",
      "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 169\n",
      "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 169\n",
      "[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 169\n",
      "[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 170\n",
      "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 170\n",
      "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 174\n",
      "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 174\n",
      "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 175\n",
      "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 175\n",
      "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 175\n",
      "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 175\n",
      "[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 176\n",
      "[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 176\n",
      "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 176\n",
      "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 179\n",
      "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 180\n",
      "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 180\n",
      "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 180\n",
      "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 180\n",
      "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+ 180\n",
      "[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+ 180\n",
      "[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+ 180\n",
      "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+ 180\n",
      "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+ 183\n",
      "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+ 184\n",
      "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+ 184\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 184\n",
      "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 184\n",
      "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 184\n",
      "[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+ 189\n",
      "Wrote english_llama-7b-v0.1.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py  \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\llama_output\" \\\n",
    "  --outfile english_llama-7b-v0.1.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c7ad2a-feb7-452d-ba13-c8a3768113d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: quantize.py [-h] fname_inp fname_out type\n",
      "\n",
      "positional arguments:\n",
      "  fname_inp   Path to input model\n",
      "  fname_out   Path to output model\n",
      "  type        Type of quantization (2: q4_0, 3: q4_1)\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "!python C:/Users/msamwelmollel/llama-cpp-python-main/examples/low_level_api/quantize.py -h ./swahili_llama-7b-v0.1.ggu ./swahili_llama-7b-v0.1-Q4_K_M.gguf  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "380387e0-b690-466e-b951-d14470aa0352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\llama_output\n",
      "LoRA model(s) ['C:\\\\\\\\Users\\\\\\\\msamwelmollel\\\\\\\\Documents\\\\\\\\checkpoint-956\\\\\\\\pt_lora_model']:\n",
      "Loading C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\checkpoint-956\\\\pt_lora_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\msamwelmollel\\Documents\\GitHub\\swahili_llama\\merge_llama_with_chinese_lora_low_mem.py\", line 246, in <module>\n",
      "    lora_state_dict = torch.load(os.path.join(lora_model_path,'adapter_model.bin'),map_location='cpu')\n",
      "  File \"C:\\Users\\msamwelmollel\\anaconda3\\envs\\python310\\lib\\site-packages\\torch\\serialization.py\", line 1005, in load\n",
      "    with _open_zipfile_reader(opened_file) as opened_zipfile:\n",
      "  File \"C:\\Users\\msamwelmollel\\anaconda3\\envs\\python310\\lib\\site-packages\\torch\\serialization.py\", line 457, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n",
      "RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\n"
     ]
    }
   ],
   "source": [
    "!python merge_llama_with_chinese_lora_low_mem.py \\\n",
    "    --base_model \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\llama_output\" \\\n",
    "    --lora_model \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\checkpoint-956\\\\pt_lora_model\" \\\n",
    "    --output_type huggingface \\\n",
    "    --output_dir \"C:\\\\Users\\\\msamwelmollel\\\\Documents\\\\GitHub\\\\swahili_llama\\\\output_llama\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
