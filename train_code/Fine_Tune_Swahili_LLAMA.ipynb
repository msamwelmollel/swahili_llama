{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.28.1\n",
    "# !pip install git+https://github.com/huggingface/peft.git@13e53fc\n",
    "# !pip install datasets\n",
    "# !pip install sentencepiece\n",
    "# !pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ee2627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 18:24:46,296] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-27 18:24:46,447] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-27 18:24:46,447] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "02/27/2024 18:24:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:666] 2024-02-27 18:24:47,626 >> loading configuration file llama_output/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-02-27 18:24:47,626 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama_output\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-27 18:24:47,626 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-27 18:24:47,626 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-27 18:24:47,626 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-27 18:24:47,626 >> loading file tokenizer_config.json\n",
      "[ERROR|tokenization_utils_base.py:1042] 2024-02-27 18:24:47,644 >> Using pad_token, but it is not set yet.\n",
      "Adding pad token [PAD]\n",
      "[INFO|tokenization_utils_base.py:907] 2024-02-27 18:24:47,644 >> Assigning [PAD] to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2024-02-27 18:24:47,644 >> Adding [PAD] to the vocabulary\n",
      "02/27/2024 18:24:47 - INFO - __main__ - Training files: sft_data/swahili_summarizer.json\n",
      "02/27/2024 18:24:47 - WARNING - root - building dataset...\n",
      "02/27/2024 18:24:47 - INFO - __name__ - training datasets-sft_data/swahili_summarizer.json has been loaded from disk\n",
      "02/27/2024 18:24:47 - INFO - __main__ - Num train_samples  257\n",
      "02/27/2024 18:24:47 - INFO - __main__ - training example:\n",
      "02/27/2024 18:24:47 - INFO - __main__ - <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Fupisha maandishi kwa Kiswahili\n",
      "Katika jamii nyingi za Kiafrika, mila na desturi zimekuwa zikichukua nafasi kubwa katika kuunda maisha na tabia za watu. Hizi zinajumuisha sherehe za jadi, ndoa, mazishi, na matambiko. Mila hizi, zinazopitishwa kutoka kizazi kimoja hadi kingine, zimekuwa zikisaidia katika kuhifadhi utamaduni na kuimarisha mshikamano wa jamii. Hata hivyo, katika zama za utandawazi, baadhi ya mila hizi zinakabiliwa na changamoto za kisasa kama vile muingiliano wa tamaduni, ambapo watu wanazidi kupokea na kuchanganya tamaduni za kigeni na za jadi.\n",
      "\n",
      "### Response:  Mila na desturi za Kiafrika, zikiwemo sherehe za jadi na matambiko, zinachangia kuhifadhi utamaduni na mshikamano wa jamii lakini zinakabiliwa na changamoto za utandawazi.</s>\n",
      "[INFO|modeling_utils.py:2531] 2024-02-27 18:24:47,653 >> loading weights file llama_output/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1176] 2024-02-27 18:24:47,654 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:575] 2024-02-27 18:24:47,654 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.31s/it]\n",
      "[INFO|modeling_utils.py:3190] 2024-02-27 18:24:54,770 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[WARNING|modeling_utils.py:3192] 2024-02-27 18:24:54,770 >> Some weights of LlamaForCausalLM were not initialized from the model checkpoint at llama_output and are newly initialized: ['model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|configuration_utils.py:535] 2024-02-27 18:24:54,771 >> loading configuration file llama_output/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-02-27 18:24:54,771 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "02/27/2024 18:24:54 - INFO - __main__ - len(tokenizer):48315\n",
      "02/27/2024 18:24:54 - INFO - __main__ - resize the embedding size by the size of the tokenizer\n",
      "02/27/2024 18:25:04 - INFO - __main__ - Init new peft model\n",
      "02/27/2024 18:25:04 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
      "02/27/2024 18:25:04 - INFO - __main__ - lora_rank: 8\n",
      "trainable params: 415784960 || all params: 6892056576 || trainable%: 6.032814086986392\n",
      "02/27/2024 18:25:57 - INFO - __main__ - model.modules_to_save: ['embed_tokens', 'lm_head']\n",
      "[INFO|trainer.py:564] 2024-02-27 18:25:57,772 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:621] 2024-02-27 18:25:57,772 >> Using cuda_amp half precision backend\n",
      "/home/ai4d/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2024-02-27 18:25:57,783] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-02-27 18:25:59,593] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-02-27 18:25:59,593] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-02-27 18:25:59,593] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-02-27 18:25:59,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-02-27 18:25:59,609] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n",
      "[2024-02-27 18:25:59,609] [WARNING] [engine.py:1189:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2024-02-27 18:25:59,609] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2024-02-27 18:25:59,609] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 100000000\n",
      "[2024-02-27 18:25:59,609] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 100000000\n",
      "[2024-02-27 18:25:59,609] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-02-27 18:25:59,609] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 18:26:00,289] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-02-27 18:26:00,289] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 15.19 GB         CA 15.23 GB         Max_CA 15 GB \n",
      "[2024-02-27 18:26:00,289] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 3.8%\n",
      "[2024-02-27 18:26:00,391] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-02-27 18:26:00,392] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 15.97 GB         CA 16.79 GB         Max_CA 17 GB \n",
      "[2024-02-27 18:26:00,392] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 3.8%\n",
      "[2024-02-27 18:26:00,392] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-02-27 18:26:00,485] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-02-27 18:26:00,485] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 14.42 GB         CA 16.79 GB         Max_CA 17 GB \n",
      "[2024-02-27 18:26:00,485] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 9.64 GB, percent = 3.8%\n",
      "[2024-02-27 18:26:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2024-02-27 18:26:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-02-27 18:26:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fb224679650>\n",
      "[2024-02-27 18:26:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:987:print] DeepSpeedEngine configuration:\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   amp_enabled .................. False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   amp_params ................... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   bfloat16_enabled ............. False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb24df2dfd0>\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   communication_data_type ...... None\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   curriculum_enabled_legacy .... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   curriculum_params_legacy ..... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   data_efficiency_enabled ...... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   dataloader_drop_last ......... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   disable_allgather ............ False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   dump_state ................... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1e-10}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_enabled ........... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   eigenvalue_verbose ........... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   elasticity_enabled ........... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   fp16_auto_cast ............... False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   fp16_enabled ................. True\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-02-27 18:26:00,491] [INFO] [config.py:991:print]   global_rank .................. 0\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   grad_accum_dtype ............. None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   gradient_accumulation_steps .. 1\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   gradient_clipping ............ 1.0\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   graph_harvesting ............. False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   load_universal_checkpoint .... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   loss_scale ................... 0\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   memory_breakdown ............. False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   mics_hierarchial_params_gather  False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   mics_shard_size .............. -1\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   optimizer_name ............... None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   optimizer_params ............. None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   pld_enabled .................. False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   pld_params ................... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   prescale_gradients ........... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   scheduler_name ............... None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   scheduler_params ............. None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   sparse_attention ............. None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   sparse_gradients_enabled ..... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   steps_per_print .............. 2000\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   train_batch_size ............. 1\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   use_data_before_expert_parallel_  False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   use_node_local_storage ....... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   wall_clock_breakdown ......... False\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   weight_quantization_config ... None\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   world_size ................... 1\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   zero_allow_untested_optimizer  True\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   zero_enabled ................. True\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:991:print]   zero_optimization_stage ...... 2\n",
      "[2024-02-27 18:26:00,492] [INFO] [config.py:977:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 100, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1e-10\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 1.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 1.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[INFO|trainer.py:1769] 2024-02-27 18:26:00,492 >> ***** Running training *****\n",
      "[INFO|trainer.py:1770] 2024-02-27 18:26:00,492 >>   Num examples = 257\n",
      "[INFO|trainer.py:1771] 2024-02-27 18:26:00,492 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1772] 2024-02-27 18:26:00,492 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1773] 2024-02-27 18:26:00,492 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1774] 2024-02-27 18:26:00,492 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1775] 2024-02-27 18:26:00,492 >>   Total optimization steps = 500\n",
      "[INFO|trainer.py:1776] 2024-02-27 18:26:00,494 >>   Number of trainable parameters = 415,784,960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/home/ai4d/anaconda3/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1990: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "[2024-02-27 18:26:01,069] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 5.4688, 'learning_rate': 0.0, 'epoch': 0.0}                            \n",
      "  0%|                                           | 1/500 [00:00<04:46,  1.74it/s][2024-02-27 18:26:01,259] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  0%|▏                                          | 2/500 [00:00<02:53,  2.87it/s][2024-02-27 18:26:01,418] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  1%|▎                                          | 3/500 [00:00<02:10,  3.82it/s][2024-02-27 18:26:01,562] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  1%|▎                                          | 4/500 [00:01<01:46,  4.64it/s][2024-02-27 18:26:01,723] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  1%|▌                                          | 7/500 [00:01<01:59,  4.13it/s][2024-02-27 18:26:02,446] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 6.6567, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.04}        \n",
      "{'loss': 5.577, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.08}          \n",
      "  4%|█▊                                        | 21/500 [00:05<02:08,  3.74it/s][2024-02-27 18:26:06,104] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "{'loss': 4.6443, 'learning_rate': 9.993288199121283e-05, 'epoch': 0.12}         \n",
      "{'loss': 3.706, 'learning_rate': 9.966052387815923e-05, 'epoch': 0.16}          \n",
      "{'loss': 3.6508, 'learning_rate': 9.91798722697081e-05, 'epoch': 0.19}          \n",
      " 10%|████▏                                     | 50/500 [00:13<01:58,  3.78it/s][INFO|trainer.py:2868] 2024-02-27 18:26:13,547 >> Saving model checkpoint to sft_output_model/checkpoint-50\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:26:13,550 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:26:14,780 >> tokenizer config file saved in sft_output_model/checkpoint-50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:26:14,780 >> Special tokens file saved in sft_output_model/checkpoint-50/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:26:14,780 >> added tokens file saved in sft_output_model/checkpoint-50/added_tokens.json\n",
      "[2024-02-27 18:26:14,782] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!\n",
      "[2024-02-27 18:26:20,010] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-50/global_step50/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:26:20,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-50/global_step50/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:26:33,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-50/global_step50/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:26:34,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:26:41,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:26:41,758] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:26:41,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:26:42,982 >> tokenizer config file saved in sft_output_model/checkpoint-50/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:26:42,982 >> Special tokens file saved in sft_output_model/checkpoint-50/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:26:42,982 >> added tokens file saved in sft_output_model/checkpoint-50/sft_lora_model/added_tokens.json\n",
      "{'loss': 3.6826, 'learning_rate': 9.849294318574351e-05, 'epoch': 0.23}         \n",
      "{'loss': 3.4043, 'learning_rate': 9.760261784556839e-05, 'epoch': 0.27}         \n",
      "{'loss': 3.298, 'learning_rate': 9.651263058306932e-05, 'epoch': 0.31}          \n",
      "{'loss': 2.865, 'learning_rate': 9.52275531836226e-05, 'epoch': 0.35}           \n",
      "{'loss': 2.6773, 'learning_rate': 9.37527757084375e-05, 'epoch': 0.39}          \n",
      " 20%|████████▏                                | 100/500 [00:55<01:41,  3.96it/s][INFO|trainer.py:2868] 2024-02-27 18:26:56,038 >> Saving model checkpoint to sft_output_model/checkpoint-100\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:26:56,040 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:26:57,267 >> tokenizer config file saved in sft_output_model/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:26:57,267 >> Special tokens file saved in sft_output_model/checkpoint-100/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:26:57,267 >> added tokens file saved in sft_output_model/checkpoint-100/added_tokens.json\n",
      "[2024-02-27 18:26:57,269] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!\n",
      "[2024-02-27 18:27:02,551] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-100/global_step100/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:27:02,551] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-100/global_step100/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:27:16,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-100/global_step100/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:27:17,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:27:24,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:27:24,438] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:27:24,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:27:25,662 >> tokenizer config file saved in sft_output_model/checkpoint-100/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:27:25,662 >> Special tokens file saved in sft_output_model/checkpoint-100/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:27:25,662 >> added tokens file saved in sft_output_model/checkpoint-100/sft_lora_model/added_tokens.json\n",
      "{'loss': 2.3796, 'learning_rate': 9.209448388676635e-05, 'epoch': 0.43}         \n",
      "{'loss': 2.3737, 'learning_rate': 9.025963317080641e-05, 'epoch': 0.47}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1765, 'learning_rate': 8.825591956211614e-05, 'epoch': 0.51}         \n",
      "{'loss': 3.1275, 'learning_rate': 8.609174733191011e-05, 'epoch': 0.54}         \n",
      "{'loss': 2.8616, 'learning_rate': 8.377619377062482e-05, 'epoch': 0.58}         \n",
      " 30%|████████████▎                            | 150/500 [01:37<01:28,  3.95it/s][INFO|trainer.py:2868] 2024-02-27 18:27:38,430 >> Saving model checkpoint to sft_output_model/checkpoint-150\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:27:38,433 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:27:39,775 >> tokenizer config file saved in sft_output_model/checkpoint-150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:27:39,775 >> Special tokens file saved in sft_output_model/checkpoint-150/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:27:39,775 >> added tokens file saved in sft_output_model/checkpoint-150/added_tokens.json\n",
      "[2024-02-27 18:27:39,777] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is about to be saved!\n",
      "[2024-02-27 18:27:45,042] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-150/global_step150/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:27:45,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-150/global_step150/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:27:58,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-150/global_step150/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:27:59,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:28:06,877] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:28:06,878] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:28:06,878] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:28:08,102 >> tokenizer config file saved in sft_output_model/checkpoint-150/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:28:08,102 >> Special tokens file saved in sft_output_model/checkpoint-150/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:28:08,102 >> added tokens file saved in sft_output_model/checkpoint-150/sft_lora_model/added_tokens.json\n",
      " 32%|████████████▉                            | 158/500 [02:09<05:44,  1.01s/it][2024-02-27 18:28:10,401] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6749, 'learning_rate': 8.157077922018537e-05, 'epoch': 0.62}         \n",
      "{'loss': 2.0699, 'learning_rate': 7.89948503374835e-05, 'epoch': 0.66}          \n",
      "{'loss': 3.1891, 'learning_rate': 7.629730697604314e-05, 'epoch': 0.7}          \n",
      "{'loss': 2.7127, 'learning_rate': 7.348946357018479e-05, 'epoch': 0.74}         \n",
      "{'loss': 2.5396, 'learning_rate': 7.05830971909472e-05, 'epoch': 0.78}          \n",
      " 40%|████████████████▍                        | 200/500 [02:20<01:20,  3.73it/s][INFO|trainer.py:2868] 2024-02-27 18:28:21,177 >> Saving model checkpoint to sft_output_model/checkpoint-200\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:28:21,180 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:28:22,408 >> tokenizer config file saved in sft_output_model/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:28:22,408 >> Special tokens file saved in sft_output_model/checkpoint-200/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:28:22,409 >> added tokens file saved in sft_output_model/checkpoint-200/added_tokens.json\n",
      "[2024-02-27 18:28:22,410] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2024-02-27 18:28:27,722] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-200/global_step200/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:28:27,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-200/global_step200/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:28:41,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-200/global_step200/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:28:42,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:28:49,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:28:49,635] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:28:49,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:28:49,636 >> Deleting older checkpoint [sft_output_model/checkpoint-50] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:28:53,436 >> tokenizer config file saved in sft_output_model/checkpoint-200/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:28:53,436 >> Special tokens file saved in sft_output_model/checkpoint-200/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:28:53,436 >> added tokens file saved in sft_output_model/checkpoint-200/sft_lora_model/added_tokens.json\n",
      " 41%|████████████████▋                        | 203/500 [02:53<24:46,  5.00s/it][2024-02-27 18:28:54,359] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "{'loss': 2.4323, 'learning_rate': 6.789319787993979e-05, 'epoch': 0.82}         \n",
      "{'loss': 2.1667, 'learning_rate': 6.483352252191584e-05, 'epoch': 0.86}         \n",
      "{'loss': 2.0628, 'learning_rate': 6.171163021542133e-05, 'epoch': 0.89}         \n",
      "{'loss': 2.3509, 'learning_rate': 5.854061526172402e-05, 'epoch': 0.93}         \n",
      "{'loss': 1.8861, 'learning_rate': 5.533377799954532e-05, 'epoch': 0.97}         \n",
      " 50%|████████████████████▌                    | 250/500 [03:05<01:06,  3.79it/s][INFO|trainer.py:2868] 2024-02-27 18:29:06,352 >> Saving model checkpoint to sft_output_model/checkpoint-250\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:29:06,355 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:29:07,573 >> tokenizer config file saved in sft_output_model/checkpoint-250/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:29:07,573 >> Special tokens file saved in sft_output_model/checkpoint-250/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:29:07,573 >> added tokens file saved in sft_output_model/checkpoint-250/added_tokens.json\n",
      "[2024-02-27 18:29:07,575] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step250 is about to be saved!\n",
      "[2024-02-27 18:29:12,787] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-250/global_step250/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:29:12,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-250/global_step250/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:29:26,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-250/global_step250/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:29:27,421] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-250/global_step250/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 18:29:34,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-250/global_step250/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:29:34,600] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-250/global_step250/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:29:34,600] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:29:34,602 >> Deleting older checkpoint [sft_output_model/checkpoint-100] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:29:38,423 >> tokenizer config file saved in sft_output_model/checkpoint-250/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:29:38,423 >> Special tokens file saved in sft_output_model/checkpoint-250/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:29:38,423 >> added tokens file saved in sft_output_model/checkpoint-250/sft_lora_model/added_tokens.json\n",
      "{'loss': 2.2211, 'learning_rate': 5.210456901881761e-05, 'epoch': 1.01}         \n",
      "{'loss': 1.0881, 'learning_rate': 4.88665327442355e-05, 'epoch': 1.05}          \n",
      "{'loss': 1.3879, 'learning_rate': 4.5633250625231806e-05, 'epoch': 1.09}        \n",
      "{'loss': 1.4186, 'learning_rate': 4.24182841706586e-05, 'epoch': 1.13}          \n",
      "{'loss': 1.6352, 'learning_rate': 3.9235118067106255e-05, 'epoch': 1.17}        \n",
      " 60%|████████████████████████▌                | 300/500 [03:50<00:51,  3.89it/s][INFO|trainer.py:2868] 2024-02-27 18:29:51,271 >> Saving model checkpoint to sft_output_model/checkpoint-300\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:29:51,274 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:29:52,506 >> tokenizer config file saved in sft_output_model/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:29:52,506 >> Special tokens file saved in sft_output_model/checkpoint-300/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:29:52,506 >> added tokens file saved in sft_output_model/checkpoint-300/added_tokens.json\n",
      "[2024-02-27 18:29:52,507] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!\n",
      "[2024-02-27 18:29:57,717] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-300/global_step300/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:29:57,717] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-300/global_step300/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:30:11,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-300/global_step300/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:30:12,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:30:19,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:30:19,449] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:30:19,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:30:19,450 >> Deleting older checkpoint [sft_output_model/checkpoint-150] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:30:23,263 >> tokenizer config file saved in sft_output_model/checkpoint-300/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:30:23,263 >> Special tokens file saved in sft_output_model/checkpoint-300/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:30:23,263 >> added tokens file saved in sft_output_model/checkpoint-300/sft_lora_model/added_tokens.json\n",
      "{'loss': 1.0536, 'learning_rate': 3.60971036194415e-05, 'epoch': 1.21}          \n",
      "{'loss': 1.6386, 'learning_rate': 3.3017402750794974e-05, 'epoch': 1.25}        \n",
      "{'loss': 1.6365, 'learning_rate': 3.000893279688155e-05, 'epoch': 1.28}         \n",
      "{'loss': 0.9659, 'learning_rate': 2.7084312326205165e-05, 'epoch': 1.32}        \n",
      "{'loss': 1.1456, 'learning_rate': 2.425580821339733e-05, 'epoch': 1.36}         \n",
      " 70%|████████████████████████████▋            | 350/500 [04:35<00:37,  3.96it/s][INFO|trainer.py:2868] 2024-02-27 18:30:36,349 >> Saving model checkpoint to sft_output_model/checkpoint-350\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:30:36,352 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:30:37,590 >> tokenizer config file saved in sft_output_model/checkpoint-350/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:30:37,590 >> Special tokens file saved in sft_output_model/checkpoint-350/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:30:37,590 >> added tokens file saved in sft_output_model/checkpoint-350/added_tokens.json\n",
      "[2024-02-27 18:30:37,592] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step350 is about to be saved!\n",
      "[2024-02-27 18:30:42,831] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-350/global_step350/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:30:42,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-350/global_step350/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:30:56,713] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-350/global_step350/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:30:57,435] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-350/global_step350/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:31:04,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-350/global_step350/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:31:04,625] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-350/global_step350/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:31:04,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step350 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:31:04,626 >> Deleting older checkpoint [sft_output_model/checkpoint-200] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:31:08,476 >> tokenizer config file saved in sft_output_model/checkpoint-350/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:31:08,476 >> Special tokens file saved in sft_output_model/checkpoint-350/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:31:08,476 >> added tokens file saved in sft_output_model/checkpoint-350/sft_lora_model/added_tokens.json\n",
      "{'loss': 1.6253, 'learning_rate': 2.1535284187681864e-05, 'epoch': 1.4}         \n",
      "{'loss': 1.6762, 'learning_rate': 1.8934151072271573e-05, 'epoch': 1.44}        \n",
      "{'loss': 1.3428, 'learning_rate': 1.646331892341018e-05, 'epoch': 1.48}         \n",
      "{'loss': 1.3021, 'learning_rate': 1.4133151269804873e-05, 'epoch': 1.52}        \n",
      "{'loss': 1.5701, 'learning_rate': 1.1953421644385443e-05, 'epoch': 1.56}        \n",
      " 80%|████████████████████████████████▊        | 400/500 [05:20<00:25,  3.95it/s][INFO|trainer.py:2868] 2024-02-27 18:31:21,290 >> Saving model checkpoint to sft_output_model/checkpoint-400\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:31:21,293 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:31:22,535 >> tokenizer config file saved in sft_output_model/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:31:22,535 >> Special tokens file saved in sft_output_model/checkpoint-400/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:31:22,535 >> added tokens file saved in sft_output_model/checkpoint-400/added_tokens.json\n",
      "[2024-02-27 18:31:22,536] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 18:31:27,771] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-400/global_step400/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:31:27,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-400/global_step400/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:31:41,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-400/global_step400/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:31:42,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:31:49,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:31:49,530] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:31:49,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:31:49,531 >> Deleting older checkpoint [sft_output_model/checkpoint-250] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:31:53,357 >> tokenizer config file saved in sft_output_model/checkpoint-400/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:31:53,357 >> Special tokens file saved in sft_output_model/checkpoint-400/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:31:53,357 >> added tokens file saved in sft_output_model/checkpoint-400/sft_lora_model/added_tokens.json\n",
      "{'loss': 1.1199, 'learning_rate': 9.933272590710507e-06, 'epoch': 1.6}          \n",
      "{'loss': 1.0029, 'learning_rate': 8.081177315962602e-06, 'epoch': 1.63}         \n",
      "{'loss': 0.8917, 'learning_rate': 6.4049041513726485e-06, 'epoch': 1.67}        \n",
      "{'loss': 1.0521, 'learning_rate': 4.9114839691390854e-06, 'epoch': 1.71}        \n",
      "{'loss': 1.0146, 'learning_rate': 3.60718069250639e-06, 'epoch': 1.75}          \n",
      " 90%|████████████████████████████████████▉    | 450/500 [06:05<00:12,  3.95it/s][INFO|trainer.py:2868] 2024-02-27 18:32:06,230 >> Saving model checkpoint to sft_output_model/checkpoint-450\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:32:06,233 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:32:07,567 >> tokenizer config file saved in sft_output_model/checkpoint-450/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:32:07,567 >> Special tokens file saved in sft_output_model/checkpoint-450/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:32:07,568 >> added tokens file saved in sft_output_model/checkpoint-450/added_tokens.json\n",
      "[2024-02-27 18:32:07,569] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step450 is about to be saved!\n",
      "[2024-02-27 18:32:12,801] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-450/global_step450/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:32:12,801] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-450/global_step450/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:32:26,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-450/global_step450/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:32:27,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-450/global_step450/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:32:34,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-450/global_step450/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:32:34,510] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-450/global_step450/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:32:34,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step450 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:32:34,512 >> Deleting older checkpoint [sft_output_model/checkpoint-300] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:32:38,332 >> tokenizer config file saved in sft_output_model/checkpoint-450/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:32:38,332 >> Special tokens file saved in sft_output_model/checkpoint-450/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:32:38,332 >> added tokens file saved in sft_output_model/checkpoint-450/sft_lora_model/added_tokens.json\n",
      "{'loss': 1.5985, 'learning_rate': 2.4974650226942064e-06, 'epoch': 1.79}        \n",
      "{'loss': 1.0971, 'learning_rate': 1.5869914928752117e-06, 'epoch': 1.83}        \n",
      "{'loss': 1.1222, 'learning_rate': 8.795789454453862e-07, 'epoch': 1.87}         \n",
      "{'loss': 1.443, 'learning_rate': 3.7819451447189126e-07, 'epoch': 1.91}         \n",
      "{'loss': 0.9926, 'learning_rate': 8.494118050164646e-08, 'epoch': 1.95}         \n",
      "100%|█████████████████████████████████████████| 500/500 [06:50<00:00,  4.03it/s][INFO|trainer.py:2868] 2024-02-27 18:32:51,407 >> Saving model checkpoint to sft_output_model/checkpoint-500\n",
      "[INFO|trainer.py:2880] 2024-02-27 18:32:51,410 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:32:52,652 >> tokenizer config file saved in sft_output_model/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:32:52,652 >> Special tokens file saved in sft_output_model/checkpoint-500/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:32:52,652 >> added tokens file saved in sft_output_model/checkpoint-500/added_tokens.json\n",
      "[2024-02-27 18:32:52,653] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!\n",
      "[2024-02-27 18:32:57,898] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: sft_output_model/checkpoint-500/global_step500/mp_rank_00_model_states.pt\n",
      "[2024-02-27 18:32:57,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-500/global_step500/mp_rank_00_model_states.pt...\n",
      "[2024-02-27 18:33:11,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-500/global_step500/mp_rank_00_model_states.pt.\n",
      "[2024-02-27 18:33:12,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving sft_output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-27 18:33:19,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved sft_output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-27 18:33:19,749] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved sft_output_model/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-27 18:33:19,749] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-27 18:33:19,751 >> Deleting older checkpoint [sft_output_model/checkpoint-350] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:33:23,584 >> tokenizer config file saved in sft_output_model/checkpoint-500/sft_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:33:23,584 >> Special tokens file saved in sft_output_model/checkpoint-500/sft_lora_model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:33:23,584 >> added tokens file saved in sft_output_model/checkpoint-500/sft_lora_model/added_tokens.json\n",
      "[INFO|trainer.py:2039] 2024-02-27 18:33:23,585 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 443.0909, 'train_samples_per_second': 1.128, 'train_steps_per_second': 1.128, 'train_loss': 2.2217895965576173, 'epoch': 1.95}\n",
      "100%|█████████████████████████████████████████| 500/500 [07:23<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2171] 2024-02-27 18:33:24,829 >> tokenizer config file saved in sft_output_model/sft_lora_model/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-27 18:33:24,829 >> Special tokens file saved in sft_output_model/sft_lora_model/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2228] 2024-02-27 18:33:24,829 >> added tokens file saved in sft_output_model/sft_lora_model/added_tokens.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       1.95\r\n",
      "  train_loss               =     2.2218\r\n",
      "  train_runtime            = 0:07:23.09\r\n",
      "  train_samples            =        257\r\n",
      "  train_samples_per_second =      1.128\r\n",
      "  train_steps_per_second   =      1.128\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \\\n",
    "    --deepspeed ds_zero2_no_offload.json \\\n",
    "    --model_name_or_path \"llama_output\" \\\n",
    "    --tokenizer_name_or_path \"merged_tokenizer_hf\" \\\n",
    "    --dataset_dir sft_data \\\n",
    "    --validation_split_percentage 0.001 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --fp16 \\\n",
    "    --seed $RANDOM \\\n",
    "    --max_steps 500 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --weight_decay 0 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --save_steps 50 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --output_dir sft_output_model \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --trainable \"q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj\" \\\n",
    "    --modules_to_save \"embed_tokens,lm_head\" \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "#     --peft_path \"output_model/peft_model\" \\\n",
    "    --gradient_checkpointing \\\n",
    "    --ddp_find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedf2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \\\n",
    "#     --deepspeed ds_zero2_no_offload.json \\\n",
    "#     --model_name_or_path \"llama_output/\"\\\n",
    "#     --tokenizer_name_or_path \"merged_tokenizer_hf\" \\\n",
    "#     --dataset_dir sft_data \\\n",
    "#     --validation_split_percentage 0.001 \\\n",
    "#     --per_device_train_batch_size 1 \\\n",
    "#     --do_train \\\n",
    "#     --do_eval \\\n",
    "#     --seed $RANDOM \\\n",
    "#     --fp16 \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --lr_scheduler_type cosine \\\n",
    "#     --learning_rate 1e-4 \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --weight_decay 0 \\\n",
    "#     --logging_strategy steps \\\n",
    "#     --logging_steps 10 \\\n",
    "#     --save_strategy steps \\\n",
    "#     --save_total_limit 3 \\\n",
    "#     --evaluation_strategy steps \\\n",
    "#     --eval_steps 100 \\\n",
    "#     --save_steps 200 \\\n",
    "#     --gradient_accumulation_steps 8 \\\n",
    "#     --preprocessing_num_workers 8 \\\n",
    "#     --max_seq_length 512 \\\n",
    "#     --output_dir \"output_model\" \\\n",
    "#     --overwrite_output_dir \\\n",
    "#     --ddp_timeout 30000 \\\n",
    "#     --logging_first_step True \\\n",
    "#     --lora_rank 8 \\\n",
    "#     --lora_alpha 32 \\\n",
    "#     --trainable q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj \\\n",
    "#     --modules_to_save embed_tokens,lm_head \\\n",
    "#     --lora_dropout 0.05 \\\n",
    "#     --torch_dtype float16 \\\n",
    "# #     --validation_file \"validation_file_name\" \\\n",
    "# #     --peft_path ${peft_model} \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --ddp_find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49100279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
