{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59444d4c-e288-46f8-91f6-917f4d550715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ninja\n",
    "# !pip install transformers==4.28.1\n",
    "# !pip install git+https://github.com/huggingface/peft.git@13e53fc\n",
    "# !pip install datasets\n",
    "# !pip install sentencepiece\n",
    "# !pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8661ec3-4303-4292-822e-c80ab9d0303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "     active environment : base\r\n",
      "    active env location : /home/ai4d/anaconda3\r\n",
      "            shell level : 1\r\n",
      "       user config file : /home/ai4d/.condarc\r\n",
      " populated config files : /home/ai4d/.condarc\r\n",
      "          conda version : 23.7.4\r\n",
      "    conda-build version : 3.26.1\r\n",
      "         python version : 3.11.5.final.0\r\n",
      "       virtual packages : __archspec=1=x86_64\r\n",
      "                          __cuda=12.2=0\r\n",
      "                          __glibc=2.35=0\r\n",
      "                          __linux=6.2.0=0\r\n",
      "                          __unix=0=0\r\n",
      "       base environment : /home/ai4d/anaconda3  (writable)\r\n",
      "      conda av data dir : /home/ai4d/anaconda3/etc/conda\r\n",
      "  conda av metadata url : None\r\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\r\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\r\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\r\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\r\n",
      "          package cache : /home/ai4d/anaconda3/pkgs\r\n",
      "                          /home/ai4d/.conda/pkgs\r\n",
      "       envs directories : /home/ai4d/anaconda3/envs\r\n",
      "                          /home/ai4d/.conda/envs\r\n",
      "               platform : linux-64\r\n",
      "             user-agent : conda/23.7.4 requests/2.31.0 CPython/3.11.5 Linux/6.2.0-35-generic ubuntu/22.04.3 glibc/2.35 aau/0.4.2 c/WXl2eUg01UCTbd5P29sRRQ s/lrPjHi8Gu6dlkPUyyIC_Pw e/wxUdwP8KP46KTikpjMY1Sw\r\n",
      "                UID:GID : 1000:1000\r\n",
      "             netrc file : None\r\n",
      "           offline mode : False\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c0e23-6292-423d-bcee-3dc2667e61f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-29 17:41:14,529] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-29 17:41:14,806] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-29 17:41:14,806] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "02/29/2024 17:41:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:666] 2024-02-29 17:41:16,562 >> loading configuration file llama_output/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-02-29 17:41:16,563 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"llama_output\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-29 17:41:16,563 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-29 17:41:16,563 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-29 17:41:16,563 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1807] 2024-02-29 17:41:16,563 >> loading file tokenizer_config.json\n",
      "02/29/2024 17:41:20 - INFO - datasets.builder - Using custom data configuration default-eb5544906f77fa73\n",
      "02/29/2024 17:41:20 - INFO - datasets.info - Loading Dataset Infos from /home/ai4d/anaconda3/lib/python3.11/site-packages/datasets/packaged_modules/text\n",
      "02/29/2024 17:41:20 - INFO - datasets.builder - Generating dataset text (/home/ai4d/Desktop/swahili_llama_pretrain/swahili_pretrained/data_cache/sw_text/text/default-eb5544906f77fa73/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Downloading and preparing dataset text/default to /home/ai4d/Desktop/swahili_llama_pretrain/swahili_pretrained/data_cache/sw_text/text/default-eb5544906f77fa73/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 15477.14it/s]\n",
      "02/29/2024 17:41:20 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/29/2024 17:41:20 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1155.77it/s]\n",
      "02/29/2024 17:41:20 - INFO - datasets.builder - Generating train split\n",
      "02/29/2024 17:41:48 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /home/ai4d/Desktop/swahili_llama_pretrain/swahili_pretrained/data_cache/sw_text/text/default-eb5544906f77fa73/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.58it/s]\n",
      "02/29/2024 17:41:48 - INFO - __main__ - sw.txt has been loaded\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #0 will write at data_cache/sw_text/tokenized_00000_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #1 will write at data_cache/sw_text/tokenized_00001_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #2 will write at data_cache/sw_text/tokenized_00002_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #3 will write at data_cache/sw_text/tokenized_00003_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #4 will write at data_cache/sw_text/tokenized_00004_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #5 will write at data_cache/sw_text/tokenized_00005_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #6 will write at data_cache/sw_text/tokenized_00006_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Process #7 will write at data_cache/sw_text/tokenized_00007_of_00008.arrow\n",
      "02/29/2024 17:41:48 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
      "Running tokenizer on dataset (num_proc=8):   0%| | 0/12660806 [00:00<?, ? exampl02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00001_of_00008.arrow\n",
      "02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00000_of_00008.arrow\n",
      "Running tokenizer on dataset (num_proc=8):   0%| | 1000/12660806 [00:00<3:02:16,02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00003_of_00008.arrow\n",
      "02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00006_of_00008.arrow\n",
      "02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00004_of_00008.arrow\n",
      "02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00007_of_00008.arrow\n",
      "Running tokenizer on dataset (num_proc=8):   0%| | 3000/12660806 [00:01<1:01:01,02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00005_of_00008.arrow\n",
      "02/29/2024 17:41:49 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/tokenized_00002_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Concatenating 8 shards    \n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #0 will write at data_cache/sw_text/grouped_00000_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #1 will write at data_cache/sw_text/grouped_00001_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #2 will write at data_cache/sw_text/grouped_00002_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #3 will write at data_cache/sw_text/grouped_00003_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #4 will write at data_cache/sw_text/grouped_00004_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #5 will write at data_cache/sw_text/grouped_00005_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #6 will write at data_cache/sw_text/grouped_00006_of_00008.arrow\n",
      "02/29/2024 18:00:42 - INFO - datasets.arrow_dataset - Process #7 will write at data_cache/sw_text/grouped_00007_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Spawning 8 processes\n",
      "Grouping texts in chunks of 512 (num_proc=8):   0%| | 0/12660806 [00:00<?, ? exa02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00000_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00001_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00002_of_00008.arrow\n",
      "Grouping texts in chunks of 512 (num_proc=8):   0%| | 1000/12660806 [00:00<57:5402/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00003_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00004_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00006_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00005_of_00008.arrow\n",
      "02/29/2024 18:00:43 - INFO - datasets.arrow_dataset - Caching processed dataset at data_cache/sw_text/grouped_00007_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/29/2024 18:05:36 - INFO - datasets.arrow_dataset - Concatenating 8 shards    \n",
      "02/29/2024 18:05:43 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/ai4d/Desktop/swahili_llama_pretrain/swahili_pretrained/data_cache/sw_text/cache-71a6d94165c438da.arrow\n",
      "02/29/2024 18:05:43 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/ai4d/Desktop/swahili_llama_pretrain/swahili_pretrained/data_cache/sw_text/cache-7c89661741a14897.arrow\n",
      "02/29/2024 18:05:43 - INFO - __main__ - Num train_samples  1061708\n",
      "02/29/2024 18:05:43 - INFO - __main__ - training example:\n",
      "02/29/2024 18:05:43 - INFO - __main__ -  lile.<s> Nikasisitiza pia umuhimu wa kuzingatia &#8216;K' tatu ili kufanikiwa katika tuyafanyayo; Kujitambua, Kujiamini na Kuthubutu. Pale Mahango nikasisitiza pia umuhimu wa kuitanguliza Tanzania kwanza katika yote tuyafanyayo. Tusiendekeze sana tofauti zetu za kisiasa. Tuwajue maadui tunaopambana nao, tuyajue malengo yetu kwenye vita iliyo mbele yetu. Tukubaliane kupambana kwa pamoja kwa maana ya kufanya kazi kwa pamoja ili kuyafikia malengo yetu.<s> Maana, hii ni nchi yetu. Ni nchi yetu sote. Hatima yetu kama watu binafsi, kama vikundi vya watu na kama taifa, inatutegemea sisi wenyewe. Umasikini wa watu wetu ni umasikini wetu.<s> Naam, sijawahi kuficha imani yangu ya kisiasa, kwa maana ya itikadi yangu. Na imani yangu inaimarika zaidi kadri ninavyozidi kutembelea na kukutana na watu wa vijijini.<s> Kiitikadi mimi ni Mjamaa ninayeamini katika uwepo wa demokrasia na kutanguliza maslahi ya walio wengi katika jamii- I am a Social Democrat. Na bado naamini kuwa hata sasa, mfumo wa enzi za Mwalimu wa Ujamaa na Kujitegemea bado una nafasi katika jamii yetu. Misingi ya Ujamaa na Kujitegemea bado ni muhimili wa maendeleo ya nchi na watu wake. Ni udhaifu na kutokujiamini kwa kuionea haya misingi iliyoijenga jamii yetu na ambayo bado ina nafasi ya kuifanya nchi yetu kuwa ya kisasa zaidi kama itaimarishwa.<s>Maana, mfumo wa Ujamaa na Kujitegemea uliofanyiwa mabadiliko ya kimsingi unaweza kutuhakikishia ulinzi wa maslahi ya nchi yetu. Ni kwa sera zenye kuhakikisha usawa wa watu. Sera zitakazohakikisha kodi inakusanywa na kutumika katika yaliyo muhimu na yenye kumsaidia pia mwananchi wa chini katika nchi yetu. Mwananchi aweze kupata elimu bora na huduma bora za afya, maji, miundo mbinu kati ya\n",
      "[INFO|modeling_utils.py:2531] 2024-02-29 18:05:43,330 >> loading weights file llama_output/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1176] 2024-02-29 18:05:43,331 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:575] 2024-02-29 18:05:43,331 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:09<00:00,  3.25s/it]\n",
      "[INFO|modeling_utils.py:3190] 2024-02-29 18:05:53,182 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[WARNING|modeling_utils.py:3192] 2024-02-29 18:05:53,182 >> Some weights of LlamaForCausalLM were not initialized from the model checkpoint at llama_output and are newly initialized: ['model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|configuration_utils.py:535] 2024-02-29 18:05:53,184 >> loading configuration file llama_output/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-02-29 18:05:53,184 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "02/29/2024 18:06:03 - INFO - __main__ - Init new peft model\n",
      "02/29/2024 18:06:03 - INFO - __main__ - target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
      "02/29/2024 18:06:03 - INFO - __main__ - lora_rank: 8\n",
      "trainable params: 415776768 || all params: 6892048384 || trainable%: 6.032702396071861\n",
      "[INFO|trainer.py:621] 2024-02-29 18:06:57,219 >> Using cuda_amp half precision backend\n",
      "/home/ai4d/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2024-02-29 18:06:57,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-02-29 18:06:59,249] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-02-29 18:06:59,249] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-02-29 18:06:59,249] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-02-29 18:06:59,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-02-29 18:06:59,265] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>\n",
      "[2024-02-29 18:06:59,265] [WARNING] [engine.py:1189:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2024-02-29 18:06:59,265] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2024-02-29 18:06:59,265] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 100000000\n",
      "[2024-02-29 18:06:59,265] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 100000000\n",
      "[2024-02-29 18:06:59,265] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-02-29 18:06:59,265] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-02-29 18:06:59,903] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-02-29 18:06:59,904] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 15.19 GB         CA 15.23 GB         Max_CA 15 GB \n",
      "[2024-02-29 18:06:59,904] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 35.77 GB, percent = 14.2%\n",
      "[2024-02-29 18:06:59,999] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-02-29 18:06:59,999] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 15.97 GB         CA 16.79 GB         Max_CA 17 GB \n",
      "[2024-02-29 18:06:59,999] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 35.77 GB, percent = 14.2%\n",
      "[2024-02-29 18:06:59,999] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-02-29 18:07:00,090] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-02-29 18:07:00,090] [INFO] [utils.py:801:see_memory_usage] MA 14.42 GB         Max_MA 14.42 GB         CA 16.79 GB         Max_CA 17 GB \n",
      "[2024-02-29 18:07:00,090] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 35.77 GB, percent = 14.2%\n",
      "[2024-02-29 18:07:00,094] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2024-02-29 18:07:00,094] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-02-29 18:07:00,094] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f4e9e2f1190>\n",
      "[2024-02-29 18:07:00,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:987:print] DeepSpeedEngine configuration:\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   amp_enabled .................. False\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   amp_params ................... False\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   bfloat16_enabled ............. False\n",
      "[2024-02-29 18:07:00,095] [INFO] [config.py:991:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4e9e12bbd0>\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   communication_data_type ...... None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   curriculum_enabled_legacy .... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   curriculum_params_legacy ..... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   data_efficiency_enabled ...... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   dataloader_drop_last ......... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   disable_allgather ............ False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   dump_state ................... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1e-10}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_enabled ........... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   eigenvalue_verbose ........... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   elasticity_enabled ........... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   fp16_auto_cast ............... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   fp16_enabled ................. True\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   global_rank .................. 0\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   grad_accum_dtype ............. None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   gradient_accumulation_steps .. 1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   gradient_clipping ............ 1.0\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   graph_harvesting ............. False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   load_universal_checkpoint .... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   loss_scale ................... 0\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   memory_breakdown ............. False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   mics_hierarchial_params_gather  False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   mics_shard_size .............. -1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   optimizer_name ............... None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   optimizer_params ............. None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   pld_enabled .................. False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   pld_params ................... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   prescale_gradients ........... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   scheduler_name ............... None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   scheduler_params ............. None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   sparse_attention ............. None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   sparse_gradients_enabled ..... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   steps_per_print .............. 2000\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   train_batch_size ............. 1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   use_data_before_expert_parallel_  False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   use_node_local_storage ....... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   wall_clock_breakdown ......... False\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   weight_quantization_config ... None\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   world_size ................... 1\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   zero_allow_untested_optimizer  True\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-02-29 18:07:00,096] [INFO] [config.py:991:print]   zero_enabled ................. True\n",
      "[2024-02-29 18:07:00,097] [INFO] [config.py:991:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-02-29 18:07:00,097] [INFO] [config.py:991:print]   zero_optimization_stage ...... 2\n",
      "[2024-02-29 18:07:00,097] [INFO] [config.py:977:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 100, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1e-10\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 1.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 1.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[INFO|trainer.py:1769] 2024-02-29 18:07:00,098 >> ***** Running training *****\n",
      "[INFO|trainer.py:1770] 2024-02-29 18:07:00,098 >>   Num examples = 1,061,708\n",
      "[INFO|trainer.py:1771] 2024-02-29 18:07:00,098 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1772] 2024-02-29 18:07:00,098 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1773] 2024-02-29 18:07:00,098 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1774] 2024-02-29 18:07:00,098 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1775] 2024-02-29 18:07:00,098 >>   Total optimization steps = 1,061,708\n",
      "[INFO|trainer.py:1776] 2024-02-29 18:07:00,099 >>   Number of trainable parameters = 415,776,768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/1061708 [00:00<?, ?it/s][WARNING|logging.py:295] 2024-02-29 18:07:00,335 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ai4d/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ai4d/anaconda3/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1990: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "[2024-02-29 18:07:01,082] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 10.0859, 'learning_rate': 0.0, 'epoch': 0.0}                           \n",
      "  0%|                                   | 1/1061708 [00:01<289:50:05,  1.02it/s][2024-02-29 18:07:01,531] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  0%|                                   | 2/1061708 [00:01<197:16:27,  1.49it/s][2024-02-29 18:07:01,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  0%|                                   | 3/1061708 [00:01<163:10:21,  1.81it/s][2024-02-29 18:07:02,365] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  0%|                                   | 4/1061708 [00:02<147:23:04,  2.00it/s][2024-02-29 18:07:02,781] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  0%|                                   | 6/1061708 [00:03<147:03:04,  2.01it/s][2024-02-29 18:07:03,752] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "  0%|                                   | 7/1061708 [00:03<139:07:19,  2.12it/s][2024-02-29 18:07:04,167] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "{'loss': 9.5304, 'learning_rate': 1.1302414949327507e-08, 'epoch': 0.0}         \n",
      "  0%|                                  | 12/1061708 [00:06<150:26:41,  1.96it/s][2024-02-29 18:07:06,856] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "  0%|                                  | 13/1061708 [00:06<157:22:54,  1.87it/s][2024-02-29 18:07:07,271] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "{'loss': 10.2859, 'learning_rate': 4.144218814753419e-08, 'epoch': 0.0}         \n",
      "{'loss': 9.6461, 'learning_rate': 7.911690464529256e-08, 'epoch': 0.0}          \n",
      "  0%|                                  | 31/1061708 [00:16<152:54:22,  1.93it/s][2024-02-29 18:07:16,787] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\n",
      "{'loss': 9.7828, 'learning_rate': 1.1302414949327505e-07, 'epoch': 0.0}         \n",
      "{'loss': 10.0094, 'learning_rate': 1.5069886599103342e-07, 'epoch': 0.0}        \n",
      "{'loss': 10.2977, 'learning_rate': 1.8837358248879178e-07, 'epoch': 0.0}        \n",
      "  0%|                                  | 61/1061708 [00:32<154:02:42,  1.91it/s][2024-02-29 18:07:32,739] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "{'loss': 9.9344, 'learning_rate': 2.2228082733677433e-07, 'epoch': 0.0}         \n",
      "{'loss': 10.3672, 'learning_rate': 2.599555438345327e-07, 'epoch': 0.0}         \n",
      "{'loss': 10.0355, 'learning_rate': 2.97630260332291e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.5492, 'learning_rate': 3.3530497683004934e-07, 'epoch': 0.0}         \n",
      "{'loss': 10.0406, 'learning_rate': 3.729796933278077e-07, 'epoch': 0.0}         \n",
      "{'loss': 9.757, 'learning_rate': 4.106544098255661e-07, 'epoch': 0.0}           \n",
      "{'loss': 10.182, 'learning_rate': 4.483291263233245e-07, 'epoch': 0.0}          \n",
      "{'loss': 10.1594, 'learning_rate': 4.860038428210828e-07, 'epoch': 0.0}         \n",
      "{'loss': 10.7344, 'learning_rate': 5.236785593188412e-07, 'epoch': 0.0}         \n",
      "{'loss': 9.3066, 'learning_rate': 5.613532758165994e-07, 'epoch': 0.0}          \n",
      "  0%|                                 | 166/1061708 [01:27<156:00:19,  1.89it/s][2024-02-29 18:08:28,093] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  0%|                                 | 169/1061708 [01:29<151:30:16,  1.95it/s][2024-02-29 18:08:29,602] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "{'loss': 9.3184, 'learning_rate': 5.914930490148062e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.3133, 'learning_rate': 6.291677655125645e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.432, 'learning_rate': 6.668424820103229e-07, 'epoch': 0.0}           \n",
      "  0%|                                 | 199/1061708 [01:44<156:05:15,  1.89it/s][2024-02-29 18:08:45,424] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
      "{'loss': 9.3828, 'learning_rate': 7.007497268583055e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.4953, 'learning_rate': 7.384244433560638e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.3324, 'learning_rate': 7.760991598538221e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.9187, 'learning_rate': 8.137738763515806e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.1824, 'learning_rate': 8.514485928493389e-07, 'epoch': 0.0}          \n",
      "{'loss': 9.3961, 'learning_rate': 8.891233093470973e-07, 'epoch': 0.0}          \n",
      "{'loss': 8.7738, 'learning_rate': 9.267980258448555e-07, 'epoch': 0.0}          \n",
      "{'loss': 8.0645, 'learning_rate': 9.644727423426139e-07, 'epoch': 0.0}          \n",
      "{'loss': 8.0379, 'learning_rate': 1.0021474588403722e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.7695, 'learning_rate': 1.0398221753381307e-06, 'epoch': 0.0}         \n",
      "{'loss': 8.5148, 'learning_rate': 1.077496891835889e-06, 'epoch': 0.0}          \n",
      "{'loss': 7.5566, 'learning_rate': 1.1151716083336472e-06, 'epoch': 0.0}         \n",
      "{'loss': 8.318, 'learning_rate': 1.1528463248314057e-06, 'epoch': 0.0}          \n",
      "{'loss': 7.7961, 'learning_rate': 1.190521041329164e-06, 'epoch': 0.0}          \n",
      "{'loss': 7.9258, 'learning_rate': 1.2281957578269224e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.7434, 'learning_rate': 1.265870474324681e-06, 'epoch': 0.0}          \n",
      "  0%|                                 | 358/1061708 [03:11<158:46:10,  1.86it/s][2024-02-29 18:10:12,502] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 7.6328, 'learning_rate': 1.2997777191726634e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.3023, 'learning_rate': 1.3374524356704215e-06, 'epoch': 0.0}         \n",
      "{'loss': 8.1406, 'learning_rate': 1.37512715216818e-06, 'epoch': 0.0}           \n",
      "{'loss': 7.3668, 'learning_rate': 1.4128018686659384e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.7172, 'learning_rate': 1.4504765851636967e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.741, 'learning_rate': 1.488151301661455e-06, 'epoch': 0.0}           \n",
      "{'loss': 8.2355, 'learning_rate': 1.5258260181592134e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.2625, 'learning_rate': 1.5635007346569717e-06, 'epoch': 0.0}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4477, 'learning_rate': 1.60117545115473e-06, 'epoch': 0.0}           \n",
      "{'loss': 7.1195, 'learning_rate': 1.6388501676524886e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.0227, 'learning_rate': 1.676524884150247e-06, 'epoch': 0.0}          \n",
      "  0%|                                 | 466/1061708 [04:09<157:09:38,  1.88it/s][2024-02-29 18:11:10,125] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 7.3445, 'learning_rate': 1.7104321289982294e-06, 'epoch': 0.0}         \n",
      "  0%|                                 | 475/1061708 [04:14<164:05:32,  1.80it/s][2024-02-29 18:11:15,009] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "{'loss': 7.2105, 'learning_rate': 1.744339373846212e-06, 'epoch': 0.0}          \n",
      "  0%|                                 | 487/1061708 [04:20<156:55:32,  1.88it/s][2024-02-29 18:11:21,332] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
      "{'loss': 7.025, 'learning_rate': 1.7782466186941946e-06, 'epoch': 0.0}          \n",
      "{'loss': 7.0156, 'learning_rate': 1.8159213351919525e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.0762, 'learning_rate': 1.853596051689711e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.6496, 'learning_rate': 1.8912707681874694e-06, 'epoch': 0.0}         \n",
      "{'loss': 7.2004, 'learning_rate': 1.9289454846852277e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.8809, 'learning_rate': 1.966620201182986e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.7609, 'learning_rate': 2.0042949176807444e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.6316, 'learning_rate': 2.041969634178503e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.8316, 'learning_rate': 2.0796443506762615e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.4988, 'learning_rate': 2.11731906717402e-06, 'epoch': 0.0}           \n",
      "{'loss': 6.407, 'learning_rate': 2.154993783671778e-06, 'epoch': 0.0}           \n",
      "{'loss': 6.134, 'learning_rate': 2.1926685001695365e-06, 'epoch': 0.0}          \n",
      "  0%|                                 | 609/1061708 [05:26<156:27:26,  1.88it/s][2024-02-29 18:12:27,283] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 6.4715, 'learning_rate': 2.2265757450175188e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.4953, 'learning_rate': 2.264250461515277e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.2922, 'learning_rate': 2.3019251780130354e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.3934, 'learning_rate': 2.3395998945107937e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.0145, 'learning_rate': 2.377274611008552e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.0918, 'learning_rate': 2.414949327506311e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.7789, 'learning_rate': 2.452624044004069e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.3047, 'learning_rate': 2.4902987605018275e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.5602, 'learning_rate': 2.527973476999586e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9875, 'learning_rate': 2.565648193497344e-06, 'epoch': 0.0}          \n",
      "{'loss': 6.259, 'learning_rate': 2.603322909995102e-06, 'epoch': 0.0}           \n",
      "{'loss': 6.1008, 'learning_rate': 2.640997626492861e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9418, 'learning_rate': 2.678672342990619e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9086, 'learning_rate': 2.7163470594883775e-06, 'epoch': 0.0}         \n",
      "{'loss': 6.268, 'learning_rate': 2.754021775986136e-06, 'epoch': 0.0}           \n",
      "{'loss': 5.768, 'learning_rate': 2.791696492483894e-06, 'epoch': 0.0}           \n",
      "  0%|                                 | 768/1061708 [06:53<158:39:39,  1.86it/s][2024-02-29 18:13:54,323] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 5.623, 'learning_rate': 2.825603737331877e-06, 'epoch': 0.0}           \n",
      "{'loss': 6.0586, 'learning_rate': 2.863278453829635e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9973, 'learning_rate': 2.9009531703273935e-06, 'epoch': 0.0}         \n",
      "  0%|                                 | 794/1061708 [07:07<167:32:31,  1.76it/s][2024-02-29 18:14:08,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "{'loss': 5.8473, 'learning_rate': 2.934860415175376e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9711, 'learning_rate': 2.9725351316731345e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.6156, 'learning_rate': 3.0102098481708924e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.8766, 'learning_rate': 3.0478845646686508e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.5957, 'learning_rate': 3.085559281166409e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.7883, 'learning_rate': 3.123233997664168e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.8508, 'learning_rate': 3.160908714161926e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.607, 'learning_rate': 3.1985834306596845e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.9488, 'learning_rate': 3.236258147157443e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.7867, 'learning_rate': 3.273932863655201e-06, 'epoch': 0.0}          \n",
      "  0%|                                 | 899/1061708 [08:03<156:13:55,  1.89it/s][2024-02-29 18:15:04,457] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 5.6352, 'learning_rate': 3.307840108503184e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.5641, 'learning_rate': 3.345514825000942e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.4047, 'learning_rate': 3.3831895414987005e-06, 'epoch': 0.0}         \n",
      "  0%|                                 | 921/1061708 [08:15<156:57:25,  1.88it/s][2024-02-29 18:15:16,363] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "  0%|                                 | 928/1061708 [08:19<155:51:00,  1.89it/s][2024-02-29 18:15:19,998] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
      "{'loss': 5.7039, 'learning_rate': 3.413329314696907e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.8016, 'learning_rate': 3.4510040311946655e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.3113, 'learning_rate': 3.488678747692424e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.618, 'learning_rate': 3.5263534641901826e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.4145, 'learning_rate': 3.564028180687941e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.5453, 'learning_rate': 3.6017028971856984e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.4793, 'learning_rate': 3.6393776136834567e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.5523, 'learning_rate': 3.677052330181215e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3066, 'learning_rate': 3.714727046678974e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.2047, 'learning_rate': 3.752401763176732e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.4883, 'learning_rate': 3.7900764796744905e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.3996, 'learning_rate': 3.827751196172249e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.4555, 'learning_rate': 3.8654259126700076e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.3887, 'learning_rate': 3.9031006291677655e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.2289, 'learning_rate': 3.940775345665524e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.175, 'learning_rate': 3.978450062163282e-06, 'epoch': 0.0}           \n",
      "{'loss': 5.432, 'learning_rate': 4.016124778661041e-06, 'epoch': 0.0}           \n",
      "{'loss': 5.4574, 'learning_rate': 4.053799495158799e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3277, 'learning_rate': 4.0914742116565575e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.4133, 'learning_rate': 4.129148928154316e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9705, 'learning_rate': 4.166823644652074e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.4277, 'learning_rate': 4.204498361149833e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3789, 'learning_rate': 4.242173077647591e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.373, 'learning_rate': 4.27984779414535e-06, 'epoch': 0.0}            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3699, 'learning_rate': 4.3175225106431075e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.0738, 'learning_rate': 4.355197227140866e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1641, 'learning_rate': 4.392871943638625e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0182, 'learning_rate': 4.430546660136382e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3605, 'learning_rate': 4.468221376634141e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1412, 'learning_rate': 4.505896093131899e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.2758, 'learning_rate': 4.5435708096296575e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.1617, 'learning_rate': 4.581245526127416e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1949, 'learning_rate': 4.618920242625174e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3199, 'learning_rate': 4.656594959122933e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1063, 'learning_rate': 4.694269675620691e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0801, 'learning_rate': 4.7319443921184496e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.277, 'learning_rate': 4.7696191086162075e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9652, 'learning_rate': 4.807293825113966e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0557, 'learning_rate': 4.844968541611725e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8891, 'learning_rate': 4.882643258109483e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0461, 'learning_rate': 4.920317974607242e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1066, 'learning_rate': 4.9579926911049995e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.9717, 'learning_rate': 4.995667407602758e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1945, 'learning_rate': 5.033342124100516e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9969, 'learning_rate': 5.071016840598275e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8051, 'learning_rate': 5.108691557096034e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0309, 'learning_rate': 5.146366273593792e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7855, 'learning_rate': 5.18404099009155e-06, 'epoch': 0.0}           \n",
      "{'loss': 5.0406, 'learning_rate': 5.2217157065893074e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.966, 'learning_rate': 5.259390423087066e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.9027, 'learning_rate': 5.297065139584825e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1242, 'learning_rate': 5.334739856082583e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.3613, 'learning_rate': 5.372414572580342e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.034, 'learning_rate': 5.4100892890780995e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7957, 'learning_rate': 5.447764005575858e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.1055, 'learning_rate': 5.485438722073616e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0773, 'learning_rate': 5.523113438571375e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9943, 'learning_rate': 5.560788155069134e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.023, 'learning_rate': 5.5984628715668916e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8961, 'learning_rate': 5.63613758806465e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5537, 'learning_rate': 5.673812304562408e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9348, 'learning_rate': 5.711487021060167e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.549, 'learning_rate': 5.749161737557925e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.6926, 'learning_rate': 5.786836454055684e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9613, 'learning_rate': 5.824511170553442e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8014, 'learning_rate': 5.8621858870512e-06, 'epoch': 0.0}            \n",
      "{'loss': 4.8703, 'learning_rate': 5.899860603548959e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6223, 'learning_rate': 5.937535320046717e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8027, 'learning_rate': 5.975210036544476e-06, 'epoch': 0.0}          \n",
      "{'loss': 5.0676, 'learning_rate': 6.012884753042234e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9137, 'learning_rate': 6.0505594695399915e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.0844, 'learning_rate': 6.08823418603775e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5619, 'learning_rate': 6.125908902535508e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 1650/1061708 [14:46<155:53:34,  1.89it/s][2024-02-29 18:21:47,105] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  0%|                                | 1654/1061708 [14:48<156:46:21,  1.88it/s][2024-02-29 18:21:49,167] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 5.0473, 'learning_rate': 6.156048675733715e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7488, 'learning_rate': 6.193723392231474e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6773, 'learning_rate': 6.231398108729232e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6059, 'learning_rate': 6.26907282522699e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.8734, 'learning_rate': 6.306747541724749e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5949, 'learning_rate': 6.344422258222507e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.9703, 'learning_rate': 6.382096974720266e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3947, 'learning_rate': 6.4197716912180236e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.6766, 'learning_rate': 6.457446407715782e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5125, 'learning_rate': 6.495121124213541e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7109, 'learning_rate': 6.532795840711299e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7785, 'learning_rate': 6.570470557209058e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 1777/1061708 [15:54<158:05:30,  1.86it/s][2024-02-29 18:22:54,704] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.6504, 'learning_rate': 6.60437780205704e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5205, 'learning_rate': 6.6420525185547975e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.824, 'learning_rate': 6.679727235052556e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5449, 'learning_rate': 6.717401951550314e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7789, 'learning_rate': 6.755076668048073e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6482, 'learning_rate': 6.792751384545832e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7564, 'learning_rate': 6.8304261010435896e-06, 'epoch': 0.0}         \n",
      "{'loss': 5.091, 'learning_rate': 6.868100817541348e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3725, 'learning_rate': 6.905775534039106e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7908, 'learning_rate': 6.943450250536865e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6781, 'learning_rate': 6.981124967034623e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6627, 'learning_rate': 7.018799683532382e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 1894/1061708 [16:56<158:43:35,  1.85it/s][2024-02-29 18:23:57,070] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  0%|                                | 1899/1061708 [16:59<156:36:53,  1.88it/s][2024-02-29 18:23:59,655] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 4.7734, 'learning_rate': 7.048939456730588e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 1908/1061708 [17:03<155:46:16,  1.89it/s][2024-02-29 18:24:04,363] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 4.4436, 'learning_rate': 7.082846701578572e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.8543, 'learning_rate': 7.12052141807633e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.533, 'learning_rate': 7.1581961345740885e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3631, 'learning_rate': 7.1958708510718464e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.6566, 'learning_rate': 7.2335455675696035e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.7309, 'learning_rate': 7.271220284067362e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6816, 'learning_rate': 7.30889500056512e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5164, 'learning_rate': 7.346569717062879e-06, 'epoch': 0.0}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5338, 'learning_rate': 7.384244433560638e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 1999/1061708 [17:52<157:17:45,  1.87it/s][2024-02-29 18:24:52,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=30, lr=[7.4219191500583956e-06], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 18:24:52,881] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=1.881392338121272, CurrSamplesPerSec=1.8970965309712258, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 4.473, 'learning_rate': 7.4219191500583956e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.616, 'learning_rate': 7.459593866556154e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.4615, 'learning_rate': 7.497268583053912e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2029/1061708 [18:08<155:53:26,  1.89it/s][2024-02-29 18:25:08,763] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.5982, 'learning_rate': 7.531175827901896e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5457, 'learning_rate': 7.568850544399654e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.4186, 'learning_rate': 7.6065252608974124e-06, 'epoch': 0.0}         \n",
      "{'loss': 4.3877, 'learning_rate': 7.644199977395171e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6051, 'learning_rate': 7.68187469389293e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.435, 'learning_rate': 7.719549410390687e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.5, 'learning_rate': 7.757224126888446e-06, 'epoch': 0.0}             \n",
      "{'loss': 4.4256, 'learning_rate': 7.794898843386205e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.4586, 'learning_rate': 7.832573559883963e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6848, 'learning_rate': 7.870248276381722e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5074, 'learning_rate': 7.907922992879479e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3764, 'learning_rate': 7.945597709377238e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3031, 'learning_rate': 7.983272425874997e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2152/1061708 [19:13<156:02:46,  1.89it/s][2024-02-29 18:26:14,270] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.5035, 'learning_rate': 8.017179670722978e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5404, 'learning_rate': 8.054854387220736e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2171/1061708 [19:23<155:45:21,  1.89it/s][2024-02-29 18:26:24,306] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 4.5893, 'learning_rate': 8.088761632068719e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.7406, 'learning_rate': 8.126436348566476e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2192/1061708 [19:34<155:48:54,  1.89it/s][2024-02-29 18:26:35,436] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 4.2246, 'learning_rate': 8.16034359341446e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.6441, 'learning_rate': 8.19801830991222e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.4357, 'learning_rate': 8.235693026409976e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.466, 'learning_rate': 8.273367742907735e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3693, 'learning_rate': 8.311042459405494e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2648, 'learning_rate': 8.348717175903253e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5879, 'learning_rate': 8.38639189240101e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3336, 'learning_rate': 8.424066608898768e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.4553, 'learning_rate': 8.461741325396527e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3521, 'learning_rate': 8.499416041894286e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.8379, 'learning_rate': 8.537090758392045e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.0883, 'learning_rate': 8.574765474889802e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3887, 'learning_rate': 8.61244019138756e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3096, 'learning_rate': 8.65011490788532e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.748, 'learning_rate': 8.687789624383078e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3295, 'learning_rate': 8.725464340880837e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2359, 'learning_rate': 8.763139057378594e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6992, 'learning_rate': 8.80081377387635e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.3385, 'learning_rate': 8.83848849037411e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.2135, 'learning_rate': 8.876163206871868e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3607, 'learning_rate': 8.913837923369627e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3357, 'learning_rate': 8.951512639867384e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2438, 'learning_rate': 8.989187356365143e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2429/1061708 [21:41<155:58:55,  1.89it/s][2024-02-29 18:28:41,665] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.2346, 'learning_rate': 9.023094601213127e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.1701, 'learning_rate': 9.060769317710884e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2609, 'learning_rate': 9.098444034208643e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.5646, 'learning_rate': 9.136118750706402e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3039, 'learning_rate': 9.17379346720416e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.4455, 'learning_rate': 9.211468183701918e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.052, 'learning_rate': 9.249142900199676e-06, 'epoch': 0.0}           \n",
      "{'loss': 4.0709, 'learning_rate': 9.286817616697435e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2885, 'learning_rate': 9.324492333195194e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.0496, 'learning_rate': 9.362167049692953e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.732, 'learning_rate': 9.39984176619071e-06, 'epoch': 0.0}            \n",
      "  0%|                                | 2535/1061708 [22:37<157:21:08,  1.87it/s][2024-02-29 18:29:38,055] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.2615, 'learning_rate': 9.433749011038692e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2547/1061708 [22:43<156:25:34,  1.88it/s][2024-02-29 18:29:44,384] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 4.0412, 'learning_rate': 9.467656255886675e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.1723, 'learning_rate': 9.505330972384434e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3186, 'learning_rate': 9.543005688882193e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.6777, 'learning_rate': 9.580680405379951e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3854, 'learning_rate': 9.618355121877708e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.2127, 'learning_rate': 9.656029838375466e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.4879, 'learning_rate': 9.693704554873224e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3895, 'learning_rate': 9.731379271370983e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.1717, 'learning_rate': 9.769053987868742e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3598, 'learning_rate': 9.806728704366499e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.9576, 'learning_rate': 9.844403420864258e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2656/1061708 [23:41<156:46:22,  1.88it/s][2024-02-29 18:30:42,363] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.4719, 'learning_rate': 9.878310665712242e-06, 'epoch': 0.0}          \n",
      "  0%|                                | 2665/1061708 [23:46<157:38:11,  1.87it/s][2024-02-29 18:30:47,112] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2066, 'learning_rate': 9.912217910560223e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.3031, 'learning_rate': 9.949892627057982e-06, 'epoch': 0.0}          \n",
      "{'loss': 4.343, 'learning_rate': 9.98756734355574e-06, 'epoch': 0.0}            \n",
      "{'loss': 4.1643, 'learning_rate': 1.0025242060053498e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3553, 'learning_rate': 1.0062916776551256e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3232, 'learning_rate': 1.0100591493049015e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 2723/1061708 [24:17<159:06:27,  1.85it/s][2024-02-29 18:31:17,998] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 4.2982, 'learning_rate': 1.0134498737896998e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2039, 'learning_rate': 1.0172173454394757e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.7125, 'learning_rate': 1.0209848170892515e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3254, 'learning_rate': 1.0247522887390274e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1365, 'learning_rate': 1.0285197603888031e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.4043, 'learning_rate': 1.032287232038579e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.3682, 'learning_rate': 1.0360547036883549e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2002, 'learning_rate': 1.0398221753381307e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.4508, 'learning_rate': 1.0435896469879064e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 2818/1061708 [25:07<155:58:29,  1.89it/s][2024-02-29 18:32:08,513] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 4.2264, 'learning_rate': 1.0469803714727047e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7069, 'learning_rate': 1.0507478431224806e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2539, 'learning_rate': 1.0545153147722565e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2912, 'learning_rate': 1.0582827864220322e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2557, 'learning_rate': 1.062050258071808e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.1113, 'learning_rate': 1.065817729721584e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.4846, 'learning_rate': 1.0695852013713598e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0754, 'learning_rate': 1.0733526730211357e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.176, 'learning_rate': 1.0771201446709114e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8416, 'learning_rate': 1.0808876163206873e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0805, 'learning_rate': 1.0846550879704631e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0234, 'learning_rate': 1.088422559620239e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0637, 'learning_rate': 1.0921900312700147e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1871, 'learning_rate': 1.0959575029197906e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0961, 'learning_rate': 1.0997249745695665e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.257, 'learning_rate': 1.1034924462193423e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.5971, 'learning_rate': 1.1072599178691182e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1867, 'learning_rate': 1.111027389518894e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.2449, 'learning_rate': 1.1147948611686698e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.941, 'learning_rate': 1.1185623328184457e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.2758, 'learning_rate': 1.1223298044682214e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3387, 'learning_rate': 1.1260972761179973e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0434, 'learning_rate': 1.129864747767773e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8631, 'learning_rate': 1.1336322194175488e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7953, 'learning_rate': 1.1373996910673247e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9715, 'learning_rate': 1.1411671627171006e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3079/1061708 [27:26<155:46:52,  1.89it/s][2024-02-29 18:34:27,386] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.1041, 'learning_rate': 1.1445578872018989e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2844, 'learning_rate': 1.1483253588516747e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.275, 'learning_rate': 1.1520928305014506e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7615, 'learning_rate': 1.1558603021512265e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0207, 'learning_rate': 1.1596277738010022e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6918, 'learning_rate': 1.163395245450778e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9389, 'learning_rate': 1.167162717100554e-05, 'epoch': 0.0}          \n",
      "  0%|                                | 3144/1061708 [28:01<158:22:59,  1.86it/s][2024-02-29 18:35:01,978] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.9463, 'learning_rate': 1.170553441585352e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.2039, 'learning_rate': 1.1743209132351279e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2582, 'learning_rate': 1.1780883848849038e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0088, 'learning_rate': 1.1818558565346797e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1682, 'learning_rate': 1.1856233281844555e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3469, 'learning_rate': 1.1893907998342312e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.224, 'learning_rate': 1.1931582714840071e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9121, 'learning_rate': 1.196925743133783e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.1031, 'learning_rate': 1.2006932147835589e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3135, 'learning_rate': 1.2044606864333346e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.3482, 'learning_rate': 1.2082281580831105e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9484, 'learning_rate': 1.2119956297328863e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0418, 'learning_rate': 1.2157631013826622e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8131, 'learning_rate': 1.219530573032438e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9979, 'learning_rate': 1.2232980446822138e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3294/1061708 [29:21<158:18:41,  1.86it/s][2024-02-29 18:36:21,910] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.0406, 'learning_rate': 1.226688769167012e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9037, 'learning_rate': 1.230456240816788e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4957, 'learning_rate': 1.2342237124665636e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0334, 'learning_rate': 1.2379911841163395e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0945, 'learning_rate': 1.2417586557661154e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1104, 'learning_rate': 1.2455261274158913e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0955, 'learning_rate': 1.2492935990656671e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9236, 'learning_rate': 1.253061070715443e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7557, 'learning_rate': 1.2568285423652187e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9854, 'learning_rate': 1.2605960140149948e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0906, 'learning_rate': 1.2643634856647705e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0723, 'learning_rate': 1.2681309573145463e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1629, 'learning_rate': 1.271898428964322e-05, 'epoch': 0.0}          \n",
      "  0%|                                | 3421/1061708 [30:28<155:27:16,  1.89it/s][2024-02-29 18:37:29,501] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.1293, 'learning_rate': 1.2752891534491205e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3432/1061708 [30:34<155:28:01,  1.89it/s][2024-02-29 18:37:35,300] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.8334, 'learning_rate': 1.2786798779339188e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8723, 'learning_rate': 1.2824473495836945e-05, 'epoch': 0.0}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8779, 'learning_rate': 1.2862148212334702e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8902, 'learning_rate': 1.289982292883246e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0455, 'learning_rate': 1.2937497645330218e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7734, 'learning_rate': 1.2975172361827978e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8598, 'learning_rate': 1.3012847078325735e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0625, 'learning_rate': 1.3050521794823495e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0822, 'learning_rate': 1.3088196511321253e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.2574, 'learning_rate': 1.3125871227819011e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0137, 'learning_rate': 1.3163545944316768e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0252, 'learning_rate': 1.3201220660814529e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7133, 'learning_rate': 1.3238895377312286e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0531, 'learning_rate': 1.3276570093810045e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9377, 'learning_rate': 1.3314244810307802e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3589/1061708 [31:58<155:47:48,  1.89it/s][2024-02-29 18:38:58,897] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.0992, 'learning_rate': 1.3348152055155786e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8687, 'learning_rate': 1.3385826771653545e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7508, 'learning_rate': 1.3423501488151302e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9178, 'learning_rate': 1.3461176204649062e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3627/1061708 [32:18<156:04:14,  1.88it/s][2024-02-29 18:39:19,050] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 4.2367, 'learning_rate': 1.3495083449497042e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3634/1061708 [32:22<157:16:06,  1.87it/s][2024-02-29 18:39:22,700] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 4.0625, 'learning_rate': 1.3528990694345026e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3649/1061708 [32:30<155:31:16,  1.89it/s][2024-02-29 18:39:30,614] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 3.7139, 'learning_rate': 1.3562897939193009e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9937, 'learning_rate': 1.3600572655690766e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.891, 'learning_rate': 1.3638247372188526e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.09, 'learning_rate': 1.3675922088686283e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.983, 'learning_rate': 1.3713596805184042e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9578, 'learning_rate': 1.3751271521681799e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0295, 'learning_rate': 1.378894623817956e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7127, 'learning_rate': 1.3826620954677317e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8902, 'learning_rate': 1.3864295671175075e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1164, 'learning_rate': 1.3901970387672832e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9107, 'learning_rate': 1.3939645104170593e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7027, 'learning_rate': 1.397731982066835e-05, 'epoch': 0.0}          \n",
      "  0%|                                | 3769/1061708 [33:33<155:55:36,  1.88it/s][2024-02-29 18:40:34,456] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.0004, 'learning_rate': 1.4011227065516333e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.2576, 'learning_rate': 1.404890178201409e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0736, 'learning_rate': 1.408657649851185e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9535, 'learning_rate': 1.4124251215009607e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5443, 'learning_rate': 1.4161925931507366e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6588, 'learning_rate': 1.4199600648005123e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7574, 'learning_rate': 1.4237275364502883e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.882, 'learning_rate': 1.427495008100064e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.8297, 'learning_rate': 1.43126247974984e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.9914, 'learning_rate': 1.4350299513996156e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7621, 'learning_rate': 1.4387974230493917e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6773, 'learning_rate': 1.4425648946991674e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8258, 'learning_rate': 1.4463323663489434e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8037, 'learning_rate': 1.4500998379987191e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0191, 'learning_rate': 1.453867309648495e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7473, 'learning_rate': 1.4576347812982707e-05, 'epoch': 0.0}         \n",
      "  0%|                                | 3923/1061708 [34:56<159:48:25,  1.84it/s][2024-02-29 18:41:56,678] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.3479, 'learning_rate': 1.461025505783069e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7283, 'learning_rate': 1.4647929774328447e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8857, 'learning_rate': 1.4685604490826207e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8393, 'learning_rate': 1.4723279207323964e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7422, 'learning_rate': 1.4760953923821725e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0676, 'learning_rate': 1.4798628640319482e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9021, 'learning_rate': 1.483630335681724e-05, 'epoch': 0.0}          \n",
      "  0%|                                | 3998/1061708 [35:36<156:06:10,  1.88it/s][2024-02-29 18:42:36,671] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  0%|                                | 3999/1061708 [35:36<147:00:12,  2.00it/s][2024-02-29 18:42:37,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=53, lr=[1.4870210601665225e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 18:42:37,204] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=1.8878550374496221, CurrSamplesPerSec=1.8935866961925594, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 3.5893, 'learning_rate': 1.4870210601665225e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6486, 'learning_rate': 1.490788531816298e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.81, 'learning_rate': 1.4945560034660741e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.7111, 'learning_rate': 1.4983234751158498e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0846, 'learning_rate': 1.5020909467656258e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.018, 'learning_rate': 1.5058584184154016e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.834, 'learning_rate': 1.5096258900651774e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.1813, 'learning_rate': 1.5133933617149531e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9088, 'learning_rate': 1.5171608333647292e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5564, 'learning_rate': 1.5209283050145049e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.74, 'learning_rate': 1.5246957766642806e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.6248, 'learning_rate': 1.5284632483140566e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9432, 'learning_rate': 1.5322307199638323e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7672, 'learning_rate': 1.535998191613608e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9215, 'learning_rate': 1.5397656632633838e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9926, 'learning_rate': 1.5435331349131598e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.0994, 'learning_rate': 1.5473006065629355e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4162/1061708 [37:03<155:48:24,  1.89it/s][2024-02-29 18:44:04,214] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5395, 'learning_rate': 1.550691331047734e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8732, 'learning_rate': 1.55445880269751e-05, 'epoch': 0.0}           \n",
      "  0%|▏                               | 4185/1061708 [37:15<157:53:50,  1.86it/s][2024-02-29 18:44:16,436] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.8045, 'learning_rate': 1.557849527182308e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8299, 'learning_rate': 1.5616169988320838e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9322, 'learning_rate': 1.5653844704818595e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1004, 'learning_rate': 1.5691519421316355e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.3613, 'learning_rate': 1.5729194137814112e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7687, 'learning_rate': 1.5766868854311873e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9625, 'learning_rate': 1.580454357080963e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5996, 'learning_rate': 1.584221828730739e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5652, 'learning_rate': 1.5879893003805148e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8779, 'learning_rate': 1.5917567720302905e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.699, 'learning_rate': 1.595524243680066e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.7316, 'learning_rate': 1.5992917153298422e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.42, 'learning_rate': 1.603059186979618e-05, 'epoch': 0.0}            \n",
      "{'loss': 3.8512, 'learning_rate': 1.606826658629394e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.3992, 'learning_rate': 1.6105941302791697e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9137, 'learning_rate': 1.6143616019289457e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7811, 'learning_rate': 1.6181290735787214e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4354/1061708 [38:46<158:09:11,  1.86it/s][2024-02-29 18:45:46,642] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.708, 'learning_rate': 1.6215197980635195e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7125, 'learning_rate': 1.6252872697132952e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8805, 'learning_rate': 1.6290547413630713e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6275, 'learning_rate': 1.632822213012847e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5484, 'learning_rate': 1.636589684662623e-05, 'epoch': 0.0}          \n",
      "  0%|▏                               | 4402/1061708 [39:11<155:54:11,  1.88it/s][2024-02-29 18:46:12,229] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.8279, 'learning_rate': 1.6399804091474215e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6641, 'learning_rate': 1.643747880797197e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.926, 'learning_rate': 1.647515352446973e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.8205, 'learning_rate': 1.6512828240967486e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6086, 'learning_rate': 1.6550502957465246e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6285, 'learning_rate': 1.6588177673963003e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8078, 'learning_rate': 1.6625852390460764e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5527, 'learning_rate': 1.666352710695852e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6672, 'learning_rate': 1.670120182345628e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0295, 'learning_rate': 1.6738876539954038e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5613, 'learning_rate': 1.67765512564518e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.8441, 'learning_rate': 1.6814225972949556e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.9158, 'learning_rate': 1.6851900689447313e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4535/1061708 [40:22<157:23:54,  1.87it/s][2024-02-29 18:47:23,204] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.0623, 'learning_rate': 1.6885807934295294e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4875, 'learning_rate': 1.6923482650793054e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8434, 'learning_rate': 1.696115736729081e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5684, 'learning_rate': 1.6998832083788572e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7514, 'learning_rate': 1.703650680028633e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8857, 'learning_rate': 1.707418151678409e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.517, 'learning_rate': 1.7111856233281846e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7178, 'learning_rate': 1.7149530949779603e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7723, 'learning_rate': 1.718720566627736e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6932, 'learning_rate': 1.722488038277512e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0131, 'learning_rate': 1.7262555099272878e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4645/1061708 [41:21<157:02:34,  1.87it/s][2024-02-29 18:48:21,881] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.6359, 'learning_rate': 1.7296462344120862e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4655/1061708 [41:26<157:11:49,  1.87it/s][2024-02-29 18:48:27,156] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.4177, 'learning_rate': 1.7330369588968843e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8832, 'learning_rate': 1.73680443054666e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.699, 'learning_rate': 1.740571902196436e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.7059, 'learning_rate': 1.7443393738462118e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8768, 'learning_rate': 1.748106845495988e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7305, 'learning_rate': 1.7518743171457635e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8076, 'learning_rate': 1.7556417887955396e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7066, 'learning_rate': 1.7594092604453153e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8242, 'learning_rate': 1.763176732095091e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.0285, 'learning_rate': 1.766944203744867e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5936, 'learning_rate': 1.7707116753946428e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7066, 'learning_rate': 1.7744791470444185e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4771/1061708 [42:28<155:41:54,  1.89it/s][2024-02-29 18:49:28,993] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 4.0475, 'learning_rate': 1.777869871529217e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7796, 'learning_rate': 1.7816373431789926e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 4798/1061708 [42:42<156:06:28,  1.88it/s][2024-02-29 18:49:43,311] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3572, 'learning_rate': 1.785028067663791e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6615, 'learning_rate': 1.7887955393135668e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6, 'learning_rate': 1.7925630109633425e-05, 'epoch': 0.0}            \n",
      "  0%|▏                               | 4827/1061708 [42:58<156:16:09,  1.88it/s][2024-02-29 18:49:58,685] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.9209, 'learning_rate': 1.795953735448141e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.383, 'learning_rate': 1.7997212070979166e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6521, 'learning_rate': 1.8034886787476926e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7691, 'learning_rate': 1.8072561503974684e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7703, 'learning_rate': 1.8110236220472444e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6281, 'learning_rate': 1.81479109369702e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.8996, 'learning_rate': 1.8185585653467958e-05, 'epoch': 0.0}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4773, 'learning_rate': 1.8223260369965715e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7309, 'learning_rate': 1.8260935086463476e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6204, 'learning_rate': 1.8298609802961233e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6984, 'learning_rate': 1.8336284519458993e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5293, 'learning_rate': 1.837395923595675e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4779, 'learning_rate': 1.8411633952454507e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8486, 'learning_rate': 1.8449308668952268e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5754, 'learning_rate': 1.8486983385450025e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.3488, 'learning_rate': 1.8524658101947785e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.3786, 'learning_rate': 1.8562332818445542e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7281, 'learning_rate': 1.86000075349433e-05, 'epoch': 0.0}           \n",
      "  0%|▏                               | 5000/1061708 [44:30<155:40:25,  1.89it/s][INFO|trainer.py:2868] 2024-02-29 18:51:30,590 >> Saving model checkpoint to output_model/checkpoint-5000\n",
      "[INFO|trainer.py:2880] 2024-02-29 18:51:30,593 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 18:51:31,808 >> tokenizer config file saved in output_model/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 18:51:31,808 >> Special tokens file saved in output_model/checkpoint-5000/special_tokens_map.json\n",
      "[2024-02-29 18:51:31,809] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!\n",
      "[2024-02-29 18:51:37,030] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 18:51:37,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 18:51:50,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5000/global_step5000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 18:51:51,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 18:51:58,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 18:51:58,698] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-5000/global_step5000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 18:51:58,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 18:51:59,913 >> tokenizer config file saved in output_model/checkpoint-5000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 18:51:59,913 >> Special tokens file saved in output_model/checkpoint-5000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 4.06, 'learning_rate': 1.8637682251441056e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.5006, 'learning_rate': 1.8675356967938817e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.615, 'learning_rate': 1.8713031684436574e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.2131, 'learning_rate': 1.8750706400934334e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.2006, 'learning_rate': 1.878838111743209e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8297, 'learning_rate': 1.8826055833929852e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7619, 'learning_rate': 1.886373055042761e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6342, 'learning_rate': 1.890140526692537e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8127, 'learning_rate': 1.8939079983423126e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8873, 'learning_rate': 1.8976754699920883e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4132, 'learning_rate': 1.901442941641864e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.9006, 'learning_rate': 1.90521041329164e-05, 'epoch': 0.0}           \n",
      "  0%|▏                               | 5120/1061708 [46:03<156:01:29,  1.88it/s][2024-02-29 18:53:04,141] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  0%|▏                               | 5126/1061708 [46:06<156:22:19,  1.88it/s][2024-02-29 18:53:07,271] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3533, 'learning_rate': 1.908224390611461e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.56, 'learning_rate': 1.9119918622612366e-05, 'epoch': 0.0}           \n",
      "{'loss': 3.8312, 'learning_rate': 1.9157593339110123e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6584, 'learning_rate': 1.919526805560788e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.5223, 'learning_rate': 1.923294277210564e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7031, 'learning_rate': 1.9270617488603398e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6014, 'learning_rate': 1.930829220510116e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.6066, 'learning_rate': 1.9345966921598916e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5551, 'learning_rate': 1.9383641638096676e-05, 'epoch': 0.0}         \n",
      "  0%|▏                               | 5210/1061708 [46:51<156:19:15,  1.88it/s][2024-02-29 18:53:52,153] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.523, 'learning_rate': 1.9417548882944657e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7436, 'learning_rate': 1.9455223599442414e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.3406, 'learning_rate': 1.949289831594017e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.3662, 'learning_rate': 1.953057303243793e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4448, 'learning_rate': 1.956824774893569e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7723, 'learning_rate': 1.960592246543345e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.4785, 'learning_rate': 1.9643597181931206e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6379, 'learning_rate': 1.9681271898428967e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4059, 'learning_rate': 1.9718946614926724e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.81, 'learning_rate': 1.9756621331424484e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.5197, 'learning_rate': 1.979429604792224e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3758, 'learning_rate': 1.9831970764419998e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1352, 'learning_rate': 1.9869645480917755e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2865, 'learning_rate': 1.9907320197415516e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6061, 'learning_rate': 1.9944994913913273e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3459, 'learning_rate': 1.9982669630411033e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4225, 'learning_rate': 2.002034434690879e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2584, 'learning_rate': 2.005801906340655e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.633, 'learning_rate': 2.0095693779904308e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 5404/1061708 [48:35<157:53:36,  1.86it/s][2024-02-29 18:55:35,766] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.5598, 'learning_rate': 2.012960102475229e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4365, 'learning_rate': 2.0167275741250046e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2906, 'learning_rate': 2.0204950457747806e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3729, 'learning_rate': 2.0242625174245563e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 5449/1061708 [48:59<155:49:13,  1.88it/s][2024-02-29 18:55:59,709] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.3822, 'learning_rate': 2.0276532419093548e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7127, 'learning_rate': 2.0314207135591308e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5238, 'learning_rate': 2.0351881852089065e-05, 'epoch': 0.01}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5396, 'learning_rate': 2.0389556568586822e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6154, 'learning_rate': 2.042723128508458e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3386, 'learning_rate': 2.046490600158234e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.859, 'learning_rate': 2.0502580718080097e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4979, 'learning_rate': 2.0540255434577857e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.627, 'learning_rate': 2.0577930151075614e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4656, 'learning_rate': 2.0615604867573375e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0544, 'learning_rate': 2.0653279584071132e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0376, 'learning_rate': 2.069095430056889e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.5707, 'learning_rate': 2.0728629017066646e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5572, 'learning_rate': 2.0766303733564406e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7127, 'learning_rate': 2.0803978450062163e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.757, 'learning_rate': 2.084165316655992e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3223, 'learning_rate': 2.087932788305768e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 5616/1061708 [50:28<156:22:52,  1.88it/s][2024-02-29 18:57:28,772] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.4389, 'learning_rate': 2.0913235127905665e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.624, 'learning_rate': 2.0950909844403422e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3004, 'learning_rate': 2.098858456090118e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.5096, 'learning_rate': 2.1026259277398937e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4598, 'learning_rate': 2.1063933993896697e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2118, 'learning_rate': 2.1101608710394454e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2436, 'learning_rate': 2.1139283426892215e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6533, 'learning_rate': 2.117695814338997e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.355, 'learning_rate': 2.1214632859887732e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.7387, 'learning_rate': 2.125230757638549e-05, 'epoch': 0.01}         \n",
      "{'loss': 4.0422, 'learning_rate': 2.128998229288325e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4951, 'learning_rate': 2.1327657009381007e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2818, 'learning_rate': 2.1365331725878764e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.122, 'learning_rate': 2.140300644237652e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.5967, 'learning_rate': 2.144068115887428e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 5768/1061708 [51:49<155:24:29,  1.89it/s][2024-02-29 18:58:49,769] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.6984, 'learning_rate': 2.1474588403722262e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7174, 'learning_rate': 2.1512263120220023e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 5788/1061708 [51:59<155:33:36,  1.89it/s][2024-02-29 18:59:00,347] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3776, 'learning_rate': 2.1546170365068004e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5836, 'learning_rate': 2.158384508156576e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3906, 'learning_rate': 2.1621519798063518e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0586, 'learning_rate': 2.1659194514561278e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 5821/1061708 [52:17<155:00:56,  1.89it/s][2024-02-29 18:59:17,807] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.2925, 'learning_rate': 2.1693101759409263e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7354, 'learning_rate': 2.173077647590702e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2053, 'learning_rate': 2.176845119240478e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4678, 'learning_rate': 2.1806125908902537e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4608, 'learning_rate': 2.1843800625400294e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5234, 'learning_rate': 2.188147534189805e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6266, 'learning_rate': 2.1919150058395812e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3486, 'learning_rate': 2.195682477489357e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4053, 'learning_rate': 2.199449949139133e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2544, 'learning_rate': 2.2032174207889086e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3336, 'learning_rate': 2.2069848924386847e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4893, 'learning_rate': 2.2107523640884604e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.8572, 'learning_rate': 2.2145198357382364e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 5952/1061708 [53:26<154:55:11,  1.89it/s][2024-02-29 19:00:27,526] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.7031, 'learning_rate': 2.2179105602230342e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5139, 'learning_rate': 2.2216780318728102e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5105, 'learning_rate': 2.225445503522586e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6344, 'learning_rate': 2.229212975172362e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 5999/1061708 [53:51<156:41:12,  1.87it/s][2024-02-29 19:00:52,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=73, lr=[2.2329804468221377e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 19:00:52,596] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=1.8890588591603261, CurrSamplesPerSec=1.8979498287941652, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 3.4893, 'learning_rate': 2.2329804468221377e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3203, 'learning_rate': 2.2367479184719137e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5193, 'learning_rate': 2.2405153901216894e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.516, 'learning_rate': 2.2442828617714655e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4981, 'learning_rate': 2.2480503334212412e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5086, 'learning_rate': 2.251817805071017e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 6055/1061708 [54:21<157:03:00,  1.87it/s][2024-02-29 19:01:22,352] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.2672, 'learning_rate': 2.255208529555815e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.592, 'learning_rate': 2.258976001205591e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.8469, 'learning_rate': 2.2627434728553667e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 6081/1061708 [54:35<155:12:56,  1.89it/s][2024-02-29 19:01:36,129] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  1%|▏                               | 6087/1061708 [54:38<155:15:26,  1.89it/s][2024-02-29 19:01:39,243] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.4934, 'learning_rate': 2.2657574501751876e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4508, 'learning_rate': 2.2695249218249633e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2693, 'learning_rate': 2.273292393474739e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3225, 'learning_rate': 2.277059865124515e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.5264, 'learning_rate': 2.2808273367742907e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3217, 'learning_rate': 2.2845948084240668e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8926, 'learning_rate': 2.2883622800738425e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3922, 'learning_rate': 2.2921297517236185e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7547, 'learning_rate': 2.2958972233733942e-05, 'epoch': 0.01}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5412, 'learning_rate': 2.29966469502317e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.7178, 'learning_rate': 2.3034321666729457e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1719, 'learning_rate': 2.3071996383227217e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1738, 'learning_rate': 2.3109671099724974e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5113, 'learning_rate': 2.3147345816222735e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4475, 'learning_rate': 2.318502053272049e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1771, 'learning_rate': 2.3222695249218252e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5465, 'learning_rate': 2.326036996571601e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.408, 'learning_rate': 2.329804468221377e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.4477, 'learning_rate': 2.3335719398711527e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.7035, 'learning_rate': 2.3373394115209284e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 6281/1061708 [56:21<155:01:50,  1.89it/s][2024-02-29 19:03:22,480] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.4734, 'learning_rate': 2.3407301360057265e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3178, 'learning_rate': 2.3444976076555025e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2809, 'learning_rate': 2.3482650793052782e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3031, 'learning_rate': 2.3520325509550543e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4791, 'learning_rate': 2.35580002260483e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3865, 'learning_rate': 2.359567494254606e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4701, 'learning_rate': 2.3633349659043817e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4045, 'learning_rate': 2.3671024375541574e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5219, 'learning_rate': 2.370869909203933e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4043, 'learning_rate': 2.3746373808537092e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.827, 'learning_rate': 2.378404852503485e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.5589, 'learning_rate': 2.382172324153261e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6711, 'learning_rate': 2.3859397958030366e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 6412/1061708 [57:31<155:29:57,  1.89it/s][2024-02-29 19:04:32,210] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▏                               | 6414/1061708 [57:32<151:51:58,  1.93it/s][2024-02-29 19:04:33,169] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.5736, 'learning_rate': 2.388953773122857e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4514, 'learning_rate': 2.3927212447726332e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.8246, 'learning_rate': 2.396488716422409e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.9316, 'learning_rate': 2.400256188072185e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2497, 'learning_rate': 2.4040236597219606e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3062, 'learning_rate': 2.4077911313717367e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2715, 'learning_rate': 2.4115586030215124e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1589, 'learning_rate': 2.4153260746712884e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2655, 'learning_rate': 2.419093546321064e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3686, 'learning_rate': 2.42286101797084e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.7682, 'learning_rate': 2.4266284896206155e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3678, 'learning_rate': 2.4303959612703916e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.385, 'learning_rate': 2.4341634329201673e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.7828, 'learning_rate': 2.4379309045699433e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1348, 'learning_rate': 2.441698376219719e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2341, 'learning_rate': 2.445465847869495e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4215, 'learning_rate': 2.4492333195192708e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9336, 'learning_rate': 2.453000791169047e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3527, 'learning_rate': 2.4567682628188225e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0551, 'learning_rate': 2.4605357344685983e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4035, 'learning_rate': 2.464303206118374e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 6627/1061708 [59:26<155:37:07,  1.88it/s][2024-02-29 19:06:26,612] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.2557, 'learning_rate': 2.4676939306031724e-05, 'epoch': 0.01}        \n",
      "  1%|▏                               | 6632/1061708 [59:28<153:36:15,  1.91it/s][2024-02-29 19:06:29,205] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.6742, 'learning_rate': 2.471084655087971e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2439, 'learning_rate': 2.4748521267377465e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4314, 'learning_rate': 2.4786195983875223e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9973, 'learning_rate': 2.482387070037298e-05, 'epoch': 0.01}         \n",
      "  1%|▏                               | 6672/1061708 [59:49<155:25:52,  1.89it/s][2024-02-29 19:06:50,541] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3707, 'learning_rate': 2.4857777945220964e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4936, 'learning_rate': 2.489545266171872e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2637, 'learning_rate': 2.493312737821648e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4847, 'learning_rate': 2.497080209471424e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3748, 'learning_rate': 2.5008476811211996e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5209, 'learning_rate': 2.5046151527709756e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9379, 'learning_rate': 2.5083826244207513e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2029, 'learning_rate': 2.5121500960705274e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5562, 'learning_rate': 2.5159175677203027e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6098, 'learning_rate': 2.5196850393700788e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 6778/1061708 [1:00:46<155:27:28,  1.88it/s][2024-02-29 19:07:46,955] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.5469, 'learning_rate': 2.523075763854877e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3225, 'learning_rate': 2.526843235504653e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 6793/1061708 [1:00:54<158:14:00,  1.85it/s][2024-02-29 19:07:54,867] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3885, 'learning_rate': 2.5302339599894514e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3029, 'learning_rate': 2.5340014316392267e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3639, 'learning_rate': 2.5377689032890028e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6748, 'learning_rate': 2.5415363749387788e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1939, 'learning_rate': 2.545303846588555e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3613, 'learning_rate': 2.5490713182383302e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1178, 'learning_rate': 2.5528387898881063e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5787, 'learning_rate': 2.5566062615378823e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1775, 'learning_rate': 2.5603737331876577e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5771, 'learning_rate': 2.5641412048374337e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4875, 'learning_rate': 2.5679086764872094e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1442, 'learning_rate': 2.5716761481369855e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9946, 'learning_rate': 2.575443619786761e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2965, 'learning_rate': 2.579211091436537e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5012, 'learning_rate': 2.582978563086313e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3412, 'learning_rate': 2.586746034736089e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 6958/1061708 [1:02:22<155:39:19,  1.88it/s][2024-02-29 19:09:22,864] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.1344, 'learning_rate': 2.590136759220887e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6109, 'learning_rate': 2.5939042308706628e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6267, 'learning_rate': 2.5976717025204388e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1191, 'learning_rate': 2.6014391741702142e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.291, 'learning_rate': 2.6052066458199902e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3842, 'learning_rate': 2.6089741174697663e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3031, 'learning_rate': 2.6127415891195423e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2434, 'learning_rate': 2.6165090607693177e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4168, 'learning_rate': 2.6202765324190937e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3773, 'learning_rate': 2.6240440040688698e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2787, 'learning_rate': 2.6278114757186455e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7062/1061708 [1:03:17<154:59:46,  1.89it/s][2024-02-29 19:10:18,201] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▏                             | 7064/1061708 [1:03:18<151:18:35,  1.94it/s][2024-02-29 19:10:19,160] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.4656, 'learning_rate': 2.6308254530384663e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0188, 'learning_rate': 2.6345929246882417e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9996, 'learning_rate': 2.6383603963380177e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2269, 'learning_rate': 2.6421278679877938e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2125, 'learning_rate': 2.645895339637569e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1084, 'learning_rate': 2.6496628112873452e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1271, 'learning_rate': 2.653430282937121e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2809, 'learning_rate': 2.657197754586897e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 7140/1061708 [1:03:59<154:53:56,  1.89it/s][2024-02-29 19:10:59,590] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3816, 'learning_rate': 2.660588479071695e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3009, 'learning_rate': 2.664355950721471e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4143, 'learning_rate': 2.668123422371247e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2753, 'learning_rate': 2.6718908940210225e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2768, 'learning_rate': 2.6756583656707985e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2166, 'learning_rate': 2.6794258373205743e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.323, 'learning_rate': 2.6831933089703503e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9982, 'learning_rate': 2.6869607806201257e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0797, 'learning_rate': 2.6907282522699017e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.6406, 'learning_rate': 2.6944957239196778e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4697, 'learning_rate': 2.6982631955694538e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4617, 'learning_rate': 2.702030667219229e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6684, 'learning_rate': 2.7057981388690052e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7275/1061708 [1:05:10<156:41:24,  1.87it/s][2024-02-29 19:12:11,424] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.9585, 'learning_rate': 2.7091888633538033e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7285/1061708 [1:05:16<156:47:37,  1.87it/s][2024-02-29 19:12:16,698] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.0876, 'learning_rate': 2.7125795878386018e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1699, 'learning_rate': 2.7163470594883778e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2344, 'learning_rate': 2.720114531138153e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9645, 'learning_rate': 2.7238820027879292e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2148, 'learning_rate': 2.7276494744377053e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0562, 'learning_rate': 2.7314169460874806e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0542, 'learning_rate': 2.7351844177372567e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2577, 'learning_rate': 2.7389518893870324e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0762, 'learning_rate': 2.7427193610368084e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0713, 'learning_rate': 2.7464868326865838e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2262, 'learning_rate': 2.7502543043363598e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4082, 'learning_rate': 2.754021775986136e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0956, 'learning_rate': 2.757789247635912e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3639, 'learning_rate': 2.7615567192856873e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1947, 'learning_rate': 2.7653241909354633e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4809, 'learning_rate': 2.7690916625852394e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2929, 'learning_rate': 2.772859134235015e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0857, 'learning_rate': 2.7766266058847908e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8923, 'learning_rate': 2.7803940775345665e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7471/1061708 [1:06:55<155:15:10,  1.89it/s][2024-02-29 19:13:55,910] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.4553, 'learning_rate': 2.7837848020193653e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0103, 'learning_rate': 2.7875522736691406e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7767, 'learning_rate': 2.7913197453189167e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5051, 'learning_rate': 2.7950872169686927e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3215, 'learning_rate': 2.7988546886184684e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0076, 'learning_rate': 2.802622160268244e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1974, 'learning_rate': 2.80638963191802e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9333, 'learning_rate': 2.810157103567796e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3814, 'learning_rate': 2.8139245752175713e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8804, 'learning_rate': 2.8176920468673473e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7570/1061708 [1:07:48<155:25:53,  1.88it/s][2024-02-29 19:14:48,660] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.4654, 'learning_rate': 2.821082771352146e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.26, 'learning_rate': 2.8248502430019214e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3781, 'learning_rate': 2.8286177146516975e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5133, 'learning_rate': 2.8323851863014732e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5908, 'learning_rate': 2.8361526579512492e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5223, 'learning_rate': 2.8399201296010246e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.259, 'learning_rate': 2.8436876012508007e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3508, 'learning_rate': 2.8474550729005767e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0553, 'learning_rate': 2.8512225445503527e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1464, 'learning_rate': 2.854990016200128e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.7893, 'learning_rate': 2.858757487849904e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3943, 'learning_rate': 2.86252495949968e-05, 'epoch': 0.01}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2938, 'learning_rate': 2.866292431149456e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2842, 'learning_rate': 2.8700599027992313e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5187, 'learning_rate': 2.8738273744490073e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9361, 'learning_rate': 2.8775948460987834e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4951, 'learning_rate': 2.8813623177485587e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2566, 'learning_rate': 2.8851297893983348e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1283, 'learning_rate': 2.8888972610481108e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7768/1061708 [1:09:33<155:33:26,  1.88it/s][2024-02-29 19:16:34,050] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.2985, 'learning_rate': 2.892287985532909e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2975, 'learning_rate': 2.8960554571826846e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1598, 'learning_rate': 2.8998229288324607e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2447, 'learning_rate': 2.9035904004822367e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.98, 'learning_rate': 2.907357872132012e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.9837, 'learning_rate': 2.911125343781788e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2357, 'learning_rate': 2.9148928154315642e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7831/1061708 [1:10:07<154:57:40,  1.89it/s][2024-02-29 19:17:07,567] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.0795, 'learning_rate': 2.9182835399163623e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0449, 'learning_rate': 2.922051011566138e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4035, 'learning_rate': 2.925818483215914e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9813, 'learning_rate': 2.9295859548656894e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4527, 'learning_rate': 2.9333534265154654e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3041, 'learning_rate': 2.9371208981652415e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4564, 'learning_rate': 2.9408883698150175e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.119, 'learning_rate': 2.944655841464793e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.2061, 'learning_rate': 2.948423313114569e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.113, 'learning_rate': 2.952190784764345e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.2775, 'learning_rate': 2.9559582564141207e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0783, 'learning_rate': 2.9597257280638964e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.501, 'learning_rate': 2.963493199713672e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.2793, 'learning_rate': 2.967260671363448e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2973, 'learning_rate': 2.9710281430132242e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 7980/1061708 [1:11:26<154:59:20,  1.89it/s][2024-02-29 19:18:26,857] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▏                             | 7986/1061708 [1:11:29<155:20:42,  1.88it/s][2024-02-29 19:18:29,978] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.8387, 'learning_rate': 2.974042120333045e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 7999/1061708 [1:11:36<155:29:44,  1.88it/s][2024-02-29 19:18:36,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=96, lr=[2.9778095919828204e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 19:18:36,951] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=1.8903949242612779, CurrSamplesPerSec=1.8950950686820511, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 3.2814, 'learning_rate': 2.9778095919828204e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2979, 'learning_rate': 2.981577063632596e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4193, 'learning_rate': 2.985344535282372e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3699, 'learning_rate': 2.9891120069321482e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3725, 'learning_rate': 2.9928794785819236e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0618, 'learning_rate': 2.9966469502316996e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 8059/1061708 [1:12:08<155:11:51,  1.89it/s][2024-02-29 19:19:08,900] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.9881, 'learning_rate': 3.0000376747164977e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3457, 'learning_rate': 3.0038051463662737e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2297, 'learning_rate': 3.0075726180160495e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.975, 'learning_rate': 3.0113400896658255e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4287, 'learning_rate': 3.015107561315601e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3254, 'learning_rate': 3.018875032965377e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9775, 'learning_rate': 3.022642504615153e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.201, 'learning_rate': 3.026409976264929e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3836, 'learning_rate': 3.0301774479147044e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2596, 'learning_rate': 3.0339449195644804e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.029, 'learning_rate': 3.0377123912142565e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.442, 'learning_rate': 3.041479862864032e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.6473, 'learning_rate': 3.045247334513808e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4973, 'learning_rate': 3.0490148061635836e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1861, 'learning_rate': 3.0527822778133596e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1545, 'learning_rate': 3.0565497494631357e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9166, 'learning_rate': 3.060317221112911e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3025, 'learning_rate': 3.064084692762687e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3338, 'learning_rate': 3.067852164412463e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.6018, 'learning_rate': 3.071619636062239e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3669, 'learning_rate': 3.0753871077120145e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2102, 'learning_rate': 3.0791545793617906e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2539, 'learning_rate': 3.0829220510115666e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0084, 'learning_rate': 3.086689522661343e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9816, 'learning_rate': 3.090456994311118e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2758, 'learning_rate': 3.094224465960894e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2486, 'learning_rate': 3.0979919376106694e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4383, 'learning_rate': 3.1017594092604455e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1633, 'learning_rate': 3.105526880910221e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4099, 'learning_rate': 3.109294352559997e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0283, 'learning_rate': 3.113061824209773e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6368, 'learning_rate': 3.116829295859549e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1126, 'learning_rate': 3.1205967675093244e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9555, 'learning_rate': 3.1243642391591004e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 8396/1061708 [1:15:10<156:27:17,  1.87it/s][2024-02-29 19:22:10,829] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.3592, 'learning_rate': 3.127754963643899e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1222, 'learning_rate': 3.131522435293674e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4479, 'learning_rate': 3.13528990694345e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1769, 'learning_rate': 3.139057378593226e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8049, 'learning_rate': 3.142824850243002e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6229, 'learning_rate': 3.146592321892778e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2928, 'learning_rate': 3.150359793542554e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0057, 'learning_rate': 3.15412726519233e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8077, 'learning_rate': 3.157894736842105e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 8480/1061708 [1:15:55<155:53:27,  1.88it/s][2024-02-29 19:22:55,608] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.148, 'learning_rate': 3.1612854613269036e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1313, 'learning_rate': 3.165052932976679e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0977, 'learning_rate': 3.168820404626455e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8975, 'learning_rate': 3.172587876276231e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3076, 'learning_rate': 3.176355347926007e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1205, 'learning_rate': 3.1801228195757825e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8438, 'learning_rate': 3.1838902912255585e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3928, 'learning_rate': 3.1876577628753346e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1309, 'learning_rate': 3.1914252345251106e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1992, 'learning_rate': 3.195192706174886e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 8586/1061708 [1:16:51<156:19:33,  1.87it/s][2024-02-29 19:23:52,107] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.2127, 'learning_rate': 3.1985834306596844e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7411, 'learning_rate': 3.20235090230946e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1851, 'learning_rate': 3.206118373959236e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8826, 'learning_rate': 3.209885845609012e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 8620/1061708 [1:17:09<155:09:19,  1.89it/s][2024-02-29 19:24:10,180] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.2074, 'learning_rate': 3.21327657009381e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1918, 'learning_rate': 3.217044041743586e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1311, 'learning_rate': 3.220811513393362e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7434, 'learning_rate': 3.224578985043138e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3813, 'learning_rate': 3.228346456692913e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9356, 'learning_rate': 3.232113928342689e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0497, 'learning_rate': 3.235881399992465e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.267, 'learning_rate': 3.239648871642241e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3324, 'learning_rate': 3.2434163432920166e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4414, 'learning_rate': 3.247183814941793e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 8725/1061708 [1:18:05<156:35:17,  1.87it/s][2024-02-29 19:25:06,155] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.0731, 'learning_rate': 3.2505745394265904e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 8735/1061708 [1:18:10<156:33:14,  1.87it/s][2024-02-29 19:25:11,431] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.6166, 'learning_rate': 3.2539652639113896e-05, 'epoch': 0.01}        \n",
      "  1%|▏                             | 8740/1061708 [1:18:13<155:38:11,  1.88it/s][2024-02-29 19:25:14,024] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.2832, 'learning_rate': 3.257355988396187e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5248, 'learning_rate': 3.2611234600459634e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0023, 'learning_rate': 3.2648909316957394e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1668, 'learning_rate': 3.268658403345515e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2656, 'learning_rate': 3.272425874995291e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9434, 'learning_rate': 3.276193346645067e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1734, 'learning_rate': 3.279960818294843e-05, 'epoch': 0.01}         \n",
      "  1%|▏                             | 8812/1061708 [1:18:51<155:10:03,  1.88it/s][2024-02-29 19:25:52,363] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.0313, 'learning_rate': 3.283351542779641e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0857, 'learning_rate': 3.287119014429417e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6994, 'learning_rate': 3.290886486079192e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.97, 'learning_rate': 3.294653957728968e-05, 'epoch': 0.01}           \n",
      "{'loss': 3.1588, 'learning_rate': 3.298421429378744e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9776, 'learning_rate': 3.30218890102852e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3432, 'learning_rate': 3.3059563726782956e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9859, 'learning_rate': 3.3097238443280716e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.5182, 'learning_rate': 3.313491315977848e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0955, 'learning_rate': 3.317258787627624e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1831, 'learning_rate': 3.321026259277399e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7347, 'learning_rate': 3.324793730927175e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4883, 'learning_rate': 3.3285612025769505e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2535, 'learning_rate': 3.3323286742267265e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5488, 'learning_rate': 3.336096145876502e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9117, 'learning_rate': 3.339863617526278e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3697, 'learning_rate': 3.343631089176054e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0021, 'learning_rate': 3.34739856082583e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.264, 'learning_rate': 3.3511660324756054e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3379, 'learning_rate': 3.3549335041253815e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.21, 'learning_rate': 3.3587009757751575e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9936, 'learning_rate': 3.3624684474249336e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9707, 'learning_rate': 3.366235919074709e-05, 'epoch': 0.01}         \n",
      "  1%|▎                             | 9045/1061708 [1:20:56<156:39:37,  1.87it/s][2024-02-29 19:27:56,778] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.0511, 'learning_rate': 3.3696266435595074e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2576, 'learning_rate': 3.373394115209283e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8985, 'learning_rate': 3.377161586859059e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0732, 'learning_rate': 3.380929058508835e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8667, 'learning_rate': 3.384696530158611e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9488, 'learning_rate': 3.388464001808386e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0615, 'learning_rate': 3.392231473458162e-05, 'epoch': 0.01}         \n",
      "  1%|▎                             | 9119/1061708 [1:21:35<155:08:43,  1.88it/s][2024-02-29 19:28:36,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.0789, 'learning_rate': 3.395622197942961e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9386, 'learning_rate': 3.399389669592736e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9049, 'learning_rate': 3.403157141242512e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2184, 'learning_rate': 3.406924612892288e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2428, 'learning_rate': 3.410692084542064e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3293, 'learning_rate': 3.4144595561918396e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0919, 'learning_rate': 3.4182270278416156e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.4631, 'learning_rate': 3.421994499491392e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9894, 'learning_rate': 3.425761971141168e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8381, 'learning_rate': 3.429529442790943e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9029, 'learning_rate': 3.433296914440719e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8944, 'learning_rate': 3.437064386090495e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8066, 'learning_rate': 3.4408318577402705e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.722, 'learning_rate': 3.4445993293900466e-05, 'epoch': 0.01}         \n",
      "  1%|▎                             | 9258/1061708 [1:22:49<155:21:32,  1.88it/s][2024-02-29 19:29:50,366] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.9338, 'learning_rate': 3.447990053874845e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3662, 'learning_rate': 3.4517575255246204e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0428, 'learning_rate': 3.4555249971743964e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2778, 'learning_rate': 3.4592924688241725e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9886, 'learning_rate': 3.4630599404739485e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.973, 'learning_rate': 3.466827412123724e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.2818, 'learning_rate': 3.4705948837735e-05, 'epoch': 0.01}           \n",
      "{'loss': 3.06, 'learning_rate': 3.474362355423276e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.9375, 'learning_rate': 3.4781298270730513e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9592, 'learning_rate': 3.4818972987228274e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8125, 'learning_rate': 3.485664770372603e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0493, 'learning_rate': 3.489432242022379e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8777, 'learning_rate': 3.493199713672155e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8642, 'learning_rate': 3.49696718532193e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.3844, 'learning_rate': 3.500734656971706e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0297, 'learning_rate': 3.504502128621482e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.124, 'learning_rate': 3.5082696002712583e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.373, 'learning_rate': 3.512037071921034e-05, 'epoch': 0.01}          \n",
      "  1%|▎                             | 9436/1061708 [1:24:24<155:56:52,  1.87it/s][2024-02-29 19:31:25,376] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.0913, 'learning_rate': 3.515427796405832e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0771, 'learning_rate': 3.5191952680556075e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8884, 'learning_rate': 3.5229627397053836e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0861, 'learning_rate': 3.5267302113551596e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8739, 'learning_rate': 3.5304976830049357e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7222, 'learning_rate': 3.534265154654711e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8111, 'learning_rate': 3.538032626304487e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1242, 'learning_rate': 3.541800097954263e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6063, 'learning_rate': 3.545567569604039e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8069, 'learning_rate': 3.5493350412538145e-05, 'epoch': 0.01}        \n",
      "  1%|▎                             | 9537/1061708 [1:25:18<155:45:25,  1.88it/s][2024-02-29 19:32:19,276] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▎                             | 9538/1061708 [1:25:19<146:05:09,  2.00it/s][2024-02-29 19:32:19,700] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 3.2584, 'learning_rate': 3.5523490185736354e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9169, 'learning_rate': 3.5561164902234114e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0047, 'learning_rate': 3.5598839618731875e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0326, 'learning_rate': 3.5636514335229635e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0535, 'learning_rate': 3.567418905172739e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0513, 'learning_rate': 3.571186376822514e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9619, 'learning_rate': 3.57495384847229e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.2004, 'learning_rate': 3.578721320122066e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0266, 'learning_rate': 3.582488791771842e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.957, 'learning_rate': 3.586256263421618e-05, 'epoch': 0.01}          \n",
      "  1%|▎                             | 9639/1061708 [1:26:13<155:10:43,  1.88it/s][2024-02-29 19:33:13,555] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.865, 'learning_rate': 3.589646987906416e-05, 'epoch': 0.01}          \n",
      "  1%|▎                             | 9640/1061708 [1:26:13<145:43:10,  2.01it/s][2024-02-29 19:33:13,979] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 3.1381, 'learning_rate': 3.5930377123912146e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.977, 'learning_rate': 3.5968051840409907e-05, 'epoch': 0.01}         \n",
      "  1%|▎                             | 9660/1061708 [1:26:24<154:55:03,  1.89it/s][2024-02-29 19:33:24,561] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.9419, 'learning_rate': 3.6001959085257884e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7505, 'learning_rate': 3.6039633801755645e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1445, 'learning_rate': 3.60773085182534e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.249, 'learning_rate': 3.611498323475116e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.0159, 'learning_rate': 3.615265795124892e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1467, 'learning_rate': 3.619033266774668e-05, 'epoch': 0.01}         \n",
      "  1%|▎                             | 9725/1061708 [1:26:58<156:21:21,  1.87it/s][2024-02-29 19:33:59,184] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.3408, 'learning_rate': 3.622423991259466e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2584, 'learning_rate': 3.626191462909242e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2562, 'learning_rate': 3.629958934559017e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9289, 'learning_rate': 3.633726406208793e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1549, 'learning_rate': 3.637493877858569e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8186, 'learning_rate': 3.641261349508345e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1559, 'learning_rate': 3.6450288211581206e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1016, 'learning_rate': 3.648796292807897e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1217, 'learning_rate': 3.652563764457673e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4812, 'learning_rate': 3.656331236107449e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1332, 'learning_rate': 3.660098707757224e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0947, 'learning_rate': 3.663866179407e-05, 'epoch': 0.01}            \n",
      "{'loss': 3.2751, 'learning_rate': 3.667633651056776e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.067, 'learning_rate': 3.671401122706552e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1174, 'learning_rate': 3.6751685943563276e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7261, 'learning_rate': 3.678936066006104e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9409, 'learning_rate': 3.682703537655879e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.4219, 'learning_rate': 3.686471009305655e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0016, 'learning_rate': 3.6902384809554305e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2223, 'learning_rate': 3.6940059526052065e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.048, 'learning_rate': 3.6977734242549826e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1354, 'learning_rate': 3.7015408959047586e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8623, 'learning_rate': 3.705308367554534e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8568, 'learning_rate': 3.70907583920431e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8568, 'learning_rate': 3.712843310854086e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3572, 'learning_rate': 3.716610782503862e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0014, 'learning_rate': 3.7203782541536375e-05, 'epoch': 0.01}        \n",
      "  1%|▎                             | 9999/1061708 [1:29:24<155:13:03,  1.88it/s][2024-02-29 19:36:25,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=115, lr=[3.7241457258034135e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 19:36:25,534] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=1.8897391106749584, CurrSamplesPerSec=1.897888853493962, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 3.187, 'learning_rate': 3.7241457258034135e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10000/1061708 [1:29:25<155:13:21,  1.88it/s][INFO|trainer.py:2868] 2024-02-29 19:36:25,536 >> Saving model checkpoint to output_model/checkpoint-10000\n",
      "[INFO|trainer.py:2880] 2024-02-29 19:36:25,539 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 19:36:26,742 >> tokenizer config file saved in output_model/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 19:36:26,742 >> Special tokens file saved in output_model/checkpoint-10000/special_tokens_map.json\n",
      "[2024-02-29 19:36:26,743] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10000 is about to be saved!\n",
      "[2024-02-29 19:36:31,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-10000/global_step10000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 19:36:31,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10000/global_step10000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 19:36:45,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10000/global_step10000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 19:36:46,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 19:36:53,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 19:36:53,783] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-10000/global_step10000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 19:36:53,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 19:36:55,001 >> tokenizer config file saved in output_model/checkpoint-10000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 19:36:55,001 >> Special tokens file saved in output_model/checkpoint-10000/pt_lora_model/special_tokens_map.json\n",
      "  1%|▎                            | 10009/1061708 [1:29:59<302:09:11,  1.03s/it][2024-02-29 19:37:00,169] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.1637, 'learning_rate': 3.727536450288211e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1901, 'learning_rate': 3.731303921937987e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4295, 'learning_rate': 3.7350713935877634e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2674, 'learning_rate': 3.7388388652375394e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0436, 'learning_rate': 3.742606336887315e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4987, 'learning_rate': 3.746373808537091e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6598, 'learning_rate': 3.750141280186867e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9089, 'learning_rate': 3.753908751836643e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0113, 'learning_rate': 3.757676223486418e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10094/1061708 [1:30:44<157:18:39,  1.86it/s][2024-02-29 19:37:45,192] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.207, 'learning_rate': 3.761066947971217e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.0021, 'learning_rate': 3.764834419620992e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9557, 'learning_rate': 3.768601891270768e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.966, 'learning_rate': 3.772369362920544e-05, 'epoch': 0.01}          \n",
      "  1%|▎                            | 10131/1061708 [1:31:04<154:54:02,  1.89it/s][2024-02-29 19:38:04,859] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6459, 'learning_rate': 3.775760087405342e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9075, 'learning_rate': 3.779527559055118e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1704, 'learning_rate': 3.783295030704894e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7851, 'learning_rate': 3.78706250235467e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.406, 'learning_rate': 3.7908299740044454e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1434, 'learning_rate': 3.7945974456542215e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7182, 'learning_rate': 3.7983649173039975e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.758, 'learning_rate': 3.8021323889537736e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7356, 'learning_rate': 3.805899860603549e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0066, 'learning_rate': 3.809667332253325e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10238/1061708 [1:32:01<155:30:01,  1.88it/s][2024-02-29 19:39:01,957] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.0432, 'learning_rate': 3.813058056738123e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0264, 'learning_rate': 3.816825528387899e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10252/1061708 [1:32:08<155:06:50,  1.88it/s][2024-02-29 19:39:09,377] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.8302, 'learning_rate': 3.820216252872698e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8732, 'learning_rate': 3.823983724522473e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9068, 'learning_rate': 3.8277511961722486e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8646, 'learning_rate': 3.831518667822025e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7208, 'learning_rate': 3.835286139471801e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2819, 'learning_rate': 3.839053611121576e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8258, 'learning_rate': 3.842821082771352e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1617, 'learning_rate': 3.846588554421128e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2977, 'learning_rate': 3.8503560260709036e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 10348/1061708 [1:33:00<155:06:27,  1.88it/s][2024-02-29 19:40:00,627] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.7021, 'learning_rate': 3.853746750555702e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1105, 'learning_rate': 3.857514222205478e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0398, 'learning_rate': 3.8612816938552534e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7506, 'learning_rate': 3.8650491655050295e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2141, 'learning_rate': 3.8688166371548055e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9635, 'learning_rate': 3.8725841088045815e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9454, 'learning_rate': 3.876351580454357e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9666, 'learning_rate': 3.880119052104133e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9441, 'learning_rate': 3.883886523753909e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9816, 'learning_rate': 3.887653995403685e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4912, 'learning_rate': 3.8914214670534604e-05, 'epoch': 0.01}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6074, 'learning_rate': 3.8951889387032365e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0583, 'learning_rate': 3.8989564103530125e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 10473/1061708 [1:34:06<158:20:12,  1.84it/s][2024-02-29 19:41:07,144] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6343, 'learning_rate': 3.90234713483781e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.7307, 'learning_rate': 3.906114606487586e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7221, 'learning_rate': 3.9098820781373624e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9888, 'learning_rate': 3.913649549787138e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9087, 'learning_rate': 3.917417021436914e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8329, 'learning_rate': 3.92118449308669e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.875, 'learning_rate': 3.924951964736466e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.54, 'learning_rate': 3.928719436386241e-05, 'epoch': 0.01}           \n",
      "{'loss': 3.077, 'learning_rate': 3.932486908036017e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9961, 'learning_rate': 3.936254379685793e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9406, 'learning_rate': 3.9400218513355694e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2689, 'learning_rate': 3.943789322985345e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2258, 'learning_rate': 3.947556794635121e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.922, 'learning_rate': 3.951324266284897e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9453, 'learning_rate': 3.955091737934672e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9461, 'learning_rate': 3.958859209584448e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1117, 'learning_rate': 3.9626266812342236e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6729, 'learning_rate': 3.9663941528839996e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9139, 'learning_rate': 3.970161624533776e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.151, 'learning_rate': 3.973929096183551e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9027, 'learning_rate': 3.977696567833327e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9933, 'learning_rate': 3.981464039483103e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0318, 'learning_rate': 3.985231511132879e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9418, 'learning_rate': 3.9889989827826545e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2439, 'learning_rate': 3.9927664544324306e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1232, 'learning_rate': 3.9965339260822066e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0467, 'learning_rate': 4.000301397731982e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10745/1061708 [1:36:31<156:25:21,  1.87it/s][2024-02-29 19:43:32,295] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.9535, 'learning_rate': 4.0036921222167804e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7652, 'learning_rate': 4.0074595938665565e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 10767/1061708 [1:36:43<155:40:50,  1.88it/s][2024-02-29 19:43:43,981] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.1887, 'learning_rate': 4.010850318351355e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0186, 'learning_rate': 4.01461779000113e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8439, 'learning_rate': 4.0183852616509063e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9491, 'learning_rate': 4.022152733300682e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6014, 'learning_rate': 4.025920204950458e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4802, 'learning_rate': 4.029687676600234e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0682, 'learning_rate': 4.033455148250009e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9158, 'learning_rate': 4.037222619899785e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8882, 'learning_rate': 4.040990091549561e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10852/1061708 [1:37:28<154:07:49,  1.89it/s][2024-02-29 19:44:29,192] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 3.0385, 'learning_rate': 4.04438081603436e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1007, 'learning_rate': 4.048148287684135e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6986, 'learning_rate': 4.051915759333911e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9235, 'learning_rate': 4.055683230983687e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9332, 'learning_rate': 4.0594507026334625e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5153, 'learning_rate': 4.0632181742832386e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9471, 'learning_rate': 4.0669856459330146e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7594, 'learning_rate': 4.0707531175827907e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9051, 'learning_rate': 4.074520589232566e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7538, 'learning_rate': 4.078288060882342e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7212, 'learning_rate': 4.082055532532118e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6174, 'learning_rate': 4.0858230041818935e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8097, 'learning_rate': 4.0895904758316695e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.024, 'learning_rate': 4.0933579474814456e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 10996/1061708 [1:38:45<155:18:27,  1.88it/s][2024-02-29 19:45:45,836] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7997, 'learning_rate': 4.096748671966243e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.04, 'learning_rate': 4.1005161436160194e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.0629, 'learning_rate': 4.1042836152657954e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7943, 'learning_rate': 4.1080510869155715e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0703, 'learning_rate': 4.111818558565347e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6357, 'learning_rate': 4.115586030215123e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0709, 'learning_rate': 4.119353501864899e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8662, 'learning_rate': 4.123120973514675e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8658, 'learning_rate': 4.12688844516445e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1068, 'learning_rate': 4.1306559168142264e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.766, 'learning_rate': 4.1344233884640024e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.881, 'learning_rate': 4.138190860113778e-05, 'epoch': 0.01}          \n",
      "  1%|▎                            | 11112/1061708 [1:39:47<154:34:25,  1.89it/s][2024-02-29 19:46:47,584] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.9063, 'learning_rate': 4.141581584598576e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1848, 'learning_rate': 4.145349056248352e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8589, 'learning_rate': 4.1491165278981276e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7215, 'learning_rate': 4.152883999547904e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9986, 'learning_rate': 4.15665147119768e-05, 'epoch': 0.01}          \n",
      "  1%|▎                            | 11166/1061708 [1:40:15<155:45:22,  1.87it/s][2024-02-29 19:47:16,320] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.0254, 'learning_rate': 4.1600421956824775e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0401, 'learning_rate': 4.1638096673322535e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1064, 'learning_rate': 4.1675771389820296e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0495, 'learning_rate': 4.171344610631805e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 11206/1061708 [1:40:37<155:39:14,  1.87it/s][2024-02-29 19:47:37,625] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6107, 'learning_rate': 4.1747353351166034e-05, 'epoch': 0.01}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5723, 'learning_rate': 4.1785028067663794e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8194, 'learning_rate': 4.182270278416155e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0578, 'learning_rate': 4.186037750065931e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0405, 'learning_rate': 4.189805221715707e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2488, 'learning_rate': 4.193572693365483e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5403, 'learning_rate': 4.197340165015258e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9201, 'learning_rate': 4.2011076366650343e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9628, 'learning_rate': 4.2048751083148104e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0023, 'learning_rate': 4.2086425799645864e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0381, 'learning_rate': 4.212410051614362e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 11311/1061708 [1:41:33<154:43:52,  1.89it/s][2024-02-29 19:48:33,614] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5504, 'learning_rate': 4.21580077609916e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5359, 'learning_rate': 4.2195682477489356e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3197, 'learning_rate': 4.2233357193987117e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8531, 'learning_rate': 4.227103191048488e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8028, 'learning_rate': 4.230870662698264e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8286, 'learning_rate': 4.234638134348039e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2885, 'learning_rate': 4.238405605997815e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7609, 'learning_rate': 4.242173077647591e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8775, 'learning_rate': 4.245940549297367e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0061, 'learning_rate': 4.2497080209471426e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9641, 'learning_rate': 4.2534754925969187e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7092, 'learning_rate': 4.257242964246694e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9248, 'learning_rate': 4.26101043589647e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1264, 'learning_rate': 4.2647779075462454e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 11451/1061708 [1:42:47<154:49:59,  1.88it/s][2024-02-29 19:49:48,326] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3027, 'learning_rate': 4.2681686320310446e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8983, 'learning_rate': 4.27193610368082e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8789, 'learning_rate': 4.275703575330596e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 11487/1061708 [1:43:06<155:15:32,  1.88it/s][2024-02-29 19:50:07,471] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.8641, 'learning_rate': 4.2790942998153944e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9749, 'learning_rate': 4.28286177146517e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.1838, 'learning_rate': 4.286629243114946e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0531, 'learning_rate': 4.290396714764722e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8784, 'learning_rate': 4.294164186414498e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9009, 'learning_rate': 4.297931658064273e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9164, 'learning_rate': 4.301699129714049e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7532, 'learning_rate': 4.3054666013638254e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8486, 'learning_rate': 4.309234073013601e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 11576/1061708 [1:43:54<157:18:14,  1.85it/s][2024-02-29 19:50:54,933] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.7718, 'learning_rate': 4.312624797498399e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0416, 'learning_rate': 4.316392269148175e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7419, 'learning_rate': 4.3201597407979506e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9683, 'learning_rate': 4.3239272124477266e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7911, 'learning_rate': 4.327694684097503e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0221, 'learning_rate': 4.331462155747279e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8062, 'learning_rate': 4.335229627397054e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7746, 'learning_rate': 4.33899709904683e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.4889, 'learning_rate': 4.3427645706966055e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0428, 'learning_rate': 4.3465320423463815e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 11670/1061708 [1:44:44<154:31:16,  1.89it/s][2024-02-29 19:51:45,046] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 3.0074, 'learning_rate': 4.34992276683118e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8841, 'learning_rate': 4.353690238480956e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1537, 'learning_rate': 4.3574577101307314e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0109, 'learning_rate': 4.3612251817805074e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9479, 'learning_rate': 4.3649926534302835e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6662, 'learning_rate': 4.368760125080059e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.684, 'learning_rate': 4.372527596729835e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.7915, 'learning_rate': 4.37629506837961e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.944, 'learning_rate': 4.380062540029386e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.7781, 'learning_rate': 4.3838300116791623e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0342, 'learning_rate': 4.387597483328938e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7672, 'learning_rate': 4.391364954978714e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0485, 'learning_rate': 4.39513242662849e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8067, 'learning_rate': 4.398899898278266e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0922, 'learning_rate': 4.402667369928041e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7674, 'learning_rate': 4.406434841577817e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9201, 'learning_rate': 4.410202313227593e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0108, 'learning_rate': 4.4139697848773694e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6623, 'learning_rate': 4.417737256527145e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8811, 'learning_rate': 4.421504728176921e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2809, 'learning_rate': 4.425272199826697e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7931, 'learning_rate': 4.429039671476473e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9529, 'learning_rate': 4.432807143126248e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7858, 'learning_rate': 4.436574614776024e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7878, 'learning_rate': 4.4403420864257996e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8713, 'learning_rate': 4.444109558075576e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8641, 'learning_rate': 4.447877029725351e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2355, 'learning_rate': 4.451644501375127e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.76, 'learning_rate': 4.455411973024903e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.7298, 'learning_rate': 4.459179444674679e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.03, 'learning_rate': 4.4629469163244545e-05, 'epoch': 0.01}          \n",
      "  1%|▎                            | 11985/1061708 [1:47:32<156:29:17,  1.86it/s][2024-02-29 19:54:33,156] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▎                            | 11987/1061708 [1:47:33<149:18:54,  1.95it/s][2024-02-29 19:54:34,115] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6762, 'learning_rate': 4.4659608936442754e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 11999/1061708 [1:47:39<154:26:22,  1.89it/s][2024-02-29 19:54:40,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=136, lr=[4.4697283652940514e-05], mom=[(0.9, 0.999)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-29 19:54:40,521] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=1.8901947777255779, CurrSamplesPerSec=1.8995615111696447, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.7808, 'learning_rate': 4.4697283652940514e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8712, 'learning_rate': 4.4734958369438275e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9179, 'learning_rate': 4.4772633085936035e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.3484, 'learning_rate': 4.481030780243379e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3032, 'learning_rate': 4.484798251893155e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6695, 'learning_rate': 4.488565723542931e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7413, 'learning_rate': 4.492333195192706e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6437, 'learning_rate': 4.4961006668424824e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8678, 'learning_rate': 4.499868138492258e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2957, 'learning_rate': 4.503635610142034e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7323, 'learning_rate': 4.507403081791809e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12109/1061708 [1:48:38<154:53:52,  1.88it/s][2024-02-29 19:55:39,183] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 3.1388, 'learning_rate': 4.510793806276608e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9913, 'learning_rate': 4.514561277926384e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2008, 'learning_rate': 4.51832874957616e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9562, 'learning_rate': 4.522096221225936e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7621, 'learning_rate': 4.525863692875711e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5924, 'learning_rate': 4.529631164525487e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9537, 'learning_rate': 4.5333986361752625e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7953, 'learning_rate': 4.5371661078250386e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0399, 'learning_rate': 4.5409335794748146e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 12193/1061708 [1:49:23<158:21:59,  1.84it/s][2024-02-29 19:56:23,957] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.8639, 'learning_rate': 4.544324303959613e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9091, 'learning_rate': 4.548091775609389e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5925, 'learning_rate': 4.5518592472591645e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4867, 'learning_rate': 4.5556267189089405e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4884, 'learning_rate': 4.559394190558716e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0149, 'learning_rate': 4.563161662208492e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5037, 'learning_rate': 4.566929133858268e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0457, 'learning_rate': 4.570696605508043e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6824, 'learning_rate': 4.5744640771578194e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8304, 'learning_rate': 4.5782315488075954e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7948, 'learning_rate': 4.5819990204573715e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 12300/1061708 [1:50:20<154:31:38,  1.89it/s][2024-02-29 19:57:21,011] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▎                            | 12302/1061708 [1:50:21<148:21:11,  1.96it/s][2024-02-29 19:57:21,971] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 3.1012, 'learning_rate': 4.585012997777192e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8366, 'learning_rate': 4.5887804694269677e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6784, 'learning_rate': 4.592547941076744e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0264, 'learning_rate': 4.59631541272652e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.0616, 'learning_rate': 4.600082884376296e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9601, 'learning_rate': 4.603850356026071e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12363/1061708 [1:50:53<158:32:40,  1.84it/s][2024-02-29 19:57:54,492] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.719, 'learning_rate': 4.6072410805108696e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0398, 'learning_rate': 4.611008552160645e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9154, 'learning_rate': 4.614776023810421e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7043, 'learning_rate': 4.618543495460197e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3343, 'learning_rate': 4.622310967109973e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5727, 'learning_rate': 4.6260784387597485e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.2238, 'learning_rate': 4.6298459104095245e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7, 'learning_rate': 4.6336133820593006e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.99, 'learning_rate': 4.6373808537090766e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9415, 'learning_rate': 4.641148325358852e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7638, 'learning_rate': 4.644915797008627e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7004, 'learning_rate': 4.6486832686584034e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.092, 'learning_rate': 4.6524507403081794e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.709, 'learning_rate': 4.656218211957955e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9494, 'learning_rate': 4.659985683607731e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.828, 'learning_rate': 4.663753155257507e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8156, 'learning_rate': 4.667520626907283e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7267, 'learning_rate': 4.671288098557058e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1018, 'learning_rate': 4.6750555702068343e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7951, 'learning_rate': 4.6788230418566104e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8146, 'learning_rate': 4.6825905135063864e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8883, 'learning_rate': 4.686357985156162e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9445, 'learning_rate': 4.690125456805938e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12591/1061708 [1:52:55<154:09:05,  1.89it/s][2024-02-29 19:59:56,085] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7681, 'learning_rate': 4.6935161812907356e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6581, 'learning_rate': 4.6972836529405116e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8693, 'learning_rate': 4.701051124590288e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6257, 'learning_rate': 4.704818596240064e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12633/1061708 [1:53:17<157:19:29,  1.85it/s][2024-02-29 20:00:18,368] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3977, 'learning_rate': 4.7082093207248615e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8836, 'learning_rate': 4.7119767923746375e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.716, 'learning_rate': 4.7157442640244136e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0322, 'learning_rate': 4.719511735674189e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6733, 'learning_rate': 4.723279207323965e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6466, 'learning_rate': 4.727046678973741e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0825, 'learning_rate': 4.7308141506235164e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0105, 'learning_rate': 4.7345816222732925e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7919, 'learning_rate': 4.7383490939230685e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.751, 'learning_rate': 4.7421165655728445e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7235, 'learning_rate': 4.74588403722262e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.0664, 'learning_rate': 4.749651508872396e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4973, 'learning_rate': 4.753418980522172e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5774, 'learning_rate': 4.757186452171948e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12778/1061708 [1:54:34<154:28:38,  1.89it/s][2024-02-29 20:01:35,477] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.515, 'learning_rate': 4.760577176656746e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9586, 'learning_rate': 4.764344648306522e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6084, 'learning_rate': 4.768112119956298e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12804/1061708 [1:54:48<156:41:46,  1.86it/s][2024-02-29 20:01:49,240] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5981, 'learning_rate': 4.771502844441096e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0074, 'learning_rate': 4.775270316090872e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6906, 'learning_rate': 4.779037787740647e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.326, 'learning_rate': 4.782805259390423e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9474, 'learning_rate': 4.786572731040199e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6004, 'learning_rate': 4.790340202689975e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3758, 'learning_rate': 4.7941076743397506e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8221, 'learning_rate': 4.7978751459895266e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1146, 'learning_rate': 4.801642617639303e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7826, 'learning_rate': 4.805410089289079e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 12901/1061708 [1:55:40<154:10:35,  1.89it/s][2024-02-29 20:02:40,761] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4928, 'learning_rate': 4.8088008137738765e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0447, 'learning_rate': 4.8125682854236525e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6424, 'learning_rate': 4.816335757073428e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6907, 'learning_rate': 4.820103228723204e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.847, 'learning_rate': 4.82387070037298e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.7603, 'learning_rate': 4.827638172022756e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.3191, 'learning_rate': 4.8314056436725314e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7689, 'learning_rate': 4.8351731153223074e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7877, 'learning_rate': 4.8389405869720835e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6684, 'learning_rate': 4.8427080586218595e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2085, 'learning_rate': 4.846475530271635e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7505, 'learning_rate': 4.850243001921411e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6579, 'learning_rate': 4.854010473571187e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6349, 'learning_rate': 4.8577779452209623e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7338, 'learning_rate': 4.8615454168707384e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8849, 'learning_rate': 4.865312888520514e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0592, 'learning_rate': 4.86908036017029e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.875, 'learning_rate': 4.872847831820066e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5652, 'learning_rate': 4.876615303469841e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7106, 'learning_rate': 4.880382775119617e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2134, 'learning_rate': 4.884150246769393e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9104, 'learning_rate': 4.8879177184191693e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6674, 'learning_rate': 4.891685190068945e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7708, 'learning_rate': 4.895452661718721e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13140/1061708 [1:57:47<154:01:40,  1.89it/s][2024-02-29 20:04:47,872] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7355, 'learning_rate': 4.8988433862035185e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8619, 'learning_rate': 4.9026108578532946e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7077, 'learning_rate': 4.9063783295030706e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0732, 'learning_rate': 4.9101458011528467e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5075, 'learning_rate': 4.913913272802622e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9504, 'learning_rate': 4.917680744452398e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9967, 'learning_rate': 4.921448216102174e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9119, 'learning_rate': 4.92521568775195e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.2406, 'learning_rate': 4.9289831594017255e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0725, 'learning_rate': 4.9327506310515016e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 13241/1061708 [1:58:40<154:07:48,  1.89it/s][2024-02-29 20:05:41,501] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▎                            | 13242/1061708 [1:58:41<144:43:19,  2.01it/s][2024-02-29 20:05:41,922] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 3.0928, 'learning_rate': 4.9357646083713224e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6676, 'learning_rate': 4.9395320800210984e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6502, 'learning_rate': 4.943299551670874e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13273/1061708 [1:58:57<156:59:57,  1.85it/s][2024-02-29 20:05:58,352] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.8669, 'learning_rate': 4.946690276155672e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3493, 'learning_rate': 4.950457747805448e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.462, 'learning_rate': 4.9542252194552243e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5814, 'learning_rate': 4.957992691105e-05, 'epoch': 0.01}            \n",
      "{'loss': 2.8883, 'learning_rate': 4.961760162754776e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9148, 'learning_rate': 4.965527634404552e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.685, 'learning_rate': 4.969295106054327e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9191, 'learning_rate': 4.973062577704103e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0168, 'learning_rate': 4.9768300493538786e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8128, 'learning_rate': 4.9805975210036546e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6249, 'learning_rate': 4.98436499265343e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8758, 'learning_rate': 4.988132464303206e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7338, 'learning_rate': 4.991899935952982e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13407/1061708 [2:00:09<154:29:58,  1.88it/s][2024-02-29 20:07:09,673] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7338, 'learning_rate': 4.9952906604377805e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.677, 'learning_rate': 4.9990581320875566e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6578, 'learning_rate': 5.002825603737332e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13432/1061708 [2:00:22<154:17:53,  1.89it/s][2024-02-29 20:07:22,902] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7634, 'learning_rate': 5.006216328222131e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5197, 'learning_rate': 5.009983799871906e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7076, 'learning_rate': 5.013751271521682e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8443, 'learning_rate': 5.017518743171458e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6363, 'learning_rate': 5.021286214821234e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5393, 'learning_rate': 5.02505368647101e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9126, 'learning_rate': 5.028821158120785e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4312, 'learning_rate': 5.032588629770561e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7081, 'learning_rate': 5.0363561014203374e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0742, 'learning_rate': 5.040123573070112e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.231, 'learning_rate': 5.043891044719888e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8548, 'learning_rate': 5.047658516369664e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13551/1061708 [2:01:25<154:19:30,  1.89it/s][2024-02-29 20:08:26,223] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4698, 'learning_rate': 5.051049240854463e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8561, 'learning_rate': 5.0548167125042386e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7977, 'learning_rate': 5.058584184154015e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7668, 'learning_rate': 5.06235165580379e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.584, 'learning_rate': 5.066119127453566e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8405, 'learning_rate': 5.0698865991033415e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8809, 'learning_rate': 5.0736540707531175e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4934, 'learning_rate': 5.0774215424028936e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.2256, 'learning_rate': 5.0811890140526696e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6891, 'learning_rate': 5.0849564857024456e-05, 'epoch': 0.01}        \n",
      "  1%|▎                            | 13652/1061708 [2:02:19<154:31:11,  1.88it/s][2024-02-29 20:09:20,049] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▎                            | 13653/1061708 [2:02:19<148:22:01,  1.96it/s][2024-02-29 20:09:20,474] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2818, 'learning_rate': 5.087970463022266e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6212, 'learning_rate': 5.091737934672042e-05, 'epoch': 0.01}         \n",
      "  1%|▎                            | 13671/1061708 [2:02:29<154:22:27,  1.89it/s][2024-02-29 20:09:29,983] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  1%|▎                            | 13676/1061708 [2:02:32<154:45:13,  1.88it/s][2024-02-29 20:09:32,574] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5339, 'learning_rate': 5.094751911991863e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.2652, 'learning_rate': 5.098519383641638e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3044, 'learning_rate': 5.102286855291414e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7384, 'learning_rate': 5.1060543269411895e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9271, 'learning_rate': 5.1098217985909655e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5452, 'learning_rate': 5.1135892702407415e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5938, 'learning_rate': 5.1173567418905176e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7763, 'learning_rate': 5.1211242135402936e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.3674, 'learning_rate': 5.12489168519007e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.95, 'learning_rate': 5.1286591568398444e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9457, 'learning_rate': 5.1324266284896204e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.716, 'learning_rate': 5.1361941001393965e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8152, 'learning_rate': 5.1399615717891725e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 13807/1061708 [2:03:41<154:31:24,  1.88it/s][2024-02-29 20:10:42,402] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.8951, 'learning_rate': 5.143352296273971e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.712, 'learning_rate': 5.147119767923747e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.675, 'learning_rate': 5.150887239573522e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8813, 'learning_rate': 5.154654711223298e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0631, 'learning_rate': 5.158422182873074e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.877, 'learning_rate': 5.16218965452285e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.4821, 'learning_rate': 5.165957126172626e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7133, 'learning_rate': 5.169724597822402e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8466, 'learning_rate': 5.173492069472178e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4986, 'learning_rate': 5.177259541121954e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6646, 'learning_rate': 5.181027012771729e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7767, 'learning_rate': 5.184794484421505e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.1427, 'learning_rate': 5.188561956071281e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3127, 'learning_rate': 5.192329427721057e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 13941/1061708 [2:04:53<154:04:10,  1.89it/s][2024-02-29 20:11:53,666] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6595, 'learning_rate': 5.195720152205855e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8766, 'learning_rate': 5.199487623855631e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5608, 'learning_rate': 5.203255095505406e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 13971/1061708 [2:05:09<153:52:21,  1.89it/s][2024-02-29 20:12:09,534] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.9921, 'learning_rate': 5.2066458199902044e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6021, 'learning_rate': 5.2104132916399805e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 13999/1061708 [2:05:23<154:00:09,  1.89it/s][2024-02-29 20:12:24,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=160, lr=[5.2141807632897565e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 20:12:24,439] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=1.8909330272675209, CurrSamplesPerSec=1.9019544180731525, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.5054, 'learning_rate': 5.2141807632897565e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7163, 'learning_rate': 5.2179482349395326e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0664, 'learning_rate': 5.2217157065893086e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0782, 'learning_rate': 5.2254831782390847e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.176, 'learning_rate': 5.2292506498888593e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8211, 'learning_rate': 5.2330181215386354e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.874, 'learning_rate': 5.2367855931884114e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5048, 'learning_rate': 5.2405530648381875e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.358, 'learning_rate': 5.2443205364879635e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7257, 'learning_rate': 5.2480880081377396e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0133, 'learning_rate': 5.251855479787515e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9643, 'learning_rate': 5.255622951437291e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8914, 'learning_rate': 5.2593904230870663e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8635, 'learning_rate': 5.2631578947368424e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0611, 'learning_rate': 5.266925366386618e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4004, 'learning_rate': 5.270692838036394e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8392, 'learning_rate': 5.27446030968617e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.6801, 'learning_rate': 5.278227781335946e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 14172/1061708 [2:06:55<154:25:55,  1.88it/s][2024-02-29 20:13:56,524] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|▍                            | 14173/1061708 [2:06:56<148:02:23,  1.97it/s][2024-02-29 20:13:56,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  1%|▍                            | 14177/1061708 [2:06:58<153:09:07,  1.90it/s][2024-02-29 20:13:59,007] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7057, 'learning_rate': 5.2808650114907884e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8729, 'learning_rate': 5.2846324831405645e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5994, 'learning_rate': 5.2883999547903405e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.783, 'learning_rate': 5.2921674264401166e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7764, 'learning_rate': 5.2959348980898926e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8384, 'learning_rate': 5.299702369739667e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5188, 'learning_rate': 5.3034698413894434e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9197, 'learning_rate': 5.3072373130392194e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5935, 'learning_rate': 5.3110047846889955e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9654, 'learning_rate': 5.3147722563387715e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7493, 'learning_rate': 5.3185397279885475e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9766, 'learning_rate': 5.3223071996383236e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8245, 'learning_rate': 5.3260746712880996e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5302, 'learning_rate': 5.329842142937874e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 14317/1061708 [2:08:12<154:38:53,  1.88it/s][2024-02-29 20:15:13,516] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.8078, 'learning_rate': 5.333232867422673e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6119, 'learning_rate': 5.337000339072449e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.949, 'learning_rate': 5.340767810722225e-05, 'epoch': 0.01}          \n",
      "  1%|▍                            | 14347/1061708 [2:08:28<154:37:38,  1.88it/s][2024-02-29 20:15:29,409] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.8419, 'learning_rate': 5.344158535207023e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3156, 'learning_rate': 5.347926006856798e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8445, 'learning_rate': 5.351693478506574e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9636, 'learning_rate': 5.35546095015635e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.8518, 'learning_rate': 5.359228421806126e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6085, 'learning_rate': 5.362995893455902e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0107, 'learning_rate': 5.366763365105678e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6182, 'learning_rate': 5.370530836755454e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0031, 'learning_rate': 5.374298308405229e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6866, 'learning_rate': 5.378065780055005e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6975, 'learning_rate': 5.381833251704781e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4816, 'learning_rate': 5.385600723354557e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.287, 'learning_rate': 5.389368195004333e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.2341, 'learning_rate': 5.393135666654109e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 14487/1061708 [2:09:43<154:37:35,  1.88it/s][2024-02-29 20:16:43,880] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2973, 'learning_rate': 5.3965263911389076e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.1414, 'learning_rate': 5.400293862788682e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7092, 'learning_rate': 5.404061334438458e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.1516, 'learning_rate': 5.4078288060882344e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4323, 'learning_rate': 5.4115962777380104e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 14534/1061708 [2:10:08<156:24:01,  1.86it/s][2024-02-29 20:17:08,870] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7323, 'learning_rate': 5.414987002222809e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5641, 'learning_rate': 5.418754473872585e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8369, 'learning_rate': 5.4225219455223596e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4171, 'learning_rate': 5.4262894171721356e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6807, 'learning_rate': 5.430056888821912e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5168, 'learning_rate': 5.433824360471688e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5437, 'learning_rate': 5.437591832121464e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6119, 'learning_rate': 5.44135930377124e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5506, 'learning_rate': 5.445126775421016e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5397, 'learning_rate': 5.448894247070791e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8482, 'learning_rate': 5.4526617187205666e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6656, 'learning_rate': 5.4564291903703426e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 14659/1061708 [2:11:14<154:06:58,  1.89it/s][2024-02-29 20:18:15,372] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.183, 'learning_rate': 5.459819914855141e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5087, 'learning_rate': 5.463587386504917e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6145, 'learning_rate': 5.467354858154693e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.834, 'learning_rate': 5.471122329804469e-05, 'epoch': 0.01}          \n",
      "  1%|▍                            | 14690/1061708 [2:11:31<154:02:59,  1.89it/s][2024-02-29 20:18:31,792] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.784, 'learning_rate': 5.474513054289266e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5146, 'learning_rate': 5.4782805259390423e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.3988, 'learning_rate': 5.4820479975888184e-05, 'epoch': 0.01}        \n",
      "{'loss': 3.0223, 'learning_rate': 5.4858154692385944e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8026, 'learning_rate': 5.4895829408883705e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7861, 'learning_rate': 5.4933504125381465e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 14750/1061708 [2:12:03<154:02:15,  1.89it/s][2024-02-29 20:19:03,664] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6643, 'learning_rate': 5.4967411370229436e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.693, 'learning_rate': 5.5005086086727197e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5663, 'learning_rate': 5.504276080322496e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5364, 'learning_rate': 5.508043551972272e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6428, 'learning_rate': 5.511811023622048e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5935, 'learning_rate': 5.515578495271824e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9752, 'learning_rate': 5.5193459669216e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.7914, 'learning_rate': 5.5231134385713746e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7161, 'learning_rate': 5.5268809102211506e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6449, 'learning_rate': 5.5306483818709267e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9348, 'learning_rate': 5.534415853520703e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2754, 'learning_rate': 5.538183325170479e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3981, 'learning_rate': 5.541950796820254e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2269, 'learning_rate': 5.54571826847003e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.888, 'learning_rate': 5.549485740119806e-05, 'epoch': 0.01}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.471, 'learning_rate': 5.5532532117695816e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8984, 'learning_rate': 5.557020683419357e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.76, 'learning_rate': 5.560788155069133e-05, 'epoch': 0.01}           \n",
      "{'loss': 2.7592, 'learning_rate': 5.564555626718909e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9648, 'learning_rate': 5.568323098368685e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9531, 'learning_rate': 5.572090570018461e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9727, 'learning_rate': 5.575858041668237e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5026, 'learning_rate': 5.579625513318013e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5622, 'learning_rate': 5.583392984967788e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7509, 'learning_rate': 5.587160456617564e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15000/1061708 [2:14:16<153:53:20,  1.89it/s][INFO|trainer.py:2868] 2024-02-29 20:21:16,281 >> Saving model checkpoint to output_model/checkpoint-15000\n",
      "[INFO|trainer.py:2880] 2024-02-29 20:21:16,284 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 20:21:17,496 >> tokenizer config file saved in output_model/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 20:21:17,497 >> Special tokens file saved in output_model/checkpoint-15000/special_tokens_map.json\n",
      "[2024-02-29 20:21:17,498] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step15000 is about to be saved!\n",
      "[2024-02-29 20:21:22,720] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-15000/global_step15000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 20:21:22,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-15000/global_step15000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 20:21:36,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-15000/global_step15000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 20:21:37,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-15000/global_step15000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 20:21:44,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-15000/global_step15000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 20:21:44,463] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-15000/global_step15000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 20:21:44,464] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-29 20:21:44,478 >> Deleting older checkpoint [output_model/checkpoint-118000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 20:21:45,707 >> tokenizer config file saved in output_model/checkpoint-15000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 20:21:45,707 >> Special tokens file saved in output_model/checkpoint-15000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 3.1605, 'learning_rate': 5.59092792826734e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5398, 'learning_rate': 5.594695399917116e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6292, 'learning_rate': 5.598462871566892e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8063, 'learning_rate': 5.602230343216668e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7158, 'learning_rate': 5.6059978148664435e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 15051/1061708 [2:15:12<152:58:54,  1.90it/s][2024-02-29 20:22:13,119] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▍                            | 15052/1061708 [2:15:13<143:34:38,  2.02it/s][2024-02-29 20:22:13,538] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1493, 'learning_rate': 5.6090117921862636e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9891, 'learning_rate': 5.61277926383604e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.2988, 'learning_rate': 5.616546735485816e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6133, 'learning_rate': 5.620314207135592e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0379, 'learning_rate': 5.624081678785368e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5982, 'learning_rate': 5.6278491504351425e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4525, 'learning_rate': 5.6316166220849186e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5597, 'learning_rate': 5.6353840937346946e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9246, 'learning_rate': 5.6391515653844706e-05, 'epoch': 0.01}        \n",
      "  1%|▍                            | 15146/1061708 [2:16:03<155:15:00,  1.87it/s][2024-02-29 20:23:03,622] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1997, 'learning_rate': 5.642542289869269e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15155/1061708 [2:16:07<156:05:07,  1.86it/s][2024-02-29 20:23:08,379] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4924, 'learning_rate': 5.645933014354067e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9977, 'learning_rate': 5.649700486003843e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2963, 'learning_rate': 5.653467957653619e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3141, 'learning_rate': 5.657235429303395e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9811, 'learning_rate': 5.6610029009531704e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.3508, 'learning_rate': 5.6647703726029464e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6807, 'learning_rate': 5.6685378442527224e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7114, 'learning_rate': 5.6723053159024985e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7497, 'learning_rate': 5.676072787552273e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8637, 'learning_rate': 5.679840259202049e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9568, 'learning_rate': 5.683607730851825e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5625, 'learning_rate': 5.687375202501601e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5647, 'learning_rate': 5.6911426741513774e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9407, 'learning_rate': 5.6949101458011534e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7936, 'learning_rate': 5.6986776174509294e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6806, 'learning_rate': 5.7024450891007055e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5799, 'learning_rate': 5.70621256075048e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.0582, 'learning_rate': 5.709980032400256e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6478, 'learning_rate': 5.713747504050032e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9004, 'learning_rate': 5.717514975699808e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8463, 'learning_rate': 5.7212824473495844e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6577, 'learning_rate': 5.72504991899936e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.5586, 'learning_rate': 5.728817390649136e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0798, 'learning_rate': 5.732584862298912e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15396/1061708 [2:18:16<154:58:26,  1.88it/s][2024-02-29 20:25:16,944] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5708, 'learning_rate': 5.7359755867837096e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4114, 'learning_rate': 5.7397430584334856e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5062, 'learning_rate': 5.743510530083262e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5207, 'learning_rate': 5.747278001733038e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7701, 'learning_rate': 5.751045473382813e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15449/1061708 [2:18:44<153:48:52,  1.89it/s][2024-02-29 20:25:45,115] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6807, 'learning_rate': 5.754436197867611e-05, 'epoch': 0.01}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6106, 'learning_rate': 5.758203669517387e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6729, 'learning_rate': 5.761971141167163e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5167, 'learning_rate': 5.765738612816939e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.9352, 'learning_rate': 5.769506084466715e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4005, 'learning_rate': 5.773273556116491e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7516, 'learning_rate': 5.7770410277662664e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6904, 'learning_rate': 5.780808499416042e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7256, 'learning_rate': 5.784575971065818e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7324, 'learning_rate': 5.788343442715594e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4574, 'learning_rate': 5.792110914365369e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15553/1061708 [2:19:39<157:43:34,  1.84it/s][2024-02-29 20:26:40,442] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.8332, 'learning_rate': 5.7955016388501684e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.3455, 'learning_rate': 5.7992691104999444e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8205, 'learning_rate': 5.8030365821497205e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4154, 'learning_rate': 5.806804053799495e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7127, 'learning_rate': 5.810571525449271e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5779, 'learning_rate': 5.814338997099047e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6781, 'learning_rate': 5.8181064687488226e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.521, 'learning_rate': 5.8218739403985987e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7752, 'learning_rate': 5.825641412048375e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6037, 'learning_rate': 5.829408883698151e-05, 'epoch': 0.01}         \n",
      "  1%|▍                            | 15654/1061708 [2:20:33<155:55:18,  1.86it/s][2024-02-29 20:27:34,206] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  1%|▍                            | 15655/1061708 [2:20:34<145:54:26,  1.99it/s][2024-02-29 20:27:34,628] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  1%|▍                            | 15657/1061708 [2:20:35<143:18:02,  2.03it/s][2024-02-29 20:27:35,584] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.784, 'learning_rate': 5.832046113852993e-05, 'epoch': 0.01}          \n",
      "  1%|▍                            | 15665/1061708 [2:20:39<154:18:52,  1.88it/s][2024-02-29 20:27:39,770] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5798, 'learning_rate': 5.8354368383377924e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.7764, 'learning_rate': 5.8392043099875685e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.9875, 'learning_rate': 5.842971781637343e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.2169, 'learning_rate': 5.846739253287119e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6426, 'learning_rate': 5.850506724936895e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.1798, 'learning_rate': 5.854274196586671e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6039, 'learning_rate': 5.8580416682364466e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.3723, 'learning_rate': 5.861809139886223e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4063, 'learning_rate': 5.865576611535999e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.3333, 'learning_rate': 5.869344083185774e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.4469, 'learning_rate': 5.8731115548355495e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.401, 'learning_rate': 5.8768790264853255e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.8752, 'learning_rate': 5.8806464981351016e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.6345, 'learning_rate': 5.8844139697848776e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.603, 'learning_rate': 5.8881814414346537e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0166, 'learning_rate': 5.89194891308443e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.9082, 'learning_rate': 5.895716384734206e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.0527, 'learning_rate': 5.8994838563839804e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5121, 'learning_rate': 5.9032513280337565e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8258, 'learning_rate': 5.9070187996835325e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8882, 'learning_rate': 5.9107862713333086e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.8155, 'learning_rate': 5.9145537429830846e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.636, 'learning_rate': 5.9183212146328607e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.7323, 'learning_rate': 5.922088686282636e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.6273, 'learning_rate': 5.925856157932412e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5541, 'learning_rate': 5.9296236295821874e-05, 'epoch': 0.01}        \n",
      "  2%|▍                            | 15926/1061708 [2:22:58<154:25:27,  1.88it/s][2024-02-29 20:29:58,657] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7771, 'learning_rate': 5.933014354066986e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4983, 'learning_rate': 5.936781825716762e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 15947/1061708 [2:23:09<154:15:42,  1.88it/s][2024-02-29 20:30:09,775] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6582, 'learning_rate': 5.9401725502015604e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.5397, 'learning_rate': 5.9439400218513364e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.8253, 'learning_rate': 5.947707493501111e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8561, 'learning_rate': 5.951474965150887e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8929, 'learning_rate': 5.955242436800663e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 15999/1061708 [2:23:36<153:46:36,  1.89it/s][2024-02-29 20:30:37,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=183, lr=[5.959009908450439e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 20:30:37,455] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=1.8915350128890016, CurrSamplesPerSec=1.9052054124988587, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.6841, 'learning_rate': 5.959009908450439e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9172, 'learning_rate': 5.962777380100215e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.596, 'learning_rate': 5.966544851749991e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7697, 'learning_rate': 5.9703123233997674e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.6506, 'learning_rate': 5.9740797950495434e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.5417, 'learning_rate': 5.977847266699318e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5478, 'learning_rate': 5.981614738349094e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3746, 'learning_rate': 5.98538220999887e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7197, 'learning_rate': 5.9891496816486455e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.1352, 'learning_rate': 5.9929171532984216e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.5082, 'learning_rate': 5.9966846249481976e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.086, 'learning_rate': 6.000452096597974e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.329, 'learning_rate': 6.0042195682477484e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6635, 'learning_rate': 6.0079870398975244e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.6133, 'learning_rate': 6.0117545115473005e-05, 'epoch': 0.02}        \n",
      "  2%|▍                            | 16148/1061708 [2:24:56<154:00:03,  1.89it/s][2024-02-29 20:31:56,685] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▍                            | 16149/1061708 [2:24:56<144:30:32,  2.01it/s][2024-02-29 20:31:57,107] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.6947, 'learning_rate': 6.014768488867122e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▍                            | 16151/1061708 [2:24:57<142:39:29,  2.04it/s][2024-02-29 20:31:58,063] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.736, 'learning_rate': 6.018159213351919e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6075, 'learning_rate': 6.021926685001695e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8777, 'learning_rate': 6.025694156651471e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.523, 'learning_rate': 6.029461628301247e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7086, 'learning_rate': 6.033229099951023e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7746, 'learning_rate': 6.036996571600799e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7025, 'learning_rate': 6.040764043250575e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8043, 'learning_rate': 6.0445315149003514e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.8018, 'learning_rate': 6.048298986550126e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16244/1061708 [2:25:46<155:56:15,  1.86it/s][2024-02-29 20:32:47,492] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5337, 'learning_rate': 6.0516897110349245e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.4355, 'learning_rate': 6.0554571826847005e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.0309, 'learning_rate': 6.0592246543344766e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.4774, 'learning_rate': 6.0629921259842526e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.9061, 'learning_rate': 6.066759597634029e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6479, 'learning_rate': 6.0705270692838034e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.5531, 'learning_rate': 6.0742945409335794e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.7268, 'learning_rate': 6.0780620125833555e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.4306, 'learning_rate': 6.0818294842331315e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.8051, 'learning_rate': 6.0855969558829076e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.7588, 'learning_rate': 6.0893644275326836e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.5111, 'learning_rate': 6.0931318991824596e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.9077, 'learning_rate': 6.096899370832235e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7802, 'learning_rate': 6.1006668424820104e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.9791, 'learning_rate': 6.104434314131786e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16397/1061708 [2:27:08<153:57:41,  1.89it/s][2024-02-29 20:34:08,853] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7222, 'learning_rate': 6.107825038616585e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8408, 'learning_rate': 6.111592510266361e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6902, 'learning_rate': 6.115359981916137e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8975, 'learning_rate': 6.119127453565913e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7482, 'learning_rate': 6.122894925215688e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3401, 'learning_rate': 6.126662396865464e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5246, 'learning_rate': 6.13042986851524e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7965, 'learning_rate': 6.134197340165016e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2666, 'learning_rate': 6.137964811814792e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5175, 'learning_rate': 6.141732283464568e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.677, 'learning_rate': 6.145499755114344e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2888, 'learning_rate': 6.14926722676412e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6942, 'learning_rate': 6.153034698413895e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9414, 'learning_rate': 6.156802170063671e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4992, 'learning_rate': 6.160569641713447e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6107, 'learning_rate': 6.164337113363223e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3938, 'learning_rate': 6.168104585012998e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16563/1061708 [2:28:36<157:23:43,  1.84it/s][2024-02-29 20:35:37,176] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5414, 'learning_rate': 6.171495309497797e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0934, 'learning_rate': 6.175262781147571e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.243, 'learning_rate': 6.179030252797347e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4427, 'learning_rate': 6.182797724447123e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4545, 'learning_rate': 6.1865651960969e-05, 'epoch': 0.02}           \n",
      "  2%|▍                            | 16617/1061708 [2:29:05<154:17:45,  1.88it/s][2024-02-29 20:36:05,877] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4946, 'learning_rate': 6.189955920581699e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9363, 'learning_rate': 6.193723392231475e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5615, 'learning_rate': 6.19749086388125e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3989, 'learning_rate': 6.201258335531025e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1708, 'learning_rate': 6.205025807180801e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5274, 'learning_rate': 6.208793278830577e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6391, 'learning_rate': 6.212560750480353e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.279, 'learning_rate': 6.21632822213013e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.4081, 'learning_rate': 6.220095693779904e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6193, 'learning_rate': 6.22386316542968e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4266, 'learning_rate': 6.227630637079456e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6515, 'learning_rate': 6.231398108729232e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16738/1061708 [2:30:09<154:03:21,  1.88it/s][2024-02-29 20:37:10,328] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7006, 'learning_rate': 6.23478883321403e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6452, 'learning_rate': 6.238556304863806e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3288, 'learning_rate': 6.242323776513582e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.416, 'learning_rate': 6.246091248163358e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6102, 'learning_rate': 6.249858719813134e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5767, 'learning_rate': 6.253626191462909e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7201, 'learning_rate': 6.257393663112685e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5578, 'learning_rate': 6.261161134762461e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16813/1061708 [2:30:49<157:24:21,  1.84it/s][2024-02-29 20:37:50,194] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7105, 'learning_rate': 6.26455185924726e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4528, 'learning_rate': 6.268319330897036e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8764, 'learning_rate': 6.272086802546811e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9354, 'learning_rate': 6.275854274196587e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16854/1061708 [2:31:11<156:03:12,  1.86it/s][2024-02-29 20:38:11,974] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4271, 'learning_rate': 6.279244998681385e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4945, 'learning_rate': 6.283012470331161e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5789, 'learning_rate': 6.286779941980937e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8789, 'learning_rate': 6.290547413630713e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5511, 'learning_rate': 6.294314885280489e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8438, 'learning_rate': 6.298082356930265e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6877, 'learning_rate': 6.30184982858004e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8914, 'learning_rate': 6.305617300229816e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3226, 'learning_rate': 6.309384771879592e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5171, 'learning_rate': 6.313152243529368e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4896, 'learning_rate': 6.316919715179144e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3929, 'learning_rate': 6.32068718682892e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 16972/1061708 [2:32:14<154:25:25,  1.88it/s][2024-02-29 20:39:14,863] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.8314, 'learning_rate': 6.324077911313718e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 16986/1061708 [2:32:21<154:51:26,  1.87it/s][2024-02-29 20:39:22,261] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.7336, 'learning_rate': 6.327468635798515e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4598, 'learning_rate': 6.331236107448291e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.614, 'learning_rate': 6.335003579098067e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5965, 'learning_rate': 6.338771050747844e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4711, 'learning_rate': 6.34253852239762e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6283, 'learning_rate': 6.346305994047394e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0496, 'learning_rate': 6.35007346569717e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4875, 'learning_rate': 6.353840937346946e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7536, 'learning_rate': 6.357608408996722e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6219, 'learning_rate': 6.361375880646498e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6787, 'learning_rate': 6.365143352296274e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5635, 'learning_rate': 6.36891082394605e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4497, 'learning_rate': 6.372678295595827e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.636, 'learning_rate': 6.376445767245601e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.9891, 'learning_rate': 6.380213238895377e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4777, 'learning_rate': 6.383980710545153e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5148, 'learning_rate': 6.38774818219493e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 17153/1061708 [2:33:50<157:17:40,  1.84it/s][2024-02-29 20:40:51,316] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4572, 'learning_rate': 6.391138906679727e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0547, 'learning_rate': 6.394906378329503e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3261, 'learning_rate': 6.398673849979279e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6246, 'learning_rate': 6.402441321629055e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4533, 'learning_rate': 6.40620879327883e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5269, 'learning_rate': 6.409976264928606e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8607, 'learning_rate': 6.413743736578382e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5618, 'learning_rate': 6.417511208228158e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4177, 'learning_rate': 6.421278679877934e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.698, 'learning_rate': 6.42504615152771e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.5885, 'learning_rate': 6.428813623177485e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7975, 'learning_rate': 6.432581094827261e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 17279/1061708 [2:34:58<154:04:36,  1.88it/s][2024-02-29 20:41:58,533] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.57, 'learning_rate': 6.43597181931206e-05, 'epoch': 0.02}            \n",
      "{'loss': 2.0665, 'learning_rate': 6.439739290961836e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.8917, 'learning_rate': 6.443506762611612e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.359, 'learning_rate': 6.447274234261388e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 17310/1061708 [2:35:14<153:46:28,  1.89it/s][2024-02-29 20:42:14,970] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4086, 'learning_rate': 6.450664958746186e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7084, 'learning_rate': 6.454432430395962e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4091, 'learning_rate': 6.458199902045737e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4117, 'learning_rate': 6.461967373695513e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 17358/1061708 [2:35:39<154:14:53,  1.88it/s][2024-02-29 20:42:40,509] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.9348, 'learning_rate': 6.465358098180312e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3713, 'learning_rate': 6.469125569830088e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7643, 'learning_rate': 6.472893041479863e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8441, 'learning_rate': 6.476660513129639e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.62, 'learning_rate': 6.480427984779415e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.692, 'learning_rate': 6.484195456429191e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8021, 'learning_rate': 6.487962928078967e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4326, 'learning_rate': 6.491730399728743e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7144, 'learning_rate': 6.495497871378519e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9279, 'learning_rate': 6.499265343028294e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7482, 'learning_rate': 6.50303281467807e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3933, 'learning_rate': 6.506800286327846e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7692, 'learning_rate': 6.510567757977622e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6066, 'learning_rate': 6.514335229627398e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6342, 'learning_rate': 6.518102701277174e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6926, 'learning_rate': 6.52187017292695e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6359, 'learning_rate': 6.525637644576726e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5345, 'learning_rate': 6.5294051162265e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.395, 'learning_rate': 6.533172587876277e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6137, 'learning_rate': 6.536940059526053e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7512, 'learning_rate': 6.540707531175829e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6604, 'learning_rate': 6.544475002825605e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6147, 'learning_rate': 6.54824247447538e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6214, 'learning_rate': 6.552009946125155e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5326, 'learning_rate': 6.555777417774932e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5521, 'learning_rate': 6.559544889424708e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.779, 'learning_rate': 6.563312361074482e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5934, 'learning_rate': 6.567079832724258e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5667, 'learning_rate': 6.570847304374034e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6168, 'learning_rate': 6.57461477602381e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 17659/1061708 [2:38:20<153:46:07,  1.89it/s][2024-02-29 20:45:21,137] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3614, 'learning_rate': 6.57800550050861e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 17660/1061708 [2:38:21<144:26:55,  2.01it/s][2024-02-29 20:45:21,560] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.5271, 'learning_rate': 6.581396224993407e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2194, 'learning_rate': 6.585163696643183e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8712, 'learning_rate': 6.58893116829296e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8109, 'learning_rate': 6.592698639942735e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9906, 'learning_rate': 6.596466111592511e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6136, 'learning_rate': 6.600233583242287e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9048, 'learning_rate': 6.604001054892062e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4633, 'learning_rate': 6.607768526541838e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.083, 'learning_rate': 6.611535998191614e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.414, 'learning_rate': 6.615303469841389e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 17761/1061708 [2:39:14<153:30:57,  1.89it/s][2024-02-29 20:46:15,346] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▍                            | 17762/1061708 [2:39:15<144:13:32,  2.01it/s][2024-02-29 20:46:15,769] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.5905, 'learning_rate': 6.618317447161211e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2308, 'learning_rate': 6.622084918810987e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1812, 'learning_rate': 6.625852390460762e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 17796/1061708 [2:39:33<154:54:46,  1.87it/s][2024-02-29 20:46:33,852] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5123, 'learning_rate': 6.62924311494556e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5652, 'learning_rate': 6.633010586595336e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7133, 'learning_rate': 6.636778058245112e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3328, 'learning_rate': 6.640545529894888e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7934, 'learning_rate': 6.644313001544664e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5101, 'learning_rate': 6.648080473194438e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.312, 'learning_rate': 6.651847944844215e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.9778, 'learning_rate': 6.65561541649399e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6188, 'learning_rate': 6.659382888143767e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6678, 'learning_rate': 6.663150359793543e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6316, 'learning_rate': 6.666917831443319e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0657, 'learning_rate': 6.670685303093095e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7134, 'learning_rate': 6.674452774742871e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4189, 'learning_rate': 6.678220246392646e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5424, 'learning_rate': 6.681987718042422e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2495, 'learning_rate': 6.685755189692198e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6783, 'learning_rate': 6.689522661341974e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3687, 'learning_rate': 6.69329013299175e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2416, 'learning_rate': 6.697057604641526e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2435, 'learning_rate': 6.700825076291302e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 17990/1061708 [2:41:16<153:41:14,  1.89it/s][2024-02-29 20:48:17,347] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▍                            | 17999/1061708 [2:41:21<153:33:42,  1.89it/s][2024-02-29 20:48:22,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=205, lr=[6.7042158007761e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 20:48:22,159] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=1.8918040073460691, CurrSamplesPerSec=1.9018000497860064, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3803, 'learning_rate': 6.7042158007761e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.5864, 'learning_rate': 6.707983272425876e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2866, 'learning_rate': 6.711750744075652e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3601, 'learning_rate': 6.715518215725428e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7117, 'learning_rate': 6.719285687375202e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5517, 'learning_rate': 6.723053159024978e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6761, 'learning_rate': 6.726820630674754e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 18060/1061708 [2:41:54<155:24:09,  1.87it/s][2024-02-29 20:48:54,634] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  2%|▍                            | 18063/1061708 [2:41:55<150:11:09,  1.93it/s][2024-02-29 20:48:56,168] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.8695, 'learning_rate': 6.729834607994575e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5392, 'learning_rate': 6.733602079644351e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4855, 'learning_rate': 6.737369551294127e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7297, 'learning_rate': 6.741137022943903e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4007, 'learning_rate': 6.74490449459368e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7414, 'learning_rate': 6.748671966243454e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.901, 'learning_rate': 6.75243943789323e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.5759, 'learning_rate': 6.756206909543006e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2182, 'learning_rate': 6.759974381192782e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4506, 'learning_rate': 6.763741852842558e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4842, 'learning_rate': 6.767509324492334e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6975, 'learning_rate': 6.77127679614211e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3884, 'learning_rate': 6.775044267791885e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7076, 'learning_rate': 6.778811739441661e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8342, 'learning_rate': 6.782579211091437e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3613, 'learning_rate': 6.786346682741212e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2259, 'learning_rate': 6.790114154390988e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2745, 'learning_rate': 6.793881626040764e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3481, 'learning_rate': 6.79764909769054e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3995, 'learning_rate': 6.801416569340316e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5731, 'learning_rate': 6.805184040990091e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5341, 'learning_rate': 6.808951512639867e-05, 'epoch': 0.02}         \n",
      "  2%|▍                            | 18285/1061708 [2:43:54<157:09:33,  1.84it/s][2024-02-29 20:50:54,608] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.364, 'learning_rate': 6.812342237124666e-05, 'epoch': 0.02}          \n",
      "  2%|▍                            | 18293/1061708 [2:43:58<157:43:39,  1.84it/s][2024-02-29 20:50:58,833] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5085, 'learning_rate': 6.815732961609465e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5938, 'learning_rate': 6.819500433259241e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2453, 'learning_rate': 6.823267904909016e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5932, 'learning_rate': 6.827035376558792e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5963, 'learning_rate': 6.830802848208568e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7348, 'learning_rate': 6.834570319858344e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7736, 'learning_rate': 6.838337791508119e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5268, 'learning_rate': 6.842105263157895e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 18377/1061708 [2:44:43<154:18:44,  1.88it/s][2024-02-29 20:51:43,593] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6656, 'learning_rate': 6.845495987642694e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4583, 'learning_rate': 6.849263459292468e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6211, 'learning_rate': 6.853030930942244e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7715, 'learning_rate': 6.85679840259202e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5545, 'learning_rate': 6.860565874241797e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7814, 'learning_rate': 6.864333345891573e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7333, 'learning_rate': 6.868100817541349e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4712, 'learning_rate': 6.871868289191125e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4558, 'learning_rate': 6.875635760840901e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5487, 'learning_rate': 6.879403232490675e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8803, 'learning_rate': 6.883170704140451e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5627, 'learning_rate': 6.886938175790228e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2712, 'learning_rate': 6.890705647440004e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4825, 'learning_rate': 6.89447311908978e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6933, 'learning_rate': 6.898240590739556e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5187, 'learning_rate': 6.902008062389332e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5186, 'learning_rate': 6.905775534039106e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.389, 'learning_rate': 6.909543005688882e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.29, 'learning_rate': 6.913310477338658e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.993, 'learning_rate': 6.917077948988435e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6839, 'learning_rate': 6.92084542063821e-05, 'epoch': 0.02}          \n",
      "{'loss': 3.0321, 'learning_rate': 6.924612892287987e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1622, 'learning_rate': 6.928380363937763e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2031, 'learning_rate': 6.932147835587537e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.225, 'learning_rate': 6.935915307237313e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6604, 'learning_rate': 6.93968277888709e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3616, 'learning_rate': 6.943450250536865e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0655, 'learning_rate': 6.94721772218664e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7253, 'learning_rate': 6.950985193836416e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 18668/1061708 [2:47:18<153:55:59,  1.88it/s][2024-02-29 20:54:18,932] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3817, 'learning_rate': 6.954375918321215e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7854, 'learning_rate': 6.958143389970991e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0492, 'learning_rate': 6.961910861620766e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 18698/1061708 [2:47:34<153:35:45,  1.89it/s][2024-02-29 20:54:34,794] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5603, 'learning_rate': 6.965301586105565e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2891, 'learning_rate': 6.969069057755341e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3477, 'learning_rate': 6.972836529405117e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6549, 'learning_rate': 6.976604001054893e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4918, 'learning_rate': 6.98037147270467e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 18746/1061708 [2:47:59<154:15:41,  1.88it/s][2024-02-29 20:55:00,284] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2184, 'learning_rate': 6.983762197189466e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3637, 'learning_rate': 6.987529668839242e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5741, 'learning_rate': 6.991297140489018e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8291, 'learning_rate': 6.995064612138794e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.057, 'learning_rate': 6.99883208378857e-05, 'epoch': 0.02}           \n",
      "{'loss': 1.9941, 'learning_rate': 7.002599555438346e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.737, 'learning_rate': 7.006367027088121e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3984, 'learning_rate': 7.010134498737897e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8895, 'learning_rate': 7.013901970387673e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4275, 'learning_rate': 7.017669442037449e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5566, 'learning_rate': 7.021436913687225e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2452, 'learning_rate': 7.025204385337001e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4794, 'learning_rate': 7.028971856986777e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4487, 'learning_rate': 7.032739328636553e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6313, 'learning_rate': 7.036506800286328e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5618, 'learning_rate': 7.040274271936104e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2691, 'learning_rate': 7.04404174358588e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3169, 'learning_rate': 7.047809215235656e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4613, 'learning_rate': 7.051576686885432e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.884, 'learning_rate': 7.055344158535208e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5549, 'learning_rate': 7.059111630184984e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 18951/1061708 [2:49:48<153:37:53,  1.89it/s][2024-02-29 20:56:49,441] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6356, 'learning_rate': 7.06250235466978e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6674, 'learning_rate': 7.066269826319556e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3469, 'learning_rate': 7.070037297969332e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 18986/1061708 [2:50:07<154:52:54,  1.87it/s][2024-02-29 20:57:08,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9124, 'learning_rate': 7.073428022454132e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7245, 'learning_rate': 7.077195494103908e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7466, 'learning_rate': 7.080962965753684e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4338, 'learning_rate': 7.084730437403458e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8104, 'learning_rate': 7.088497909053234e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6092, 'learning_rate': 7.09226538070301e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2667, 'learning_rate': 7.096032852352786e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6533, 'learning_rate': 7.099800324002563e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4358, 'learning_rate': 7.103567795652339e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3563, 'learning_rate': 7.107335267302115e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5739, 'learning_rate': 7.11110273895189e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3903, 'learning_rate': 7.114870210601665e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4867, 'learning_rate': 7.118637682251441e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8432, 'learning_rate': 7.122405153901217e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19120/1061708 [2:51:19<153:42:55,  1.88it/s][2024-02-29 20:58:19,572] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2854, 'learning_rate': 7.125795878386015e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.532, 'learning_rate': 7.129563350035791e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5135, 'learning_rate': 7.133330821685567e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6584, 'learning_rate': 7.137098293335342e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5803, 'learning_rate': 7.140865764985118e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1739, 'learning_rate': 7.144633236634894e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19182/1061708 [2:51:52<154:53:23,  1.87it/s][2024-02-29 20:58:52,644] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.311, 'learning_rate': 7.148023961119693e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2764, 'learning_rate': 7.151791432769469e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.228, 'learning_rate': 7.155558904419245e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2444, 'learning_rate': 7.15932637606902e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4582, 'learning_rate': 7.163093847718796e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3337, 'learning_rate': 7.166861319368572e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19242/1061708 [2:52:24<153:43:24,  1.88it/s][2024-02-29 20:59:24,649] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6242, 'learning_rate': 7.17025204385337e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3521, 'learning_rate': 7.174019515503146e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8889, 'learning_rate': 7.177786987152922e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5455, 'learning_rate': 7.181554458802698e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6058, 'learning_rate': 7.185321930452473e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7792, 'learning_rate': 7.189089402102249e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7643, 'learning_rate': 7.192856873752025e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1294, 'learning_rate': 7.196624345401801e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3752, 'learning_rate': 7.200391817051577e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3956, 'learning_rate': 7.204159288701353e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6743, 'learning_rate': 7.207926760351129e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6709, 'learning_rate': 7.211694232000904e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5846, 'learning_rate': 7.21546170365068e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8025, 'learning_rate': 7.219229175300456e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4833, 'learning_rate': 7.222996646950232e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1943, 'learning_rate': 7.226764118600008e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6367, 'learning_rate': 7.230531590249784e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8676, 'learning_rate': 7.23429906189956e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.329, 'learning_rate': 7.238066533549336e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4144, 'learning_rate': 7.24183400519911e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4088, 'learning_rate': 7.245601476848887e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4559, 'learning_rate': 7.249368948498663e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7158, 'learning_rate': 7.253136420148439e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.32, 'learning_rate': 7.256903891798215e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.676, 'learning_rate': 7.260671363447991e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2609, 'learning_rate': 7.264438835097767e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6125, 'learning_rate': 7.268206306747543e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3602, 'learning_rate': 7.271973778397318e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7344, 'learning_rate': 7.275741250047094e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7246, 'learning_rate': 7.27950872169687e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 19543/1061708 [2:55:04<157:27:25,  1.84it/s][2024-02-29 21:02:05,309] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▌                            | 19544/1061708 [2:55:05<147:00:03,  1.97it/s][2024-02-29 21:02:05,732] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3763, 'learning_rate': 7.28252269901669e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4295, 'learning_rate': 7.286290170666467e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6714, 'learning_rate': 7.290057642316241e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19571/1061708 [2:55:19<153:21:51,  1.89it/s][2024-02-29 21:02:20,057] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6617, 'learning_rate': 7.29344836680104e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2326, 'learning_rate': 7.297215838450816e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19590/1061708 [2:55:29<153:27:45,  1.89it/s][2024-02-29 21:02:30,099] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3279, 'learning_rate': 7.300606562935614e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.413, 'learning_rate': 7.30437403458539e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.3141, 'learning_rate': 7.308141506235165e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3519, 'learning_rate': 7.311908977884941e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7139, 'learning_rate': 7.315676449534717e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3694, 'learning_rate': 7.319443921184493e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3787, 'learning_rate': 7.323211392834269e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2645, 'learning_rate': 7.326978864484045e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5951, 'learning_rate': 7.330746336133821e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4703, 'learning_rate': 7.334513807783597e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3142, 'learning_rate': 7.338281279433372e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19701/1061708 [2:56:28<153:24:27,  1.89it/s][2024-02-29 21:03:29,252] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4585, 'learning_rate': 7.341672003918171e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3707, 'learning_rate': 7.345439475567947e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19728/1061708 [2:56:43<153:47:13,  1.88it/s][2024-02-29 21:03:43,574] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4546, 'learning_rate': 7.348830200052745e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7873, 'learning_rate': 7.352597671702521e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5988, 'learning_rate': 7.356365143352296e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7454, 'learning_rate': 7.360132615002072e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4947, 'learning_rate': 7.363900086651848e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5832, 'learning_rate': 7.367667558301624e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7589, 'learning_rate': 7.3714350299514e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.4187, 'learning_rate': 7.375202501601176e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3993, 'learning_rate': 7.378969973250952e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3995, 'learning_rate': 7.382737444900727e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3754, 'learning_rate': 7.386504916550503e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5284, 'learning_rate': 7.390272388200279e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4474, 'learning_rate': 7.394039859850055e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3148, 'learning_rate': 7.397807331499831e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7473, 'learning_rate': 7.401574803149607e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9633, 'learning_rate': 7.405342274799383e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6987, 'learning_rate': 7.409109746449159e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5751, 'learning_rate': 7.412877218098934e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2273, 'learning_rate': 7.41664468974871e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 19913/1061708 [2:58:21<157:08:16,  1.84it/s][2024-02-29 21:05:22,285] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6564, 'learning_rate': 7.420035414233509e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7405, 'learning_rate': 7.423802885883285e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8981, 'learning_rate': 7.427570357533061e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9572, 'learning_rate': 7.431337829182836e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7415, 'learning_rate': 7.435105300832612e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0731, 'learning_rate': 7.438872772482388e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19972/1061708 [2:58:53<153:35:31,  1.88it/s][2024-02-29 21:05:53,722] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4036, 'learning_rate': 7.442263496967185e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4795, 'learning_rate': 7.446030968616961e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 19999/1061708 [2:59:07<153:44:22,  1.88it/s][2024-02-29 21:06:08,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=226, lr=[7.449798440266737e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 21:06:08,163] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=1.8918087249242892, CurrSamplesPerSec=1.8963014403431722, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.5343, 'learning_rate': 7.449798440266737e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20000/1061708 [2:59:08<153:47:35,  1.88it/s][INFO|trainer.py:2868] 2024-02-29 21:06:08,165 >> Saving model checkpoint to output_model/checkpoint-20000\n",
      "[INFO|trainer.py:2880] 2024-02-29 21:06:08,168 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 21:06:09,368 >> tokenizer config file saved in output_model/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 21:06:09,368 >> Special tokens file saved in output_model/checkpoint-20000/special_tokens_map.json\n",
      "[2024-02-29 21:06:09,370] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20000 is about to be saved!\n",
      "[2024-02-29 21:06:14,583] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-20000/global_step20000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 21:06:14,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-20000/global_step20000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 21:06:28,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-20000/global_step20000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 21:06:29,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-20000/global_step20000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 21:06:36,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-20000/global_step20000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 21:06:36,263] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-20000/global_step20000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 21:06:36,264] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-29 21:06:36,282 >> Deleting older checkpoint [output_model/checkpoint-5000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 21:06:40,022 >> tokenizer config file saved in output_model/checkpoint-20000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 21:06:40,022 >> Special tokens file saved in output_model/checkpoint-20000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.2667, 'learning_rate': 7.453565911916513e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6107, 'learning_rate': 7.45733338356629e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.0039, 'learning_rate': 7.461100855216064e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7615, 'learning_rate': 7.46486832686584e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5935, 'learning_rate': 7.468635798515616e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.706, 'learning_rate': 7.472403270165392e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.0788, 'learning_rate': 7.476170741815168e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.486, 'learning_rate': 7.479938213464944e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3569, 'learning_rate': 7.48370568511472e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4907, 'learning_rate': 7.487473156764497e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3072, 'learning_rate': 7.491240628414271e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6004, 'learning_rate': 7.495008100064047e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9645, 'learning_rate': 7.498775571713823e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0546, 'learning_rate': 7.5025430433636e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.1162, 'learning_rate': 7.506310515013375e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20156/1061708 [3:01:02<154:48:42,  1.87it/s][2024-02-29 21:08:03,489] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4662, 'learning_rate': 7.509701239498173e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4499, 'learning_rate': 7.513468711147948e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3932, 'learning_rate': 7.517236182797724e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3555, 'learning_rate': 7.5210036544475e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.4787, 'learning_rate': 7.524771126097276e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.319, 'learning_rate': 7.528538597747052e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4275, 'learning_rate': 7.532306069396828e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5722, 'learning_rate': 7.536073541046604e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3498, 'learning_rate': 7.53984101269638e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4371, 'learning_rate': 7.543608484346155e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5182, 'learning_rate': 7.547375955995931e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7047, 'learning_rate': 7.551143427645707e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6801, 'learning_rate': 7.554910899295483e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20288/1061708 [3:02:13<153:48:05,  1.88it/s][2024-02-29 21:09:13,918] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5512, 'learning_rate': 7.558301623780282e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2333, 'learning_rate': 7.562069095430058e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3088, 'learning_rate': 7.565836567079833e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20312/1061708 [3:02:26<153:15:04,  1.89it/s][2024-02-29 21:09:26,654] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0488, 'learning_rate': 7.56922729156463e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8297, 'learning_rate': 7.572994763214407e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5133, 'learning_rate': 7.576762234864183e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6575, 'learning_rate': 7.580529706513959e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.342, 'learning_rate': 7.584297178163735e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.493, 'learning_rate': 7.588064649813511e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3788, 'learning_rate': 7.591832121463286e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3678, 'learning_rate': 7.595599593113062e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7466, 'learning_rate': 7.599367064762838e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8494, 'learning_rate': 7.603134536412614e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3032, 'learning_rate': 7.60690200806239e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4912, 'learning_rate': 7.610669479712166e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6239, 'learning_rate': 7.614436951361942e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1162, 'learning_rate': 7.618204423011716e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6412, 'learning_rate': 7.621971894661493e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3849, 'learning_rate': 7.625739366311269e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6273, 'learning_rate': 7.629506837961045e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.369, 'learning_rate': 7.63327430961082e-05, 'epoch': 0.02}           \n",
      "  2%|▌                            | 20495/1061708 [3:04:03<155:16:22,  1.86it/s][2024-02-29 21:11:04,382] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7436, 'learning_rate': 7.63666503409562e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5827, 'learning_rate': 7.640432505745396e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20513/1061708 [3:04:13<157:22:28,  1.84it/s][2024-02-29 21:11:13,952] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6367, 'learning_rate': 7.643823230230192e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.526, 'learning_rate': 7.647590701879968e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6235, 'learning_rate': 7.651358173529744e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6863, 'learning_rate': 7.65512564517952e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 20556/1061708 [3:04:36<154:29:23,  1.87it/s][2024-02-29 21:11:36,843] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1483, 'learning_rate': 7.65851636966432e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5077, 'learning_rate': 7.662283841314094e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5352, 'learning_rate': 7.66605131296387e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1207, 'learning_rate': 7.669818784613646e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1599, 'learning_rate': 7.673586256263422e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5109, 'learning_rate': 7.677353727913198e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4356, 'learning_rate': 7.681121199562974e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5492, 'learning_rate': 7.68488867121275e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5872, 'learning_rate': 7.688656142862525e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0168, 'learning_rate': 7.692423614512301e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3904, 'learning_rate': 7.696191086162077e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1415, 'learning_rate': 7.699958557811853e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6397, 'learning_rate': 7.703726029461629e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.279, 'learning_rate': 7.707493501111404e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.211, 'learning_rate': 7.71126097276118e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.8921, 'learning_rate': 7.715028444410956e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2963, 'learning_rate': 7.718795916060732e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2464, 'learning_rate': 7.722563387710507e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5114, 'learning_rate': 7.726330859360283e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4422, 'learning_rate': 7.730098331010059e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20755/1061708 [3:06:22<155:22:10,  1.86it/s][2024-02-29 21:13:23,025] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7733, 'learning_rate': 7.733489055494858e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8122, 'learning_rate': 7.737256527144634e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2685, 'learning_rate': 7.74102399879441e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2569, 'learning_rate': 7.744791470444185e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5173, 'learning_rate': 7.748558942093961e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5701, 'learning_rate': 7.752326413743737e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4238, 'learning_rate': 7.756093885393513e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 20825/1061708 [3:06:59<154:36:14,  1.87it/s][2024-02-29 21:14:00,303] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.7544, 'learning_rate': 7.759484609878312e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5664, 'learning_rate': 7.763252081528087e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3833, 'learning_rate': 7.767019553177863e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1817, 'learning_rate': 7.770787024827639e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4891, 'learning_rate': 7.774554496477414e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4264, 'learning_rate': 7.77832196812719e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7801, 'learning_rate': 7.782089439776966e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3095, 'learning_rate': 7.785856911426742e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4949, 'learning_rate': 7.789624383076518e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1788, 'learning_rate': 7.793391854726294e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2563, 'learning_rate': 7.797159326376068e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7473, 'learning_rate': 7.800926798025844e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4756, 'learning_rate': 7.80469426967562e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.418, 'learning_rate': 7.808461741325397e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7657, 'learning_rate': 7.812229212975173e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3276, 'learning_rate': 7.815996684624949e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7535, 'learning_rate': 7.819764156274725e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3723, 'learning_rate': 7.823531627924501e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5719, 'learning_rate': 7.827299099574275e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3073, 'learning_rate': 7.831066571224051e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4422, 'learning_rate': 7.834834042873828e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.138, 'learning_rate': 7.838601514523604e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 21047/1061708 [3:08:58<153:48:07,  1.88it/s][2024-02-29 21:15:58,580] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7783, 'learning_rate': 7.841992239008403e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21052/1061708 [3:09:00<151:52:56,  1.90it/s][2024-02-29 21:16:01,180] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3857, 'learning_rate': 7.845382963493199e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2705, 'learning_rate': 7.849150435142975e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4399, 'learning_rate': 7.852917906792751e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.252, 'learning_rate': 7.856685378442527e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2798, 'learning_rate': 7.860452850092303e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5059, 'learning_rate': 7.864220321742079e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8229, 'learning_rate': 7.867987793391855e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2584, 'learning_rate': 7.87175526504163e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3015, 'learning_rate': 7.875522736691406e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21141/1061708 [3:09:48<152:50:58,  1.89it/s][2024-02-29 21:16:48,549] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6447, 'learning_rate': 7.878913461176205e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6122, 'learning_rate': 7.882680932825981e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4064, 'learning_rate': 7.886448404475757e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.313, 'learning_rate': 7.890215876125533e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2646, 'learning_rate': 7.89398334777531e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4909, 'learning_rate': 7.897750819425084e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4763, 'learning_rate': 7.90151829107486e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5257, 'learning_rate': 7.905285762724636e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4492, 'learning_rate': 7.909053234374412e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7047, 'learning_rate': 7.912820706024188e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.268, 'learning_rate': 7.916588177673964e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3872, 'learning_rate': 7.920355649323739e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5525, 'learning_rate': 7.924123120973515e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4295, 'learning_rate': 7.927890592623291e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.491, 'learning_rate': 7.931658064273067e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3768, 'learning_rate': 7.935425535922842e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7288, 'learning_rate': 7.939193007572618e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9773, 'learning_rate': 7.942960479222394e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2337, 'learning_rate': 7.94672795087217e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1961, 'learning_rate': 7.950495422521946e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21348/1061708 [3:11:38<153:14:51,  1.89it/s][2024-02-29 21:18:38,739] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7785, 'learning_rate': 7.953886147006744e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4133, 'learning_rate': 7.95765361865652e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3672, 'learning_rate': 7.961421090306296e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2541, 'learning_rate': 7.965188561956072e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2273, 'learning_rate': 7.968956033605848e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5835, 'learning_rate': 7.972723505255624e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4255, 'learning_rate': 7.9764909769054e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.6028, 'learning_rate': 7.980258448555175e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5428, 'learning_rate': 7.984025920204951e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3555, 'learning_rate': 7.987793391854727e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21449/1061708 [3:12:31<153:07:50,  1.89it/s][2024-02-29 21:19:32,464] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6928, 'learning_rate': 7.991184116339525e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21450/1061708 [3:12:32<143:46:32,  2.01it/s][2024-02-29 21:19:32,887] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3945, 'learning_rate': 7.994574840824324e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2247, 'learning_rate': 7.998342312474098e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3989, 'learning_rate': 8.002109784123874e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6114, 'learning_rate': 8.00587725577365e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1872, 'learning_rate': 8.009644727423427e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1958, 'learning_rate': 8.013412199073203e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6486, 'learning_rate': 8.017179670722979e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4653, 'learning_rate': 8.020947142372755e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21533/1061708 [3:13:16<156:27:56,  1.85it/s][2024-02-29 21:20:17,050] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5689, 'learning_rate': 8.024337866857552e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3813, 'learning_rate': 8.028105338507328e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9423, 'learning_rate': 8.031872810157105e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1925, 'learning_rate': 8.03564028180688e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1876, 'learning_rate': 8.039407753456655e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4072, 'learning_rate': 8.043175225106431e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7528, 'learning_rate': 8.046942696756207e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.79, 'learning_rate': 8.050710168405982e-05, 'epoch': 0.02}           \n",
      "{'loss': 1.9877, 'learning_rate': 8.054477640055758e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1851, 'learning_rate': 8.058245111705534e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4487, 'learning_rate': 8.06201258335531e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 21645/1061708 [3:14:16<154:28:05,  1.87it/s][2024-02-29 21:21:16,691] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▌                            | 21649/1061708 [3:14:18<153:54:00,  1.88it/s][2024-02-29 21:21:18,752] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.309, 'learning_rate': 8.065026560675132e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2606, 'learning_rate': 8.068794032324907e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3897, 'learning_rate': 8.072561503974683e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2324, 'learning_rate': 8.076328975624459e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8227, 'learning_rate': 8.080096447274235e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5614, 'learning_rate': 8.083863918924011e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3899, 'learning_rate': 8.087631390573787e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3757, 'learning_rate': 8.091398862223562e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6108, 'learning_rate': 8.095166333873338e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9563, 'learning_rate': 8.098933805523114e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6277, 'learning_rate': 8.10270127717289e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6115, 'learning_rate': 8.106468748822665e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0575, 'learning_rate': 8.110236220472441e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5042, 'learning_rate': 8.114003692122217e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4846, 'learning_rate': 8.117771163771993e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2619, 'learning_rate': 8.121538635421769e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3806, 'learning_rate': 8.125306107071544e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9218, 'learning_rate': 8.12907357872132e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 21821/1061708 [3:15:49<153:11:02,  1.89it/s][2024-02-29 21:22:50,413] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▌                            | 21827/1061708 [3:15:53<154:20:19,  1.87it/s][2024-02-29 21:22:53,555] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  2%|▌                            | 21829/1061708 [3:15:54<149:00:23,  1.94it/s][2024-02-29 21:22:54,529] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6145, 'learning_rate': 8.131710808876164e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5604, 'learning_rate': 8.13547828052594e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4438, 'learning_rate': 8.139245752175714e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4859, 'learning_rate': 8.14301322382549e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3474, 'learning_rate': 8.146780695475266e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4926, 'learning_rate': 8.150548167125042e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3074, 'learning_rate': 8.154315638774819e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4537, 'learning_rate': 8.158083110424595e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7484, 'learning_rate': 8.16185058207437e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 21915/1061708 [3:16:39<154:46:18,  1.87it/s][2024-02-29 21:23:40,369] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.7729, 'learning_rate': 8.165241306559168e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3604, 'learning_rate': 8.169008778208944e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5289, 'learning_rate': 8.17277624985872e-05, 'epoch': 0.02}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4637, 'learning_rate': 8.176543721508496e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3912, 'learning_rate': 8.180311193158273e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8078, 'learning_rate': 8.184078664808049e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2441, 'learning_rate': 8.187846136457825e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1756, 'learning_rate': 8.191613608107599e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 21999/1061708 [3:17:24<153:22:38,  1.88it/s][2024-02-29 21:24:25,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=247, lr=[8.195381079757375e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 21:24:25,232] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=1.8919199370956614, CurrSamplesPerSec=1.8980116669027318, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.4534, 'learning_rate': 8.195381079757375e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5088, 'learning_rate': 8.199148551407151e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4597, 'learning_rate': 8.202916023056927e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1703, 'learning_rate': 8.206683494706703e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2816, 'learning_rate': 8.210450966356478e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5117, 'learning_rate': 8.214218438006254e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2363, 'learning_rate': 8.21798590965603e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4608, 'learning_rate': 8.221753381305806e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6714, 'learning_rate': 8.225520852955581e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5553, 'learning_rate': 8.229288324605357e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5007, 'learning_rate': 8.233055796255133e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3805, 'learning_rate': 8.236823267904909e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3947, 'learning_rate': 8.240590739554685e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3062, 'learning_rate': 8.244358211204461e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6386, 'learning_rate': 8.248125682854236e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3372, 'learning_rate': 8.251893154504012e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2221, 'learning_rate': 8.255660626153788e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3469, 'learning_rate': 8.259428097803564e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4027, 'learning_rate': 8.26319556945334e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2501, 'learning_rate': 8.266963041103116e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6758, 'learning_rate': 8.270730512752892e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.408, 'learning_rate': 8.274497984402668e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3766, 'learning_rate': 8.278265456052443e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8017, 'learning_rate': 8.282032927702219e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3259, 'learning_rate': 8.285800399351995e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5018, 'learning_rate': 8.289567871001771e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5909, 'learning_rate': 8.293335342651547e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3696, 'learning_rate': 8.297102814301323e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3454, 'learning_rate': 8.300870285951099e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22283/1061708 [3:19:56<157:09:07,  1.84it/s][2024-02-29 21:26:56,554] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4591, 'learning_rate': 8.304261010435897e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22291/1061708 [3:20:00<153:14:06,  1.88it/s][2024-02-29 21:27:00,726] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7381, 'learning_rate': 8.307651734920695e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2006, 'learning_rate': 8.311419206570471e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6553, 'learning_rate': 8.315186678220247e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.445, 'learning_rate': 8.318954149870023e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4945, 'learning_rate': 8.322721621519799e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5342, 'learning_rate': 8.326489093169574e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3172, 'learning_rate': 8.33025656481935e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7941, 'learning_rate': 8.334024036469126e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22373/1061708 [3:20:43<156:53:04,  1.84it/s][2024-02-29 21:27:44,373] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.502, 'learning_rate': 8.337414760953925e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4659, 'learning_rate': 8.341182232603701e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3951, 'learning_rate': 8.344949704253477e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5559, 'learning_rate': 8.348717175903252e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2925, 'learning_rate': 8.352484647553028e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3004, 'learning_rate': 8.356252119202804e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7781, 'learning_rate': 8.36001959085258e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4411, 'learning_rate': 8.363787062502356e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5147, 'learning_rate': 8.36755453415213e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5408, 'learning_rate': 8.371322005801906e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4385, 'learning_rate': 8.375089477451683e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3744, 'learning_rate': 8.378856949101459e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6652, 'learning_rate': 8.382624420751233e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1375, 'learning_rate': 8.386391892401009e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22511/1061708 [3:21:57<153:18:20,  1.88it/s][2024-02-29 21:28:57,960] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7026, 'learning_rate': 8.389782616885808e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7079, 'learning_rate': 8.393550088535584e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2379, 'learning_rate': 8.39731756018536e-05, 'epoch': 0.02}          \n",
      "  2%|▌                            | 22545/1061708 [3:22:15<155:03:21,  1.86it/s][2024-02-29 21:29:16,051] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3749, 'learning_rate': 8.400708284670158e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8266, 'learning_rate': 8.404475756319934e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6605, 'learning_rate': 8.40824322796971e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7854, 'learning_rate': 8.412010699619486e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1915, 'learning_rate': 8.415778171269262e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4515, 'learning_rate': 8.419545642919037e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4141, 'learning_rate': 8.423313114568813e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2859, 'learning_rate': 8.427080586218589e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9727, 'learning_rate': 8.430848057868365e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4587, 'learning_rate': 8.43461552951814e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4435, 'learning_rate': 8.438383001167916e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2931, 'learning_rate': 8.442150472817692e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4218, 'learning_rate': 8.445917944467468e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0232, 'learning_rate': 8.449685416117244e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4374, 'learning_rate': 8.45345288776702e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3565, 'learning_rate': 8.457220359416795e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22703/1061708 [3:23:39<155:59:22,  1.85it/s][2024-02-29 21:30:40,262] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.159, 'learning_rate': 8.460611083901594e-05, 'epoch': 0.02}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5846, 'learning_rate': 8.46437855555137e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5564, 'learning_rate': 8.468146027201146e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5969, 'learning_rate': 8.471913498850922e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.577, 'learning_rate': 8.475680970500698e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5288, 'learning_rate': 8.479448442150473e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2729, 'learning_rate': 8.483215913800249e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.222, 'learning_rate': 8.486983385450025e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4646, 'learning_rate': 8.490750857099801e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.401, 'learning_rate': 8.494518328749577e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.0256, 'learning_rate': 8.498285800399353e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.389, 'learning_rate': 8.502053272049129e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6093, 'learning_rate': 8.505820743698905e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.403, 'learning_rate': 8.50958821534868e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.1322, 'learning_rate': 8.513355686998456e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5926, 'learning_rate': 8.517123158648232e-05, 'epoch': 0.02}         \n",
      "  2%|▌                            | 22869/1061708 [3:25:08<152:38:37,  1.89it/s][2024-02-29 21:32:08,634] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4617, 'learning_rate': 8.52051388313303e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7662, 'learning_rate': 8.524281354782806e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6368, 'learning_rate': 8.528048826432582e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 22899/1061708 [3:25:23<152:48:59,  1.89it/s][2024-02-29 21:32:24,514] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4465, 'learning_rate': 8.53143955091738e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7721, 'learning_rate': 8.535207022567156e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1231, 'learning_rate': 8.538974494216932e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 22924/1061708 [3:25:37<155:01:59,  1.86it/s][2024-02-29 21:32:37,750] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3274, 'learning_rate': 8.54236521870173e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3733, 'learning_rate': 8.546132690351505e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0248, 'learning_rate': 8.549900162001282e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4854, 'learning_rate': 8.553667633651056e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4501, 'learning_rate': 8.557435105300832e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.506, 'learning_rate': 8.561202576950608e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3407, 'learning_rate': 8.564970048600384e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.805, 'learning_rate': 8.56873752025016e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.5097, 'learning_rate': 8.572504991899936e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4806, 'learning_rate': 8.576272463549712e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4847, 'learning_rate': 8.580039935199487e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2475, 'learning_rate': 8.583807406849263e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2139, 'learning_rate': 8.587574878499039e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1151, 'learning_rate': 8.591342350148815e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5064, 'learning_rate': 8.595109821798591e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5557, 'learning_rate': 8.598877293448367e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4255, 'learning_rate': 8.602644765098143e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.672, 'learning_rate': 8.60641223674792e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.4026, 'learning_rate': 8.610179708397694e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6169, 'learning_rate': 8.61394718004747e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6034, 'learning_rate': 8.617714651697246e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.7836, 'learning_rate': 8.621482123347022e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5152, 'learning_rate': 8.625249594996798e-05, 'epoch': 0.02}         \n",
      "{'loss': 3.0186, 'learning_rate': 8.629017066646574e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23162/1061708 [3:27:43<152:50:35,  1.89it/s][2024-02-29 21:34:44,479] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▋                            | 23166/1061708 [3:27:46<154:54:42,  1.86it/s][2024-02-29 21:34:46,541] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4089, 'learning_rate': 8.632031043966394e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4742, 'learning_rate': 8.63579851561617e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8732, 'learning_rate': 8.639565987265946e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2554, 'learning_rate': 8.643333458915722e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23201/1061708 [3:28:04<152:40:06,  1.89it/s][2024-02-29 21:35:05,146] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.8292, 'learning_rate': 8.646724183400521e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6481, 'learning_rate': 8.650491655050296e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3039, 'learning_rate': 8.654259126700072e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4948, 'learning_rate': 8.658026598349848e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0829, 'learning_rate': 8.661794069999624e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4683, 'learning_rate': 8.6655615416494e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.6417, 'learning_rate': 8.669329013299176e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1165, 'learning_rate': 8.673096484948952e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3502, 'learning_rate': 8.676863956598728e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6036, 'learning_rate': 8.680631428248503e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1648, 'learning_rate': 8.684398899898279e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4495, 'learning_rate': 8.688166371548055e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0588, 'learning_rate': 8.691933843197831e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2989, 'learning_rate': 8.695701314847606e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5547, 'learning_rate': 8.699468786497382e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3827, 'learning_rate': 8.703236258147158e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4664, 'learning_rate': 8.707003729796934e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4892, 'learning_rate': 8.710771201446708e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2223, 'learning_rate': 8.714538673096484e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4016, 'learning_rate': 8.71830614474626e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6758, 'learning_rate': 8.722073616396037e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2617, 'learning_rate': 8.725841088045813e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2207, 'learning_rate': 8.729608559695589e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2405, 'learning_rate': 8.733376031345365e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23444/1061708 [3:30:13<155:24:46,  1.86it/s][2024-02-29 21:37:14,520] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4196, 'learning_rate': 8.736766755830162e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4009, 'learning_rate': 8.740534227479939e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1961, 'learning_rate': 8.744301699129715e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2337, 'learning_rate': 8.74806917077949e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5909, 'learning_rate': 8.751836642429267e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5262, 'learning_rate': 8.755604114079043e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3773, 'learning_rate': 8.759371585728819e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3435, 'learning_rate': 8.763139057378593e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23524/1061708 [3:30:56<155:09:31,  1.86it/s][2024-02-29 21:37:57,157] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5067, 'learning_rate': 8.766529781863391e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2712, 'learning_rate': 8.770297253513167e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2529, 'learning_rate': 8.774064725162943e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5219, 'learning_rate': 8.77783219681272e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1898, 'learning_rate': 8.781599668462495e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4063, 'learning_rate': 8.78536714011227e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.9264, 'learning_rate': 8.789134611762046e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7886, 'learning_rate': 8.792902083411822e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4635, 'learning_rate': 8.796669555061598e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.188, 'learning_rate': 8.800437026711374e-05, 'epoch': 0.02}          \n",
      "{'loss': 1.9455, 'learning_rate': 8.80420449836115e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4874, 'learning_rate': 8.807971970010926e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6614, 'learning_rate': 8.811739441660702e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0722, 'learning_rate': 8.815506913310477e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2326, 'learning_rate': 8.819274384960253e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2614, 'learning_rate': 8.823041856610029e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.31, 'learning_rate': 8.826809328259805e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.2738, 'learning_rate': 8.830576799909581e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.193, 'learning_rate': 8.834344271559357e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 23717/1061708 [3:32:39<153:11:03,  1.88it/s][2024-02-29 21:39:39,923] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6181, 'learning_rate': 8.837734996044155e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7604, 'learning_rate': 8.841502467693931e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6604, 'learning_rate': 8.845269939343707e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1567, 'learning_rate': 8.849037410993483e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6482, 'learning_rate': 8.852804882643259e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2944, 'learning_rate': 8.856572354293034e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9451, 'learning_rate': 8.86033982594281e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.564, 'learning_rate': 8.864107297592586e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5416, 'learning_rate': 8.867874769242361e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2361, 'learning_rate': 8.871642240892137e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23818/1061708 [3:33:33<152:51:09,  1.89it/s][2024-02-29 21:40:33,637] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▋                            | 23819/1061708 [3:33:33<143:21:55,  2.01it/s][2024-02-29 21:40:34,059] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.539, 'learning_rate': 8.874656218211959e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2536, 'learning_rate': 8.878423689861735e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2261, 'learning_rate': 8.882191161511511e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2611, 'learning_rate': 8.885958633161286e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4151, 'learning_rate': 8.889726104811062e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2513, 'learning_rate': 8.893493576460838e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2838, 'learning_rate': 8.897261048110614e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7771, 'learning_rate': 8.90102851976039e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7208, 'learning_rate': 8.904795991410166e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4741, 'learning_rate': 8.90856346305994e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6117, 'learning_rate': 8.912330934709717e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23920/1061708 [3:34:27<152:31:06,  1.89it/s][2024-02-29 21:41:27,801] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▋                            | 23921/1061708 [3:34:27<143:14:27,  2.01it/s][2024-02-29 21:41:28,222] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1942, 'learning_rate': 8.915344912029538e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3883, 'learning_rate': 8.919112383679314e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23942/1061708 [3:34:38<153:00:58,  1.88it/s][2024-02-29 21:41:39,360] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3883, 'learning_rate': 8.922503108164111e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1069, 'learning_rate': 8.926270579813887e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6785, 'learning_rate': 8.930038051463663e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4866, 'learning_rate': 8.933805523113438e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6052, 'learning_rate': 8.937572994763214e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 23999/1061708 [3:35:09<152:38:27,  1.89it/s][2024-02-29 21:42:09,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=267, lr=[8.94134046641299e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 21:42:09,753] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=1.892104100503082, CurrSamplesPerSec=1.8850906162190537, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.5795, 'learning_rate': 8.94134046641299e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.2105, 'learning_rate': 8.945107938062766e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4076, 'learning_rate': 8.948875409712542e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3872, 'learning_rate': 8.952642881362318e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6146, 'learning_rate': 8.956410353012093e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6756, 'learning_rate': 8.960177824661869e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3655, 'learning_rate': 8.963945296311645e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1104, 'learning_rate': 8.967712767961421e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5872, 'learning_rate': 8.971480239611197e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3805, 'learning_rate': 8.975247711260973e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24094/1061708 [3:35:59<154:48:47,  1.86it/s][2024-02-29 21:43:00,301] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3352, 'learning_rate': 8.978638435745771e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2491, 'learning_rate': 8.982405907395547e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3704, 'learning_rate': 8.986173379045323e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2789, 'learning_rate': 8.989940850695099e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.9301, 'learning_rate': 8.993708322344875e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.344, 'learning_rate': 8.997475793994651e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6523, 'learning_rate': 9.001243265644427e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.19, 'learning_rate': 9.005010737294203e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.1315, 'learning_rate': 9.008778208943978e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6115, 'learning_rate': 9.012545680593754e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24195/1061708 [3:36:53<153:55:12,  1.87it/s][2024-02-29 21:43:54,007] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▋                            | 24196/1061708 [3:36:53<144:15:32,  2.00it/s][2024-02-29 21:43:54,429] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.211, 'learning_rate': 9.015559657913575e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.8177, 'learning_rate': 9.019327129563351e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4112, 'learning_rate': 9.023094601213127e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.31, 'learning_rate': 9.026862072862902e-05, 'epoch': 0.02}           \n",
      "  2%|▋                            | 24239/1061708 [3:37:16<152:37:17,  1.89it/s][2024-02-29 21:44:17,261] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6258, 'learning_rate': 9.030252797347701e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1482, 'learning_rate': 9.034020268997477e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7729, 'learning_rate': 9.037787740647253e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24263/1061708 [3:37:29<156:19:17,  1.84it/s][2024-02-29 21:44:29,969] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2131, 'learning_rate': 9.04117846513205e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3273, 'learning_rate': 9.044945936781825e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3075, 'learning_rate': 9.048713408431601e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6468, 'learning_rate': 9.052480880081377e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4495, 'learning_rate': 9.056248351731153e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4056, 'learning_rate': 9.06001582338093e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3595, 'learning_rate': 9.063783295030706e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3512, 'learning_rate': 9.067550766680482e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1417, 'learning_rate': 9.071318238330258e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5283, 'learning_rate': 9.075085709980032e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4002, 'learning_rate': 9.078853181629808e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5321, 'learning_rate': 9.082620653279584e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4571, 'learning_rate': 9.08638812492936e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3303, 'learning_rate': 9.090155596579137e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1736, 'learning_rate': 9.093923068228913e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24414/1061708 [3:38:49<154:36:53,  1.86it/s][2024-02-29 21:45:50,311] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5109, 'learning_rate': 9.09731379271371e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5479, 'learning_rate': 9.101081264363486e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0362, 'learning_rate': 9.104848736013261e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24444/1061708 [3:39:05<154:45:31,  1.86it/s][2024-02-29 21:46:06,223] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4317, 'learning_rate': 9.10823946049806e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.599, 'learning_rate': 9.112006932147836e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4525, 'learning_rate': 9.115774403797612e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8522, 'learning_rate': 9.119541875447388e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3337, 'learning_rate': 9.123309347097163e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4587, 'learning_rate': 9.127076818746939e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3096, 'learning_rate': 9.130844290396715e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3201, 'learning_rate': 9.134611762046491e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7051, 'learning_rate': 9.138379233696267e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2788, 'learning_rate': 9.142146705346043e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4205, 'learning_rate': 9.145914176995819e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3848, 'learning_rate': 9.149681648645594e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4623, 'learning_rate': 9.15344912029537e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 24572/1061708 [3:40:13<152:32:20,  1.89it/s][2024-02-29 21:47:14,358] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.408, 'learning_rate': 9.156839844780168e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5914, 'learning_rate': 9.160607316429944e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6204, 'learning_rate': 9.16437478807972e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4099, 'learning_rate': 9.168142259729496e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4109, 'learning_rate': 9.171909731379272e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3027, 'learning_rate': 9.175677203029047e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5655, 'learning_rate': 9.179444674678823e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7404, 'learning_rate': 9.183212146328599e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.356, 'learning_rate': 9.186979617978375e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1938, 'learning_rate': 9.190747089628151e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4091, 'learning_rate': 9.194514561277927e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3373, 'learning_rate': 9.198282032927703e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2638, 'learning_rate': 9.202049504577478e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3019, 'learning_rate': 9.205816976227254e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4987, 'learning_rate': 9.20958444787703e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7008, 'learning_rate': 9.213351919526806e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24736/1061708 [3:41:41<153:32:53,  1.88it/s][2024-02-29 21:48:41,610] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.523, 'learning_rate': 9.216742644011605e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 24746/1061708 [3:41:46<153:40:03,  1.87it/s][2024-02-29 21:48:46,883] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.9355, 'learning_rate': 9.220133368496403e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3725, 'learning_rate': 9.223900840146177e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6961, 'learning_rate': 9.227668311795953e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4133, 'learning_rate': 9.23143578344573e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3615, 'learning_rate': 9.235203255095505e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5212, 'learning_rate': 9.238970726745281e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 24801/1061708 [3:42:15<152:35:05,  1.89it/s][2024-02-29 21:49:16,135] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2424, 'learning_rate': 9.24236145123008e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5378, 'learning_rate': 9.246128922879855e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.126, 'learning_rate': 9.249896394529631e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.342, 'learning_rate': 9.253663866179407e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5061, 'learning_rate': 9.257431337829183e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3028, 'learning_rate': 9.26119880947896e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3078, 'learning_rate': 9.264966281128735e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2691, 'learning_rate': 9.268733752778512e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6939, 'learning_rate': 9.272501224428288e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3187, 'learning_rate': 9.276268696078062e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5883, 'learning_rate': 9.280036167727838e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2195, 'learning_rate': 9.283803639377614e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2411, 'learning_rate': 9.28757111102739e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5904, 'learning_rate': 9.291338582677166e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0525, 'learning_rate': 9.295106054326942e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0286, 'learning_rate': 9.298873525976719e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6938, 'learning_rate': 9.302640997626493e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.461, 'learning_rate': 9.306408469276269e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1646, 'learning_rate': 9.310175940926045e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3383, 'learning_rate': 9.313943412575821e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25000/1061708 [3:44:01<152:28:07,  1.89it/s][INFO|trainer.py:2868] 2024-02-29 21:51:01,634 >> Saving model checkpoint to output_model/checkpoint-25000\n",
      "[INFO|trainer.py:2880] 2024-02-29 21:51:01,637 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 21:51:02,838 >> tokenizer config file saved in output_model/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 21:51:02,838 >> Special tokens file saved in output_model/checkpoint-25000/special_tokens_map.json\n",
      "[2024-02-29 21:51:02,839] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step25000 is about to be saved!\n",
      "[2024-02-29 21:51:08,068] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-25000/global_step25000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 21:51:08,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-25000/global_step25000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 21:51:21,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-25000/global_step25000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 21:51:22,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-25000/global_step25000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 21:51:29,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-25000/global_step25000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 21:51:29,769] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-25000/global_step25000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 21:51:29,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-29 21:51:29,792 >> Deleting older checkpoint [output_model/checkpoint-10000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 21:51:33,533 >> tokenizer config file saved in output_model/checkpoint-25000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 21:51:33,534 >> Special tokens file saved in output_model/checkpoint-25000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.4938, 'learning_rate': 9.317710884225596e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25013/1061708 [3:44:40<192:32:13,  1.50it/s][2024-02-29 21:51:40,826] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4382, 'learning_rate': 9.321101608710395e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5171, 'learning_rate': 9.324869080360171e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3113, 'learning_rate': 9.328636552009946e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2562, 'learning_rate': 9.332404023659722e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3536, 'learning_rate': 9.336171495309498e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.611, 'learning_rate': 9.339938966959274e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.419, 'learning_rate': 9.34370643860905e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.259, 'learning_rate': 9.347473910258826e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25097/1061708 [3:45:24<152:58:52,  1.88it/s][2024-02-29 21:52:25,271] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6102, 'learning_rate': 9.350864634743624e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4846, 'learning_rate': 9.3546321063934e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.8168, 'learning_rate': 9.358399578043176e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4602, 'learning_rate': 9.362167049692952e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6647, 'learning_rate': 9.365934521342728e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3156, 'learning_rate': 9.369701992992503e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.67, 'learning_rate': 9.373469464642279e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.1371, 'learning_rate': 9.377236936292055e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5916, 'learning_rate': 9.381004407941831e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2426, 'learning_rate': 9.384771879591606e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0352, 'learning_rate': 9.388539351241382e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2861, 'learning_rate': 9.392306822891158e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4423, 'learning_rate': 9.396074294540934e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3827, 'learning_rate': 9.39984176619071e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25234/1061708 [3:46:37<154:56:55,  1.86it/s][2024-02-29 21:53:38,398] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5202, 'learning_rate': 9.403232490675508e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.033, 'learning_rate': 9.406999962325284e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25259/1061708 [3:46:51<152:39:01,  1.89it/s][2024-02-29 21:53:51,641] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4494, 'learning_rate': 9.410390686810083e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4881, 'learning_rate': 9.414158158459859e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9324, 'learning_rate': 9.417925630109635e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2789, 'learning_rate': 9.42169310175941e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4305, 'learning_rate': 9.425460573409186e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.8598, 'learning_rate': 9.429228045058962e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1971, 'learning_rate': 9.432995516708738e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.656, 'learning_rate': 9.436762988358512e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25339/1061708 [3:47:33<152:36:16,  1.89it/s][2024-02-29 21:54:34,190] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2049, 'learning_rate': 9.440153712843311e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.371, 'learning_rate': 9.443921184493087e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.6377, 'learning_rate': 9.447688656142863e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4612, 'learning_rate': 9.451456127792638e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4621, 'learning_rate': 9.455223599442414e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7861, 'learning_rate': 9.45899107109219e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3223, 'learning_rate': 9.462758542741966e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5814, 'learning_rate': 9.466526014391742e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.753, 'learning_rate': 9.470293486041518e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.7188, 'learning_rate': 9.474060957691294e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1377, 'learning_rate': 9.47782842934107e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25443/1061708 [3:48:29<155:53:49,  1.85it/s][2024-02-29 21:55:29,553] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3726, 'learning_rate': 9.481219153825868e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▋                            | 25454/1061708 [3:48:34<155:40:16,  1.85it/s][2024-02-29 21:55:35,344] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1964, 'learning_rate': 9.484609878310666e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2225, 'learning_rate': 9.488377349960442e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5164, 'learning_rate': 9.492144821610218e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4452, 'learning_rate': 9.495912293259994e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7246, 'learning_rate': 9.499679764909769e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4773, 'learning_rate': 9.503447236559545e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2362, 'learning_rate': 9.507214708209321e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2395, 'learning_rate': 9.510982179859097e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4388, 'learning_rate': 9.514749651508873e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7052, 'learning_rate': 9.518517123158649e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4212, 'learning_rate': 9.522284594808425e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.688, 'learning_rate': 9.5260520664582e-05, 'epoch': 0.02}            \n",
      "{'loss': 2.2435, 'learning_rate': 9.529819538107976e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4267, 'learning_rate': 9.533587009757752e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4591, 'learning_rate': 9.537354481407528e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3113, 'learning_rate': 9.541121953057304e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2595, 'learning_rate': 9.54488942470708e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1997, 'learning_rate': 9.548656896356856e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0861, 'learning_rate': 9.552424368006632e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4529, 'learning_rate': 9.556191839656407e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0622, 'learning_rate': 9.559959311306183e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3418, 'learning_rate': 9.563726782955959e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2355, 'learning_rate': 9.567494254605735e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.224, 'learning_rate': 9.571261726255511e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3557, 'learning_rate': 9.575029197905287e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5498, 'learning_rate': 9.578796669555062e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2452, 'learning_rate': 9.582564141204838e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4641, 'learning_rate': 9.586331612854614e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2079, 'learning_rate': 9.59009908450439e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5221, 'learning_rate': 9.593866556154165e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25755/1061708 [3:51:15<154:05:22,  1.87it/s][2024-02-29 21:58:15,713] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  2%|▋                            | 25756/1061708 [3:51:15<144:15:05,  1.99it/s][2024-02-29 21:58:16,135] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3954, 'learning_rate': 9.596880533473987e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25768/1061708 [3:51:21<152:24:33,  1.89it/s][2024-02-29 21:58:22,466] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2589, 'learning_rate': 9.600271257958784e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2106, 'learning_rate': 9.60403872960856e-05, 'epoch': 0.02}          \n",
      "{'loss': 1.9866, 'learning_rate': 9.607806201258335e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1165, 'learning_rate': 9.611573672908111e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25800/1061708 [3:51:38<152:16:23,  1.89it/s][2024-02-29 21:58:39,416] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5534, 'learning_rate': 9.61496439739291e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3847, 'learning_rate': 9.618731869042686e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3216, 'learning_rate': 9.622499340692461e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3308, 'learning_rate': 9.626266812342237e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7336, 'learning_rate': 9.630034283992013e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4422, 'learning_rate': 9.633801755641789e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1793, 'learning_rate': 9.637569227291565e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1411, 'learning_rate': 9.641336698941341e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3651, 'learning_rate': 9.645104170591117e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7293, 'learning_rate': 9.648871642240893e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25903/1061708 [3:52:33<156:19:03,  1.84it/s][2024-02-29 21:59:34,352] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4188, 'learning_rate': 9.652262366725691e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6756, 'learning_rate': 9.656029838375467e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5445, 'learning_rate': 9.659797310025242e-05, 'epoch': 0.02}         \n",
      "{'loss': 1.9339, 'learning_rate': 9.663564781675018e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 25941/1061708 [3:52:54<152:29:21,  1.89it/s][2024-02-29 21:59:54,549] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3454, 'learning_rate': 9.666955506159817e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4888, 'learning_rate': 9.670722977809592e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2413, 'learning_rate': 9.674490449459368e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6422, 'learning_rate': 9.678257921109144e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3025, 'learning_rate': 9.68202539275892e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 25999/1061708 [3:53:24<152:44:24,  1.88it/s][2024-02-29 22:00:25,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=291, lr=[9.685792864408696e-05], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 22:00:25,513] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=1.8923438193215043, CurrSamplesPerSec=1.8934627451865562, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.6706, 'learning_rate': 9.685792864408696e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4617, 'learning_rate': 9.689560336058472e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4852, 'learning_rate': 9.693327807708248e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4625, 'learning_rate': 9.697095279358023e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3479, 'learning_rate': 9.700862751007799e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3262, 'learning_rate': 9.704630222657575e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1099, 'learning_rate': 9.708397694307351e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.4497, 'learning_rate': 9.712165165957127e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3572, 'learning_rate': 9.715932637606903e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3548, 'learning_rate': 9.719700109256679e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.265, 'learning_rate': 9.723467580906455e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.391, 'learning_rate': 9.72723505255623e-05, 'epoch': 0.02}           \n",
      "{'loss': 2.3253, 'learning_rate': 9.731002524206006e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 26123/1061708 [3:54:31<156:33:16,  1.84it/s][2024-02-29 22:01:31,661] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2075, 'learning_rate': 9.734393248690804e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7437, 'learning_rate': 9.73816072034058e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5314, 'learning_rate': 9.741928191990356e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2555, 'learning_rate': 9.745695663640132e-05, 'epoch': 0.02}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2605, 'learning_rate': 9.749463135289908e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3192, 'learning_rate': 9.753230606939682e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0732, 'learning_rate': 9.756998078589458e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5796, 'learning_rate': 9.760765550239235e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.0102, 'learning_rate': 9.76453302188901e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.1946, 'learning_rate': 9.768300493538787e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5217, 'learning_rate': 9.772067965188563e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.7749, 'learning_rate': 9.775835436838339e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1313, 'learning_rate': 9.779602908488113e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3115, 'learning_rate': 9.78337038013789e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 26260/1061708 [3:55:44<152:35:30,  1.88it/s][2024-02-29 22:02:44,721] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2347, 'learning_rate': 9.786761104622689e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1607, 'learning_rate': 9.790528576272465e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 26280/1061708 [3:55:54<153:45:14,  1.87it/s][2024-02-29 22:02:55,337] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6386, 'learning_rate': 9.793919300757262e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.683, 'learning_rate': 9.797686772407037e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4868, 'learning_rate': 9.801454244056813e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5544, 'learning_rate': 9.805221715706589e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2419, 'learning_rate': 9.808989187356365e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1682, 'learning_rate': 9.812756659006141e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1078, 'learning_rate': 9.816524130655917e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3717, 'learning_rate': 9.820291602305693e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5104, 'learning_rate': 9.82405907395547e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3282, 'learning_rate': 9.827826545605244e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2377, 'learning_rate': 9.83159401725502e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.3771, 'learning_rate': 9.835361488904796e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5039, 'learning_rate': 9.839128960554572e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.125, 'learning_rate': 9.842896432204348e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4383, 'learning_rate': 9.846663903854124e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 26436/1061708 [3:57:18<153:38:56,  1.87it/s][2024-02-29 22:04:18,552] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1402, 'learning_rate': 9.850054628338922e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.5345, 'learning_rate': 9.853822099988698e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.1169, 'learning_rate': 9.857589571638474e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3606, 'learning_rate': 9.86135704328825e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.5371, 'learning_rate': 9.865124514938026e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.6947, 'learning_rate': 9.868891986587801e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.409, 'learning_rate': 9.872659458237577e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.476, 'learning_rate': 9.876426929887353e-05, 'epoch': 0.02}          \n",
      "  2%|▋                            | 26519/1061708 [3:58:02<152:39:09,  1.88it/s][2024-02-29 22:05:02,872] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4535, 'learning_rate': 9.879817654372151e-05, 'epoch': 0.02}         \n",
      "  2%|▋                            | 26529/1061708 [3:58:07<152:12:35,  1.89it/s][2024-02-29 22:05:08,112] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3695, 'learning_rate': 9.88320837885695e-05, 'epoch': 0.02}          \n",
      "{'loss': 2.4354, 'learning_rate': 9.886975850506726e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.288, 'learning_rate': 9.890743322156502e-05, 'epoch': 0.03}          \n",
      "{'loss': 2.4664, 'learning_rate': 9.894510793806278e-05, 'epoch': 0.03}         \n",
      "  3%|▋                            | 26567/1061708 [3:58:27<153:13:12,  1.88it/s][2024-02-29 22:05:28,315] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.3058, 'learning_rate': 9.897901518291074e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.43, 'learning_rate': 9.90166898994085e-05, 'epoch': 0.03}            \n",
      "{'loss': 2.3336, 'learning_rate': 9.905436461590627e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.6575, 'learning_rate': 9.909203933240403e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.3719, 'learning_rate': 9.912971404890179e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.1357, 'learning_rate': 9.916738876539955e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.3617, 'learning_rate': 9.92050634818973e-05, 'epoch': 0.03}          \n",
      "{'loss': 2.3507, 'learning_rate': 9.924273819839505e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.42, 'learning_rate': 9.928041291489281e-05, 'epoch': 0.03}           \n",
      "{'loss': 2.5657, 'learning_rate': 9.931808763139057e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.2313, 'learning_rate': 9.935576234788834e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.7942, 'learning_rate': 9.93934370643861e-05, 'epoch': 0.03}          \n",
      "{'loss': 2.5675, 'learning_rate': 9.943111178088386e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.5519, 'learning_rate': 9.946878649738162e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.2092, 'learning_rate': 9.950646121387936e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.1917, 'learning_rate': 9.954413593037712e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.7553, 'learning_rate': 9.958181064687488e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.2487, 'learning_rate': 9.961948536337264e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.032, 'learning_rate': 9.96571600798704e-05, 'epoch': 0.03}           \n",
      "{'loss': 2.5629, 'learning_rate': 9.969483479636817e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.1852, 'learning_rate': 9.973250951286593e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.4384, 'learning_rate': 9.977018422936369e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.3935, 'learning_rate': 9.980785894586143e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.1696, 'learning_rate': 9.98455336623592e-05, 'epoch': 0.03}          \n",
      "{'loss': 2.2179, 'learning_rate': 9.988320837885695e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.4, 'learning_rate': 9.992088309535471e-05, 'epoch': 0.03}            \n",
      "{'loss': 2.7135, 'learning_rate': 9.995855781185248e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.1693, 'learning_rate': 9.999623252835024e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.4783, 'learning_rate': 0.00010003390724484798, 'epoch': 0.03}        \n",
      "{'loss': 2.5007, 'learning_rate': 0.00010007158196134574, 'epoch': 0.03}        \n",
      "{'loss': 2.1792, 'learning_rate': 0.0001001092566778435, 'epoch': 0.03}         \n",
      "{'loss': 2.355, 'learning_rate': 0.00010014693139434126, 'epoch': 0.03}         \n",
      "{'loss': 2.1015, 'learning_rate': 0.00010018460611083902, 'epoch': 0.03}        \n",
      "{'loss': 2.6146, 'learning_rate': 0.00010022228082733678, 'epoch': 0.03}        \n",
      "{'loss': 2.2134, 'learning_rate': 0.00010025995554383455, 'epoch': 0.03}        \n",
      "{'loss': 2.6691, 'learning_rate': 0.00010029763026033229, 'epoch': 0.03}        \n",
      "{'loss': 2.1492, 'learning_rate': 0.00010033530497683005, 'epoch': 0.03}        \n",
      "{'loss': 2.2941, 'learning_rate': 0.00010037297969332781, 'epoch': 0.03}        \n",
      "  3%|▋                            | 26941/1061708 [4:01:47<152:32:43,  1.88it/s][2024-02-29 22:08:47,898] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3105, 'learning_rate': 0.00010040688693817579, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5699, 'learning_rate': 0.00010044456165467355, 'epoch': 0.03}        \n",
      "{'loss': 2.5595, 'learning_rate': 0.00010048223637117131, 'epoch': 0.03}        \n",
      "{'loss': 1.8476, 'learning_rate': 0.00010051991108766907, 'epoch': 0.03}        \n",
      "{'loss': 2.2818, 'learning_rate': 0.00010055758580416683, 'epoch': 0.03}        \n",
      "{'loss': 2.4521, 'learning_rate': 0.00010059526052066459, 'epoch': 0.03}        \n",
      "{'loss': 2.4744, 'learning_rate': 0.00010063293523716235, 'epoch': 0.03}        \n",
      "{'loss': 2.1997, 'learning_rate': 0.00010067060995366011, 'epoch': 0.03}        \n",
      "{'loss': 2.5814, 'learning_rate': 0.00010070828467015787, 'epoch': 0.03}        \n",
      "{'loss': 2.1821, 'learning_rate': 0.00010074595938665563, 'epoch': 0.03}        \n",
      "  3%|▋                            | 27042/1061708 [4:02:41<152:39:06,  1.88it/s][2024-02-29 22:09:41,809] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▋                            | 27043/1061708 [4:02:41<146:46:37,  1.96it/s][2024-02-29 22:09:42,234] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1363, 'learning_rate': 0.00010077609915985382, 'epoch': 0.03}        \n",
      "{'loss': 2.1639, 'learning_rate': 0.00010081377387635158, 'epoch': 0.03}        \n",
      "{'loss': 2.5442, 'learning_rate': 0.00010085144859284934, 'epoch': 0.03}        \n",
      "  3%|▋                            | 27072/1061708 [4:02:57<152:56:10,  1.88it/s][2024-02-29 22:09:57,661] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0576, 'learning_rate': 0.00010088535583769731, 'epoch': 0.03}        \n",
      "{'loss': 2.2226, 'learning_rate': 0.00010092303055419507, 'epoch': 0.03}        \n",
      "{'loss': 2.6654, 'learning_rate': 0.00010096070527069284, 'epoch': 0.03}        \n",
      "{'loss': 2.0999, 'learning_rate': 0.0001009983799871906, 'epoch': 0.03}         \n",
      "{'loss': 2.283, 'learning_rate': 0.00010103605470368836, 'epoch': 0.03}         \n",
      "{'loss': 2.1358, 'learning_rate': 0.00010107372942018612, 'epoch': 0.03}        \n",
      "{'loss': 2.2963, 'learning_rate': 0.00010111140413668388, 'epoch': 0.03}        \n",
      "{'loss': 2.1691, 'learning_rate': 0.00010114907885318164, 'epoch': 0.03}        \n",
      "{'loss': 2.5243, 'learning_rate': 0.0001011867535696794, 'epoch': 0.03}         \n",
      "{'loss': 2.2048, 'learning_rate': 0.00010122442828617716, 'epoch': 0.03}        \n",
      "{'loss': 2.4439, 'learning_rate': 0.00010126210300267492, 'epoch': 0.03}        \n",
      "{'loss': 2.2465, 'learning_rate': 0.00010129977771917268, 'epoch': 0.03}        \n",
      "  3%|▋                            | 27196/1061708 [4:04:03<153:37:29,  1.87it/s][2024-02-29 22:11:03,900] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3028, 'learning_rate': 0.00010133368496402064, 'epoch': 0.03}        \n",
      "{'loss': 2.5173, 'learning_rate': 0.0001013713596805184, 'epoch': 0.03}         \n",
      "{'loss': 2.5796, 'learning_rate': 0.00010140903439701616, 'epoch': 0.03}        \n",
      "{'loss': 2.3698, 'learning_rate': 0.00010144670911351392, 'epoch': 0.03}        \n",
      "{'loss': 2.7999, 'learning_rate': 0.00010148438383001169, 'epoch': 0.03}        \n",
      "{'loss': 2.4584, 'learning_rate': 0.00010152205854650945, 'epoch': 0.03}        \n",
      "{'loss': 2.5205, 'learning_rate': 0.0001015597332630072, 'epoch': 0.03}         \n",
      "{'loss': 2.4529, 'learning_rate': 0.00010159740797950497, 'epoch': 0.03}        \n",
      "{'loss': 2.2881, 'learning_rate': 0.00010163508269600273, 'epoch': 0.03}        \n",
      "{'loss': 2.4375, 'learning_rate': 0.00010167275741250049, 'epoch': 0.03}        \n",
      "  3%|▋                            | 27297/1061708 [4:04:57<152:58:11,  1.88it/s][2024-02-29 22:11:57,809] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▋                            | 27298/1061708 [4:04:57<143:32:06,  2.00it/s][2024-02-29 22:11:58,234] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4713, 'learning_rate': 0.00010170289718569868, 'epoch': 0.03}        \n",
      "{'loss': 2.1419, 'learning_rate': 0.00010174057190219643, 'epoch': 0.03}        \n",
      "{'loss': 2.1829, 'learning_rate': 0.00010177824661869419, 'epoch': 0.03}        \n",
      "{'loss': 2.2607, 'learning_rate': 0.00010181592133519195, 'epoch': 0.03}        \n",
      "{'loss': 2.2475, 'learning_rate': 0.00010185359605168971, 'epoch': 0.03}        \n",
      "{'loss': 2.3705, 'learning_rate': 0.00010189127076818747, 'epoch': 0.03}        \n",
      "{'loss': 2.2993, 'learning_rate': 0.00010192894548468523, 'epoch': 0.03}        \n",
      "{'loss': 2.2353, 'learning_rate': 0.00010196662020118299, 'epoch': 0.03}        \n",
      "  3%|▋                            | 27379/1061708 [4:05:40<152:47:05,  1.88it/s][2024-02-29 22:12:41,427] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4189, 'learning_rate': 0.00010200052744603097, 'epoch': 0.03}        \n",
      "{'loss': 2.4173, 'learning_rate': 0.00010203820216252873, 'epoch': 0.03}        \n",
      "{'loss': 2.4101, 'learning_rate': 0.00010207587687902649, 'epoch': 0.03}        \n",
      "{'loss': 2.3475, 'learning_rate': 0.00010211355159552425, 'epoch': 0.03}        \n",
      "{'loss': 2.5408, 'learning_rate': 0.00010215122631202201, 'epoch': 0.03}        \n",
      "{'loss': 2.4159, 'learning_rate': 0.00010218890102851977, 'epoch': 0.03}        \n",
      "{'loss': 2.1036, 'learning_rate': 0.00010222657574501753, 'epoch': 0.03}        \n",
      "{'loss': 2.2332, 'learning_rate': 0.00010226425046151529, 'epoch': 0.03}        \n",
      "{'loss': 2.2219, 'learning_rate': 0.00010230192517801305, 'epoch': 0.03}        \n",
      "{'loss': 2.2196, 'learning_rate': 0.00010233959989451081, 'epoch': 0.03}        \n",
      "{'loss': 2.2563, 'learning_rate': 0.00010237727461100856, 'epoch': 0.03}        \n",
      "{'loss': 2.6537, 'learning_rate': 0.00010241494932750631, 'epoch': 0.03}        \n",
      "{'loss': 2.1084, 'learning_rate': 0.00010245262404400407, 'epoch': 0.03}        \n",
      "{'loss': 2.205, 'learning_rate': 0.00010249029876050183, 'epoch': 0.03}         \n",
      "{'loss': 2.4369, 'learning_rate': 0.00010252797347699959, 'epoch': 0.03}        \n",
      "{'loss': 2.4887, 'learning_rate': 0.00010256564819349735, 'epoch': 0.03}        \n",
      "{'loss': 2.3962, 'learning_rate': 0.00010260332290999511, 'epoch': 0.03}        \n",
      "{'loss': 2.3918, 'learning_rate': 0.00010264099762649286, 'epoch': 0.03}        \n",
      "{'loss': 2.2366, 'learning_rate': 0.00010267867234299062, 'epoch': 0.03}        \n",
      "{'loss': 2.7632, 'learning_rate': 0.00010271634705948838, 'epoch': 0.03}        \n",
      "{'loss': 2.1223, 'learning_rate': 0.00010275402177598614, 'epoch': 0.03}        \n",
      "  3%|▊                            | 27580/1061708 [4:07:28<152:33:53,  1.88it/s][2024-02-29 22:14:28,759] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 27581/1061708 [4:07:28<143:18:58,  2.00it/s][2024-02-29 22:14:29,182] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.7137, 'learning_rate': 0.00010278416154918436, 'epoch': 0.03}        \n",
      "{'loss': 2.6788, 'learning_rate': 0.00010282183626568212, 'epoch': 0.03}        \n",
      "{'loss': 2.566, 'learning_rate': 0.00010285951098217988, 'epoch': 0.03}         \n",
      "{'loss': 2.3385, 'learning_rate': 0.00010289718569867761, 'epoch': 0.03}        \n",
      "  3%|▊                            | 27626/1061708 [4:07:52<154:23:05,  1.86it/s][2024-02-29 22:14:53,173] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8634, 'learning_rate': 0.0001029310929435256, 'epoch': 0.03}         \n",
      "{'loss': 2.256, 'learning_rate': 0.00010296876766002337, 'epoch': 0.03}         \n",
      "{'loss': 1.7983, 'learning_rate': 0.00010300644237652113, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0555, 'learning_rate': 0.00010304411709301889, 'epoch': 0.03}        \n",
      "{'loss': 2.3411, 'learning_rate': 0.00010308179180951665, 'epoch': 0.03}        \n",
      "{'loss': 2.4432, 'learning_rate': 0.00010311946652601441, 'epoch': 0.03}        \n",
      "{'loss': 2.8803, 'learning_rate': 0.00010315714124251214, 'epoch': 0.03}        \n",
      "{'loss': 2.2767, 'learning_rate': 0.0001031948159590099, 'epoch': 0.03}         \n",
      "{'loss': 2.162, 'learning_rate': 0.00010323249067550766, 'epoch': 0.03}         \n",
      "{'loss': 2.3973, 'learning_rate': 0.00010327016539200542, 'epoch': 0.03}        \n",
      "  3%|▊                            | 27728/1061708 [4:08:47<152:49:55,  1.88it/s][2024-02-29 22:15:47,625] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1273, 'learning_rate': 0.00010330407263685343, 'epoch': 0.03}        \n",
      "  3%|▊                            | 27730/1061708 [4:08:48<146:34:39,  1.96it/s][2024-02-29 22:15:48,587] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0182, 'learning_rate': 0.00010333797988170139, 'epoch': 0.03}        \n",
      "{'loss': 2.4326, 'learning_rate': 0.00010337565459819915, 'epoch': 0.03}        \n",
      "{'loss': 2.2882, 'learning_rate': 0.00010341332931469691, 'epoch': 0.03}        \n",
      "{'loss': 2.1711, 'learning_rate': 0.00010345100403119467, 'epoch': 0.03}        \n",
      "{'loss': 2.325, 'learning_rate': 0.00010348867874769243, 'epoch': 0.03}         \n",
      "{'loss': 2.1984, 'learning_rate': 0.00010352635346419019, 'epoch': 0.03}        \n",
      "{'loss': 2.5347, 'learning_rate': 0.00010356402818068795, 'epoch': 0.03}        \n",
      "{'loss': 2.3347, 'learning_rate': 0.00010360170289718569, 'epoch': 0.03}        \n",
      "{'loss': 2.7865, 'learning_rate': 0.00010363937761368345, 'epoch': 0.03}        \n",
      "{'loss': 2.2376, 'learning_rate': 0.00010367705233018121, 'epoch': 0.03}        \n",
      "{'loss': 2.2, 'learning_rate': 0.00010371472704667897, 'epoch': 0.03}           \n",
      "  3%|▊                            | 27841/1061708 [4:09:47<152:24:55,  1.88it/s][2024-02-29 22:16:47,772] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3463, 'learning_rate': 0.00010374863429152697, 'epoch': 0.03}        \n",
      "{'loss': 2.623, 'learning_rate': 0.00010378630900802473, 'epoch': 0.03}         \n",
      "{'loss': 2.2408, 'learning_rate': 0.0001038239837245225, 'epoch': 0.03}         \n",
      "{'loss': 2.7614, 'learning_rate': 0.00010386165844102023, 'epoch': 0.03}        \n",
      "{'loss': 2.2462, 'learning_rate': 0.00010389933315751799, 'epoch': 0.03}        \n",
      "{'loss': 2.4937, 'learning_rate': 0.00010393700787401575, 'epoch': 0.03}        \n",
      "{'loss': 2.258, 'learning_rate': 0.00010397468259051351, 'epoch': 0.03}         \n",
      "{'loss': 2.2439, 'learning_rate': 0.00010401235730701127, 'epoch': 0.03}        \n",
      "{'loss': 2.337, 'learning_rate': 0.00010405003202350903, 'epoch': 0.03}         \n",
      "{'loss': 2.4717, 'learning_rate': 0.00010408770674000679, 'epoch': 0.03}        \n",
      "  3%|▊                            | 27942/1061708 [4:10:41<152:16:26,  1.89it/s][2024-02-29 22:17:41,662] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 27943/1061708 [4:10:41<146:12:22,  1.96it/s][2024-02-29 22:17:42,086] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1645, 'learning_rate': 0.000104117846513205, 'epoch': 0.03}          \n",
      "{'loss': 2.1805, 'learning_rate': 0.00010415552122970276, 'epoch': 0.03}        \n",
      "{'loss': 2.3098, 'learning_rate': 0.00010419319594620052, 'epoch': 0.03}        \n",
      "{'loss': 2.2271, 'learning_rate': 0.00010423087066269828, 'epoch': 0.03}        \n",
      "{'loss': 2.268, 'learning_rate': 0.00010426854537919604, 'epoch': 0.03}         \n",
      "  3%|▊                            | 27999/1061708 [4:11:11<152:06:31,  1.89it/s][2024-02-29 22:18:11,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=314, lr=[0.00010430622009569377], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 22:18:11,960] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=1.8922528145343898, CurrSamplesPerSec=1.899036876858603, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.2149, 'learning_rate': 0.00010430622009569377, 'epoch': 0.03}        \n",
      "{'loss': 2.1418, 'learning_rate': 0.00010434389481219153, 'epoch': 0.03}        \n",
      "{'loss': 2.2844, 'learning_rate': 0.0001043815695286893, 'epoch': 0.03}         \n",
      "{'loss': 2.7148, 'learning_rate': 0.00010441924424518705, 'epoch': 0.03}        \n",
      "{'loss': 2.5946, 'learning_rate': 0.00010445691896168481, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28044/1061708 [4:11:35<154:40:58,  1.86it/s][2024-02-29 22:18:35,857] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 28045/1061708 [4:11:35<144:44:16,  1.98it/s][2024-02-29 22:18:36,282] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2972, 'learning_rate': 0.00010448705873488302, 'epoch': 0.03}        \n",
      "{'loss': 2.396, 'learning_rate': 0.00010452473345138078, 'epoch': 0.03}         \n",
      "  3%|▊                            | 28067/1061708 [4:11:47<153:03:43,  1.88it/s][2024-02-29 22:18:47,966] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0938, 'learning_rate': 0.00010455864069622876, 'epoch': 0.03}        \n",
      "{'loss': 2.5153, 'learning_rate': 0.00010459631541272652, 'epoch': 0.03}        \n",
      "{'loss': 2.2683, 'learning_rate': 0.00010463399012922428, 'epoch': 0.03}        \n",
      "{'loss': 2.0917, 'learning_rate': 0.00010467166484572204, 'epoch': 0.03}        \n",
      "{'loss': 2.3701, 'learning_rate': 0.0001047093395622198, 'epoch': 0.03}         \n",
      "{'loss': 2.5667, 'learning_rate': 0.00010474701427871756, 'epoch': 0.03}        \n",
      "{'loss': 2.0142, 'learning_rate': 0.00010478468899521532, 'epoch': 0.03}        \n",
      "{'loss': 2.3758, 'learning_rate': 0.00010482236371171308, 'epoch': 0.03}        \n",
      "{'loss': 2.4622, 'learning_rate': 0.00010486003842821084, 'epoch': 0.03}        \n",
      "{'loss': 2.6483, 'learning_rate': 0.00010489771314470859, 'epoch': 0.03}        \n",
      "{'loss': 2.5464, 'learning_rate': 0.00010493538786120635, 'epoch': 0.03}        \n",
      "{'loss': 2.5248, 'learning_rate': 0.00010497306257770411, 'epoch': 0.03}        \n",
      "{'loss': 2.1487, 'learning_rate': 0.00010501073729420186, 'epoch': 0.03}        \n",
      "{'loss': 2.4408, 'learning_rate': 0.00010504841201069962, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28202/1061708 [4:12:59<152:18:26,  1.88it/s][2024-02-29 22:20:00,000] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7584, 'learning_rate': 0.00010508231925554761, 'epoch': 0.03}        \n",
      "{'loss': 2.4996, 'learning_rate': 0.00010511999397204537, 'epoch': 0.03}        \n",
      "{'loss': 2.4455, 'learning_rate': 0.00010515766868854313, 'epoch': 0.03}        \n",
      "{'loss': 2.0976, 'learning_rate': 0.00010519534340504089, 'epoch': 0.03}        \n",
      "{'loss': 2.6498, 'learning_rate': 0.00010523301812153865, 'epoch': 0.03}        \n",
      "{'loss': 2.6308, 'learning_rate': 0.00010527069283803639, 'epoch': 0.03}        \n",
      "{'loss': 2.4585, 'learning_rate': 0.00010530836755453415, 'epoch': 0.03}        \n",
      "{'loss': 2.7191, 'learning_rate': 0.00010534604227103191, 'epoch': 0.03}        \n",
      "{'loss': 2.349, 'learning_rate': 0.00010538371698752967, 'epoch': 0.03}         \n",
      "{'loss': 2.3169, 'learning_rate': 0.00010542139170402743, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28303/1061708 [4:13:53<156:21:11,  1.84it/s][2024-02-29 22:20:53,869] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▊                            | 28304/1061708 [4:13:53<145:54:57,  1.97it/s][2024-02-29 22:20:54,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.659, 'learning_rate': 0.00010545153147722564, 'epoch': 0.03}         \n",
      "{'loss': 2.3697, 'learning_rate': 0.0001054892061937234, 'epoch': 0.03}         \n",
      "{'loss': 1.9794, 'learning_rate': 0.00010552688091022116, 'epoch': 0.03}        \n",
      "{'loss': 2.1729, 'learning_rate': 0.00010556455562671892, 'epoch': 0.03}        \n",
      "{'loss': 2.2291, 'learning_rate': 0.00010560223034321668, 'epoch': 0.03}        \n",
      "{'loss': 2.2343, 'learning_rate': 0.00010563990505971444, 'epoch': 0.03}        \n",
      "{'loss': 2.337, 'learning_rate': 0.0001056775797762122, 'epoch': 0.03}          \n",
      "{'loss': 2.767, 'learning_rate': 0.00010571525449270993, 'epoch': 0.03}         \n",
      "{'loss': 2.3137, 'learning_rate': 0.00010575292920920769, 'epoch': 0.03}        \n",
      "{'loss': 2.5996, 'learning_rate': 0.00010579060392570545, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28405/1061708 [4:14:47<153:44:02,  1.87it/s][2024-02-29 22:21:48,025] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 28406/1061708 [4:14:47<143:51:41,  2.00it/s][2024-02-29 22:21:48,447] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.6617, 'learning_rate': 0.00010582074369890366, 'epoch': 0.03}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.00010585841841540142, 'epoch': 0.03}        \n",
      "{'loss': 2.0587, 'learning_rate': 0.00010589609313189918, 'epoch': 0.03}        \n",
      "{'loss': 2.4541, 'learning_rate': 0.00010593376784839694, 'epoch': 0.03}        \n",
      "{'loss': 1.9977, 'learning_rate': 0.0001059714425648947, 'epoch': 0.03}         \n",
      "  3%|▊                            | 28455/1061708 [4:15:13<153:44:24,  1.87it/s][2024-02-29 22:22:14,502] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.346, 'learning_rate': 0.00010600534980974268, 'epoch': 0.03}         \n",
      "{'loss': 2.5099, 'learning_rate': 0.00010604302452624044, 'epoch': 0.03}        \n",
      "{'loss': 2.5917, 'learning_rate': 0.0001060806992427382, 'epoch': 0.03}         \n",
      "{'loss': 2.1912, 'learning_rate': 0.00010611837395923596, 'epoch': 0.03}        \n",
      "{'loss': 2.2787, 'learning_rate': 0.00010615604867573372, 'epoch': 0.03}        \n",
      "{'loss': 2.273, 'learning_rate': 0.00010619372339223148, 'epoch': 0.03}         \n",
      "{'loss': 2.2608, 'learning_rate': 0.00010623139810872924, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28523/1061708 [4:15:50<155:23:44,  1.85it/s][2024-02-29 22:22:50,709] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4563, 'learning_rate': 0.00010626530535357722, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28536/1061708 [4:15:57<152:54:16,  1.88it/s][2024-02-29 22:22:57,578] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.3589, 'learning_rate': 0.00010629921259842521, 'epoch': 0.03}        \n",
      "{'loss': 2.6789, 'learning_rate': 0.00010633688731492295, 'epoch': 0.03}        \n",
      "{'loss': 2.2281, 'learning_rate': 0.0001063745620314207, 'epoch': 0.03}         \n",
      "{'loss': 1.9571, 'learning_rate': 0.00010641223674791847, 'epoch': 0.03}        \n",
      "{'loss': 2.3339, 'learning_rate': 0.00010644991146441623, 'epoch': 0.03}        \n",
      "{'loss': 2.7692, 'learning_rate': 0.00010648758618091399, 'epoch': 0.03}        \n",
      "{'loss': 2.0667, 'learning_rate': 0.00010652526089741175, 'epoch': 0.03}        \n",
      "{'loss': 2.5213, 'learning_rate': 0.00010656293561390951, 'epoch': 0.03}        \n",
      "{'loss': 2.2634, 'learning_rate': 0.00010660061033040727, 'epoch': 0.03}        \n",
      "{'loss': 2.9106, 'learning_rate': 0.00010663828504690503, 'epoch': 0.03}        \n",
      "{'loss': 1.8163, 'learning_rate': 0.00010667595976340279, 'epoch': 0.03}        \n",
      "{'loss': 2.3938, 'learning_rate': 0.00010671363447990055, 'epoch': 0.03}        \n",
      "{'loss': 2.2778, 'learning_rate': 0.00010675130919639831, 'epoch': 0.03}        \n",
      "{'loss': 2.4653, 'learning_rate': 0.00010678898391289607, 'epoch': 0.03}        \n",
      "{'loss': 2.3236, 'learning_rate': 0.00010682665862939383, 'epoch': 0.03}        \n",
      "{'loss': 2.2551, 'learning_rate': 0.00010686433334589159, 'epoch': 0.03}        \n",
      "{'loss': 2.4769, 'learning_rate': 0.00010690200806238933, 'epoch': 0.03}        \n",
      "{'loss': 2.4288, 'learning_rate': 0.00010693968277888709, 'epoch': 0.03}        \n",
      "{'loss': 2.4902, 'learning_rate': 0.00010697735749538485, 'epoch': 0.03}        \n",
      "{'loss': 2.046, 'learning_rate': 0.00010701503221188261, 'epoch': 0.03}         \n",
      "{'loss': 2.3077, 'learning_rate': 0.00010705270692838037, 'epoch': 0.03}        \n",
      "{'loss': 2.3239, 'learning_rate': 0.00010709038164487813, 'epoch': 0.03}        \n",
      "{'loss': 2.0766, 'learning_rate': 0.00010712805636137589, 'epoch': 0.03}        \n",
      "{'loss': 2.1391, 'learning_rate': 0.00010716573107787365, 'epoch': 0.03}        \n",
      "{'loss': 2.2874, 'learning_rate': 0.00010720340579437141, 'epoch': 0.03}        \n",
      "{'loss': 2.3297, 'learning_rate': 0.00010724108051086917, 'epoch': 0.03}        \n",
      "{'loss': 2.6343, 'learning_rate': 0.00010727875522736692, 'epoch': 0.03}        \n",
      "{'loss': 2.3049, 'learning_rate': 0.00010731642994386468, 'epoch': 0.03}        \n",
      "{'loss': 1.8745, 'learning_rate': 0.00010735410466036244, 'epoch': 0.03}        \n",
      "{'loss': 2.4224, 'learning_rate': 0.0001073917793768602, 'epoch': 0.03}         \n",
      "{'loss': 2.3066, 'learning_rate': 0.00010742945409335796, 'epoch': 0.03}        \n",
      "{'loss': 2.4762, 'learning_rate': 0.00010746712880985572, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28858/1061708 [4:18:48<152:30:47,  1.88it/s][2024-02-29 22:25:49,220] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5904, 'learning_rate': 0.0001075010360547037, 'epoch': 0.03}         \n",
      "{'loss': 2.4428, 'learning_rate': 0.00010753871077120146, 'epoch': 0.03}        \n",
      "{'loss': 1.859, 'learning_rate': 0.00010757638548769922, 'epoch': 0.03}         \n",
      "{'loss': 1.9533, 'learning_rate': 0.00010761406020419698, 'epoch': 0.03}        \n",
      "{'loss': 2.5461, 'learning_rate': 0.00010765173492069474, 'epoch': 0.03}        \n",
      "  3%|▊                            | 28905/1061708 [4:19:13<153:45:04,  1.87it/s][2024-02-29 22:26:14,238] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.595, 'learning_rate': 0.00010768564216554272, 'epoch': 0.03}         \n",
      "{'loss': 2.6302, 'learning_rate': 0.00010772331688204048, 'epoch': 0.03}        \n",
      "{'loss': 2.5125, 'learning_rate': 0.00010776099159853824, 'epoch': 0.03}        \n",
      "{'loss': 2.4278, 'learning_rate': 0.00010779866631503598, 'epoch': 0.03}        \n",
      "{'loss': 2.248, 'learning_rate': 0.00010783634103153374, 'epoch': 0.03}         \n",
      "{'loss': 2.3995, 'learning_rate': 0.0001078740157480315, 'epoch': 0.03}         \n",
      "{'loss': 2.3619, 'learning_rate': 0.00010791169046452926, 'epoch': 0.03}        \n",
      "{'loss': 2.3625, 'learning_rate': 0.00010794936518102703, 'epoch': 0.03}        \n",
      "{'loss': 2.304, 'learning_rate': 0.00010798703989752477, 'epoch': 0.03}         \n",
      "{'loss': 2.2271, 'learning_rate': 0.00010802471461402253, 'epoch': 0.03}        \n",
      "{'loss': 2.4584, 'learning_rate': 0.00010806238933052028, 'epoch': 0.03}        \n",
      "{'loss': 2.3548, 'learning_rate': 0.00010810006404701804, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29022/1061708 [4:20:16<152:32:18,  1.88it/s][2024-02-29 22:27:16,737] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1236, 'learning_rate': 0.00010813397129186604, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.327, 'learning_rate': 0.0001081716460083638, 'epoch': 0.03}          \n",
      "{'loss': 2.2533, 'learning_rate': 0.00010820932072486154, 'epoch': 0.03}        \n",
      "{'loss': 2.4622, 'learning_rate': 0.0001082469954413593, 'epoch': 0.03}         \n",
      "{'loss': 2.5459, 'learning_rate': 0.00010828467015785706, 'epoch': 0.03}        \n",
      "{'loss': 2.5387, 'learning_rate': 0.00010832234487435482, 'epoch': 0.03}        \n",
      "{'loss': 2.4611, 'learning_rate': 0.00010836001959085258, 'epoch': 0.03}        \n",
      "{'loss': 2.3798, 'learning_rate': 0.00010839769430735034, 'epoch': 0.03}        \n",
      "{'loss': 1.9441, 'learning_rate': 0.0001084353690238481, 'epoch': 0.03}         \n",
      "{'loss': 2.5508, 'learning_rate': 0.00010847304374034586, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29123/1061708 [4:21:10<155:43:06,  1.84it/s][2024-02-29 22:28:10,656] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 29124/1061708 [4:21:10<145:26:57,  1.97it/s][2024-02-29 22:28:11,080] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4129, 'learning_rate': 0.00010850318351354407, 'epoch': 0.03}        \n",
      "{'loss': 2.7036, 'learning_rate': 0.00010854085823004183, 'epoch': 0.03}        \n",
      "{'loss': 2.2593, 'learning_rate': 0.00010857853294653959, 'epoch': 0.03}        \n",
      "{'loss': 2.4577, 'learning_rate': 0.00010861620766303735, 'epoch': 0.03}        \n",
      "{'loss': 1.9221, 'learning_rate': 0.00010865388237953511, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29173/1061708 [4:21:36<155:22:20,  1.85it/s][2024-02-29 22:28:37,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3634, 'learning_rate': 0.00010868778962438309, 'epoch': 0.03}        \n",
      "{'loss': 2.2591, 'learning_rate': 0.00010872546434088085, 'epoch': 0.03}        \n",
      "{'loss': 2.2343, 'learning_rate': 0.00010876313905737861, 'epoch': 0.03}        \n",
      "{'loss': 2.6489, 'learning_rate': 0.00010880081377387637, 'epoch': 0.03}        \n",
      "{'loss': 1.9935, 'learning_rate': 0.00010883848849037412, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29229/1061708 [4:22:06<152:09:23,  1.88it/s][2024-02-29 22:29:07,020] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4938, 'learning_rate': 0.0001088723957352221, 'epoch': 0.03}         \n",
      "{'loss': 2.2684, 'learning_rate': 0.00010891007045171986, 'epoch': 0.03}        \n",
      "{'loss': 2.5642, 'learning_rate': 0.00010894774516821762, 'epoch': 0.03}        \n",
      "{'loss': 2.0901, 'learning_rate': 0.00010898541988471538, 'epoch': 0.03}        \n",
      "{'loss': 2.4332, 'learning_rate': 0.00010902309460121314, 'epoch': 0.03}        \n",
      "{'loss': 2.4221, 'learning_rate': 0.0001090607693177109, 'epoch': 0.03}         \n",
      "{'loss': 2.1073, 'learning_rate': 0.00010909844403420866, 'epoch': 0.03}        \n",
      "{'loss': 2.0885, 'learning_rate': 0.00010913611875070639, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29305/1061708 [4:22:46<153:32:11,  1.87it/s][2024-02-29 22:29:47,508] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.6648, 'learning_rate': 0.0001091700259955544, 'epoch': 0.03}         \n",
      "{'loss': 2.4003, 'learning_rate': 0.00010920770071205216, 'epoch': 0.03}        \n",
      "{'loss': 2.4424, 'learning_rate': 0.00010924537542854992, 'epoch': 0.03}        \n",
      "{'loss': 2.1826, 'learning_rate': 0.00010928305014504768, 'epoch': 0.03}        \n",
      "{'loss': 2.2893, 'learning_rate': 0.00010932072486154544, 'epoch': 0.03}        \n",
      "{'loss': 2.6584, 'learning_rate': 0.00010935839957804318, 'epoch': 0.03}        \n",
      "{'loss': 2.4316, 'learning_rate': 0.00010939607429454093, 'epoch': 0.03}        \n",
      "{'loss': 2.244, 'learning_rate': 0.00010943374901103869, 'epoch': 0.03}         \n",
      "{'loss': 2.2777, 'learning_rate': 0.00010947142372753645, 'epoch': 0.03}        \n",
      "{'loss': 2.3479, 'learning_rate': 0.00010950909844403421, 'epoch': 0.03}        \n",
      "{'loss': 2.4293, 'learning_rate': 0.00010954677316053197, 'epoch': 0.03}        \n",
      "{'loss': 2.2354, 'learning_rate': 0.00010958444787702973, 'epoch': 0.03}        \n",
      "{'loss': 2.1107, 'learning_rate': 0.00010962212259352748, 'epoch': 0.03}        \n",
      "{'loss': 2.5164, 'learning_rate': 0.00010965979731002524, 'epoch': 0.03}        \n",
      "{'loss': 2.3544, 'learning_rate': 0.000109697472026523, 'epoch': 0.03}          \n",
      "{'loss': 2.4575, 'learning_rate': 0.00010973514674302076, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29467/1061708 [4:24:13<152:50:47,  1.88it/s][2024-02-29 22:31:13,941] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1028, 'learning_rate': 0.00010976905398786874, 'epoch': 0.03}        \n",
      "{'loss': 2.2245, 'learning_rate': 0.0001098067287043665, 'epoch': 0.03}         \n",
      "{'loss': 2.4916, 'learning_rate': 0.00010984440342086426, 'epoch': 0.03}        \n",
      "{'loss': 2.1774, 'learning_rate': 0.00010988207813736202, 'epoch': 0.03}        \n",
      "{'loss': 2.2182, 'learning_rate': 0.00010991975285385978, 'epoch': 0.03}        \n",
      "{'loss': 2.2722, 'learning_rate': 0.00010995742757035754, 'epoch': 0.03}        \n",
      "{'loss': 2.2173, 'learning_rate': 0.0001099951022868553, 'epoch': 0.03}         \n",
      "{'loss': 2.2645, 'learning_rate': 0.00011003277700335306, 'epoch': 0.03}        \n",
      "{'loss': 2.2435, 'learning_rate': 0.00011007045171985082, 'epoch': 0.03}        \n",
      "{'loss': 2.354, 'learning_rate': 0.00011010812643634858, 'epoch': 0.03}         \n",
      "{'loss': 2.5165, 'learning_rate': 0.00011014580115284634, 'epoch': 0.03}        \n",
      "{'loss': 2.4997, 'learning_rate': 0.0001101834758693441, 'epoch': 0.03}         \n",
      "{'loss': 2.368, 'learning_rate': 0.00011022115058584184, 'epoch': 0.03}         \n",
      "{'loss': 2.0578, 'learning_rate': 0.0001102588253023396, 'epoch': 0.03}         \n",
      "{'loss': 2.2422, 'learning_rate': 0.00011029650001883736, 'epoch': 0.03}        \n",
      "{'loss': 2.7801, 'learning_rate': 0.00011033417473533512, 'epoch': 0.03}        \n",
      "{'loss': 2.2184, 'learning_rate': 0.00011037184945183288, 'epoch': 0.03}        \n",
      "{'loss': 2.3772, 'learning_rate': 0.00011040952416833064, 'epoch': 0.03}        \n",
      "{'loss': 2.1746, 'learning_rate': 0.0001104471988848284, 'epoch': 0.03}         \n",
      "{'loss': 2.1999, 'learning_rate': 0.00011048487360132616, 'epoch': 0.03}        \n",
      "{'loss': 2.2824, 'learning_rate': 0.00011052254831782392, 'epoch': 0.03}        \n",
      "{'loss': 2.3676, 'learning_rate': 0.00011056022303432167, 'epoch': 0.03}        \n",
      "{'loss': 1.9005, 'learning_rate': 0.00011059789775081943, 'epoch': 0.03}        \n",
      "{'loss': 2.5423, 'learning_rate': 0.00011063557246731719, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29708/1061708 [4:26:22<152:26:10,  1.88it/s][2024-02-29 22:33:22,542] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5851, 'learning_rate': 0.00011066947971216517, 'epoch': 0.03}        \n",
      "{'loss': 1.8951, 'learning_rate': 0.00011070715442866293, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29724/1061708 [4:26:30<154:32:38,  1.85it/s][2024-02-29 22:33:31,013] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4121, 'learning_rate': 0.0001107410616735109, 'epoch': 0.03}         \n",
      "{'loss': 2.2751, 'learning_rate': 0.00011077873639000867, 'epoch': 0.03}        \n",
      "{'loss': 2.5284, 'learning_rate': 0.00011081641110650643, 'epoch': 0.03}        \n",
      "{'loss': 2.4549, 'learning_rate': 0.00011085408582300419, 'epoch': 0.03}        \n",
      "{'loss': 2.2018, 'learning_rate': 0.00011089176053950195, 'epoch': 0.03}        \n",
      "{'loss': 2.6682, 'learning_rate': 0.00011092943525599971, 'epoch': 0.03}        \n",
      "{'loss': 2.1297, 'learning_rate': 0.00011096710997249747, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3658, 'learning_rate': 0.00011100478468899523, 'epoch': 0.03}        \n",
      "{'loss': 2.3808, 'learning_rate': 0.00011104245940549299, 'epoch': 0.03}        \n",
      "{'loss': 1.8901, 'learning_rate': 0.00011108013412199074, 'epoch': 0.03}        \n",
      "{'loss': 2.2187, 'learning_rate': 0.0001111178088384885, 'epoch': 0.03}         \n",
      "{'loss': 2.3543, 'learning_rate': 0.00011115548355498626, 'epoch': 0.03}        \n",
      "{'loss': 2.1852, 'learning_rate': 0.00011119315827148402, 'epoch': 0.03}        \n",
      "{'loss': 2.4116, 'learning_rate': 0.00011123083298798178, 'epoch': 0.03}        \n",
      "{'loss': 2.1789, 'learning_rate': 0.00011126850770447952, 'epoch': 0.03}        \n",
      "{'loss': 2.5165, 'learning_rate': 0.00011130618242097728, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29886/1061708 [4:27:57<153:18:08,  1.87it/s][2024-02-29 22:34:57,532] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3975, 'learning_rate': 0.00011134008966582528, 'epoch': 0.03}        \n",
      "{'loss': 2.1608, 'learning_rate': 0.00011137776438232304, 'epoch': 0.03}        \n",
      "{'loss': 2.3326, 'learning_rate': 0.0001114154390988208, 'epoch': 0.03}         \n",
      "{'loss': 2.148, 'learning_rate': 0.00011145311381531856, 'epoch': 0.03}         \n",
      "{'loss': 2.1302, 'learning_rate': 0.00011149078853181629, 'epoch': 0.03}        \n",
      "{'loss': 2.4404, 'learning_rate': 0.00011152846324831405, 'epoch': 0.03}        \n",
      "{'loss': 1.7242, 'learning_rate': 0.00011156613796481181, 'epoch': 0.03}        \n",
      "{'loss': 2.1985, 'learning_rate': 0.00011160381268130957, 'epoch': 0.03}        \n",
      "{'loss': 2.5592, 'learning_rate': 0.00011164148739780733, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29979/1061708 [4:28:46<152:02:28,  1.88it/s][2024-02-29 22:35:47,154] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4025, 'learning_rate': 0.00011167539464265532, 'epoch': 0.03}        \n",
      "{'loss': 2.2955, 'learning_rate': 0.00011171306935915308, 'epoch': 0.03}        \n",
      "  3%|▊                            | 29999/1061708 [4:28:57<152:19:04,  1.88it/s][2024-02-29 22:35:57,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=338, lr=[0.00011175074407565083], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 22:35:57,840] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=1.892231915282409, CurrSamplesPerSec=1.8955369982546415, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.317, 'learning_rate': 0.00011175074407565083, 'epoch': 0.03}         \n",
      "  3%|▊                            | 30000/1061708 [4:28:57<152:22:37,  1.88it/s][INFO|trainer.py:2868] 2024-02-29 22:35:57,842 >> Saving model checkpoint to output_model/checkpoint-30000\n",
      "[INFO|trainer.py:2880] 2024-02-29 22:35:57,845 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 22:35:59,055 >> tokenizer config file saved in output_model/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 22:35:59,055 >> Special tokens file saved in output_model/checkpoint-30000/special_tokens_map.json\n",
      "[2024-02-29 22:35:59,057] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30000 is about to be saved!\n",
      "[2024-02-29 22:36:04,279] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-30000/global_step30000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 22:36:04,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-30000/global_step30000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 22:36:18,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-30000/global_step30000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 22:36:18,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-30000/global_step30000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 22:36:25,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-30000/global_step30000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 22:36:25,944] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-30000/global_step30000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 22:36:25,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-29 22:36:25,972 >> Deleting older checkpoint [output_model/checkpoint-15000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 22:36:29,728 >> tokenizer config file saved in output_model/checkpoint-30000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 22:36:29,729 >> Special tokens file saved in output_model/checkpoint-30000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.465, 'learning_rate': 0.00011178841879214859, 'epoch': 0.03}         \n",
      "{'loss': 1.9594, 'learning_rate': 0.00011182609350864635, 'epoch': 0.03}        \n",
      "{'loss': 2.3482, 'learning_rate': 0.0001118637682251441, 'epoch': 0.03}         \n",
      "{'loss': 1.9971, 'learning_rate': 0.00011190144294164186, 'epoch': 0.03}        \n",
      "{'loss': 2.4393, 'learning_rate': 0.00011193911765813962, 'epoch': 0.03}        \n",
      "{'loss': 2.0438, 'learning_rate': 0.00011197679237463738, 'epoch': 0.03}        \n",
      "{'loss': 2.2405, 'learning_rate': 0.00011201446709113514, 'epoch': 0.03}        \n",
      "{'loss': 2.1057, 'learning_rate': 0.0001120521418076329, 'epoch': 0.03}         \n",
      "{'loss': 2.2847, 'learning_rate': 0.00011208981652413066, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30095/1061708 [4:30:20<153:26:28,  1.87it/s][2024-02-29 22:37:20,669] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3141, 'learning_rate': 0.00011212372376897864, 'epoch': 0.03}        \n",
      "{'loss': 1.9546, 'learning_rate': 0.0001121613984854764, 'epoch': 0.03}         \n",
      "{'loss': 2.5764, 'learning_rate': 0.00011219907320197416, 'epoch': 0.03}        \n",
      "{'loss': 2.0326, 'learning_rate': 0.00011223674791847192, 'epoch': 0.03}        \n",
      "{'loss': 2.2718, 'learning_rate': 0.00011227442263496968, 'epoch': 0.03}        \n",
      "{'loss': 2.2574, 'learning_rate': 0.00011231209735146744, 'epoch': 0.03}        \n",
      "{'loss': 2.4019, 'learning_rate': 0.0001123497720679652, 'epoch': 0.03}         \n",
      "{'loss': 2.3078, 'learning_rate': 0.00011238744678446296, 'epoch': 0.03}        \n",
      "{'loss': 2.2004, 'learning_rate': 0.00011242512150096072, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30184/1061708 [4:31:07<154:51:36,  1.85it/s][2024-02-29 22:38:08,258] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.244, 'learning_rate': 0.00011245902874580869, 'epoch': 0.03}         \n",
      "{'loss': 2.2851, 'learning_rate': 0.00011249670346230645, 'epoch': 0.03}        \n",
      "{'loss': 2.4762, 'learning_rate': 0.00011253437817880421, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30213/1061708 [4:31:23<155:48:29,  1.84it/s][2024-02-29 22:38:23,725] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3484, 'learning_rate': 0.00011256828542365219, 'epoch': 0.03}        \n",
      "{'loss': 2.5469, 'learning_rate': 0.00011260596014014995, 'epoch': 0.03}        \n",
      "{'loss': 2.1003, 'learning_rate': 0.0001126436348566477, 'epoch': 0.03}         \n",
      "{'loss': 2.2636, 'learning_rate': 0.00011268130957314547, 'epoch': 0.03}        \n",
      "{'loss': 2.1736, 'learning_rate': 0.00011271898428964323, 'epoch': 0.03}        \n",
      "{'loss': 2.1791, 'learning_rate': 0.00011275665900614099, 'epoch': 0.03}        \n",
      "{'loss': 2.5007, 'learning_rate': 0.00011279433372263875, 'epoch': 0.03}        \n",
      "{'loss': 2.3441, 'learning_rate': 0.00011283200843913651, 'epoch': 0.03}        \n",
      "{'loss': 2.3074, 'learning_rate': 0.00011286968315563427, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0759, 'learning_rate': 0.00011290735787213203, 'epoch': 0.03}        \n",
      "{'loss': 2.445, 'learning_rate': 0.00011294503258862979, 'epoch': 0.03}         \n",
      "{'loss': 2.3042, 'learning_rate': 0.00011298270730512755, 'epoch': 0.03}        \n",
      "{'loss': 2.0895, 'learning_rate': 0.00011302038202162528, 'epoch': 0.03}        \n",
      "{'loss': 1.9383, 'learning_rate': 0.00011305805673812304, 'epoch': 0.03}        \n",
      "{'loss': 2.3005, 'learning_rate': 0.0001130957314546208, 'epoch': 0.03}         \n",
      "{'loss': 2.2909, 'learning_rate': 0.00011313340617111856, 'epoch': 0.03}        \n",
      "{'loss': 2.2866, 'learning_rate': 0.00011317108088761633, 'epoch': 0.03}        \n",
      "{'loss': 2.3387, 'learning_rate': 0.00011320875560411409, 'epoch': 0.03}        \n",
      "{'loss': 2.1898, 'learning_rate': 0.00011324643032061185, 'epoch': 0.03}        \n",
      "{'loss': 2.2738, 'learning_rate': 0.0001132841050371096, 'epoch': 0.03}         \n",
      "{'loss': 2.1645, 'learning_rate': 0.00011332177975360735, 'epoch': 0.03}        \n",
      "{'loss': 2.6451, 'learning_rate': 0.00011335945447010511, 'epoch': 0.03}        \n",
      "{'loss': 2.4144, 'learning_rate': 0.00011339712918660287, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30444/1061708 [4:33:26<154:38:50,  1.85it/s][2024-02-29 22:40:27,437] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3063, 'learning_rate': 0.00011343103643145085, 'epoch': 0.03}        \n",
      "{'loss': 2.1893, 'learning_rate': 0.00011346871114794861, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30466/1061708 [4:33:38<152:54:05,  1.87it/s][2024-02-29 22:40:39,120] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3302, 'learning_rate': 0.00011350261839279659, 'epoch': 0.03}        \n",
      "{'loss': 2.3265, 'learning_rate': 0.00011354029310929435, 'epoch': 0.03}        \n",
      "{'loss': 2.6854, 'learning_rate': 0.00011357796782579211, 'epoch': 0.03}        \n",
      "{'loss': 2.5084, 'learning_rate': 0.00011361564254228987, 'epoch': 0.03}        \n",
      "{'loss': 1.9914, 'learning_rate': 0.00011365331725878763, 'epoch': 0.03}        \n",
      "{'loss': 1.9155, 'learning_rate': 0.00011369099197528539, 'epoch': 0.03}        \n",
      "{'loss': 2.1432, 'learning_rate': 0.00011372866669178315, 'epoch': 0.03}        \n",
      "{'loss': 2.5414, 'learning_rate': 0.00011376634140828091, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30543/1061708 [4:34:19<155:45:13,  1.84it/s][2024-02-29 22:41:20,248] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5312, 'learning_rate': 0.00011380024865312888, 'epoch': 0.03}        \n",
      "{'loss': 2.3602, 'learning_rate': 0.00011383792336962664, 'epoch': 0.03}        \n",
      "{'loss': 2.2647, 'learning_rate': 0.0001138755980861244, 'epoch': 0.03}         \n",
      "{'loss': 2.1775, 'learning_rate': 0.00011391327280262216, 'epoch': 0.03}        \n",
      "{'loss': 1.8688, 'learning_rate': 0.00011395094751911992, 'epoch': 0.03}        \n",
      "{'loss': 2.3259, 'learning_rate': 0.00011398862223561768, 'epoch': 0.03}        \n",
      "{'loss': 2.5981, 'learning_rate': 0.00011402629695211544, 'epoch': 0.03}        \n",
      "{'loss': 2.6406, 'learning_rate': 0.0001140639716686132, 'epoch': 0.03}         \n",
      "{'loss': 2.3104, 'learning_rate': 0.00011410164638511096, 'epoch': 0.03}        \n",
      "{'loss': 2.1268, 'learning_rate': 0.00011413932110160872, 'epoch': 0.03}        \n",
      "{'loss': 2.4113, 'learning_rate': 0.00011417699581810648, 'epoch': 0.03}        \n",
      "{'loss': 2.5181, 'learning_rate': 0.00011421467053460424, 'epoch': 0.03}        \n",
      "{'loss': 2.2946, 'learning_rate': 0.000114252345251102, 'epoch': 0.03}          \n",
      "{'loss': 2.3988, 'learning_rate': 0.00011429001996759976, 'epoch': 0.03}        \n",
      "{'loss': 2.1567, 'learning_rate': 0.0001143276946840975, 'epoch': 0.03}         \n",
      "{'loss': 2.4705, 'learning_rate': 0.00011436536940059526, 'epoch': 0.03}        \n",
      "{'loss': 2.0562, 'learning_rate': 0.00011440304411709302, 'epoch': 0.03}        \n",
      "{'loss': 1.9868, 'learning_rate': 0.00011444071883359078, 'epoch': 0.03}        \n",
      "{'loss': 2.5273, 'learning_rate': 0.00011447839355008854, 'epoch': 0.03}        \n",
      "{'loss': 1.9514, 'learning_rate': 0.0001145160682665863, 'epoch': 0.03}         \n",
      "  3%|▊                            | 30742/1061708 [4:36:05<151:57:18,  1.88it/s][2024-02-29 22:43:06,509] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.572, 'learning_rate': 0.00011454997551143428, 'epoch': 0.03}         \n",
      "{'loss': 2.1117, 'learning_rate': 0.00011458765022793204, 'epoch': 0.03}        \n",
      "{'loss': 2.2159, 'learning_rate': 0.0001146253249444298, 'epoch': 0.03}         \n",
      "{'loss': 2.5758, 'learning_rate': 0.00011466299966092754, 'epoch': 0.03}        \n",
      "{'loss': 2.3071, 'learning_rate': 0.0001147006743774253, 'epoch': 0.03}         \n",
      "{'loss': 2.3277, 'learning_rate': 0.00011473834909392306, 'epoch': 0.03}        \n",
      "{'loss': 2.0744, 'learning_rate': 0.00011477602381042083, 'epoch': 0.03}        \n",
      "{'loss': 2.2866, 'learning_rate': 0.00011481369852691859, 'epoch': 0.03}        \n",
      "{'loss': 2.444, 'learning_rate': 0.00011485137324341635, 'epoch': 0.03}         \n",
      "{'loss': 2.1301, 'learning_rate': 0.0001148890479599141, 'epoch': 0.03}         \n",
      "{'loss': 2.2083, 'learning_rate': 0.00011492672267641187, 'epoch': 0.03}        \n",
      "{'loss': 1.926, 'learning_rate': 0.00011496439739290963, 'epoch': 0.03}         \n",
      "{'loss': 2.0855, 'learning_rate': 0.00011500207210940739, 'epoch': 0.03}        \n",
      "{'loss': 2.3222, 'learning_rate': 0.00011503974682590515, 'epoch': 0.03}        \n",
      "{'loss': 2.2181, 'learning_rate': 0.00011507742154240291, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30894/1061708 [4:37:27<154:45:04,  1.85it/s][2024-02-29 22:44:27,654] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3593, 'learning_rate': 0.00011511132878725089, 'epoch': 0.03}        \n",
      "{'loss': 2.0087, 'learning_rate': 0.00011514900350374865, 'epoch': 0.03}        \n",
      "{'loss': 2.1388, 'learning_rate': 0.00011518667822024641, 'epoch': 0.03}        \n",
      "{'loss': 2.2725, 'learning_rate': 0.00011522435293674417, 'epoch': 0.03}        \n",
      "{'loss': 2.0458, 'learning_rate': 0.00011526202765324193, 'epoch': 0.03}        \n",
      "{'loss': 2.3568, 'learning_rate': 0.00011529970236973968, 'epoch': 0.03}        \n",
      "  3%|▊                            | 30959/1061708 [4:38:01<152:25:31,  1.88it/s][2024-02-29 22:45:02,340] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4847, 'learning_rate': 0.00011533360961458765, 'epoch': 0.03}        \n",
      "{'loss': 2.4428, 'learning_rate': 0.00011537128433108541, 'epoch': 0.03}        \n",
      "{'loss': 2.3374, 'learning_rate': 0.00011540895904758317, 'epoch': 0.03}        \n",
      "{'loss': 2.0038, 'learning_rate': 0.00011544663376408093, 'epoch': 0.03}        \n",
      "{'loss': 2.1646, 'learning_rate': 0.0001154843084805787, 'epoch': 0.03}         \n",
      "{'loss': 2.6387, 'learning_rate': 0.00011552198319707646, 'epoch': 0.03}        \n",
      "{'loss': 2.3128, 'learning_rate': 0.00011555965791357422, 'epoch': 0.03}        \n",
      "{'loss': 2.5158, 'learning_rate': 0.00011559733263007198, 'epoch': 0.03}        \n",
      "{'loss': 2.2671, 'learning_rate': 0.00011563500734656971, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31045/1061708 [4:38:47<153:56:12,  1.86it/s][2024-02-29 22:45:48,211] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2378, 'learning_rate': 0.00011566891459141771, 'epoch': 0.03}        \n",
      "{'loss': 2.3117, 'learning_rate': 0.00011570658930791547, 'epoch': 0.03}        \n",
      "{'loss': 2.4364, 'learning_rate': 0.00011574426402441324, 'epoch': 0.03}        \n",
      "{'loss': 2.1943, 'learning_rate': 0.000115781938740911, 'epoch': 0.03}          \n",
      "{'loss': 2.1795, 'learning_rate': 0.00011581961345740874, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.366, 'learning_rate': 0.00011585728817390649, 'epoch': 0.03}         \n",
      "{'loss': 2.2282, 'learning_rate': 0.00011589496289040425, 'epoch': 0.03}        \n",
      "{'loss': 2.0393, 'learning_rate': 0.00011593263760690201, 'epoch': 0.03}        \n",
      "{'loss': 2.413, 'learning_rate': 0.00011597031232339977, 'epoch': 0.03}         \n",
      "{'loss': 2.2574, 'learning_rate': 0.00011600798703989753, 'epoch': 0.03}        \n",
      "{'loss': 2.2745, 'learning_rate': 0.00011604566175639529, 'epoch': 0.03}        \n",
      "{'loss': 2.4072, 'learning_rate': 0.00011608333647289304, 'epoch': 0.03}        \n",
      "{'loss': 2.6318, 'learning_rate': 0.0001161210111893908, 'epoch': 0.03}         \n",
      "{'loss': 2.3697, 'learning_rate': 0.00011615868590588856, 'epoch': 0.03}        \n",
      "{'loss': 2.2924, 'learning_rate': 0.00011619636062238632, 'epoch': 0.03}        \n",
      "{'loss': 2.3548, 'learning_rate': 0.00011623403533888408, 'epoch': 0.03}        \n",
      "{'loss': 2.3954, 'learning_rate': 0.00011627171005538184, 'epoch': 0.03}        \n",
      "{'loss': 2.4762, 'learning_rate': 0.0001163093847718796, 'epoch': 0.03}         \n",
      "{'loss': 2.0546, 'learning_rate': 0.00011634705948837736, 'epoch': 0.03}        \n",
      "{'loss': 2.1855, 'learning_rate': 0.00011638473420487512, 'epoch': 0.03}        \n",
      "{'loss': 1.7625, 'learning_rate': 0.00011642240892137288, 'epoch': 0.03}        \n",
      "{'loss': 2.0479, 'learning_rate': 0.00011646008363787062, 'epoch': 0.03}        \n",
      "{'loss': 2.3134, 'learning_rate': 0.00011649775835436838, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31277/1061708 [4:40:51<152:23:26,  1.88it/s][2024-02-29 22:47:52,106] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2188, 'learning_rate': 0.00011653166559921638, 'epoch': 0.03}        \n",
      "{'loss': 2.1987, 'learning_rate': 0.00011656934031571414, 'epoch': 0.03}        \n",
      "{'loss': 2.1769, 'learning_rate': 0.0001166070150322119, 'epoch': 0.03}         \n",
      "  3%|▊                            | 31308/1061708 [4:41:08<152:02:49,  1.88it/s][2024-02-29 22:48:08,572] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2901, 'learning_rate': 0.00011664092227705987, 'epoch': 0.03}        \n",
      "{'loss': 1.9194, 'learning_rate': 0.00011667859699355763, 'epoch': 0.03}        \n",
      "{'loss': 2.3523, 'learning_rate': 0.00011671627171005539, 'epoch': 0.03}        \n",
      "{'loss': 2.2636, 'learning_rate': 0.00011675394642655315, 'epoch': 0.03}        \n",
      "{'loss': 2.2621, 'learning_rate': 0.00011679162114305091, 'epoch': 0.03}        \n",
      "{'loss': 2.4821, 'learning_rate': 0.00011682929585954867, 'epoch': 0.03}        \n",
      "{'loss': 2.7062, 'learning_rate': 0.00011686697057604643, 'epoch': 0.03}        \n",
      "{'loss': 1.9363, 'learning_rate': 0.00011690464529254416, 'epoch': 0.03}        \n",
      "{'loss': 2.3802, 'learning_rate': 0.00011694232000904192, 'epoch': 0.03}        \n",
      "{'loss': 2.0737, 'learning_rate': 0.00011697999472553968, 'epoch': 0.03}        \n",
      "{'loss': 2.6353, 'learning_rate': 0.00011701766944203744, 'epoch': 0.03}        \n",
      "{'loss': 2.3595, 'learning_rate': 0.0001170553441585352, 'epoch': 0.03}         \n",
      "{'loss': 2.361, 'learning_rate': 0.00011709301887503296, 'epoch': 0.03}         \n",
      "{'loss': 2.2105, 'learning_rate': 0.00011713069359153072, 'epoch': 0.03}        \n",
      "{'loss': 2.2128, 'learning_rate': 0.00011716836830802849, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31459/1061708 [4:42:28<152:01:25,  1.88it/s][2024-02-29 22:49:29,147] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6378, 'learning_rate': 0.00011720227555287646, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31460/1061708 [4:42:29<143:15:11,  2.00it/s][2024-02-29 22:49:29,578] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1962, 'learning_rate': 0.00011723618279772445, 'epoch': 0.03}        \n",
      "{'loss': 2.1637, 'learning_rate': 0.00011727385751422221, 'epoch': 0.03}        \n",
      "{'loss': 2.2414, 'learning_rate': 0.00011731153223071997, 'epoch': 0.03}        \n",
      "{'loss': 2.2483, 'learning_rate': 0.00011734920694721774, 'epoch': 0.03}        \n",
      "{'loss': 2.0575, 'learning_rate': 0.00011738688166371548, 'epoch': 0.03}        \n",
      "{'loss': 2.3251, 'learning_rate': 0.00011742455638021323, 'epoch': 0.03}        \n",
      "{'loss': 2.0182, 'learning_rate': 0.00011746223109671099, 'epoch': 0.03}        \n",
      "{'loss': 2.4373, 'learning_rate': 0.00011749990581320875, 'epoch': 0.03}        \n",
      "{'loss': 2.3829, 'learning_rate': 0.00011753758052970651, 'epoch': 0.03}        \n",
      "{'loss': 2.1978, 'learning_rate': 0.00011757525524620427, 'epoch': 0.03}        \n",
      "{'loss': 2.0779, 'learning_rate': 0.00011761292996270203, 'epoch': 0.03}        \n",
      "{'loss': 2.005, 'learning_rate': 0.00011765060467919979, 'epoch': 0.03}         \n",
      "{'loss': 2.1949, 'learning_rate': 0.00011768827939569755, 'epoch': 0.03}        \n",
      "{'loss': 2.0901, 'learning_rate': 0.00011772595411219531, 'epoch': 0.03}        \n",
      "{'loss': 2.4119, 'learning_rate': 0.00011776362882869307, 'epoch': 0.03}        \n",
      "{'loss': 1.9421, 'learning_rate': 0.00011780130354519083, 'epoch': 0.03}        \n",
      "{'loss': 2.316, 'learning_rate': 0.0001178389782616886, 'epoch': 0.03}          \n",
      "{'loss': 2.4613, 'learning_rate': 0.00011787665297818635, 'epoch': 0.03}        \n",
      "{'loss': 2.3016, 'learning_rate': 0.00011791432769468411, 'epoch': 0.03}        \n",
      "{'loss': 2.1579, 'learning_rate': 0.00011795200241118188, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31661/1061708 [4:44:16<151:52:00,  1.88it/s][2024-02-29 22:51:16,884] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▊                            | 31662/1061708 [4:44:16<142:35:31,  2.01it/s][2024-02-29 22:51:17,307] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.6486, 'learning_rate': 0.00011798214218438006, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31676/1061708 [4:44:24<152:59:30,  1.87it/s][2024-02-29 22:51:24,719] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.29, 'learning_rate': 0.00011801604942922806, 'epoch': 0.03}          \n",
      "{'loss': 2.4116, 'learning_rate': 0.00011805372414572582, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31690/1061708 [4:44:31<151:46:34,  1.89it/s][2024-02-29 22:51:32,126] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5819, 'learning_rate': 0.0001180876313905738, 'epoch': 0.03}         \n",
      "{'loss': 2.1985, 'learning_rate': 0.00011812530610707156, 'epoch': 0.03}        \n",
      "{'loss': 2.5744, 'learning_rate': 0.00011816298082356932, 'epoch': 0.03}        \n",
      "{'loss': 2.5229, 'learning_rate': 0.00011820065554006707, 'epoch': 0.03}        \n",
      "{'loss': 2.2768, 'learning_rate': 0.00011823833025656483, 'epoch': 0.03}        \n",
      "{'loss': 2.3034, 'learning_rate': 0.00011827600497306259, 'epoch': 0.03}        \n",
      "{'loss': 2.2932, 'learning_rate': 0.00011831367968956035, 'epoch': 0.03}        \n",
      "{'loss': 1.9836, 'learning_rate': 0.0001183513544060581, 'epoch': 0.03}         \n",
      "{'loss': 2.1141, 'learning_rate': 0.00011838902912255586, 'epoch': 0.03}        \n",
      "{'loss': 2.1521, 'learning_rate': 0.00011842670383905362, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31795/1061708 [4:45:27<153:34:57,  1.86it/s][2024-02-29 22:52:28,125] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5621, 'learning_rate': 0.00011846061108390161, 'epoch': 0.03}        \n",
      "{'loss': 2.5172, 'learning_rate': 0.00011849828580039937, 'epoch': 0.03}        \n",
      "{'loss': 2.2753, 'learning_rate': 0.00011853596051689713, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2959, 'learning_rate': 0.00011857363523339486, 'epoch': 0.03}        \n",
      "{'loss': 2.4294, 'learning_rate': 0.00011861130994989262, 'epoch': 0.03}        \n",
      "{'loss': 2.1244, 'learning_rate': 0.00011864898466639038, 'epoch': 0.03}        \n",
      "{'loss': 2.3487, 'learning_rate': 0.00011868665938288814, 'epoch': 0.03}        \n",
      "{'loss': 2.5257, 'learning_rate': 0.0001187243340993859, 'epoch': 0.03}         \n",
      "{'loss': 2.4045, 'learning_rate': 0.00011876200881588366, 'epoch': 0.03}        \n",
      "{'loss': 2.4865, 'learning_rate': 0.00011879968353238142, 'epoch': 0.03}        \n",
      "{'loss': 2.7456, 'learning_rate': 0.00011883735824887918, 'epoch': 0.03}        \n",
      "{'loss': 2.1551, 'learning_rate': 0.00011887503296537695, 'epoch': 0.03}        \n",
      "{'loss': 1.8215, 'learning_rate': 0.0001189127076818747, 'epoch': 0.03}         \n",
      "{'loss': 2.146, 'learning_rate': 0.00011895038239837247, 'epoch': 0.03}         \n",
      "  3%|▊                            | 31936/1061708 [4:46:42<152:42:12,  1.87it/s][2024-02-29 22:53:43,424] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3839, 'learning_rate': 0.00011898428964322043, 'epoch': 0.03}        \n",
      "{'loss': 2.2883, 'learning_rate': 0.00011902196435971819, 'epoch': 0.03}        \n",
      "{'loss': 2.394, 'learning_rate': 0.00011905963907621595, 'epoch': 0.03}         \n",
      "{'loss': 2.3869, 'learning_rate': 0.00011909731379271371, 'epoch': 0.03}        \n",
      "{'loss': 2.0116, 'learning_rate': 0.00011913498850921147, 'epoch': 0.03}        \n",
      "{'loss': 2.6232, 'learning_rate': 0.00011917266322570923, 'epoch': 0.03}        \n",
      "  3%|▊                            | 31999/1061708 [4:47:16<151:56:49,  1.88it/s][2024-02-29 22:54:17,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=358, lr=[0.00011921033794220699], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 22:54:17,141] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=1.8920664062872878, CurrSamplesPerSec=1.894475344246479, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.235, 'learning_rate': 0.00011921033794220699, 'epoch': 0.03}         \n",
      "  3%|▊                            | 32002/1061708 [4:47:18<152:13:05,  1.88it/s][2024-02-29 22:54:18,675] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  3%|▊                            | 32005/1061708 [4:47:19<149:34:53,  1.91it/s][2024-02-29 22:54:20,215] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2909, 'learning_rate': 0.0001192404777154052, 'epoch': 0.03}         \n",
      "{'loss': 2.519, 'learning_rate': 0.00011927815243190295, 'epoch': 0.03}         \n",
      "{'loss': 2.3326, 'learning_rate': 0.00011931582714840071, 'epoch': 0.03}        \n",
      "{'loss': 2.2402, 'learning_rate': 0.00011935350186489847, 'epoch': 0.03}        \n",
      "{'loss': 2.5015, 'learning_rate': 0.00011939117658139623, 'epoch': 0.03}        \n",
      "{'loss': 2.0794, 'learning_rate': 0.00011942885129789399, 'epoch': 0.03}        \n",
      "{'loss': 2.3519, 'learning_rate': 0.00011946652601439175, 'epoch': 0.03}        \n",
      "{'loss': 2.4176, 'learning_rate': 0.0001195042007308895, 'epoch': 0.03}         \n",
      "{'loss': 1.7012, 'learning_rate': 0.00011954187544738726, 'epoch': 0.03}        \n",
      "{'loss': 2.0604, 'learning_rate': 0.00011957955016388502, 'epoch': 0.03}        \n",
      "{'loss': 2.173, 'learning_rate': 0.00011961722488038278, 'epoch': 0.03}         \n",
      "{'loss': 2.4266, 'learning_rate': 0.00011965489959688054, 'epoch': 0.03}        \n",
      "{'loss': 2.5108, 'learning_rate': 0.0001196925743133783, 'epoch': 0.03}         \n",
      "{'loss': 2.3665, 'learning_rate': 0.00011973024902987606, 'epoch': 0.03}        \n",
      "{'loss': 2.3407, 'learning_rate': 0.00011976792374637382, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32150/1061708 [4:48:37<151:47:59,  1.88it/s][2024-02-29 22:55:37,618] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9872, 'learning_rate': 0.0001198018309912218, 'epoch': 0.03}         \n",
      "{'loss': 2.1026, 'learning_rate': 0.00011983950570771956, 'epoch': 0.03}        \n",
      "{'loss': 2.6463, 'learning_rate': 0.00011987718042421732, 'epoch': 0.03}        \n",
      "{'loss': 2.1112, 'learning_rate': 0.00011991485514071508, 'epoch': 0.03}        \n",
      "{'loss': 2.5211, 'learning_rate': 0.00011995252985721284, 'epoch': 0.03}        \n",
      "{'loss': 2.1563, 'learning_rate': 0.0001199902045737106, 'epoch': 0.03}         \n",
      "{'loss': 2.0381, 'learning_rate': 0.00012002787929020836, 'epoch': 0.03}        \n",
      "{'loss': 2.226, 'learning_rate': 0.00012006555400670612, 'epoch': 0.03}         \n",
      "{'loss': 2.349, 'learning_rate': 0.00012010322872320385, 'epoch': 0.03}         \n",
      "{'loss': 2.5261, 'learning_rate': 0.00012014090343970161, 'epoch': 0.03}        \n",
      "{'loss': 2.2329, 'learning_rate': 0.00012017857815619938, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32268/1061708 [4:49:40<152:10:54,  1.88it/s][2024-02-29 22:56:40,600] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6872, 'learning_rate': 0.00012021248540104737, 'epoch': 0.03}        \n",
      "{'loss': 2.2195, 'learning_rate': 0.00012025016011754513, 'epoch': 0.03}        \n",
      "{'loss': 2.1681, 'learning_rate': 0.00012028783483404289, 'epoch': 0.03}        \n",
      "{'loss': 2.1185, 'learning_rate': 0.00012032550955054062, 'epoch': 0.03}        \n",
      "{'loss': 2.1269, 'learning_rate': 0.00012036318426703838, 'epoch': 0.03}        \n",
      "{'loss': 2.3612, 'learning_rate': 0.00012040085898353614, 'epoch': 0.03}        \n",
      "{'loss': 2.5196, 'learning_rate': 0.0001204385337000339, 'epoch': 0.03}         \n",
      "{'loss': 2.1389, 'learning_rate': 0.00012047620841653166, 'epoch': 0.03}        \n",
      "{'loss': 2.1236, 'learning_rate': 0.00012051388313302942, 'epoch': 0.03}        \n",
      "{'loss': 2.0739, 'learning_rate': 0.00012055155784952718, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32369/1061708 [4:50:33<151:48:11,  1.88it/s][2024-02-29 22:57:34,482] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7321, 'learning_rate': 0.00012058546509437516, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32370/1061708 [4:50:34<142:29:58,  2.01it/s][2024-02-29 22:57:34,905] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.6971, 'learning_rate': 0.00012061937233922315, 'epoch': 0.03}        \n",
      "{'loss': 2.6172, 'learning_rate': 0.00012065704705572091, 'epoch': 0.03}        \n",
      "{'loss': 2.4736, 'learning_rate': 0.00012069472177221867, 'epoch': 0.03}        \n",
      "{'loss': 2.2301, 'learning_rate': 0.00012073239648871643, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32415/1061708 [4:50:58<153:40:17,  1.86it/s][2024-02-29 22:57:58,885] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0716, 'learning_rate': 0.00012076630373356441, 'epoch': 0.03}        \n",
      "{'loss': 2.1947, 'learning_rate': 0.00012080397845006217, 'epoch': 0.03}        \n",
      "{'loss': 2.1176, 'learning_rate': 0.00012084165316655993, 'epoch': 0.03}        \n",
      "{'loss': 2.1814, 'learning_rate': 0.00012087932788305769, 'epoch': 0.03}        \n",
      "{'loss': 2.474, 'learning_rate': 0.00012091700259955545, 'epoch': 0.03}         \n",
      "{'loss': 2.4024, 'learning_rate': 0.00012095467731605321, 'epoch': 0.03}        \n",
      "{'loss': 2.6984, 'learning_rate': 0.00012099235203255097, 'epoch': 0.03}        \n",
      "{'loss': 2.0055, 'learning_rate': 0.00012103002674904871, 'epoch': 0.03}        \n",
      "{'loss': 2.2843, 'learning_rate': 0.00012106770146554647, 'epoch': 0.03}        \n",
      "{'loss': 2.0945, 'learning_rate': 0.00012110537618204423, 'epoch': 0.03}        \n",
      "{'loss': 2.2664, 'learning_rate': 0.00012114305089854199, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▉                            | 32523/1061708 [4:51:55<152:11:31,  1.88it/s][2024-02-29 22:58:56,565] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4688, 'learning_rate': 0.00012117695814338998, 'epoch': 0.03}        \n",
      "{'loss': 2.0085, 'learning_rate': 0.00012121463285988774, 'epoch': 0.03}        \n",
      "{'loss': 2.2729, 'learning_rate': 0.0001212523075763855, 'epoch': 0.03}         \n",
      "{'loss': 2.0669, 'learning_rate': 0.00012128998229288325, 'epoch': 0.03}        \n",
      "{'loss': 2.2544, 'learning_rate': 0.00012132765700938101, 'epoch': 0.03}        \n",
      "{'loss': 1.9037, 'learning_rate': 0.00012136533172587875, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32585/1061708 [4:52:29<153:25:34,  1.86it/s][2024-02-29 22:59:29,631] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3196, 'learning_rate': 0.00012139923897072676, 'epoch': 0.03}        \n",
      "{'loss': 2.4496, 'learning_rate': 0.00012143691368722452, 'epoch': 0.03}        \n",
      "{'loss': 2.3282, 'learning_rate': 0.00012147458840372228, 'epoch': 0.03}        \n",
      "{'loss': 1.9625, 'learning_rate': 0.00012151226312022001, 'epoch': 0.03}        \n",
      "{'loss': 2.3477, 'learning_rate': 0.00012154993783671777, 'epoch': 0.03}        \n",
      "{'loss': 2.3242, 'learning_rate': 0.00012158761255321553, 'epoch': 0.03}        \n",
      "{'loss': 2.0952, 'learning_rate': 0.0001216252872697133, 'epoch': 0.03}         \n",
      "{'loss': 2.2547, 'learning_rate': 0.00012166296198621106, 'epoch': 0.03}        \n",
      "{'loss': 2.515, 'learning_rate': 0.00012170063670270882, 'epoch': 0.03}         \n",
      "{'loss': 2.3447, 'learning_rate': 0.00012173831141920658, 'epoch': 0.03}        \n",
      "{'loss': 2.3585, 'learning_rate': 0.00012177598613570434, 'epoch': 0.03}        \n",
      "{'loss': 2.3612, 'learning_rate': 0.0001218136608522021, 'epoch': 0.03}         \n",
      "{'loss': 2.4973, 'learning_rate': 0.00012185133556869986, 'epoch': 0.03}        \n",
      "{'loss': 2.1948, 'learning_rate': 0.00012188901028519762, 'epoch': 0.03}        \n",
      "{'loss': 1.9964, 'learning_rate': 0.00012192668500169538, 'epoch': 0.03}        \n",
      "{'loss': 1.9569, 'learning_rate': 0.00012196435971819314, 'epoch': 0.03}        \n",
      "  3%|▉                            | 32740/1061708 [4:53:51<151:38:05,  1.88it/s][2024-02-29 23:00:52,370] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▉                            | 32741/1061708 [4:53:52<144:11:48,  1.98it/s][2024-02-29 23:00:52,800] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4068, 'learning_rate': 0.00012199449949139132, 'epoch': 0.03}        \n",
      "{'loss': 2.1676, 'learning_rate': 0.00012203217420788908, 'epoch': 0.03}        \n",
      "{'loss': 2.1729, 'learning_rate': 0.00012206984892438684, 'epoch': 0.03}        \n",
      "{'loss': 2.0178, 'learning_rate': 0.0001221075236408846, 'epoch': 0.03}         \n",
      "{'loss': 2.3258, 'learning_rate': 0.00012214519835738235, 'epoch': 0.03}        \n",
      "{'loss': 2.5677, 'learning_rate': 0.00012218287307388012, 'epoch': 0.03}        \n",
      "{'loss': 2.523, 'learning_rate': 0.00012222054779037787, 'epoch': 0.03}         \n",
      "  3%|▉                            | 32818/1061708 [4:54:33<151:56:58,  1.88it/s][2024-02-29 23:01:33,867] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1708, 'learning_rate': 0.00012225445503522586, 'epoch': 0.03}        \n",
      "{'loss': 2.3668, 'learning_rate': 0.0001222921297517236, 'epoch': 0.03}         \n",
      "{'loss': 2.3929, 'learning_rate': 0.00012232980446822138, 'epoch': 0.03}        \n",
      "{'loss': 2.0017, 'learning_rate': 0.00012236747918471913, 'epoch': 0.03}        \n",
      "{'loss': 1.8073, 'learning_rate': 0.0001224051539012169, 'epoch': 0.03}         \n",
      "{'loss': 2.0765, 'learning_rate': 0.00012244282861771465, 'epoch': 0.03}        \n",
      "{'loss': 2.0062, 'learning_rate': 0.00012248050333421242, 'epoch': 0.03}        \n",
      "{'loss': 2.3848, 'learning_rate': 0.00012251817805071017, 'epoch': 0.03}        \n",
      "{'loss': 2.0797, 'learning_rate': 0.00012255585276720794, 'epoch': 0.03}        \n",
      "{'loss': 2.236, 'learning_rate': 0.0001225935274837057, 'epoch': 0.03}          \n",
      "{'loss': 1.8886, 'learning_rate': 0.00012263120220020347, 'epoch': 0.03}        \n",
      "{'loss': 2.1869, 'learning_rate': 0.0001226688769167012, 'epoch': 0.03}         \n",
      "{'loss': 1.9955, 'learning_rate': 0.00012270655163319899, 'epoch': 0.03}        \n",
      "{'loss': 2.6488, 'learning_rate': 0.00012274422634969673, 'epoch': 0.03}        \n",
      "{'loss': 2.3718, 'learning_rate': 0.0001227819010661945, 'epoch': 0.03}         \n",
      "{'loss': 2.3847, 'learning_rate': 0.00012281957578269223, 'epoch': 0.03}        \n",
      "{'loss': 2.2112, 'learning_rate': 0.00012285725049919, 'epoch': 0.03}           \n",
      "{'loss': 2.2919, 'learning_rate': 0.00012289492521568775, 'epoch': 0.03}        \n",
      "{'loss': 2.1963, 'learning_rate': 0.00012293259993218552, 'epoch': 0.03}        \n",
      "{'loss': 2.447, 'learning_rate': 0.00012297027464868327, 'epoch': 0.03}         \n",
      "{'loss': 2.0274, 'learning_rate': 0.00012300794936518104, 'epoch': 0.03}        \n",
      "{'loss': 2.0875, 'learning_rate': 0.0001230456240816788, 'epoch': 0.03}         \n",
      "  3%|▉                            | 33038/1061708 [4:56:30<151:46:21,  1.88it/s][2024-02-29 23:03:31,313] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0709, 'learning_rate': 0.00012307953132652675, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33042/1061708 [4:56:32<148:32:32,  1.92it/s][2024-02-29 23:03:33,376] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.347, 'learning_rate': 0.00012311343857137477, 'epoch': 0.03}         \n",
      "{'loss': 2.5877, 'learning_rate': 0.00012315111328787252, 'epoch': 0.03}        \n",
      "{'loss': 2.4637, 'learning_rate': 0.0001231887880043703, 'epoch': 0.03}         \n",
      "  3%|▉                            | 33077/1061708 [4:56:51<152:20:18,  1.88it/s][2024-02-29 23:03:52,010] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.5387, 'learning_rate': 0.00012322269524921826, 'epoch': 0.03}        \n",
      "{'loss': 2.0591, 'learning_rate': 0.000123260369965716, 'epoch': 0.03}          \n",
      "{'loss': 2.3605, 'learning_rate': 0.00012329804468221378, 'epoch': 0.03}        \n",
      "{'loss': 2.0232, 'learning_rate': 0.00012333571939871152, 'epoch': 0.03}        \n",
      "{'loss': 2.3358, 'learning_rate': 0.0001233733941152093, 'epoch': 0.03}         \n",
      "{'loss': 2.2805, 'learning_rate': 0.00012341106883170705, 'epoch': 0.03}        \n",
      "{'loss': 2.0963, 'learning_rate': 0.00012344874354820482, 'epoch': 0.03}        \n",
      "{'loss': 2.4608, 'learning_rate': 0.00012348641826470257, 'epoch': 0.03}        \n",
      "{'loss': 1.79, 'learning_rate': 0.0001235240929812003, 'epoch': 0.03}           \n",
      "{'loss': 2.4906, 'learning_rate': 0.00012356176769769806, 'epoch': 0.03}        \n",
      "{'loss': 2.0952, 'learning_rate': 0.00012359944241419583, 'epoch': 0.03}        \n",
      "{'loss': 2.1354, 'learning_rate': 0.00012363711713069358, 'epoch': 0.03}        \n",
      "{'loss': 2.1996, 'learning_rate': 0.00012367479184719136, 'epoch': 0.03}        \n",
      "{'loss': 2.0674, 'learning_rate': 0.0001237124665636891, 'epoch': 0.03}         \n",
      "{'loss': 2.5521, 'learning_rate': 0.00012375014128018688, 'epoch': 0.03}        \n",
      "{'loss': 2.4894, 'learning_rate': 0.00012378781599668462, 'epoch': 0.03}        \n",
      "{'loss': 2.123, 'learning_rate': 0.0001238254907131824, 'epoch': 0.03}          \n",
      "{'loss': 2.1437, 'learning_rate': 0.00012386316542968014, 'epoch': 0.03}        \n",
      "{'loss': 2.0668, 'learning_rate': 0.00012390084014617792, 'epoch': 0.03}        \n",
      "{'loss': 2.5705, 'learning_rate': 0.00012393851486267566, 'epoch': 0.03}        \n",
      "{'loss': 2.0611, 'learning_rate': 0.00012397618957917344, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6704, 'learning_rate': 0.00012401386429567119, 'epoch': 0.03}        \n",
      "{'loss': 2.2073, 'learning_rate': 0.00012405153901216896, 'epoch': 0.03}        \n",
      "{'loss': 2.3425, 'learning_rate': 0.00012408921372866668, 'epoch': 0.03}        \n",
      "{'loss': 1.9621, 'learning_rate': 0.00012412688844516445, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33328/1061708 [4:59:05<151:59:44,  1.88it/s][2024-02-29 23:06:06,076] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1004, 'learning_rate': 0.00012416079569001244, 'epoch': 0.03}        \n",
      "{'loss': 2.3698, 'learning_rate': 0.0001241984704065102, 'epoch': 0.03}         \n",
      "{'loss': 2.0187, 'learning_rate': 0.00012423614512300797, 'epoch': 0.03}        \n",
      "{'loss': 2.1979, 'learning_rate': 0.0001242738198395057, 'epoch': 0.03}         \n",
      "{'loss': 2.5646, 'learning_rate': 0.00012431149455600349, 'epoch': 0.03}        \n",
      "{'loss': 2.2476, 'learning_rate': 0.00012434916927250123, 'epoch': 0.03}        \n",
      "{'loss': 2.2874, 'learning_rate': 0.00012438684398899898, 'epoch': 0.03}        \n",
      "{'loss': 2.18, 'learning_rate': 0.00012442451870549673, 'epoch': 0.03}          \n",
      "  3%|▉                            | 33407/1061708 [4:59:47<152:17:14,  1.88it/s][2024-02-29 23:06:48,175] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.556, 'learning_rate': 0.00012445842595034475, 'epoch': 0.03}         \n",
      "{'loss': 2.1309, 'learning_rate': 0.0001244961006668425, 'epoch': 0.03}         \n",
      "{'loss': 2.1844, 'learning_rate': 0.00012453377538334027, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33438/1061708 [5:00:04<152:05:15,  1.88it/s][2024-02-29 23:07:04,677] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2704, 'learning_rate': 0.00012456768262818823, 'epoch': 0.03}        \n",
      "{'loss': 2.2646, 'learning_rate': 0.000124605357344686, 'epoch': 0.03}          \n",
      "{'loss': 2.3559, 'learning_rate': 0.00012464303206118375, 'epoch': 0.03}        \n",
      "{'loss': 2.0849, 'learning_rate': 0.0001246807067776815, 'epoch': 0.03}         \n",
      "{'loss': 2.0248, 'learning_rate': 0.00012471838149417927, 'epoch': 0.03}        \n",
      "{'loss': 2.0296, 'learning_rate': 0.00012475605621067702, 'epoch': 0.03}        \n",
      "{'loss': 2.2551, 'learning_rate': 0.0001247937309271748, 'epoch': 0.03}         \n",
      "{'loss': 2.0619, 'learning_rate': 0.00012483140564367254, 'epoch': 0.03}        \n",
      "{'loss': 2.1484, 'learning_rate': 0.0001248690803601703, 'epoch': 0.03}         \n",
      "{'loss': 2.6314, 'learning_rate': 0.00012490675507666806, 'epoch': 0.03}        \n",
      "{'loss': 2.6514, 'learning_rate': 0.0001249444297931658, 'epoch': 0.03}         \n",
      "{'loss': 2.2609, 'learning_rate': 0.00012498210450966355, 'epoch': 0.03}        \n",
      "{'loss': 2.3377, 'learning_rate': 0.00012501977922616133, 'epoch': 0.03}        \n",
      "{'loss': 2.0594, 'learning_rate': 0.00012505745394265908, 'epoch': 0.03}        \n",
      "{'loss': 2.1401, 'learning_rate': 0.00012509512865915685, 'epoch': 0.03}        \n",
      "{'loss': 2.3996, 'learning_rate': 0.0001251328033756546, 'epoch': 0.03}         \n",
      "{'loss': 2.4658, 'learning_rate': 0.00012517047809215237, 'epoch': 0.03}        \n",
      "{'loss': 2.3261, 'learning_rate': 0.00012520815280865012, 'epoch': 0.03}        \n",
      "{'loss': 2.0694, 'learning_rate': 0.0001252458275251479, 'epoch': 0.03}         \n",
      "{'loss': 2.2828, 'learning_rate': 0.00012528350224164564, 'epoch': 0.03}        \n",
      "{'loss': 2.1223, 'learning_rate': 0.0001253211769581434, 'epoch': 0.03}         \n",
      "{'loss': 2.1618, 'learning_rate': 0.00012535885167464116, 'epoch': 0.03}        \n",
      "{'loss': 2.5459, 'learning_rate': 0.0001253965263911389, 'epoch': 0.03}         \n",
      "  3%|▉                            | 33669/1061708 [5:02:07<151:34:18,  1.88it/s][2024-02-29 23:09:08,038] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3244, 'learning_rate': 0.0001254304336359869, 'epoch': 0.03}         \n",
      "{'loss': 2.2391, 'learning_rate': 0.00012546810835248467, 'epoch': 0.03}        \n",
      "{'loss': 2.3245, 'learning_rate': 0.00012550578306898242, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33692/1061708 [5:02:19<151:38:40,  1.88it/s][2024-02-29 23:09:20,252] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9678, 'learning_rate': 0.00012553969031383038, 'epoch': 0.03}        \n",
      "{'loss': 2.2705, 'learning_rate': 0.00012557736503032816, 'epoch': 0.03}        \n",
      "{'loss': 2.1255, 'learning_rate': 0.0001256150397468259, 'epoch': 0.03}         \n",
      "{'loss': 2.264, 'learning_rate': 0.00012565271446332368, 'epoch': 0.03}         \n",
      "{'loss': 2.612, 'learning_rate': 0.00012569038917982142, 'epoch': 0.03}         \n",
      "{'loss': 2.5112, 'learning_rate': 0.0001257280638963192, 'epoch': 0.03}         \n",
      "{'loss': 2.2671, 'learning_rate': 0.00012576573861281694, 'epoch': 0.03}        \n",
      "{'loss': 1.951, 'learning_rate': 0.00012580341332931472, 'epoch': 0.03}         \n",
      "{'loss': 2.3835, 'learning_rate': 0.00012584108804581247, 'epoch': 0.03}        \n",
      "{'loss': 2.1353, 'learning_rate': 0.0001258787627623102, 'epoch': 0.03}         \n",
      "{'loss': 2.2099, 'learning_rate': 0.00012591643747880796, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33805/1061708 [5:03:20<153:05:03,  1.87it/s][2024-02-29 23:10:20,543] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.35, 'learning_rate': 0.00012595034472365598, 'epoch': 0.03}          \n",
      "{'loss': 2.1583, 'learning_rate': 0.00012598801944015372, 'epoch': 0.03}        \n",
      "{'loss': 2.1397, 'learning_rate': 0.0001260256941566515, 'epoch': 0.03}         \n",
      "{'loss': 2.1185, 'learning_rate': 0.00012606336887314925, 'epoch': 0.03}        \n",
      "{'loss': 2.2874, 'learning_rate': 0.000126101043589647, 'epoch': 0.03}          \n",
      "{'loss': 2.3865, 'learning_rate': 0.00012613871830614474, 'epoch': 0.03}        \n",
      "{'loss': 2.2191, 'learning_rate': 0.0001261763930226425, 'epoch': 0.03}         \n",
      "{'loss': 2.1454, 'learning_rate': 0.00012621406773914026, 'epoch': 0.03}        \n",
      "{'loss': 2.0205, 'learning_rate': 0.00012625174245563803, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33894/1061708 [5:04:07<154:06:52,  1.85it/s][2024-02-29 23:11:08,078] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5825, 'learning_rate': 0.00012628564970048603, 'epoch': 0.03}        \n",
      "{'loss': 2.1626, 'learning_rate': 0.00012632332441698377, 'epoch': 0.03}        \n",
      "{'loss': 2.3134, 'learning_rate': 0.00012636099913348152, 'epoch': 0.03}        \n",
      "{'loss': 1.8184, 'learning_rate': 0.00012639867384997927, 'epoch': 0.03}        \n",
      "{'loss': 2.0845, 'learning_rate': 0.00012643634856647704, 'epoch': 0.03}        \n",
      "{'loss': 1.983, 'learning_rate': 0.0001264740232829748, 'epoch': 0.03}          \n",
      "{'loss': 2.4551, 'learning_rate': 0.00012651169799947256, 'epoch': 0.03}        \n",
      "{'loss': 2.2581, 'learning_rate': 0.0001265493727159703, 'epoch': 0.03}         \n",
      "{'loss': 2.2206, 'learning_rate': 0.00012658704743246808, 'epoch': 0.03}        \n",
      "{'loss': 2.0224, 'learning_rate': 0.00012662472214896583, 'epoch': 0.03}        \n",
      "  3%|▉                            | 33999/1061708 [5:05:03<151:32:29,  1.88it/s][2024-02-29 23:12:04,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=380, lr=[0.0001266623968654636], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 23:12:04,179] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=1.8919357938658743, CurrSamplesPerSec=1.8948116913243882, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.8952, 'learning_rate': 0.0001266623968654636, 'epoch': 0.03}         \n",
      "  3%|▉                            | 34008/1061708 [5:05:08<151:49:06,  1.88it/s][2024-02-29 23:12:08,895] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3599, 'learning_rate': 0.00012669630411031157, 'epoch': 0.03}        \n",
      "{'loss': 2.29, 'learning_rate': 0.00012673397882680934, 'epoch': 0.03}          \n",
      "{'loss': 2.3119, 'learning_rate': 0.0001267716535433071, 'epoch': 0.03}         \n",
      "{'loss': 2.3229, 'learning_rate': 0.00012680932825980486, 'epoch': 0.03}        \n",
      "  3%|▉                            | 34046/1061708 [5:05:28<152:36:45,  1.87it/s][2024-02-29 23:12:29,113] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9979, 'learning_rate': 0.00012684323550465283, 'epoch': 0.03}        \n",
      "{'loss': 2.5106, 'learning_rate': 0.00012688091022115057, 'epoch': 0.03}        \n",
      "{'loss': 2.1922, 'learning_rate': 0.00012691858493764835, 'epoch': 0.03}        \n",
      "{'loss': 2.2381, 'learning_rate': 0.0001269562596541461, 'epoch': 0.03}         \n",
      "  3%|▉                            | 34081/1061708 [5:05:47<151:20:06,  1.89it/s][2024-02-29 23:12:47,731] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0664, 'learning_rate': 0.0001269901668989941, 'epoch': 0.03}         \n",
      "{'loss': 2.4652, 'learning_rate': 0.00012702784161549186, 'epoch': 0.03}        \n",
      "{'loss': 2.4459, 'learning_rate': 0.0001270655163319896, 'epoch': 0.03}         \n",
      "{'loss': 1.9461, 'learning_rate': 0.00012710319104848735, 'epoch': 0.03}        \n",
      "{'loss': 1.9774, 'learning_rate': 0.00012714086576498513, 'epoch': 0.03}        \n",
      "  3%|▉                            | 34137/1061708 [5:06:17<152:03:36,  1.88it/s][2024-02-29 23:13:17,563] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.2035, 'learning_rate': 0.00012717477300983312, 'epoch': 0.03}        \n",
      "{'loss': 2.1375, 'learning_rate': 0.00012721244772633086, 'epoch': 0.03}        \n",
      "{'loss': 2.3594, 'learning_rate': 0.00012725012244282864, 'epoch': 0.03}        \n",
      "{'loss': 2.399, 'learning_rate': 0.00012728779715932636, 'epoch': 0.03}         \n",
      "{'loss': 2.273, 'learning_rate': 0.00012732547187582413, 'epoch': 0.03}         \n",
      "{'loss': 2.0485, 'learning_rate': 0.00012736314659232188, 'epoch': 0.03}        \n",
      "{'loss': 2.0233, 'learning_rate': 0.00012740082130881965, 'epoch': 0.03}        \n",
      "{'loss': 2.2594, 'learning_rate': 0.0001274384960253174, 'epoch': 0.03}         \n",
      "{'loss': 2.159, 'learning_rate': 0.00012747617074181517, 'epoch': 0.03}         \n",
      "{'loss': 2.459, 'learning_rate': 0.00012751384545831292, 'epoch': 0.03}         \n",
      "{'loss': 1.7933, 'learning_rate': 0.0001275515201748107, 'epoch': 0.03}         \n",
      "{'loss': 2.8116, 'learning_rate': 0.00012758919489130844, 'epoch': 0.03}        \n",
      "{'loss': 2.5398, 'learning_rate': 0.00012762686960780622, 'epoch': 0.03}        \n",
      "{'loss': 2.079, 'learning_rate': 0.00012766454432430396, 'epoch': 0.03}         \n",
      "{'loss': 2.2721, 'learning_rate': 0.00012770221904080174, 'epoch': 0.03}        \n",
      "{'loss': 1.8459, 'learning_rate': 0.00012773989375729948, 'epoch': 0.03}        \n",
      "{'loss': 2.3653, 'learning_rate': 0.00012777756847379726, 'epoch': 0.03}        \n",
      "{'loss': 2.3288, 'learning_rate': 0.000127815243190295, 'epoch': 0.03}          \n",
      "{'loss': 2.3535, 'learning_rate': 0.00012785291790679278, 'epoch': 0.03}        \n",
      "{'loss': 2.3128, 'learning_rate': 0.0001278905926232905, 'epoch': 0.03}         \n",
      "{'loss': 2.0561, 'learning_rate': 0.00012792826733978827, 'epoch': 0.03}        \n",
      "{'loss': 1.7718, 'learning_rate': 0.00012796594205628602, 'epoch': 0.03}        \n",
      "{'loss': 2.2243, 'learning_rate': 0.0001280036167727838, 'epoch': 0.03}         \n",
      "{'loss': 2.6466, 'learning_rate': 0.00012804129148928154, 'epoch': 0.03}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00012807896620577931, 'epoch': 0.03}        \n",
      "{'loss': 2.4645, 'learning_rate': 0.00012811664092227706, 'epoch': 0.03}        \n",
      "{'loss': 2.2923, 'learning_rate': 0.00012815431563877484, 'epoch': 0.03}        \n",
      "{'loss': 2.0669, 'learning_rate': 0.00012819199035527258, 'epoch': 0.03}        \n",
      "{'loss': 2.4043, 'learning_rate': 0.00012822966507177036, 'epoch': 0.03}        \n",
      "{'loss': 2.2153, 'learning_rate': 0.0001282673397882681, 'epoch': 0.03}         \n",
      "  3%|▉                            | 34435/1061708 [5:08:56<152:47:42,  1.87it/s][2024-02-29 23:15:56,624] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5175, 'learning_rate': 0.00012830124703311607, 'epoch': 0.03}        \n",
      "{'loss': 2.0105, 'learning_rate': 0.00012833892174961384, 'epoch': 0.03}        \n",
      "{'loss': 2.7121, 'learning_rate': 0.0001283765964661116, 'epoch': 0.03}         \n",
      "{'loss': 2.357, 'learning_rate': 0.00012841427118260936, 'epoch': 0.03}         \n",
      "{'loss': 2.2417, 'learning_rate': 0.0001284519458991071, 'epoch': 0.03}         \n",
      "{'loss': 2.1755, 'learning_rate': 0.00012848962061560488, 'epoch': 0.03}        \n",
      "{'loss': 2.4175, 'learning_rate': 0.00012852729533210263, 'epoch': 0.03}        \n",
      "{'loss': 2.07, 'learning_rate': 0.0001285649700486004, 'epoch': 0.03}           \n",
      "  3%|▉                            | 34510/1061708 [5:09:36<151:24:13,  1.88it/s][2024-02-29 23:16:36,586] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.8396, 'learning_rate': 0.00012859887729344837, 'epoch': 0.03}        \n",
      "{'loss': 2.0262, 'learning_rate': 0.00012863655200994614, 'epoch': 0.03}        \n",
      "{'loss': 2.3635, 'learning_rate': 0.0001286742267264439, 'epoch': 0.03}         \n",
      "{'loss': 1.9288, 'learning_rate': 0.00012871190144294166, 'epoch': 0.03}        \n",
      "{'loss': 2.2668, 'learning_rate': 0.0001287495761594394, 'epoch': 0.03}         \n",
      "{'loss': 2.2446, 'learning_rate': 0.00012878725087593718, 'epoch': 0.03}        \n",
      "{'loss': 1.7633, 'learning_rate': 0.00012882492559243493, 'epoch': 0.03}        \n",
      "{'loss': 2.5434, 'learning_rate': 0.00012886260030893268, 'epoch': 0.03}        \n",
      "{'loss': 2.5125, 'learning_rate': 0.00012890027502543045, 'epoch': 0.03}        \n",
      "{'loss': 2.2913, 'learning_rate': 0.0001289379497419282, 'epoch': 0.03}         \n",
      "{'loss': 2.4672, 'learning_rate': 0.00012897562445842595, 'epoch': 0.03}        \n",
      "{'loss': 2.3314, 'learning_rate': 0.00012901329917492372, 'epoch': 0.03}        \n",
      "{'loss': 2.0915, 'learning_rate': 0.00012905097389142147, 'epoch': 0.03}        \n",
      "{'loss': 2.5715, 'learning_rate': 0.00012908864860791924, 'epoch': 0.03}        \n",
      "{'loss': 2.7031, 'learning_rate': 0.000129126323324417, 'epoch': 0.03}          \n",
      "{'loss': 2.4126, 'learning_rate': 0.00012916399804091473, 'epoch': 0.03}        \n",
      "{'loss': 2.3703, 'learning_rate': 0.0001292016727574125, 'epoch': 0.03}         \n",
      "{'loss': 2.4082, 'learning_rate': 0.00012923934747391025, 'epoch': 0.03}        \n",
      "{'loss': 2.4606, 'learning_rate': 0.00012927702219040803, 'epoch': 0.03}        \n",
      "{'loss': 2.2854, 'learning_rate': 0.00012931469690690578, 'epoch': 0.03}        \n",
      "  3%|▉                            | 34719/1061708 [5:11:27<151:25:58,  1.88it/s][2024-02-29 23:18:28,093] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4553, 'learning_rate': 0.00012934860415175377, 'epoch': 0.03}        \n",
      "{'loss': 2.6519, 'learning_rate': 0.00012938627886825151, 'epoch': 0.03}        \n",
      "  3%|▉                            | 34731/1061708 [5:11:33<151:21:57,  1.88it/s][2024-02-29 23:18:34,394] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1595, 'learning_rate': 0.0001294201861130995, 'epoch': 0.03}         \n",
      "{'loss': 1.9921, 'learning_rate': 0.00012945786082959725, 'epoch': 0.03}        \n",
      "{'loss': 1.7925, 'learning_rate': 0.00012949553554609503, 'epoch': 0.03}        \n",
      "{'loss': 2.2288, 'learning_rate': 0.00012953321026259277, 'epoch': 0.03}        \n",
      "{'loss': 2.0763, 'learning_rate': 0.00012957088497909055, 'epoch': 0.03}        \n",
      "{'loss': 2.2042, 'learning_rate': 0.0001296085596955883, 'epoch': 0.03}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3184, 'learning_rate': 0.00012964623441208604, 'epoch': 0.03}        \n",
      "{'loss': 2.3355, 'learning_rate': 0.00012968390912858381, 'epoch': 0.03}        \n",
      "{'loss': 2.5094, 'learning_rate': 0.00012972158384508156, 'epoch': 0.03}        \n",
      "{'loss': 2.379, 'learning_rate': 0.00012975925856157934, 'epoch': 0.03}         \n",
      "{'loss': 2.0435, 'learning_rate': 0.00012979693327807708, 'epoch': 0.03}        \n",
      "{'loss': 1.9539, 'learning_rate': 0.00012983460799457486, 'epoch': 0.03}        \n",
      "{'loss': 2.2285, 'learning_rate': 0.0001298722827110726, 'epoch': 0.03}         \n",
      "{'loss': 2.2809, 'learning_rate': 0.00012990995742757038, 'epoch': 0.03}        \n",
      "{'loss': 2.3104, 'learning_rate': 0.00012994763214406812, 'epoch': 0.03}        \n",
      "{'loss': 2.3217, 'learning_rate': 0.00012998530686056587, 'epoch': 0.03}        \n",
      "{'loss': 2.2856, 'learning_rate': 0.00013002298157706362, 'epoch': 0.03}        \n",
      "{'loss': 2.293, 'learning_rate': 0.0001300606562935614, 'epoch': 0.03}          \n",
      "{'loss': 2.1284, 'learning_rate': 0.00013009833101005914, 'epoch': 0.03}        \n",
      "{'loss': 2.1368, 'learning_rate': 0.0001301360057265569, 'epoch': 0.03}         \n",
      "  3%|▉                            | 34932/1061708 [5:13:21<151:42:16,  1.88it/s][2024-02-29 23:20:21,685] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▉                            | 34933/1061708 [5:13:21<145:50:52,  1.96it/s][2024-02-29 23:20:22,110] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2602, 'learning_rate': 0.00013016614549975512, 'epoch': 0.03}        \n",
      "{'loss': 2.379, 'learning_rate': 0.00013020382021625287, 'epoch': 0.03}         \n",
      "  3%|▉                            | 34957/1061708 [5:13:34<151:48:15,  1.88it/s][2024-02-29 23:20:34,851] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0282, 'learning_rate': 0.00013023772746110086, 'epoch': 0.03}        \n",
      "{'loss': 2.2755, 'learning_rate': 0.0001302754021775986, 'epoch': 0.03}         \n",
      "{'loss': 2.3927, 'learning_rate': 0.00013031307689409638, 'epoch': 0.03}        \n",
      "{'loss': 2.2743, 'learning_rate': 0.00013035075161059413, 'epoch': 0.03}        \n",
      "{'loss': 1.87, 'learning_rate': 0.0001303884263270919, 'epoch': 0.03}           \n",
      "  3%|▉                            | 35000/1061708 [5:13:57<151:21:50,  1.88it/s][INFO|trainer.py:2868] 2024-02-29 23:20:57,280 >> Saving model checkpoint to output_model/checkpoint-35000\n",
      "[INFO|trainer.py:2880] 2024-02-29 23:20:57,283 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 23:20:58,492 >> tokenizer config file saved in output_model/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 23:20:58,492 >> Special tokens file saved in output_model/checkpoint-35000/special_tokens_map.json\n",
      "[2024-02-29 23:20:58,493] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step35000 is about to be saved!\n",
      "[2024-02-29 23:21:03,720] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-35000/global_step35000/mp_rank_00_model_states.pt\n",
      "[2024-02-29 23:21:03,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-35000/global_step35000/mp_rank_00_model_states.pt...\n",
      "[2024-02-29 23:21:17,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-35000/global_step35000/mp_rank_00_model_states.pt.\n",
      "[2024-02-29 23:21:18,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-35000/global_step35000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-02-29 23:21:25,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-35000/global_step35000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-02-29 23:21:25,401] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-35000/global_step35000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-02-29 23:21:25,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-02-29 23:21:25,433 >> Deleting older checkpoint [output_model/checkpoint-20000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-02-29 23:21:29,186 >> tokenizer config file saved in output_model/checkpoint-35000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-02-29 23:21:29,186 >> Special tokens file saved in output_model/checkpoint-35000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.5217, 'learning_rate': 0.00013042610104358965, 'epoch': 0.03}        \n",
      "{'loss': 2.7281, 'learning_rate': 0.00013046377576008742, 'epoch': 0.03}        \n",
      "{'loss': 2.4451, 'learning_rate': 0.00013050145047658517, 'epoch': 0.03}        \n",
      "{'loss': 2.1538, 'learning_rate': 0.00013053912519308294, 'epoch': 0.03}        \n",
      "{'loss': 2.4506, 'learning_rate': 0.0001305767999095807, 'epoch': 0.03}         \n",
      "{'loss': 1.9185, 'learning_rate': 0.00013061447462607846, 'epoch': 0.03}        \n",
      "{'loss': 2.2611, 'learning_rate': 0.0001306521493425762, 'epoch': 0.03}         \n",
      "{'loss': 2.4571, 'learning_rate': 0.00013068982405907396, 'epoch': 0.03}        \n",
      "{'loss': 2.1324, 'learning_rate': 0.0001307274987755717, 'epoch': 0.03}         \n",
      "{'loss': 2.3991, 'learning_rate': 0.00013076517349206948, 'epoch': 0.03}        \n",
      "{'loss': 2.4842, 'learning_rate': 0.00013080284820856723, 'epoch': 0.03}        \n",
      "{'loss': 2.0342, 'learning_rate': 0.000130840522925065, 'epoch': 0.03}          \n",
      "  3%|▉                            | 35123/1061708 [5:15:34<154:55:19,  1.84it/s][2024-02-29 23:22:35,057] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3462, 'learning_rate': 0.000130874430169913, 'epoch': 0.03}          \n",
      "{'loss': 2.2881, 'learning_rate': 0.00013091210488641074, 'epoch': 0.03}        \n",
      "{'loss': 2.2908, 'learning_rate': 0.00013094977960290848, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35157/1061708 [5:15:52<153:12:23,  1.86it/s][2024-02-29 23:22:53,207] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3605, 'learning_rate': 0.00013098368684775648, 'epoch': 0.03}        \n",
      "{'loss': 2.2676, 'learning_rate': 0.00013102136156425425, 'epoch': 0.03}        \n",
      "{'loss': 2.3486, 'learning_rate': 0.000131059036280752, 'epoch': 0.03}          \n",
      "{'loss': 2.1352, 'learning_rate': 0.00013109671099724977, 'epoch': 0.03}        \n",
      "{'loss': 2.5109, 'learning_rate': 0.00013113438571374752, 'epoch': 0.03}        \n",
      "{'loss': 2.2301, 'learning_rate': 0.00013117206043024526, 'epoch': 0.03}        \n",
      "{'loss': 2.1775, 'learning_rate': 0.000131209735146743, 'epoch': 0.03}          \n",
      "{'loss': 2.2953, 'learning_rate': 0.00013124740986324078, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35230/1061708 [5:16:31<151:24:31,  1.88it/s][2024-02-29 23:23:32,163] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0928, 'learning_rate': 0.00013128131710808878, 'epoch': 0.03}        \n",
      "{'loss': 2.6726, 'learning_rate': 0.00013131899182458652, 'epoch': 0.03}        \n",
      "{'loss': 2.34, 'learning_rate': 0.0001313566665410843, 'epoch': 0.03}           \n",
      "{'loss': 2.4808, 'learning_rate': 0.00013139434125758204, 'epoch': 0.03}        \n",
      "{'loss': 2.2621, 'learning_rate': 0.0001314320159740798, 'epoch': 0.03}         \n",
      "{'loss': 2.2611, 'learning_rate': 0.00013146969069057756, 'epoch': 0.03}        \n",
      "{'loss': 2.3854, 'learning_rate': 0.0001315073654070753, 'epoch': 0.03}         \n",
      "{'loss': 2.0447, 'learning_rate': 0.00013154504012357306, 'epoch': 0.03}        \n",
      "{'loss': 2.3247, 'learning_rate': 0.00013158271484007083, 'epoch': 0.03}        \n",
      "{'loss': 2.4209, 'learning_rate': 0.00013162038955656858, 'epoch': 0.03}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6638, 'learning_rate': 0.00013165806427306635, 'epoch': 0.03}        \n",
      "{'loss': 2.6201, 'learning_rate': 0.0001316957389895641, 'epoch': 0.03}         \n",
      "{'loss': 2.2874, 'learning_rate': 0.00013173341370606187, 'epoch': 0.03}        \n",
      "{'loss': 2.4652, 'learning_rate': 0.00013177108842255962, 'epoch': 0.03}        \n",
      "{'loss': 2.1953, 'learning_rate': 0.0001318087631390574, 'epoch': 0.03}         \n",
      "{'loss': 1.8976, 'learning_rate': 0.00013184643785555514, 'epoch': 0.03}        \n",
      "{'loss': 2.1326, 'learning_rate': 0.00013188411257205292, 'epoch': 0.03}        \n",
      "{'loss': 2.2485, 'learning_rate': 0.00013192178728855066, 'epoch': 0.03}        \n",
      "{'loss': 2.3974, 'learning_rate': 0.00013195946200504844, 'epoch': 0.03}        \n",
      "{'loss': 2.2514, 'learning_rate': 0.00013199713672154616, 'epoch': 0.03}        \n",
      "{'loss': 1.9837, 'learning_rate': 0.00013203481143804393, 'epoch': 0.03}        \n",
      "{'loss': 2.0032, 'learning_rate': 0.00013207248615454168, 'epoch': 0.03}        \n",
      "{'loss': 2.2517, 'learning_rate': 0.00013211016087103945, 'epoch': 0.03}        \n",
      "{'loss': 2.1237, 'learning_rate': 0.0001321478355875372, 'epoch': 0.03}         \n",
      "{'loss': 2.625, 'learning_rate': 0.00013218551030403497, 'epoch': 0.03}         \n",
      "{'loss': 1.964, 'learning_rate': 0.00013222318502053272, 'epoch': 0.03}         \n",
      "{'loss': 2.7166, 'learning_rate': 0.0001322608597370305, 'epoch': 0.03}         \n",
      "{'loss': 1.9353, 'learning_rate': 0.00013229853445352824, 'epoch': 0.03}        \n",
      "{'loss': 2.1014, 'learning_rate': 0.00013233620917002601, 'epoch': 0.03}        \n",
      "{'loss': 2.2388, 'learning_rate': 0.00013237388388652376, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35531/1061708 [5:19:12<151:18:12,  1.88it/s][2024-02-29 23:26:13,039] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▉                            | 35532/1061708 [5:19:12<142:09:35,  2.01it/s][2024-02-29 23:26:13,463] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1451, 'learning_rate': 0.00013240402365972197, 'epoch': 0.03}        \n",
      "{'loss': 2.1433, 'learning_rate': 0.00013244169837621974, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35553/1061708 [5:19:24<154:44:00,  1.84it/s][2024-02-29 23:26:24,632] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2731, 'learning_rate': 0.0001324756056210677, 'epoch': 0.03}         \n",
      "{'loss': 2.2859, 'learning_rate': 0.00013251328033756548, 'epoch': 0.03}        \n",
      "{'loss': 2.0727, 'learning_rate': 0.00013255095505406323, 'epoch': 0.03}        \n",
      "{'loss': 2.1726, 'learning_rate': 0.000132588629770561, 'epoch': 0.03}          \n",
      "{'loss': 2.3535, 'learning_rate': 0.00013262630448705875, 'epoch': 0.03}        \n",
      "{'loss': 2.262, 'learning_rate': 0.00013266397920355652, 'epoch': 0.03}         \n",
      "{'loss': 2.1994, 'learning_rate': 0.00013270165392005424, 'epoch': 0.03}        \n",
      "{'loss': 1.8647, 'learning_rate': 0.00013273932863655202, 'epoch': 0.03}        \n",
      "{'loss': 2.1844, 'learning_rate': 0.00013277700335304976, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35646/1061708 [5:20:13<152:19:46,  1.87it/s][2024-02-29 23:27:14,301] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4557, 'learning_rate': 0.00013281091059789776, 'epoch': 0.03}        \n",
      "{'loss': 2.6098, 'learning_rate': 0.00013284858531439553, 'epoch': 0.03}        \n",
      "{'loss': 2.4479, 'learning_rate': 0.00013288626003089328, 'epoch': 0.03}        \n",
      "{'loss': 2.3795, 'learning_rate': 0.00013292393474739102, 'epoch': 0.03}        \n",
      "{'loss': 2.0039, 'learning_rate': 0.00013296160946388877, 'epoch': 0.03}        \n",
      "{'loss': 2.1144, 'learning_rate': 0.00013299928418038654, 'epoch': 0.03}        \n",
      "  3%|▉                            | 35701/1061708 [5:20:43<151:12:57,  1.88it/s][2024-02-29 23:27:43,592] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.5297, 'learning_rate': 0.00013303319142523454, 'epoch': 0.03}        \n",
      "{'loss': 2.1204, 'learning_rate': 0.0001330708661417323, 'epoch': 0.03}         \n",
      "{'loss': 2.2849, 'learning_rate': 0.00013310854085823006, 'epoch': 0.03}        \n",
      "{'loss': 2.4611, 'learning_rate': 0.00013314621557472783, 'epoch': 0.03}        \n",
      "{'loss': 2.007, 'learning_rate': 0.00013318389029122555, 'epoch': 0.03}         \n",
      "{'loss': 1.9335, 'learning_rate': 0.00013322156500772332, 'epoch': 0.03}        \n",
      "{'loss': 2.4054, 'learning_rate': 0.00013325923972422107, 'epoch': 0.03}        \n",
      "{'loss': 2.3322, 'learning_rate': 0.00013329691444071884, 'epoch': 0.03}        \n",
      "{'loss': 1.8405, 'learning_rate': 0.0001333345891572166, 'epoch': 0.03}         \n",
      "{'loss': 2.1549, 'learning_rate': 0.00013337226387371437, 'epoch': 0.03}        \n",
      "{'loss': 2.497, 'learning_rate': 0.0001334099385902121, 'epoch': 0.03}          \n",
      "{'loss': 2.0464, 'learning_rate': 0.0001334476133067099, 'epoch': 0.03}         \n",
      "{'loss': 2.2686, 'learning_rate': 0.00013348528802320763, 'epoch': 0.03}        \n",
      "{'loss': 2.0968, 'learning_rate': 0.00013352296273970538, 'epoch': 0.03}        \n",
      "{'loss': 2.204, 'learning_rate': 0.00013356063745620315, 'epoch': 0.03}         \n",
      "{'loss': 2.0414, 'learning_rate': 0.0001335983121727009, 'epoch': 0.03}         \n",
      "{'loss': 2.3524, 'learning_rate': 0.00013363598688919868, 'epoch': 0.03}        \n",
      "{'loss': 2.2316, 'learning_rate': 0.00013367366160569642, 'epoch': 0.03}        \n",
      "{'loss': 2.3544, 'learning_rate': 0.0001337113363221942, 'epoch': 0.03}         \n",
      "{'loss': 2.6289, 'learning_rate': 0.00013374901103869192, 'epoch': 0.03}        \n",
      "{'loss': 2.0718, 'learning_rate': 0.0001337866857551897, 'epoch': 0.03}         \n",
      "{'loss': 2.2896, 'learning_rate': 0.00013382436047168744, 'epoch': 0.03}        \n",
      "{'loss': 2.1438, 'learning_rate': 0.0001338620351881852, 'epoch': 0.03}         \n",
      "  3%|▉                            | 35931/1061708 [5:22:45<150:49:45,  1.89it/s][2024-02-29 23:29:46,146] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4203, 'learning_rate': 0.0001338959424330332, 'epoch': 0.03}         \n",
      "{'loss': 2.1891, 'learning_rate': 0.00013393361714953098, 'epoch': 0.03}        \n",
      "{'loss': 1.9545, 'learning_rate': 0.00013397129186602872, 'epoch': 0.03}        \n",
      "{'loss': 1.8724, 'learning_rate': 0.00013400896658252647, 'epoch': 0.03}        \n",
      "{'loss': 2.6426, 'learning_rate': 0.00013404664129902422, 'epoch': 0.03}        \n",
      "{'loss': 1.9106, 'learning_rate': 0.000134084316015522, 'epoch': 0.03}          \n",
      "  3%|▉                            | 35999/1061708 [5:23:21<151:00:31,  1.89it/s][2024-02-29 23:30:22,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=400, lr=[0.00013412199073201974], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 23:30:22,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=1.891889683810252, CurrSamplesPerSec=1.9032075389507617, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.2505, 'learning_rate': 0.00013412199073201974, 'epoch': 0.03}        \n",
      "{'loss': 2.1911, 'learning_rate': 0.0001341596654485175, 'epoch': 0.03}         \n",
      "{'loss': 2.3461, 'learning_rate': 0.00013419734016501526, 'epoch': 0.03}        \n",
      "{'loss': 1.9194, 'learning_rate': 0.00013423501488151303, 'epoch': 0.03}        \n",
      "{'loss': 2.0504, 'learning_rate': 0.00013427268959801078, 'epoch': 0.03}        \n",
      "  3%|▉                            | 36044/1061708 [5:23:45<153:46:44,  1.85it/s][2024-02-29 23:30:46,302] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|▉                            | 36045/1061708 [5:23:46<144:07:20,  1.98it/s][2024-02-29 23:30:46,731] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0647, 'learning_rate': 0.000134302829371209, 'epoch': 0.03}          \n",
      "{'loss': 2.1745, 'learning_rate': 0.00013434050408770676, 'epoch': 0.03}        \n",
      "{'loss': 2.1804, 'learning_rate': 0.0001343781788042045, 'epoch': 0.03}         \n",
      "{'loss': 2.2769, 'learning_rate': 0.00013441585352070228, 'epoch': 0.03}        \n",
      "{'loss': 2.0282, 'learning_rate': 0.0001344535282372, 'epoch': 0.03}            \n",
      "{'loss': 1.7399, 'learning_rate': 0.00013449120295369778, 'epoch': 0.03}        \n",
      "{'loss': 2.2682, 'learning_rate': 0.00013452887767019552, 'epoch': 0.03}        \n",
      "{'loss': 2.0836, 'learning_rate': 0.0001345665523866933, 'epoch': 0.03}         \n",
      "  3%|▉                            | 36122/1061708 [5:24:27<150:51:17,  1.89it/s][2024-02-29 23:31:27,755] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3174, 'learning_rate': 0.0001346004596315413, 'epoch': 0.03}         \n",
      "{'loss': 1.9737, 'learning_rate': 0.00013463813434803904, 'epoch': 0.03}        \n",
      "{'loss': 2.2832, 'learning_rate': 0.0001346758090645368, 'epoch': 0.03}         \n",
      "{'loss': 2.451, 'learning_rate': 0.00013471348378103456, 'epoch': 0.03}         \n",
      "{'loss': 1.9556, 'learning_rate': 0.0001347511584975323, 'epoch': 0.03}         \n",
      "{'loss': 2.3087, 'learning_rate': 0.00013478883321403008, 'epoch': 0.03}        \n",
      "{'loss': 2.291, 'learning_rate': 0.00013482650793052782, 'epoch': 0.03}         \n",
      "{'loss': 1.9898, 'learning_rate': 0.00013486418264702557, 'epoch': 0.03}        \n",
      "{'loss': 2.3272, 'learning_rate': 0.00013490185736352334, 'epoch': 0.03}        \n",
      "{'loss': 2.1345, 'learning_rate': 0.0001349395320800211, 'epoch': 0.03}         \n",
      "{'loss': 2.0002, 'learning_rate': 0.00013497720679651887, 'epoch': 0.03}        \n",
      "{'loss': 2.2463, 'learning_rate': 0.0001350148815130166, 'epoch': 0.03}         \n",
      "{'loss': 2.259, 'learning_rate': 0.0001350525562295144, 'epoch': 0.03}          \n",
      "{'loss': 2.3765, 'learning_rate': 0.00013509023094601213, 'epoch': 0.03}        \n",
      "{'loss': 2.5114, 'learning_rate': 0.0001351279056625099, 'epoch': 0.03}         \n",
      "  3%|▉                            | 36277/1061708 [5:25:49<151:31:24,  1.88it/s][2024-02-29 23:32:50,269] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1075, 'learning_rate': 0.00013516181290735787, 'epoch': 0.03}        \n",
      "  3%|▉                            | 36289/1061708 [5:25:56<151:28:03,  1.88it/s][2024-02-29 23:32:56,597] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.7336, 'learning_rate': 0.00013519572015220586, 'epoch': 0.03}        \n",
      "{'loss': 2.0663, 'learning_rate': 0.0001352333948687036, 'epoch': 0.03}         \n",
      "{'loss': 2.3977, 'learning_rate': 0.00013527106958520138, 'epoch': 0.03}        \n",
      "{'loss': 2.3109, 'learning_rate': 0.00013530874430169913, 'epoch': 0.03}        \n",
      "{'loss': 2.3014, 'learning_rate': 0.00013534641901819688, 'epoch': 0.03}        \n",
      "{'loss': 2.315, 'learning_rate': 0.00013538409373469465, 'epoch': 0.03}         \n",
      "{'loss': 2.3595, 'learning_rate': 0.0001354217684511924, 'epoch': 0.03}         \n",
      "{'loss': 2.1026, 'learning_rate': 0.00013545944316769017, 'epoch': 0.03}        \n",
      "{'loss': 1.9053, 'learning_rate': 0.00013549711788418792, 'epoch': 0.03}        \n",
      "{'loss': 1.7756, 'learning_rate': 0.0001355347926006857, 'epoch': 0.03}         \n",
      "{'loss': 1.7893, 'learning_rate': 0.00013557246731718344, 'epoch': 0.03}        \n",
      "{'loss': 2.6896, 'learning_rate': 0.00013561014203368121, 'epoch': 0.03}        \n",
      "{'loss': 2.4125, 'learning_rate': 0.00013564781675017896, 'epoch': 0.03}        \n",
      "{'loss': 2.4995, 'learning_rate': 0.00013568549146667674, 'epoch': 0.03}        \n",
      "{'loss': 2.0787, 'learning_rate': 0.00013572316618317448, 'epoch': 0.03}        \n",
      "{'loss': 2.4358, 'learning_rate': 0.00013576084089967223, 'epoch': 0.03}        \n",
      "{'loss': 2.1061, 'learning_rate': 0.00013579851561616998, 'epoch': 0.03}        \n",
      "{'loss': 2.0616, 'learning_rate': 0.00013583619033266775, 'epoch': 0.03}        \n",
      "{'loss': 1.9905, 'learning_rate': 0.0001358738650491655, 'epoch': 0.03}         \n",
      "{'loss': 2.4302, 'learning_rate': 0.00013591153976566327, 'epoch': 0.03}        \n",
      "  3%|▉                            | 36489/1061708 [5:27:42<150:52:02,  1.89it/s][2024-02-29 23:34:43,069] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2939, 'learning_rate': 0.00013594544701051126, 'epoch': 0.03}        \n",
      "{'loss': 2.2467, 'learning_rate': 0.000135983121727009, 'epoch': 0.03}          \n",
      "{'loss': 2.3434, 'learning_rate': 0.00013602079644350676, 'epoch': 0.03}        \n",
      "{'loss': 2.3119, 'learning_rate': 0.00013605847116000453, 'epoch': 0.03}        \n",
      "{'loss': 1.8134, 'learning_rate': 0.00013609614587650228, 'epoch': 0.03}        \n",
      "{'loss': 2.3245, 'learning_rate': 0.00013613382059300005, 'epoch': 0.03}        \n",
      "{'loss': 2.535, 'learning_rate': 0.0001361714953094978, 'epoch': 0.03}          \n",
      "{'loss': 1.9765, 'learning_rate': 0.00013620917002599557, 'epoch': 0.03}        \n",
      "{'loss': 2.1545, 'learning_rate': 0.00013624684474249332, 'epoch': 0.03}        \n",
      "{'loss': 2.4598, 'learning_rate': 0.00013628451945899107, 'epoch': 0.03}        \n",
      "{'loss': 2.1071, 'learning_rate': 0.00013632219417548884, 'epoch': 0.03}        \n",
      "{'loss': 2.551, 'learning_rate': 0.00013635986889198659, 'epoch': 0.03}         \n",
      "{'loss': 2.085, 'learning_rate': 0.00013639754360848436, 'epoch': 0.03}         \n",
      "{'loss': 2.1452, 'learning_rate': 0.0001364352183249821, 'epoch': 0.03}         \n",
      "{'loss': 2.1551, 'learning_rate': 0.00013647289304147988, 'epoch': 0.03}        \n",
      "  3%|█                            | 36633/1061708 [5:28:59<154:42:33,  1.84it/s][2024-02-29 23:35:59,764] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4929, 'learning_rate': 0.00013650680028632785, 'epoch': 0.03}        \n",
      "{'loss': 2.403, 'learning_rate': 0.00013654447500282562, 'epoch': 0.03}         \n",
      "  3%|█                            | 36655/1061708 [5:29:10<152:19:37,  1.87it/s][2024-02-29 23:36:11,431] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1501, 'learning_rate': 0.00013657838224767358, 'epoch': 0.03}        \n",
      "{'loss': 2.3179, 'learning_rate': 0.00013661605696417136, 'epoch': 0.03}        \n",
      "{'loss': 2.1283, 'learning_rate': 0.0001366537316806691, 'epoch': 0.03}         \n",
      "{'loss': 2.2846, 'learning_rate': 0.00013669140639716688, 'epoch': 0.03}        \n",
      "{'loss': 2.064, 'learning_rate': 0.00013672908111366462, 'epoch': 0.03}         \n",
      "{'loss': 2.3509, 'learning_rate': 0.00013676675583016237, 'epoch': 0.03}        \n",
      "{'loss': 1.878, 'learning_rate': 0.00013680443054666015, 'epoch': 0.03}         \n",
      "{'loss': 1.949, 'learning_rate': 0.0001368421052631579, 'epoch': 0.03}          \n",
      "{'loss': 2.6259, 'learning_rate': 0.00013687977997965567, 'epoch': 0.03}        \n",
      "{'loss': 2.3908, 'learning_rate': 0.00013691745469615341, 'epoch': 0.03}        \n",
      "{'loss': 2.2807, 'learning_rate': 0.0001369551294126512, 'epoch': 0.03}         \n",
      "{'loss': 2.0764, 'learning_rate': 0.00013699280412914893, 'epoch': 0.03}        \n",
      "{'loss': 2.2863, 'learning_rate': 0.0001370304788456467, 'epoch': 0.03}         \n",
      "{'loss': 2.2003, 'learning_rate': 0.00013706815356214443, 'epoch': 0.03}        \n",
      "{'loss': 1.92, 'learning_rate': 0.0001371058282786422, 'epoch': 0.03}           \n",
      "{'loss': 2.1839, 'learning_rate': 0.00013714350299513995, 'epoch': 0.03}        \n",
      "{'loss': 2.2543, 'learning_rate': 0.00013718117771163772, 'epoch': 0.03}        \n",
      "{'loss': 2.4165, 'learning_rate': 0.00013721885242813547, 'epoch': 0.03}        \n",
      "{'loss': 2.1644, 'learning_rate': 0.00013725652714463324, 'epoch': 0.03}        \n",
      "{'loss': 2.4289, 'learning_rate': 0.000137294201861131, 'epoch': 0.03}          \n",
      "  3%|█                            | 36855/1061708 [5:30:57<152:17:02,  1.87it/s][2024-02-29 23:37:57,949] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4119, 'learning_rate': 0.00013732810910597898, 'epoch': 0.03}        \n",
      "{'loss': 2.1102, 'learning_rate': 0.00013736578382247673, 'epoch': 0.03}        \n",
      "{'loss': 2.5529, 'learning_rate': 0.0001374034585389745, 'epoch': 0.03}         \n",
      "  3%|█                            | 36882/1061708 [5:31:11<151:09:00,  1.88it/s][2024-02-29 23:38:12,295] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9993, 'learning_rate': 0.0001374373657838225, 'epoch': 0.03}         \n",
      "{'loss': 2.4108, 'learning_rate': 0.00013747504050032024, 'epoch': 0.03}        \n",
      "{'loss': 2.3617, 'learning_rate': 0.00013751271521681802, 'epoch': 0.03}        \n",
      "{'loss': 1.9213, 'learning_rate': 0.00013755038993331576, 'epoch': 0.03}        \n",
      "{'loss': 1.8664, 'learning_rate': 0.0001375880646498135, 'epoch': 0.03}         \n",
      "{'loss': 2.2946, 'learning_rate': 0.00013762573936631126, 'epoch': 0.03}        \n",
      "{'loss': 2.3621, 'learning_rate': 0.00013766341408280903, 'epoch': 0.03}        \n",
      "{'loss': 2.2582, 'learning_rate': 0.00013770108879930678, 'epoch': 0.03}        \n",
      "{'loss': 1.9924, 'learning_rate': 0.00013773876351580455, 'epoch': 0.03}        \n",
      "{'loss': 1.8738, 'learning_rate': 0.0001377764382323023, 'epoch': 0.03}         \n",
      "{'loss': 2.0953, 'learning_rate': 0.00013781411294880007, 'epoch': 0.03}        \n",
      "{'loss': 2.2038, 'learning_rate': 0.00013785178766529782, 'epoch': 0.03}        \n",
      "{'loss': 2.4305, 'learning_rate': 0.0001378894623817956, 'epoch': 0.03}         \n",
      "{'loss': 2.0595, 'learning_rate': 0.00013792713709829334, 'epoch': 0.03}        \n",
      "{'loss': 2.2525, 'learning_rate': 0.0001379648118147911, 'epoch': 0.03}         \n",
      "{'loss': 2.0435, 'learning_rate': 0.00013800248653128886, 'epoch': 0.03}        \n",
      "{'loss': 2.6035, 'learning_rate': 0.00013804016124778663, 'epoch': 0.03}        \n",
      "{'loss': 2.3662, 'learning_rate': 0.00013807783596428438, 'epoch': 0.03}        \n",
      "{'loss': 2.3538, 'learning_rate': 0.00013811551068078213, 'epoch': 0.03}        \n",
      "{'loss': 2.3815, 'learning_rate': 0.00013815318539727987, 'epoch': 0.03}        \n",
      "  3%|█                            | 37083/1061708 [5:32:58<154:15:59,  1.84it/s][2024-02-29 23:39:59,458] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  3%|█                            | 37084/1061708 [5:32:59<144:05:52,  1.98it/s][2024-02-29 23:39:59,882] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2962, 'learning_rate': 0.00013818332517047808, 'epoch': 0.03}        \n",
      "{'loss': 2.0335, 'learning_rate': 0.00013822099988697586, 'epoch': 0.03}        \n",
      "{'loss': 2.221, 'learning_rate': 0.0001382586746034736, 'epoch': 0.03}          \n",
      "{'loss': 2.3538, 'learning_rate': 0.00013829634931997138, 'epoch': 0.03}        \n",
      "{'loss': 2.0246, 'learning_rate': 0.00013833402403646913, 'epoch': 0.03}        \n",
      "  3%|█                            | 37137/1061708 [5:33:27<151:22:02,  1.88it/s][2024-02-29 23:40:28,082] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2792, 'learning_rate': 0.00013836793128131712, 'epoch': 0.03}        \n",
      "{'loss': 2.0104, 'learning_rate': 0.00013840560599781486, 'epoch': 0.03}        \n",
      "{'loss': 2.5004, 'learning_rate': 0.00013844328071431264, 'epoch': 0.04}        \n",
      "{'loss': 2.3707, 'learning_rate': 0.00013848095543081038, 'epoch': 0.04}        \n",
      "{'loss': 2.6547, 'learning_rate': 0.00013851863014730816, 'epoch': 0.04}        \n",
      "{'loss': 2.191, 'learning_rate': 0.0001385563048638059, 'epoch': 0.04}          \n",
      "{'loss': 2.1437, 'learning_rate': 0.00013859397958030368, 'epoch': 0.04}        \n",
      "{'loss': 2.0894, 'learning_rate': 0.00013863165429680143, 'epoch': 0.04}        \n",
      "{'loss': 2.146, 'learning_rate': 0.0001386693290132992, 'epoch': 0.04}          \n",
      "{'loss': 2.0305, 'learning_rate': 0.00013870700372979695, 'epoch': 0.04}        \n",
      "{'loss': 2.0364, 'learning_rate': 0.0001387446784462947, 'epoch': 0.04}         \n",
      "{'loss': 2.4205, 'learning_rate': 0.00013878235316279247, 'epoch': 0.04}        \n",
      "{'loss': 2.341, 'learning_rate': 0.00013882002787929021, 'epoch': 0.04}         \n",
      "{'loss': 2.4717, 'learning_rate': 0.00013885770259578796, 'epoch': 0.04}        \n",
      "{'loss': 2.2151, 'learning_rate': 0.00013889537731228574, 'epoch': 0.04}        \n",
      "  4%|█                            | 37285/1061708 [5:34:46<152:09:38,  1.87it/s][2024-02-29 23:41:46,898] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.384, 'learning_rate': 0.00013892928455713373, 'epoch': 0.04}         \n",
      "{'loss': 2.0975, 'learning_rate': 0.00013896695927363147, 'epoch': 0.04}        \n",
      "  4%|█                            | 37306/1061708 [5:34:57<151:34:44,  1.88it/s][2024-02-29 23:41:58,038] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2319, 'learning_rate': 0.00013900086651847946, 'epoch': 0.04}        \n",
      "{'loss': 2.0576, 'learning_rate': 0.0001390385412349772, 'epoch': 0.04}         \n",
      "{'loss': 2.2816, 'learning_rate': 0.00013907621595147499, 'epoch': 0.04}        \n",
      "{'loss': 2.4792, 'learning_rate': 0.00013911389066797273, 'epoch': 0.04}        \n",
      "  4%|█                            | 37344/1061708 [5:35:17<153:00:32,  1.86it/s][2024-02-29 23:42:18,237] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.9611, 'learning_rate': 0.0001391477979128207, 'epoch': 0.04}         \n",
      "{'loss': 2.2874, 'learning_rate': 0.00013918547262931847, 'epoch': 0.04}        \n",
      "{'loss': 1.9848, 'learning_rate': 0.00013922314734581622, 'epoch': 0.04}        \n",
      "{'loss': 2.4852, 'learning_rate': 0.000139260822062314, 'epoch': 0.04}          \n",
      "{'loss': 2.0021, 'learning_rate': 0.00013929849677881174, 'epoch': 0.04}        \n",
      "{'loss': 2.1967, 'learning_rate': 0.0001393361714953095, 'epoch': 0.04}         \n",
      "{'loss': 2.2443, 'learning_rate': 0.00013937384621180726, 'epoch': 0.04}        \n",
      "{'loss': 2.5247, 'learning_rate': 0.00013941152092830503, 'epoch': 0.04}        \n",
      "{'loss': 2.275, 'learning_rate': 0.00013944919564480278, 'epoch': 0.04}         \n",
      "{'loss': 2.0063, 'learning_rate': 0.00013948687036130055, 'epoch': 0.04}        \n",
      "{'loss': 2.302, 'learning_rate': 0.00013952454507779827, 'epoch': 0.04}         \n",
      "{'loss': 2.6007, 'learning_rate': 0.00013956221979429605, 'epoch': 0.04}        \n",
      "{'loss': 2.5898, 'learning_rate': 0.0001395998945107938, 'epoch': 0.04}         \n",
      "{'loss': 2.5864, 'learning_rate': 0.00013963756922729157, 'epoch': 0.04}        \n",
      "  4%|█                            | 37488/1061708 [5:36:34<150:56:11,  1.88it/s][2024-02-29 23:43:34,888] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3532, 'learning_rate': 0.00013967147647213956, 'epoch': 0.04}        \n",
      "{'loss': 2.4077, 'learning_rate': 0.00013970915118863733, 'epoch': 0.04}        \n",
      "{'loss': 2.2732, 'learning_rate': 0.00013974682590513508, 'epoch': 0.04}        \n",
      "{'loss': 2.2026, 'learning_rate': 0.00013978450062163283, 'epoch': 0.04}        \n",
      "{'loss': 2.4693, 'learning_rate': 0.00013982217533813057, 'epoch': 0.04}        \n",
      "{'loss': 2.3136, 'learning_rate': 0.00013985985005462835, 'epoch': 0.04}        \n",
      "{'loss': 2.0642, 'learning_rate': 0.0001398975247711261, 'epoch': 0.04}         \n",
      "{'loss': 2.1638, 'learning_rate': 0.00013993519948762387, 'epoch': 0.04}        \n",
      "{'loss': 1.7954, 'learning_rate': 0.00013997287420412162, 'epoch': 0.04}        \n",
      "{'loss': 2.2376, 'learning_rate': 0.0001400105489206194, 'epoch': 0.04}         \n",
      "{'loss': 2.1848, 'learning_rate': 0.00014004822363711714, 'epoch': 0.04}        \n",
      "{'loss': 2.2184, 'learning_rate': 0.00014008589835361488, 'epoch': 0.04}        \n",
      "{'loss': 2.5108, 'learning_rate': 0.00014012357307011266, 'epoch': 0.04}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4372, 'learning_rate': 0.0001401612477866104, 'epoch': 0.04}         \n",
      "{'loss': 2.8179, 'learning_rate': 0.00014019892250310818, 'epoch': 0.04}        \n",
      "{'loss': 2.1992, 'learning_rate': 0.00014023659721960593, 'epoch': 0.04}        \n",
      "  4%|█                            | 37643/1061708 [5:37:56<154:22:45,  1.84it/s][2024-02-29 23:44:57,445] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2729, 'learning_rate': 0.00014027050446445392, 'epoch': 0.04}        \n",
      "{'loss': 2.4222, 'learning_rate': 0.00014030817918095166, 'epoch': 0.04}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00014034585389744944, 'epoch': 0.04}        \n",
      "{'loss': 2.4658, 'learning_rate': 0.00014038352861394719, 'epoch': 0.04}        \n",
      "{'loss': 2.3757, 'learning_rate': 0.00014042120333044496, 'epoch': 0.04}        \n",
      "  4%|█                            | 37690/1061708 [5:38:21<150:31:22,  1.89it/s][2024-02-29 23:45:22,409] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1171, 'learning_rate': 0.00014045511057529292, 'epoch': 0.04}        \n",
      "{'loss': 2.1799, 'learning_rate': 0.0001404927852917907, 'epoch': 0.04}         \n",
      "{'loss': 2.2185, 'learning_rate': 0.00014053046000828844, 'epoch': 0.04}        \n",
      "{'loss': 2.5385, 'learning_rate': 0.00014056813472478622, 'epoch': 0.04}        \n",
      "{'loss': 2.0283, 'learning_rate': 0.00014060580944128396, 'epoch': 0.04}        \n",
      "{'loss': 2.0362, 'learning_rate': 0.0001406434841577817, 'epoch': 0.04}         \n",
      "{'loss': 2.3823, 'learning_rate': 0.00014068115887427949, 'epoch': 0.04}        \n",
      "{'loss': 2.1991, 'learning_rate': 0.00014071883359077723, 'epoch': 0.04}        \n",
      "{'loss': 2.1876, 'learning_rate': 0.000140756508307275, 'epoch': 0.04}          \n",
      "{'loss': 2.6053, 'learning_rate': 0.00014079418302377275, 'epoch': 0.04}        \n",
      "{'loss': 2.2651, 'learning_rate': 0.0001408318577402705, 'epoch': 0.04}         \n",
      "{'loss': 2.0703, 'learning_rate': 0.00014086953245676825, 'epoch': 0.04}        \n",
      "{'loss': 2.4556, 'learning_rate': 0.00014090720717326602, 'epoch': 0.04}        \n",
      "{'loss': 2.0009, 'learning_rate': 0.00014094488188976377, 'epoch': 0.04}        \n",
      "{'loss': 2.3198, 'learning_rate': 0.00014098255660626154, 'epoch': 0.04}        \n",
      "{'loss': 2.3367, 'learning_rate': 0.0001410202313227593, 'epoch': 0.04}         \n",
      "  4%|█                            | 37855/1061708 [5:39:49<152:13:32,  1.87it/s][2024-02-29 23:46:50,252] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8595, 'learning_rate': 0.00014105413856760728, 'epoch': 0.04}        \n",
      "  4%|█                            | 37861/1061708 [5:39:52<152:00:34,  1.87it/s][2024-02-29 23:46:53,387] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2045, 'learning_rate': 0.00014108804581245527, 'epoch': 0.04}        \n",
      "{'loss': 2.1745, 'learning_rate': 0.00014112572052895302, 'epoch': 0.04}        \n",
      "{'loss': 2.6045, 'learning_rate': 0.0001411633952454508, 'epoch': 0.04}         \n",
      "{'loss': 2.3963, 'learning_rate': 0.00014120106996194854, 'epoch': 0.04}        \n",
      "{'loss': 2.5268, 'learning_rate': 0.0001412387446784463, 'epoch': 0.04}         \n",
      "{'loss': 2.3478, 'learning_rate': 0.00014127641939494406, 'epoch': 0.04}        \n",
      "{'loss': 2.1843, 'learning_rate': 0.0001413140941114418, 'epoch': 0.04}         \n",
      "{'loss': 2.2685, 'learning_rate': 0.00014135176882793958, 'epoch': 0.04}        \n",
      "{'loss': 2.3536, 'learning_rate': 0.00014138944354443733, 'epoch': 0.04}        \n",
      "{'loss': 2.2082, 'learning_rate': 0.00014142711826093507, 'epoch': 0.04}        \n",
      "{'loss': 2.1049, 'learning_rate': 0.00014146479297743285, 'epoch': 0.04}        \n",
      "{'loss': 2.1511, 'learning_rate': 0.0001415024676939306, 'epoch': 0.04}         \n",
      "{'loss': 2.1267, 'learning_rate': 0.00014154014241042837, 'epoch': 0.04}        \n",
      "  4%|█                            | 37999/1061708 [5:41:06<150:40:52,  1.89it/s][2024-02-29 23:48:06,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=421, lr=[0.00014157781712692612], mom=[(0.9, 0.999)]\n",
      "[2024-02-29 23:48:06,945] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=1.891994080850014, CurrSamplesPerSec=1.9001587428148943, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1191, 'learning_rate': 0.00014157781712692612, 'epoch': 0.04}        \n",
      "{'loss': 2.6428, 'learning_rate': 0.0001416154918434239, 'epoch': 0.04}         \n",
      "{'loss': 1.9822, 'learning_rate': 0.00014165316655992164, 'epoch': 0.04}        \n",
      "{'loss': 2.0951, 'learning_rate': 0.0001416908412764194, 'epoch': 0.04}         \n",
      "{'loss': 2.1542, 'learning_rate': 0.00014172851599291716, 'epoch': 0.04}        \n",
      "{'loss': 2.0777, 'learning_rate': 0.00014176619070941493, 'epoch': 0.04}        \n",
      "{'loss': 1.9769, 'learning_rate': 0.00014180386542591268, 'epoch': 0.04}        \n",
      "  4%|█                            | 38062/1061708 [5:41:39<150:31:30,  1.89it/s][2024-02-29 23:48:40,436] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█                            | 38063/1061708 [5:41:40<144:54:09,  1.96it/s][2024-02-29 23:48:40,860] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1714, 'learning_rate': 0.0001418340051991109, 'epoch': 0.04}         \n",
      "{'loss': 2.2825, 'learning_rate': 0.00014187167991560863, 'epoch': 0.04}        \n",
      "{'loss': 2.2489, 'learning_rate': 0.00014190935463210638, 'epoch': 0.04}        \n",
      "  4%|█                            | 38092/1061708 [5:41:55<151:37:50,  1.88it/s][2024-02-29 23:48:56,297] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7632, 'learning_rate': 0.0001419432618769544, 'epoch': 0.04}         \n",
      "{'loss': 2.3002, 'learning_rate': 0.00014198093659345215, 'epoch': 0.04}        \n",
      "{'loss': 2.15, 'learning_rate': 0.0001420186113099499, 'epoch': 0.04}           \n",
      "{'loss': 2.0925, 'learning_rate': 0.00014205628602644764, 'epoch': 0.04}        \n",
      "{'loss': 2.2209, 'learning_rate': 0.00014209396074294541, 'epoch': 0.04}        \n",
      "{'loss': 1.8494, 'learning_rate': 0.00014213163545944316, 'epoch': 0.04}        \n",
      "{'loss': 2.2188, 'learning_rate': 0.00014216931017594094, 'epoch': 0.04}        \n",
      "{'loss': 1.9083, 'learning_rate': 0.00014220698489243868, 'epoch': 0.04}        \n",
      "{'loss': 2.2096, 'learning_rate': 0.00014224465960893646, 'epoch': 0.04}        \n",
      "{'loss': 2.1707, 'learning_rate': 0.0001422823343254342, 'epoch': 0.04}         \n",
      "{'loss': 1.9468, 'learning_rate': 0.00014232000904193198, 'epoch': 0.04}        \n",
      "{'loss': 2.4442, 'learning_rate': 0.00014235768375842972, 'epoch': 0.04}        \n",
      "{'loss': 2.4032, 'learning_rate': 0.0001423953584749275, 'epoch': 0.04}         \n",
      "{'loss': 2.0418, 'learning_rate': 0.00014243303319142524, 'epoch': 0.04}        \n",
      "{'loss': 2.508, 'learning_rate': 0.00014247070790792302, 'epoch': 0.04}         \n",
      "{'loss': 2.2447, 'learning_rate': 0.00014250838262442077, 'epoch': 0.04}        \n",
      "{'loss': 2.3786, 'learning_rate': 0.00014254605734091854, 'epoch': 0.04}        \n",
      "  4%|█                            | 38263/1061708 [5:43:26<153:52:00,  1.85it/s][2024-02-29 23:50:27,343] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0728, 'learning_rate': 0.0001425799645857665, 'epoch': 0.04}         \n",
      "  4%|█                            | 38278/1061708 [5:43:34<150:48:55,  1.88it/s][2024-02-29 23:50:35,256] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2411, 'learning_rate': 0.00014261387183061447, 'epoch': 0.04}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3263, 'learning_rate': 0.00014265154654711224, 'epoch': 0.04}        \n",
      "{'loss': 2.6872, 'learning_rate': 0.00014268922126361, 'epoch': 0.04}           \n",
      "{'loss': 2.1895, 'learning_rate': 0.00014272689598010776, 'epoch': 0.04}        \n",
      "{'loss': 1.9963, 'learning_rate': 0.0001427645706966055, 'epoch': 0.04}         \n",
      "{'loss': 2.1385, 'learning_rate': 0.00014280224541310328, 'epoch': 0.04}        \n",
      "{'loss': 2.6307, 'learning_rate': 0.00014283992012960103, 'epoch': 0.04}        \n",
      "{'loss': 2.1522, 'learning_rate': 0.0001428775948460988, 'epoch': 0.04}         \n",
      "{'loss': 2.2701, 'learning_rate': 0.00014291526956259655, 'epoch': 0.04}        \n",
      "{'loss': 2.2793, 'learning_rate': 0.00014295294427909433, 'epoch': 0.04}        \n",
      "{'loss': 2.4261, 'learning_rate': 0.00014299061899559207, 'epoch': 0.04}        \n",
      "{'loss': 1.7823, 'learning_rate': 0.00014302829371208985, 'epoch': 0.04}        \n",
      "{'loss': 2.372, 'learning_rate': 0.00014306596842858757, 'epoch': 0.04}         \n",
      "{'loss': 2.0415, 'learning_rate': 0.00014310364314508534, 'epoch': 0.04}        \n",
      "  4%|█                            | 38412/1061708 [5:44:46<151:00:43,  1.88it/s][2024-02-29 23:51:46,735] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8059, 'learning_rate': 0.00014313755038993333, 'epoch': 0.04}        \n",
      "  4%|█                            | 38422/1061708 [5:44:51<150:57:21,  1.88it/s][2024-02-29 23:51:52,023] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1731, 'learning_rate': 0.0001431714576347813, 'epoch': 0.04}         \n",
      "{'loss': 2.294, 'learning_rate': 0.00014320913235127907, 'epoch': 0.04}         \n",
      "{'loss': 2.2093, 'learning_rate': 0.00014324680706777682, 'epoch': 0.04}        \n",
      "{'loss': 2.265, 'learning_rate': 0.0001432844817842746, 'epoch': 0.04}          \n",
      "{'loss': 2.2178, 'learning_rate': 0.00014332215650077234, 'epoch': 0.04}        \n",
      "{'loss': 2.3906, 'learning_rate': 0.0001433598312172701, 'epoch': 0.04}         \n",
      "{'loss': 2.3158, 'learning_rate': 0.00014339750593376786, 'epoch': 0.04}        \n",
      "{'loss': 2.0671, 'learning_rate': 0.00014343518065026563, 'epoch': 0.04}        \n",
      "{'loss': 2.0856, 'learning_rate': 0.00014347285536676338, 'epoch': 0.04}        \n",
      "{'loss': 2.5019, 'learning_rate': 0.00014351053008326115, 'epoch': 0.04}        \n",
      "{'loss': 2.341, 'learning_rate': 0.00014354820479975887, 'epoch': 0.04}         \n",
      "{'loss': 2.434, 'learning_rate': 0.00014358587951625665, 'epoch': 0.04}         \n",
      "{'loss': 2.2012, 'learning_rate': 0.0001436235542327544, 'epoch': 0.04}         \n",
      "{'loss': 2.4694, 'learning_rate': 0.00014366122894925217, 'epoch': 0.04}        \n",
      "{'loss': 2.0011, 'learning_rate': 0.00014369890366574991, 'epoch': 0.04}        \n",
      "{'loss': 2.0284, 'learning_rate': 0.0001437365783822477, 'epoch': 0.04}         \n",
      "{'loss': 2.3651, 'learning_rate': 0.00014377425309874544, 'epoch': 0.04}        \n",
      "  4%|█                            | 38592/1061708 [5:46:22<151:18:03,  1.88it/s][2024-02-29 23:53:22,827] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.559, 'learning_rate': 0.0001438081603435934, 'epoch': 0.04}          \n",
      "{'loss': 2.046, 'learning_rate': 0.00014384583506009117, 'epoch': 0.04}         \n",
      "{'loss': 2.4141, 'learning_rate': 0.00014388350977658892, 'epoch': 0.04}        \n",
      "{'loss': 1.9623, 'learning_rate': 0.0001439211844930867, 'epoch': 0.04}         \n",
      "  4%|█                            | 38636/1061708 [5:46:45<152:20:48,  1.87it/s][2024-02-29 23:53:46,332] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4462, 'learning_rate': 0.00014395509173793469, 'epoch': 0.04}        \n",
      "{'loss': 2.0053, 'learning_rate': 0.00014399276645443246, 'epoch': 0.04}        \n",
      "{'loss': 2.1569, 'learning_rate': 0.00014403044117093018, 'epoch': 0.04}        \n",
      "{'loss': 2.358, 'learning_rate': 0.00014406811588742795, 'epoch': 0.04}         \n",
      "{'loss': 1.9923, 'learning_rate': 0.0001441057906039257, 'epoch': 0.04}         \n",
      "{'loss': 2.1282, 'learning_rate': 0.00014414346532042347, 'epoch': 0.04}        \n",
      "{'loss': 2.2601, 'learning_rate': 0.00014418114003692122, 'epoch': 0.04}        \n",
      "{'loss': 2.6997, 'learning_rate': 0.000144218814753419, 'epoch': 0.04}          \n",
      "{'loss': 2.4285, 'learning_rate': 0.00014425648946991674, 'epoch': 0.04}        \n",
      "  4%|█                            | 38727/1061708 [5:47:34<151:25:32,  1.88it/s][2024-02-29 23:54:34,891] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1714, 'learning_rate': 0.0001442903967147647, 'epoch': 0.04}         \n",
      "{'loss': 1.9636, 'learning_rate': 0.00014432807143126248, 'epoch': 0.04}        \n",
      "{'loss': 2.2158, 'learning_rate': 0.00014436574614776023, 'epoch': 0.04}        \n",
      "{'loss': 2.434, 'learning_rate': 0.000144403420864258, 'epoch': 0.04}           \n",
      "{'loss': 2.2826, 'learning_rate': 0.00014444109558075575, 'epoch': 0.04}        \n",
      "{'loss': 2.4472, 'learning_rate': 0.00014447877029725352, 'epoch': 0.04}        \n",
      "{'loss': 2.0238, 'learning_rate': 0.00014451644501375127, 'epoch': 0.04}        \n",
      "{'loss': 2.0207, 'learning_rate': 0.00014455411973024904, 'epoch': 0.04}        \n",
      "{'loss': 1.97, 'learning_rate': 0.0001445917944467468, 'epoch': 0.04}           \n",
      "{'loss': 2.1838, 'learning_rate': 0.00014462946916324456, 'epoch': 0.04}        \n",
      "{'loss': 2.2051, 'learning_rate': 0.0001446671438797423, 'epoch': 0.04}         \n",
      "{'loss': 2.0735, 'learning_rate': 0.00014470481859624008, 'epoch': 0.04}        \n",
      "{'loss': 2.1779, 'learning_rate': 0.00014474249331273783, 'epoch': 0.04}        \n",
      "  4%|█                            | 38857/1061708 [5:48:43<151:21:49,  1.88it/s][2024-02-29 23:55:44,239] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1277, 'learning_rate': 0.00014477640055758582, 'epoch': 0.04}        \n",
      "{'loss': 2.0221, 'learning_rate': 0.00014481407527408357, 'epoch': 0.04}        \n",
      "{'loss': 2.3597, 'learning_rate': 0.00014485174999058134, 'epoch': 0.04}        \n",
      "{'loss': 2.5833, 'learning_rate': 0.0001448894247070791, 'epoch': 0.04}         \n",
      "{'loss': 1.9384, 'learning_rate': 0.00014492709942357684, 'epoch': 0.04}        \n",
      "{'loss': 2.0619, 'learning_rate': 0.0001449647741400746, 'epoch': 0.04}         \n",
      "  4%|█                            | 38916/1061708 [5:49:15<151:46:01,  1.87it/s][2024-02-29 23:56:15,649] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.955, 'learning_rate': 0.00014499868138492258, 'epoch': 0.04}         \n",
      "{'loss': 2.2892, 'learning_rate': 0.00014503635610142035, 'epoch': 0.04}        \n",
      "{'loss': 2.6757, 'learning_rate': 0.0001450740308179181, 'epoch': 0.04}         \n",
      "{'loss': 2.035, 'learning_rate': 0.00014511170553441587, 'epoch': 0.04}         \n",
      "{'loss': 2.5193, 'learning_rate': 0.00014514938025091362, 'epoch': 0.04}        \n",
      "{'loss': 1.9562, 'learning_rate': 0.0001451870549674114, 'epoch': 0.04}         \n",
      "{'loss': 2.3211, 'learning_rate': 0.00014522472968390914, 'epoch': 0.04}        \n",
      "{'loss': 2.3478, 'learning_rate': 0.0001452624044004069, 'epoch': 0.04}         \n",
      "{'loss': 2.6678, 'learning_rate': 0.00014530007911690463, 'epoch': 0.04}        \n",
      "{'loss': 2.587, 'learning_rate': 0.0001453377538334024, 'epoch': 0.04}          \n",
      "{'loss': 2.4767, 'learning_rate': 0.00014537542854990015, 'epoch': 0.04}        \n",
      "{'loss': 1.765, 'learning_rate': 0.00014541310326639793, 'epoch': 0.04}         \n",
      "{'loss': 2.2445, 'learning_rate': 0.00014545077798289567, 'epoch': 0.04}        \n",
      "{'loss': 2.2325, 'learning_rate': 0.00014548845269939345, 'epoch': 0.04}        \n",
      "{'loss': 2.377, 'learning_rate': 0.0001455261274158912, 'epoch': 0.04}          \n",
      "{'loss': 2.463, 'learning_rate': 0.00014556380213238897, 'epoch': 0.04}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4983, 'learning_rate': 0.00014560147684888672, 'epoch': 0.04}        \n",
      "{'loss': 1.8638, 'learning_rate': 0.0001456391515653845, 'epoch': 0.04}         \n",
      "{'loss': 2.3549, 'learning_rate': 0.00014567682628188224, 'epoch': 0.04}        \n",
      "{'loss': 2.0764, 'learning_rate': 0.00014571450099838, 'epoch': 0.04}           \n",
      "{'loss': 2.0931, 'learning_rate': 0.00014575217571487776, 'epoch': 0.04}        \n",
      "  4%|█                            | 39120/1061708 [5:51:03<150:57:21,  1.88it/s][2024-02-29 23:58:04,513] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5457, 'learning_rate': 0.00014578608295972572, 'epoch': 0.04}        \n",
      "{'loss': 2.3872, 'learning_rate': 0.0001458237576762235, 'epoch': 0.04}         \n",
      "{'loss': 2.0501, 'learning_rate': 0.00014586143239272124, 'epoch': 0.04}        \n",
      "{'loss': 2.4097, 'learning_rate': 0.00014589910710921902, 'epoch': 0.04}        \n",
      "{'loss': 2.4081, 'learning_rate': 0.00014593678182571676, 'epoch': 0.04}        \n",
      "{'loss': 2.3331, 'learning_rate': 0.00014597445654221454, 'epoch': 0.04}        \n",
      "{'loss': 2.6508, 'learning_rate': 0.00014601213125871228, 'epoch': 0.04}        \n",
      "{'loss': 2.1494, 'learning_rate': 0.00014604980597521006, 'epoch': 0.04}        \n",
      "  4%|█                            | 39209/1061708 [5:51:51<150:52:49,  1.88it/s][2024-02-29 23:58:51,970] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3914, 'learning_rate': 0.00014608371322005802, 'epoch': 0.04}        \n",
      "{'loss': 2.1114, 'learning_rate': 0.0001461213879365558, 'epoch': 0.04}         \n",
      "{'loss': 1.9574, 'learning_rate': 0.00014615906265305354, 'epoch': 0.04}        \n",
      "{'loss': 2.0581, 'learning_rate': 0.00014619673736955132, 'epoch': 0.04}        \n",
      "{'loss': 2.4964, 'learning_rate': 0.00014623441208604906, 'epoch': 0.04}        \n",
      "{'loss': 2.436, 'learning_rate': 0.00014627208680254684, 'epoch': 0.04}         \n",
      "{'loss': 2.0789, 'learning_rate': 0.00014630976151904458, 'epoch': 0.04}        \n",
      "{'loss': 2.3874, 'learning_rate': 0.00014634743623554236, 'epoch': 0.04}        \n",
      "{'loss': 2.2391, 'learning_rate': 0.00014638511095204008, 'epoch': 0.04}        \n",
      "{'loss': 2.2328, 'learning_rate': 0.00014642278566853785, 'epoch': 0.04}        \n",
      "{'loss': 2.3176, 'learning_rate': 0.0001464604603850356, 'epoch': 0.04}         \n",
      "{'loss': 2.5801, 'learning_rate': 0.00014649813510153337, 'epoch': 0.04}        \n",
      "{'loss': 2.0643, 'learning_rate': 0.00014653580981803112, 'epoch': 0.04}        \n",
      "{'loss': 2.0938, 'learning_rate': 0.0001465734845345289, 'epoch': 0.04}         \n",
      "{'loss': 2.4509, 'learning_rate': 0.00014661115925102664, 'epoch': 0.04}        \n",
      "{'loss': 2.1648, 'learning_rate': 0.0001466488339675244, 'epoch': 0.04}         \n",
      "{'loss': 2.1978, 'learning_rate': 0.00014668650868402216, 'epoch': 0.04}        \n",
      "{'loss': 1.9703, 'learning_rate': 0.0001467241834005199, 'epoch': 0.04}         \n",
      "{'loss': 2.1323, 'learning_rate': 0.00014676185811701768, 'epoch': 0.04}        \n",
      "{'loss': 2.0905, 'learning_rate': 0.00014679953283351543, 'epoch': 0.04}        \n",
      "{'loss': 2.3333, 'learning_rate': 0.0001468372075500132, 'epoch': 0.04}         \n",
      "  4%|█                            | 39410/1061708 [5:53:38<150:48:47,  1.88it/s][2024-03-01 00:00:39,264] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█                            | 39411/1061708 [5:53:39<141:38:29,  2.00it/s][2024-03-01 00:00:39,687] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2692, 'learning_rate': 0.00014686734732321139, 'epoch': 0.04}        \n",
      "{'loss': 2.3866, 'learning_rate': 0.00014690502203970916, 'epoch': 0.04}        \n",
      "{'loss': 2.3122, 'learning_rate': 0.0001469426967562069, 'epoch': 0.04}         \n",
      "{'loss': 2.3677, 'learning_rate': 0.00014698037147270468, 'epoch': 0.04}        \n",
      "{'loss': 2.2465, 'learning_rate': 0.00014701804618920243, 'epoch': 0.04}        \n",
      "  4%|█                            | 39463/1061708 [5:54:06<154:26:44,  1.84it/s][2024-03-01 00:01:07,433] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3519, 'learning_rate': 0.00014705195343405042, 'epoch': 0.04}        \n",
      "{'loss': 2.3194, 'learning_rate': 0.00014708962815054817, 'epoch': 0.04}        \n",
      "{'loss': 2.0643, 'learning_rate': 0.0001471273028670459, 'epoch': 0.04}         \n",
      "{'loss': 2.6183, 'learning_rate': 0.00014716497758354369, 'epoch': 0.04}        \n",
      "{'loss': 2.0238, 'learning_rate': 0.00014720265230004143, 'epoch': 0.04}        \n",
      "{'loss': 2.1786, 'learning_rate': 0.0001472403270165392, 'epoch': 0.04}         \n",
      "  4%|█                            | 39526/1061708 [5:54:40<151:52:32,  1.87it/s][2024-03-01 00:01:41,018] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1805, 'learning_rate': 0.0001472742342613872, 'epoch': 0.04}         \n",
      "{'loss': 2.1483, 'learning_rate': 0.00014731190897788494, 'epoch': 0.04}        \n",
      "{'loss': 1.9304, 'learning_rate': 0.0001473495836943827, 'epoch': 0.04}         \n",
      "{'loss': 2.0752, 'learning_rate': 0.00014738725841088047, 'epoch': 0.04}        \n",
      "{'loss': 2.0702, 'learning_rate': 0.0001474249331273782, 'epoch': 0.04}         \n",
      "{'loss': 1.9935, 'learning_rate': 0.000147462607843876, 'epoch': 0.04}          \n",
      "{'loss': 2.3894, 'learning_rate': 0.00014750028256037373, 'epoch': 0.04}        \n",
      "{'loss': 2.5802, 'learning_rate': 0.0001475379572768715, 'epoch': 0.04}         \n",
      "{'loss': 2.1579, 'learning_rate': 0.00014757563199336925, 'epoch': 0.04}        \n",
      "{'loss': 2.3292, 'learning_rate': 0.00014761330670986703, 'epoch': 0.04}        \n",
      "{'loss': 2.4114, 'learning_rate': 0.00014765098142636478, 'epoch': 0.04}        \n",
      "{'loss': 1.9218, 'learning_rate': 0.00014768865614286255, 'epoch': 0.04}        \n",
      "{'loss': 1.8655, 'learning_rate': 0.0001477263308593603, 'epoch': 0.04}         \n",
      "{'loss': 1.9373, 'learning_rate': 0.00014776400557585804, 'epoch': 0.04}        \n",
      "{'loss': 2.2772, 'learning_rate': 0.00014780168029235582, 'epoch': 0.04}        \n",
      "{'loss': 2.1482, 'learning_rate': 0.00014783935500885356, 'epoch': 0.04}        \n",
      "{'loss': 2.057, 'learning_rate': 0.00014787702972535134, 'epoch': 0.04}         \n",
      "{'loss': 2.3375, 'learning_rate': 0.00014791470444184909, 'epoch': 0.04}        \n",
      "{'loss': 2.7053, 'learning_rate': 0.00014795237915834683, 'epoch': 0.04}        \n",
      "{'loss': 1.7765, 'learning_rate': 0.00014799005387484458, 'epoch': 0.04}        \n",
      "{'loss': 2.3204, 'learning_rate': 0.00014802772859134235, 'epoch': 0.04}        \n",
      "{'loss': 2.246, 'learning_rate': 0.0001480654033078401, 'epoch': 0.04}          \n",
      "{'loss': 2.4686, 'learning_rate': 0.00014810307802433787, 'epoch': 0.04}        \n",
      "{'loss': 2.7166, 'learning_rate': 0.00014814075274083562, 'epoch': 0.04}        \n",
      "{'loss': 2.2646, 'learning_rate': 0.0001481784274573334, 'epoch': 0.04}         \n",
      "{'loss': 2.3966, 'learning_rate': 0.00014821610217383114, 'epoch': 0.04}        \n",
      "  4%|█                            | 39789/1061708 [5:57:00<150:50:12,  1.88it/s][2024-03-01 00:04:01,471] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9638, 'learning_rate': 0.00014825000941867913, 'epoch': 0.04}        \n",
      "  4%|█                            | 39792/1061708 [5:57:02<146:36:09,  1.94it/s][2024-03-01 00:04:03,005] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0314, 'learning_rate': 0.00014828391666352712, 'epoch': 0.04}        \n",
      "{'loss': 2.0808, 'learning_rate': 0.00014832159138002487, 'epoch': 0.04}        \n",
      "{'loss': 2.3066, 'learning_rate': 0.00014835926609652262, 'epoch': 0.04}        \n",
      "{'loss': 2.6103, 'learning_rate': 0.0001483969408130204, 'epoch': 0.04}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2106, 'learning_rate': 0.00014843461552951814, 'epoch': 0.04}        \n",
      "  4%|█                            | 39840/1061708 [5:57:28<150:45:34,  1.88it/s][2024-03-01 00:04:28,539] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.049, 'learning_rate': 0.00014846852277436613, 'epoch': 0.04}         \n",
      "{'loss': 2.1442, 'learning_rate': 0.0001485061974908639, 'epoch': 0.04}         \n",
      "{'loss': 2.2227, 'learning_rate': 0.00014854387220736165, 'epoch': 0.04}        \n",
      "{'loss': 2.3523, 'learning_rate': 0.00014858154692385942, 'epoch': 0.04}        \n",
      "{'loss': 2.2824, 'learning_rate': 0.00014861922164035714, 'epoch': 0.04}        \n",
      "{'loss': 2.0268, 'learning_rate': 0.00014865689635685492, 'epoch': 0.04}        \n",
      "{'loss': 2.2712, 'learning_rate': 0.00014869457107335267, 'epoch': 0.04}        \n",
      "{'loss': 1.9698, 'learning_rate': 0.00014873224578985044, 'epoch': 0.04}        \n",
      "{'loss': 2.3499, 'learning_rate': 0.00014876992050634819, 'epoch': 0.04}        \n",
      "{'loss': 2.1267, 'learning_rate': 0.00014880759522284596, 'epoch': 0.04}        \n",
      "{'loss': 1.8848, 'learning_rate': 0.0001488452699393437, 'epoch': 0.04}         \n",
      "{'loss': 2.337, 'learning_rate': 0.00014888294465584148, 'epoch': 0.04}         \n",
      "{'loss': 2.3757, 'learning_rate': 0.00014892061937233923, 'epoch': 0.04}        \n",
      "{'loss': 2.0318, 'learning_rate': 0.000148958294088837, 'epoch': 0.04}          \n",
      "{'loss': 2.0603, 'learning_rate': 0.00014899596880533475, 'epoch': 0.04}        \n",
      "  4%|█                            | 39999/1061708 [5:58:52<150:21:34,  1.89it/s][2024-03-01 00:05:53,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=442, lr=[0.00014903364352183252], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 00:05:53,426] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=1.8919387521866977, CurrSamplesPerSec=1.9021666075589543, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1781, 'learning_rate': 0.00014903364352183252, 'epoch': 0.04}        \n",
      "  4%|█                            | 40000/1061708 [5:58:53<150:23:36,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 00:05:53,428 >> Saving model checkpoint to output_model/checkpoint-40000\n",
      "[INFO|trainer.py:2880] 2024-03-01 00:05:53,431 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 00:05:54,659 >> tokenizer config file saved in output_model/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 00:05:54,659 >> Special tokens file saved in output_model/checkpoint-40000/special_tokens_map.json\n",
      "[2024-03-01 00:05:54,661] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40000 is about to be saved!\n",
      "[2024-03-01 00:05:59,947] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-40000/global_step40000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 00:05:59,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-40000/global_step40000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 00:06:14,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-40000/global_step40000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 00:06:14,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-40000/global_step40000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 00:06:21,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-40000/global_step40000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 00:06:21,911] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-40000/global_step40000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 00:06:21,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 00:06:21,947 >> Deleting older checkpoint [output_model/checkpoint-25000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 00:06:25,688 >> tokenizer config file saved in output_model/checkpoint-40000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 00:06:25,688 >> Special tokens file saved in output_model/checkpoint-40000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.1968, 'learning_rate': 0.00014907131823833027, 'epoch': 0.04}        \n",
      "{'loss': 2.3087, 'learning_rate': 0.00014910899295482804, 'epoch': 0.04}        \n",
      "{'loss': 2.3856, 'learning_rate': 0.0001491466676713258, 'epoch': 0.04}         \n",
      "{'loss': 2.6193, 'learning_rate': 0.00014918434238782354, 'epoch': 0.04}        \n",
      "{'loss': 2.2674, 'learning_rate': 0.00014922201710432128, 'epoch': 0.04}        \n",
      "{'loss': 1.9588, 'learning_rate': 0.00014925969182081906, 'epoch': 0.04}        \n",
      "{'loss': 1.9886, 'learning_rate': 0.0001492973665373168, 'epoch': 0.04}         \n",
      "{'loss': 2.2146, 'learning_rate': 0.00014933504125381458, 'epoch': 0.04}        \n",
      "{'loss': 2.3624, 'learning_rate': 0.00014937271597031233, 'epoch': 0.04}        \n",
      "{'loss': 2.4773, 'learning_rate': 0.00014941039068681007, 'epoch': 0.04}        \n",
      "{'loss': 2.3299, 'learning_rate': 0.00014944806540330785, 'epoch': 0.04}        \n",
      "  4%|█                            | 40115/1061708 [6:00:26<152:04:39,  1.87it/s][2024-03-01 00:07:27,224] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1325, 'learning_rate': 0.0001494819726481558, 'epoch': 0.04}         \n",
      "{'loss': 2.0772, 'learning_rate': 0.00014951964736465359, 'epoch': 0.04}        \n",
      "  4%|█                            | 40131/1061708 [6:00:35<150:31:24,  1.89it/s][2024-03-01 00:07:35,693] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1319, 'learning_rate': 0.00014955355460950158, 'epoch': 0.04}        \n",
      "{'loss': 2.3291, 'learning_rate': 0.00014959122932599935, 'epoch': 0.04}        \n",
      "{'loss': 1.7442, 'learning_rate': 0.0001496289040424971, 'epoch': 0.04}         \n",
      "{'loss': 2.1486, 'learning_rate': 0.00014966657875899484, 'epoch': 0.04}        \n",
      "{'loss': 2.0295, 'learning_rate': 0.0001497042534754926, 'epoch': 0.04}         \n",
      "{'loss': 2.1765, 'learning_rate': 0.00014974192819199037, 'epoch': 0.04}        \n",
      "{'loss': 2.3751, 'learning_rate': 0.0001497796029084881, 'epoch': 0.04}         \n",
      "{'loss': 2.3203, 'learning_rate': 0.00014981727762498589, 'epoch': 0.04}        \n",
      "{'loss': 2.3832, 'learning_rate': 0.00014985495234148363, 'epoch': 0.04}        \n",
      "{'loss': 2.3462, 'learning_rate': 0.0001498926270579814, 'epoch': 0.04}         \n",
      "{'loss': 2.1731, 'learning_rate': 0.00014993030177447915, 'epoch': 0.04}        \n",
      "{'loss': 2.4144, 'learning_rate': 0.0001499679764909769, 'epoch': 0.04}         \n",
      "  4%|█                            | 40259/1061708 [6:01:43<150:58:20,  1.88it/s][2024-03-01 00:08:43,997] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2086, 'learning_rate': 0.0001500018837358249, 'epoch': 0.04}         \n",
      "{'loss': 1.9103, 'learning_rate': 0.00015003955845232264, 'epoch': 0.04}        \n",
      "{'loss': 2.4771, 'learning_rate': 0.0001500772331688204, 'epoch': 0.04}         \n",
      "{'loss': 2.4859, 'learning_rate': 0.00015011490788531816, 'epoch': 0.04}        \n",
      "  4%|█                            | 40298/1061708 [6:02:04<150:31:29,  1.88it/s][2024-03-01 00:09:04,807] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4702, 'learning_rate': 0.00015014881513016615, 'epoch': 0.04}        \n",
      "{'loss': 2.405, 'learning_rate': 0.0001501864898466639, 'epoch': 0.04}          \n",
      "{'loss': 2.3094, 'learning_rate': 0.00015022416456316167, 'epoch': 0.04}        \n",
      "{'loss': 2.133, 'learning_rate': 0.00015026183927965942, 'epoch': 0.04}         \n",
      "  4%|█                            | 40334/1061708 [6:02:23<152:55:43,  1.86it/s][2024-03-01 00:09:23,934] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0675, 'learning_rate': 0.0001502957465245074, 'epoch': 0.04}         \n",
      "{'loss': 2.3406, 'learning_rate': 0.00015033342124100518, 'epoch': 0.04}        \n",
      "{'loss': 2.7707, 'learning_rate': 0.0001503710959575029, 'epoch': 0.04}         \n",
      "{'loss': 2.219, 'learning_rate': 0.00015040877067400068, 'epoch': 0.04}         \n",
      "{'loss': 1.7961, 'learning_rate': 0.00015044644539049842, 'epoch': 0.04}        \n",
      "{'loss': 2.1128, 'learning_rate': 0.0001504841201069962, 'epoch': 0.04}         \n",
      "{'loss': 2.1061, 'learning_rate': 0.00015052179482349395, 'epoch': 0.04}        \n",
      "{'loss': 2.3335, 'learning_rate': 0.00015055946953999172, 'epoch': 0.04}        \n",
      "{'loss': 1.8577, 'learning_rate': 0.00015059714425648947, 'epoch': 0.04}        \n",
      "{'loss': 2.1812, 'learning_rate': 0.00015063481897298724, 'epoch': 0.04}        \n",
      "{'loss': 2.3936, 'learning_rate': 0.000150672493689485, 'epoch': 0.04}          \n",
      "{'loss': 2.291, 'learning_rate': 0.00015071016840598276, 'epoch': 0.04}         \n",
      "  4%|█                            | 40451/1061708 [6:03:25<150:11:25,  1.89it/s][2024-03-01 00:10:26,216] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5784, 'learning_rate': 0.00015074407565083073, 'epoch': 0.04}        \n",
      "  4%|█                            | 40464/1061708 [6:03:32<152:48:10,  1.86it/s][2024-03-01 00:10:33,080] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2503, 'learning_rate': 0.00015077798289567872, 'epoch': 0.04}        \n",
      "{'loss': 2.4114, 'learning_rate': 0.0001508156576121765, 'epoch': 0.04}         \n",
      "{'loss': 2.2412, 'learning_rate': 0.00015085333232867424, 'epoch': 0.04}        \n",
      "{'loss': 2.3734, 'learning_rate': 0.00015089100704517198, 'epoch': 0.04}        \n",
      "{'loss': 2.0198, 'learning_rate': 0.00015092868176166973, 'epoch': 0.04}        \n",
      "{'loss': 2.6899, 'learning_rate': 0.0001509663564781675, 'epoch': 0.04}         \n",
      "{'loss': 2.4146, 'learning_rate': 0.00015100403119466525, 'epoch': 0.04}        \n",
      "{'loss': 2.0935, 'learning_rate': 0.00015104170591116303, 'epoch': 0.04}        \n",
      "{'loss': 2.0525, 'learning_rate': 0.00015107938062766077, 'epoch': 0.04}        \n",
      "{'loss': 2.3243, 'learning_rate': 0.00015111705534415855, 'epoch': 0.04}        \n",
      "{'loss': 1.7505, 'learning_rate': 0.0001511547300606563, 'epoch': 0.04}         \n",
      "{'loss': 1.8379, 'learning_rate': 0.00015119240477715407, 'epoch': 0.04}        \n",
      "{'loss': 2.2464, 'learning_rate': 0.00015123007949365181, 'epoch': 0.04}        \n",
      "{'loss': 2.0539, 'learning_rate': 0.0001512677542101496, 'epoch': 0.04}         \n",
      "{'loss': 2.464, 'learning_rate': 0.00015130542892664734, 'epoch': 0.04}         \n",
      "{'loss': 2.238, 'learning_rate': 0.0001513431036431451, 'epoch': 0.04}          \n",
      "  4%|█                            | 40629/1061708 [6:05:00<150:19:39,  1.89it/s][2024-03-01 00:12:00,963] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4443, 'learning_rate': 0.00015137701088799307, 'epoch': 0.04}        \n",
      "{'loss': 2.0581, 'learning_rate': 0.00015141468560449085, 'epoch': 0.04}        \n",
      "  4%|█                            | 40642/1061708 [6:05:07<150:09:05,  1.89it/s][2024-03-01 00:12:07,828] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9022, 'learning_rate': 0.0001514485928493388, 'epoch': 0.04}         \n",
      "{'loss': 2.4555, 'learning_rate': 0.00015148626756583656, 'epoch': 0.04}        \n",
      "{'loss': 2.4302, 'learning_rate': 0.00015152394228233433, 'epoch': 0.04}        \n",
      "{'loss': 2.2958, 'learning_rate': 0.00015156161699883208, 'epoch': 0.04}        \n",
      "{'loss': 2.1042, 'learning_rate': 0.00015159929171532985, 'epoch': 0.04}        \n",
      "{'loss': 2.1482, 'learning_rate': 0.0001516369664318276, 'epoch': 0.04}         \n",
      "{'loss': 2.1319, 'learning_rate': 0.00015167464114832537, 'epoch': 0.04}        \n",
      "{'loss': 2.6117, 'learning_rate': 0.00015171231586482312, 'epoch': 0.04}        \n",
      "{'loss': 2.4558, 'learning_rate': 0.0001517499905813209, 'epoch': 0.04}         \n",
      "{'loss': 1.9339, 'learning_rate': 0.00015178766529781864, 'epoch': 0.04}        \n",
      "{'loss': 1.9574, 'learning_rate': 0.00015182534001431642, 'epoch': 0.04}        \n",
      "{'loss': 2.4941, 'learning_rate': 0.00015186301473081416, 'epoch': 0.04}        \n",
      "{'loss': 2.6541, 'learning_rate': 0.0001519006894473119, 'epoch': 0.04}         \n",
      "{'loss': 1.9937, 'learning_rate': 0.00015193836416380966, 'epoch': 0.04}        \n",
      "{'loss': 2.6306, 'learning_rate': 0.00015197603888030743, 'epoch': 0.04}        \n",
      "{'loss': 2.4981, 'learning_rate': 0.00015201371359680518, 'epoch': 0.04}        \n",
      "{'loss': 1.9325, 'learning_rate': 0.00015205138831330295, 'epoch': 0.04}        \n",
      "{'loss': 2.1842, 'learning_rate': 0.0001520890630298007, 'epoch': 0.04}         \n",
      "{'loss': 2.0281, 'learning_rate': 0.00015212673774629847, 'epoch': 0.04}        \n",
      "{'loss': 2.2584, 'learning_rate': 0.00015216441246279622, 'epoch': 0.04}        \n",
      "{'loss': 2.1293, 'learning_rate': 0.000152202087179294, 'epoch': 0.04}          \n",
      "  4%|█                            | 40859/1061708 [6:07:02<150:18:36,  1.89it/s][2024-03-01 00:14:03,371] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1138, 'learning_rate': 0.00015223599442414196, 'epoch': 0.04}        \n",
      "{'loss': 2.4195, 'learning_rate': 0.00015227366914063973, 'epoch': 0.04}        \n",
      "{'loss': 1.8342, 'learning_rate': 0.00015231134385713748, 'epoch': 0.04}        \n",
      "{'loss': 2.5678, 'learning_rate': 0.00015234901857363523, 'epoch': 0.04}        \n",
      "{'loss': 2.1928, 'learning_rate': 0.000152386693290133, 'epoch': 0.04}          \n",
      "{'loss': 2.2584, 'learning_rate': 0.00015242436800663075, 'epoch': 0.04}        \n",
      "  4%|█                            | 40913/1061708 [6:07:31<154:07:56,  1.84it/s][2024-03-01 00:14:32,097] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0241, 'learning_rate': 0.00015245827525147874, 'epoch': 0.04}        \n",
      "{'loss': 1.8556, 'learning_rate': 0.00015249594996797648, 'epoch': 0.04}        \n",
      "{'loss': 2.192, 'learning_rate': 0.00015253362468447426, 'epoch': 0.04}         \n",
      "{'loss': 2.3535, 'learning_rate': 0.000152571299400972, 'epoch': 0.04}          \n",
      "{'loss': 2.2657, 'learning_rate': 0.00015260897411746978, 'epoch': 0.04}        \n",
      "{'loss': 1.8725, 'learning_rate': 0.00015264664883396753, 'epoch': 0.04}        \n",
      "{'loss': 2.2766, 'learning_rate': 0.0001526843235504653, 'epoch': 0.04}         \n",
      "  4%|█                            | 40984/1061708 [6:08:09<152:45:18,  1.86it/s][2024-03-01 00:15:09,932] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  4%|█                            | 40989/1061708 [6:08:11<151:18:31,  1.87it/s][2024-03-01 00:15:12,526] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.0138, 'learning_rate': 0.0001527144633236635, 'epoch': 0.04}         \n",
      "{'loss': 1.9775, 'learning_rate': 0.00015275213804016126, 'epoch': 0.04}        \n",
      "{'loss': 2.3484, 'learning_rate': 0.00015278981275665903, 'epoch': 0.04}        \n",
      "{'loss': 2.1761, 'learning_rate': 0.00015282748747315675, 'epoch': 0.04}        \n",
      "{'loss': 2.1412, 'learning_rate': 0.00015286516218965452, 'epoch': 0.04}        \n",
      "{'loss': 2.1179, 'learning_rate': 0.00015290283690615227, 'epoch': 0.04}        \n",
      "{'loss': 2.4697, 'learning_rate': 0.00015294051162265004, 'epoch': 0.04}        \n",
      "{'loss': 2.4366, 'learning_rate': 0.0001529781863391478, 'epoch': 0.04}         \n",
      "{'loss': 2.2729, 'learning_rate': 0.00015301586105564556, 'epoch': 0.04}        \n",
      "{'loss': 2.1477, 'learning_rate': 0.0001530535357721433, 'epoch': 0.04}         \n",
      "{'loss': 2.187, 'learning_rate': 0.00015309121048864109, 'epoch': 0.04}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3407, 'learning_rate': 0.00015312888520513883, 'epoch': 0.04}        \n",
      "{'loss': 2.1983, 'learning_rate': 0.0001531665599216366, 'epoch': 0.04}         \n",
      "{'loss': 2.191, 'learning_rate': 0.00015320423463813435, 'epoch': 0.04}         \n",
      "{'loss': 2.2605, 'learning_rate': 0.00015324190935463213, 'epoch': 0.04}        \n",
      "{'loss': 2.3396, 'learning_rate': 0.00015327958407112987, 'epoch': 0.04}        \n",
      "{'loss': 1.9618, 'learning_rate': 0.00015331725878762765, 'epoch': 0.04}        \n",
      "{'loss': 2.0234, 'learning_rate': 0.0001533549335041254, 'epoch': 0.04}         \n",
      "{'loss': 2.1014, 'learning_rate': 0.00015339260822062317, 'epoch': 0.04}        \n",
      "{'loss': 2.2324, 'learning_rate': 0.0001534302829371209, 'epoch': 0.04}         \n",
      "{'loss': 2.1709, 'learning_rate': 0.00015346795765361866, 'epoch': 0.04}        \n",
      "{'loss': 2.5117, 'learning_rate': 0.0001535056323701164, 'epoch': 0.04}         \n",
      "{'loss': 2.3384, 'learning_rate': 0.00015354330708661418, 'epoch': 0.04}        \n",
      "{'loss': 2.0623, 'learning_rate': 0.00015358098180311193, 'epoch': 0.04}        \n",
      "{'loss': 2.5691, 'learning_rate': 0.0001536186565196097, 'epoch': 0.04}         \n",
      "{'loss': 1.982, 'learning_rate': 0.00015365633123610745, 'epoch': 0.04}         \n",
      "{'loss': 2.2841, 'learning_rate': 0.00015369400595260523, 'epoch': 0.04}        \n",
      "{'loss': 2.3728, 'learning_rate': 0.00015373168066910297, 'epoch': 0.04}        \n",
      "{'loss': 2.6241, 'learning_rate': 0.00015376935538560072, 'epoch': 0.04}        \n",
      "{'loss': 2.0384, 'learning_rate': 0.0001538070301020985, 'epoch': 0.04}         \n",
      "{'loss': 2.6356, 'learning_rate': 0.00015384470481859624, 'epoch': 0.04}        \n",
      "{'loss': 2.0868, 'learning_rate': 0.00015388237953509401, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41309/1061708 [6:11:02<150:43:14,  1.88it/s][2024-03-01 00:18:03,096] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3358, 'learning_rate': 0.00015391628677994198, 'epoch': 0.04}        \n",
      "{'loss': 2.0479, 'learning_rate': 0.00015395396149643975, 'epoch': 0.04}        \n",
      "{'loss': 2.3534, 'learning_rate': 0.0001539916362129375, 'epoch': 0.04}         \n",
      "{'loss': 2.6253, 'learning_rate': 0.00015402931092943527, 'epoch': 0.04}        \n",
      "{'loss': 1.7718, 'learning_rate': 0.00015406698564593302, 'epoch': 0.04}        \n",
      "{'loss': 2.2086, 'learning_rate': 0.0001541046603624308, 'epoch': 0.04}         \n",
      "{'loss': 2.3822, 'learning_rate': 0.00015414233507892854, 'epoch': 0.04}        \n",
      "{'loss': 2.2994, 'learning_rate': 0.00015418000979542632, 'epoch': 0.04}        \n",
      "{'loss': 2.3255, 'learning_rate': 0.00015421768451192406, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41391/1061708 [6:11:46<150:09:14,  1.89it/s][2024-03-01 00:18:46,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.333, 'learning_rate': 0.00015425159175677205, 'epoch': 0.04}         \n",
      "{'loss': 2.0338, 'learning_rate': 0.0001542892664732698, 'epoch': 0.04}         \n",
      "{'loss': 2.4884, 'learning_rate': 0.00015432694118976755, 'epoch': 0.04}        \n",
      "{'loss': 2.2829, 'learning_rate': 0.00015436461590626532, 'epoch': 0.04}        \n",
      "{'loss': 2.1465, 'learning_rate': 0.00015440229062276307, 'epoch': 0.04}        \n",
      "{'loss': 2.3735, 'learning_rate': 0.00015443996533926084, 'epoch': 0.04}        \n",
      "{'loss': 2.2702, 'learning_rate': 0.0001544776400557586, 'epoch': 0.04}         \n",
      "{'loss': 2.4072, 'learning_rate': 0.00015451531477225634, 'epoch': 0.04}        \n",
      "{'loss': 2.3662, 'learning_rate': 0.0001545529894887541, 'epoch': 0.04}         \n",
      "{'loss': 2.3445, 'learning_rate': 0.00015459066420525186, 'epoch': 0.04}        \n",
      "{'loss': 1.9546, 'learning_rate': 0.0001546283389217496, 'epoch': 0.04}         \n",
      "{'loss': 2.2858, 'learning_rate': 0.00015466601363824738, 'epoch': 0.04}        \n",
      "{'loss': 2.0943, 'learning_rate': 0.00015470368835474512, 'epoch': 0.04}        \n",
      "{'loss': 2.2366, 'learning_rate': 0.0001547413630712429, 'epoch': 0.04}         \n",
      "{'loss': 1.8616, 'learning_rate': 0.00015477903778774065, 'epoch': 0.04}        \n",
      "{'loss': 2.2097, 'learning_rate': 0.00015481671250423842, 'epoch': 0.04}        \n",
      "{'loss': 1.7726, 'learning_rate': 0.00015485438722073617, 'epoch': 0.04}        \n",
      "{'loss': 2.0535, 'learning_rate': 0.00015489206193723394, 'epoch': 0.04}        \n",
      "{'loss': 2.4489, 'learning_rate': 0.0001549297366537317, 'epoch': 0.04}         \n",
      "{'loss': 2.1473, 'learning_rate': 0.00015496741137022946, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41592/1061708 [6:13:33<149:55:21,  1.89it/s][2024-03-01 00:20:33,819] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 41593/1061708 [6:13:33<144:51:21,  1.96it/s][2024-03-01 00:20:34,242] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9932, 'learning_rate': 0.00015499755114342764, 'epoch': 0.04}        \n",
      "{'loss': 2.1873, 'learning_rate': 0.00015503522585992542, 'epoch': 0.04}        \n",
      "{'loss': 2.2575, 'learning_rate': 0.00015507290057642316, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41620/1061708 [6:13:48<149:56:28,  1.89it/s][2024-03-01 00:20:48,544] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0617, 'learning_rate': 0.00015510680782127115, 'epoch': 0.04}        \n",
      "{'loss': 2.3593, 'learning_rate': 0.00015514448253776893, 'epoch': 0.04}        \n",
      "{'loss': 2.1651, 'learning_rate': 0.00015518215725426668, 'epoch': 0.04}        \n",
      "{'loss': 2.0803, 'learning_rate': 0.00015521983197076442, 'epoch': 0.04}        \n",
      "{'loss': 2.4568, 'learning_rate': 0.00015525750668726217, 'epoch': 0.04}        \n",
      "{'loss': 1.9318, 'learning_rate': 0.00015529518140375994, 'epoch': 0.04}        \n",
      "{'loss': 2.1697, 'learning_rate': 0.0001553328561202577, 'epoch': 0.04}         \n",
      "{'loss': 2.4592, 'learning_rate': 0.00015537053083675546, 'epoch': 0.04}        \n",
      "{'loss': 2.1825, 'learning_rate': 0.0001554082055532532, 'epoch': 0.04}         \n",
      "{'loss': 2.0779, 'learning_rate': 0.00015544588026975099, 'epoch': 0.04}        \n",
      "{'loss': 2.3846, 'learning_rate': 0.00015548355498624873, 'epoch': 0.04}        \n",
      "{'loss': 2.1199, 'learning_rate': 0.0001555212297027465, 'epoch': 0.04}         \n",
      "{'loss': 2.2399, 'learning_rate': 0.00015555890441924425, 'epoch': 0.04}        \n",
      "{'loss': 2.1617, 'learning_rate': 0.00015559657913574203, 'epoch': 0.04}        \n",
      "{'loss': 2.1782, 'learning_rate': 0.00015563425385223977, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41773/1061708 [6:15:09<153:48:27,  1.84it/s][2024-03-01 00:22:10,070] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1402, 'learning_rate': 0.00015566816109708774, 'epoch': 0.04}        \n",
      "{'loss': 2.5279, 'learning_rate': 0.0001557058358135855, 'epoch': 0.04}         \n",
      "{'loss': 2.0752, 'learning_rate': 0.00015574351053008326, 'epoch': 0.04}        \n",
      "{'loss': 2.4001, 'learning_rate': 0.00015578118524658103, 'epoch': 0.04}        \n",
      "{'loss': 2.5753, 'learning_rate': 0.00015581885996307878, 'epoch': 0.04}        \n",
      "{'loss': 2.1593, 'learning_rate': 0.00015585653467957655, 'epoch': 0.04}        \n",
      "{'loss': 2.2737, 'learning_rate': 0.0001558942093960743, 'epoch': 0.04}         \n",
      "{'loss': 2.0423, 'learning_rate': 0.00015593188411257207, 'epoch': 0.04}        \n",
      "{'loss': 2.1724, 'learning_rate': 0.00015596955882906982, 'epoch': 0.04}        \n",
      "{'loss': 2.0419, 'learning_rate': 0.00015600723354556757, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41874/1061708 [6:16:03<152:25:31,  1.86it/s][2024-03-01 00:23:03,862] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 41875/1061708 [6:16:03<142:34:09,  1.99it/s][2024-03-01 00:23:04,284] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0936, 'learning_rate': 0.00015603737331876578, 'epoch': 0.04}        \n",
      "{'loss': 2.6393, 'learning_rate': 0.00015607504803526355, 'epoch': 0.04}        \n",
      "{'loss': 2.8294, 'learning_rate': 0.0001561127227517613, 'epoch': 0.04}         \n",
      "{'loss': 2.2376, 'learning_rate': 0.00015615039746825904, 'epoch': 0.04}        \n",
      "{'loss': 2.3054, 'learning_rate': 0.00015618807218475682, 'epoch': 0.04}        \n",
      "{'loss': 2.3954, 'learning_rate': 0.00015622574690125457, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41932/1061708 [6:16:34<149:58:05,  1.89it/s][2024-03-01 00:23:34,622] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1092, 'learning_rate': 0.00015625965414610256, 'epoch': 0.04}        \n",
      "{'loss': 2.21, 'learning_rate': 0.0001562973288626003, 'epoch': 0.04}           \n",
      "{'loss': 2.1061, 'learning_rate': 0.00015633500357909808, 'epoch': 0.04}        \n",
      "{'loss': 1.9334, 'learning_rate': 0.00015637267829559582, 'epoch': 0.04}        \n",
      "{'loss': 2.3418, 'learning_rate': 0.0001564103530120936, 'epoch': 0.04}         \n",
      "{'loss': 2.0495, 'learning_rate': 0.00015644802772859135, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 41999/1061708 [6:17:09<150:08:01,  1.89it/s][2024-03-01 00:24:10,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=464, lr=[0.00015648570244508912], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 00:24:10,428] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=1.8920180065962555, CurrSamplesPerSec=1.8982728055696736, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.4188, 'learning_rate': 0.00015648570244508912, 'epoch': 0.04}        \n",
      "{'loss': 2.078, 'learning_rate': 0.00015652337716158687, 'epoch': 0.04}         \n",
      "{'loss': 2.3271, 'learning_rate': 0.00015656105187808464, 'epoch': 0.04}        \n",
      "{'loss': 2.3519, 'learning_rate': 0.0001565987265945824, 'epoch': 0.04}         \n",
      "{'loss': 2.2802, 'learning_rate': 0.00015663640131108016, 'epoch': 0.04}        \n",
      "{'loss': 2.0214, 'learning_rate': 0.0001566740760275779, 'epoch': 0.04}         \n",
      "{'loss': 2.1883, 'learning_rate': 0.00015671175074407568, 'epoch': 0.04}        \n",
      "{'loss': 2.2486, 'learning_rate': 0.0001567494254605734, 'epoch': 0.04}         \n",
      "{'loss': 2.5008, 'learning_rate': 0.00015678710017707118, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 42080/1061708 [6:17:52<150:49:35,  1.88it/s][2024-03-01 00:24:53,509] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9763, 'learning_rate': 0.00015682100742191917, 'epoch': 0.04}        \n",
      "{'loss': 2.203, 'learning_rate': 0.00015685868213841691, 'epoch': 0.04}         \n",
      "{'loss': 1.9873, 'learning_rate': 0.0001568963568549147, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42111/1061708 [6:18:09<149:42:41,  1.89it/s][2024-03-01 00:25:09,939] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2231, 'learning_rate': 0.00015693026409976265, 'epoch': 0.04}        \n",
      "{'loss': 2.1538, 'learning_rate': 0.00015696793881626043, 'epoch': 0.04}        \n",
      "{'loss': 2.1782, 'learning_rate': 0.00015700561353275817, 'epoch': 0.04}        \n",
      "{'loss': 2.1779, 'learning_rate': 0.00015704328824925595, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 42156/1061708 [6:18:33<151:04:04,  1.87it/s][2024-03-01 00:25:33,845] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3055, 'learning_rate': 0.0001570771954941039, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42169/1061708 [6:18:40<149:53:32,  1.89it/s][2024-03-01 00:25:40,699] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.2504, 'learning_rate': 0.00015711110273895187, 'epoch': 0.04}        \n",
      "{'loss': 2.4287, 'learning_rate': 0.00015714877745544965, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 42186/1061708 [6:18:49<150:51:22,  1.88it/s][2024-03-01 00:25:49,672] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "{'loss': 2.3751, 'learning_rate': 0.00015718268470029764, 'epoch': 0.04}        \n",
      "{'loss': 2.3967, 'learning_rate': 0.00015722035941679541, 'epoch': 0.04}        \n",
      "{'loss': 2.2485, 'learning_rate': 0.00015725803413329316, 'epoch': 0.04}        \n",
      "{'loss': 2.1574, 'learning_rate': 0.00015729570884979093, 'epoch': 0.04}        \n",
      "{'loss': 2.2631, 'learning_rate': 0.00015733338356628865, 'epoch': 0.04}        \n",
      "{'loss': 2.4921, 'learning_rate': 0.00015737105828278643, 'epoch': 0.04}        \n",
      "{'loss': 1.9985, 'learning_rate': 0.00015740873299928418, 'epoch': 0.04}        \n",
      "{'loss': 2.2238, 'learning_rate': 0.00015744640771578195, 'epoch': 0.04}        \n",
      "{'loss': 2.1403, 'learning_rate': 0.0001574840824322797, 'epoch': 0.04}         \n",
      "{'loss': 2.4316, 'learning_rate': 0.00015752175714877747, 'epoch': 0.04}        \n",
      "{'loss': 2.421, 'learning_rate': 0.00015755943186527522, 'epoch': 0.04}         \n",
      "{'loss': 2.3419, 'learning_rate': 0.000157597106581773, 'epoch': 0.04}          \n",
      "{'loss': 2.1692, 'learning_rate': 0.00015763478129827074, 'epoch': 0.04}        \n",
      "{'loss': 2.3284, 'learning_rate': 0.0001576724560147685, 'epoch': 0.04}         \n",
      "{'loss': 2.2885, 'learning_rate': 0.00015771013073126626, 'epoch': 0.04}        \n",
      "{'loss': 2.1693, 'learning_rate': 0.000157747805447764, 'epoch': 0.04}          \n",
      "{'loss': 1.9991, 'learning_rate': 0.00015778548016426178, 'epoch': 0.04}        \n",
      "{'loss': 2.3559, 'learning_rate': 0.00015782315488075953, 'epoch': 0.04}        \n",
      "{'loss': 2.3408, 'learning_rate': 0.0001578608295972573, 'epoch': 0.04}         \n",
      "{'loss': 2.3197, 'learning_rate': 0.00015789850431375505, 'epoch': 0.04}        \n",
      "{'loss': 2.0822, 'learning_rate': 0.0001579361790302528, 'epoch': 0.04}         \n",
      "{'loss': 1.8895, 'learning_rate': 0.00015797385374675057, 'epoch': 0.04}        \n",
      "{'loss': 2.211, 'learning_rate': 0.00015801152846324832, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42410/1061708 [6:20:48<149:53:56,  1.89it/s][2024-03-01 00:27:48,913] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1244, 'learning_rate': 0.0001580454357080963, 'epoch': 0.04}         \n",
      "{'loss': 2.0952, 'learning_rate': 0.00015808311042459408, 'epoch': 0.04}        \n",
      "{'loss': 2.3131, 'learning_rate': 0.00015812078514109183, 'epoch': 0.04}        \n",
      "{'loss': 2.7198, 'learning_rate': 0.00015815845985758957, 'epoch': 0.04}        \n",
      "{'loss': 2.5729, 'learning_rate': 0.00015819613457408732, 'epoch': 0.04}        \n",
      "{'loss': 2.1342, 'learning_rate': 0.0001582338092905851, 'epoch': 0.04}         \n",
      "{'loss': 2.066, 'learning_rate': 0.00015827148400708284, 'epoch': 0.04}         \n",
      "{'loss': 2.0837, 'learning_rate': 0.00015830915872358062, 'epoch': 0.04}        \n",
      "{'loss': 2.2842, 'learning_rate': 0.00015834683344007836, 'epoch': 0.04}        \n",
      "{'loss': 2.2221, 'learning_rate': 0.00015838450815657614, 'epoch': 0.04}        \n",
      "{'loss': 2.336, 'learning_rate': 0.00015842218287307388, 'epoch': 0.04}         \n",
      "{'loss': 2.4407, 'learning_rate': 0.00015845985758957166, 'epoch': 0.04}        \n",
      "{'loss': 2.1521, 'learning_rate': 0.0001584975323060694, 'epoch': 0.04}         \n",
      "{'loss': 1.9974, 'learning_rate': 0.00015853520702256718, 'epoch': 0.04}        \n",
      "{'loss': 2.0819, 'learning_rate': 0.00015857288173906493, 'epoch': 0.04}        \n",
      "{'loss': 2.3141, 'learning_rate': 0.0001586105564555627, 'epoch': 0.04}         \n",
      "{'loss': 2.512, 'learning_rate': 0.00015864823117206045, 'epoch': 0.04}         \n",
      "{'loss': 2.2943, 'learning_rate': 0.0001586859058885582, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42595/1061708 [6:22:27<151:59:23,  1.86it/s][2024-03-01 00:29:27,530] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1945, 'learning_rate': 0.00015871981313340618, 'epoch': 0.04}        \n",
      "{'loss': 2.2393, 'learning_rate': 0.00015875748784990393, 'epoch': 0.04}        \n",
      "{'loss': 2.1124, 'learning_rate': 0.0001587951625664017, 'epoch': 0.04}         \n",
      "{'loss': 2.1521, 'learning_rate': 0.00015883283728289945, 'epoch': 0.04}        \n",
      "{'loss': 2.3704, 'learning_rate': 0.00015887051199939723, 'epoch': 0.04}        \n",
      "{'loss': 2.4159, 'learning_rate': 0.00015890818671589497, 'epoch': 0.04}        \n",
      "{'loss': 2.1555, 'learning_rate': 0.00015894586143239275, 'epoch': 0.04}        \n",
      "{'loss': 2.1339, 'learning_rate': 0.00015898353614889047, 'epoch': 0.04}        \n",
      "{'loss': 1.946, 'learning_rate': 0.00015902121086538824, 'epoch': 0.04}         \n",
      "{'loss': 2.0675, 'learning_rate': 0.000159058885581886, 'epoch': 0.04}          \n",
      "{'loss': 2.2788, 'learning_rate': 0.00015909656029838376, 'epoch': 0.04}        \n",
      "{'loss': 2.4354, 'learning_rate': 0.0001591342350148815, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42710/1061708 [6:23:28<150:09:58,  1.88it/s][2024-03-01 00:30:28,835] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4881, 'learning_rate': 0.0001591681422597295, 'epoch': 0.04}         \n",
      "{'loss': 2.0416, 'learning_rate': 0.00015920581697622725, 'epoch': 0.04}        \n",
      "{'loss': 2.1108, 'learning_rate': 0.00015924349169272502, 'epoch': 0.04}        \n",
      "{'loss': 2.0155, 'learning_rate': 0.00015928116640922277, 'epoch': 0.04}        \n",
      "{'loss': 2.3699, 'learning_rate': 0.00015931884112572054, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 42766/1061708 [6:23:58<151:22:55,  1.87it/s][2024-03-01 00:30:58,701] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.057, 'learning_rate': 0.00015935274837056853, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42778/1061708 [6:24:04<150:34:22,  1.88it/s][2024-03-01 00:31:05,043] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.195, 'learning_rate': 0.0001593866556154165, 'epoch': 0.04}          \n",
      "{'loss': 2.0985, 'learning_rate': 0.00015942433033191427, 'epoch': 0.04}        \n",
      "{'loss': 2.2936, 'learning_rate': 0.00015946200504841202, 'epoch': 0.04}        \n",
      "{'loss': 2.1306, 'learning_rate': 0.0001594996797649098, 'epoch': 0.04}         \n",
      "{'loss': 2.1439, 'learning_rate': 0.00015953735448140754, 'epoch': 0.04}        \n",
      "{'loss': 2.0051, 'learning_rate': 0.0001595750291979053, 'epoch': 0.04}         \n",
      "{'loss': 2.2938, 'learning_rate': 0.00015961270391440306, 'epoch': 0.04}        \n",
      "{'loss': 2.2816, 'learning_rate': 0.00015965037863090083, 'epoch': 0.04}        \n",
      "{'loss': 2.1594, 'learning_rate': 0.00015968805334739855, 'epoch': 0.04}        \n",
      "{'loss': 2.0058, 'learning_rate': 0.00015972572806389633, 'epoch': 0.04}        \n",
      "{'loss': 2.3575, 'learning_rate': 0.00015976340278039407, 'epoch': 0.04}        \n",
      "{'loss': 2.0429, 'learning_rate': 0.00015980107749689185, 'epoch': 0.04}        \n",
      "{'loss': 2.284, 'learning_rate': 0.0001598387522133896, 'epoch': 0.04}          \n",
      "{'loss': 1.9063, 'learning_rate': 0.00015987642692988737, 'epoch': 0.04}        \n",
      "{'loss': 2.4859, 'learning_rate': 0.00015991410164638512, 'epoch': 0.04}        \n",
      "{'loss': 2.2956, 'learning_rate': 0.00015995177636288286, 'epoch': 0.04}        \n",
      "{'loss': 2.264, 'learning_rate': 0.00015998945107938064, 'epoch': 0.04}         \n",
      "{'loss': 2.3376, 'learning_rate': 0.00016002712579587838, 'epoch': 0.04}        \n",
      "{'loss': 2.3019, 'learning_rate': 0.00016006480051237616, 'epoch': 0.04}        \n",
      "{'loss': 2.2951, 'learning_rate': 0.0001601024752288739, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 42970/1061708 [6:25:46<150:06:43,  1.89it/s][2024-03-01 00:32:47,430] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3771, 'learning_rate': 0.0001601363824737219, 'epoch': 0.04}         \n",
      "{'loss': 2.5809, 'learning_rate': 0.00016017405719021964, 'epoch': 0.04}        \n",
      "{'loss': 2.1036, 'learning_rate': 0.00016021173190671742, 'epoch': 0.04}        \n",
      "{'loss': 2.3536, 'learning_rate': 0.00016024940662321516, 'epoch': 0.04}        \n",
      "{'loss': 2.1546, 'learning_rate': 0.00016028708133971294, 'epoch': 0.04}        \n",
      "{'loss': 2.0621, 'learning_rate': 0.00016032475605621069, 'epoch': 0.04}        \n",
      "{'loss': 2.4601, 'learning_rate': 0.00016036243077270846, 'epoch': 0.04}        \n",
      "{'loss': 2.3058, 'learning_rate': 0.0001604001054892062, 'epoch': 0.04}         \n",
      "{'loss': 2.3401, 'learning_rate': 0.00016043778020570398, 'epoch': 0.04}        \n",
      "{'loss': 2.2716, 'learning_rate': 0.00016047545492220173, 'epoch': 0.04}        \n",
      "{'loss': 2.2846, 'learning_rate': 0.00016051312963869947, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43087/1061708 [6:26:49<150:33:48,  1.88it/s][2024-03-01 00:33:49,798] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4013, 'learning_rate': 0.00016054703688354746, 'epoch': 0.04}        \n",
      "{'loss': 2.422, 'learning_rate': 0.0001605847116000452, 'epoch': 0.04}          \n",
      "{'loss': 2.6205, 'learning_rate': 0.00016062238631654299, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43110/1061708 [6:27:01<150:13:02,  1.88it/s][2024-03-01 00:34:01,968] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0086, 'learning_rate': 0.00016065629356139095, 'epoch': 0.04}        \n",
      "{'loss': 2.425, 'learning_rate': 0.00016069396827788872, 'epoch': 0.04}         \n",
      "{'loss': 2.2781, 'learning_rate': 0.00016073164299438647, 'epoch': 0.04}        \n",
      "{'loss': 2.4045, 'learning_rate': 0.00016076931771088424, 'epoch': 0.04}        \n",
      "{'loss': 2.0312, 'learning_rate': 0.000160806992427382, 'epoch': 0.04}          \n",
      "{'loss': 2.3933, 'learning_rate': 0.00016084466714387977, 'epoch': 0.04}        \n",
      "{'loss': 2.0966, 'learning_rate': 0.0001608823418603775, 'epoch': 0.04}         \n",
      "{'loss': 2.2693, 'learning_rate': 0.0001609200165768753, 'epoch': 0.04}         \n",
      "{'loss': 1.8138, 'learning_rate': 0.000160957691293373, 'epoch': 0.04}          \n",
      "{'loss': 2.1931, 'learning_rate': 0.00016099536600987078, 'epoch': 0.04}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.00016103304072636853, 'epoch': 0.04}        \n",
      "{'loss': 2.4025, 'learning_rate': 0.0001610707154428663, 'epoch': 0.04}         \n",
      "{'loss': 1.9747, 'learning_rate': 0.00016110839015936405, 'epoch': 0.04}        \n",
      "{'loss': 2.307, 'learning_rate': 0.00016114606487586182, 'epoch': 0.04}         \n",
      "{'loss': 2.0922, 'learning_rate': 0.00016118373959235957, 'epoch': 0.04}        \n",
      "{'loss': 2.1286, 'learning_rate': 0.00016122141430885734, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43276/1061708 [6:28:30<151:07:31,  1.87it/s][2024-03-01 00:35:30,543] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9677, 'learning_rate': 0.0001612553215537053, 'epoch': 0.04}         \n",
      "{'loss': 2.4907, 'learning_rate': 0.00016129299627020305, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43297/1061708 [6:28:41<150:41:14,  1.88it/s][2024-03-01 00:35:41,688] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1649, 'learning_rate': 0.00016132690351505107, 'epoch': 0.04}        \n",
      "{'loss': 2.1518, 'learning_rate': 0.00016136457823154882, 'epoch': 0.04}        \n",
      "{'loss': 1.3983, 'learning_rate': 0.0001614022529480466, 'epoch': 0.04}         \n",
      "{'loss': 2.1867, 'learning_rate': 0.0001614399276645443, 'epoch': 0.04}         \n",
      "{'loss': 2.483, 'learning_rate': 0.0001614776023810421, 'epoch': 0.04}          \n",
      "{'loss': 2.357, 'learning_rate': 0.00016151527709753983, 'epoch': 0.04}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3655, 'learning_rate': 0.0001615529518140376, 'epoch': 0.04}         \n",
      "{'loss': 2.5784, 'learning_rate': 0.00016159062653053535, 'epoch': 0.04}        \n",
      "{'loss': 2.3023, 'learning_rate': 0.00016162830124703313, 'epoch': 0.04}        \n",
      "{'loss': 2.048, 'learning_rate': 0.00016166597596353088, 'epoch': 0.04}         \n",
      "{'loss': 2.2267, 'learning_rate': 0.00016170365068002865, 'epoch': 0.04}        \n",
      "{'loss': 1.9851, 'learning_rate': 0.0001617413253965264, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 43416/1061708 [6:29:44<150:23:05,  1.88it/s][2024-03-01 00:36:44,958] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8058, 'learning_rate': 0.0001617752326413744, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 43421/1061708 [6:29:47<150:11:07,  1.88it/s][2024-03-01 00:36:47,545] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4703, 'learning_rate': 0.00016180913988622238, 'epoch': 0.04}        \n",
      "{'loss': 2.1857, 'learning_rate': 0.00016184681460272013, 'epoch': 0.04}        \n",
      "{'loss': 2.0071, 'learning_rate': 0.0001618844893192179, 'epoch': 0.04}         \n",
      "{'loss': 2.0874, 'learning_rate': 0.00016192216403571562, 'epoch': 0.04}        \n",
      "{'loss': 2.206, 'learning_rate': 0.0001619598387522134, 'epoch': 0.04}          \n",
      "{'loss': 2.3431, 'learning_rate': 0.00016199751346871114, 'epoch': 0.04}        \n",
      "{'loss': 1.9486, 'learning_rate': 0.00016203518818520891, 'epoch': 0.04}        \n",
      "{'loss': 2.196, 'learning_rate': 0.00016207286290170666, 'epoch': 0.04}         \n",
      "{'loss': 2.281, 'learning_rate': 0.00016211053761820444, 'epoch': 0.04}         \n",
      "{'loss': 2.4577, 'learning_rate': 0.00016214821233470218, 'epoch': 0.04}        \n",
      "{'loss': 2.3611, 'learning_rate': 0.00016218588705119996, 'epoch': 0.04}        \n",
      "{'loss': 2.6014, 'learning_rate': 0.0001622235617676977, 'epoch': 0.04}         \n",
      "{'loss': 2.0974, 'learning_rate': 0.00016226123648419548, 'epoch': 0.04}        \n",
      "{'loss': 2.0426, 'learning_rate': 0.00016229891120069322, 'epoch': 0.04}        \n",
      "{'loss': 2.3183, 'learning_rate': 0.000162336585917191, 'epoch': 0.04}          \n",
      "{'loss': 2.0091, 'learning_rate': 0.00016237426063368875, 'epoch': 0.04}        \n",
      "{'loss': 2.1916, 'learning_rate': 0.00016241193535018652, 'epoch': 0.04}        \n",
      "{'loss': 2.3265, 'learning_rate': 0.00016244961006668427, 'epoch': 0.04}        \n",
      "{'loss': 2.3116, 'learning_rate': 0.000162487284783182, 'epoch': 0.04}          \n",
      "{'loss': 1.9414, 'learning_rate': 0.00016252495949967976, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43622/1061708 [6:31:34<149:34:39,  1.89it/s][2024-03-01 00:38:34,591] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 43623/1061708 [6:31:34<143:55:45,  1.96it/s][2024-03-01 00:38:35,014] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9594, 'learning_rate': 0.00016255509927287797, 'epoch': 0.04}        \n",
      "{'loss': 1.8172, 'learning_rate': 0.00016259277398937574, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43642/1061708 [6:31:44<149:29:52,  1.89it/s][2024-03-01 00:38:45,073] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0416, 'learning_rate': 0.0001626266812342237, 'epoch': 0.04}         \n",
      "{'loss': 2.3695, 'learning_rate': 0.00016266435595072148, 'epoch': 0.04}        \n",
      "{'loss': 2.1017, 'learning_rate': 0.00016270203066721923, 'epoch': 0.04}        \n",
      "{'loss': 2.3125, 'learning_rate': 0.000162739705383717, 'epoch': 0.04}          \n",
      "{'loss': 2.09, 'learning_rate': 0.00016277738010021475, 'epoch': 0.04}          \n",
      "{'loss': 1.9162, 'learning_rate': 0.00016281505481671252, 'epoch': 0.04}        \n",
      "{'loss': 2.2748, 'learning_rate': 0.00016285272953321027, 'epoch': 0.04}        \n",
      "{'loss': 2.1505, 'learning_rate': 0.00016289040424970802, 'epoch': 0.04}        \n",
      "{'loss': 2.3428, 'learning_rate': 0.0001629280789662058, 'epoch': 0.04}         \n",
      "{'loss': 1.979, 'learning_rate': 0.00016296575368270354, 'epoch': 0.04}         \n",
      "{'loss': 2.3545, 'learning_rate': 0.0001630034283992013, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 43752/1061708 [6:32:43<149:55:58,  1.89it/s][2024-03-01 00:39:43,784] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.6903, 'learning_rate': 0.00016303733564404927, 'epoch': 0.04}        \n",
      "{'loss': 1.974, 'learning_rate': 0.00016307501036054705, 'epoch': 0.04}         \n",
      "{'loss': 2.3395, 'learning_rate': 0.0001631126850770448, 'epoch': 0.04}         \n",
      "{'loss': 2.3055, 'learning_rate': 0.00016315035979354257, 'epoch': 0.04}        \n",
      "{'loss': 1.5496, 'learning_rate': 0.00016318803451004032, 'epoch': 0.04}        \n",
      "{'loss': 2.3611, 'learning_rate': 0.0001632257092265381, 'epoch': 0.04}         \n",
      "{'loss': 2.0429, 'learning_rate': 0.00016326338394303584, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43824/1061708 [6:33:21<152:30:28,  1.85it/s][2024-03-01 00:40:22,143] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3766, 'learning_rate': 0.00016329729118788383, 'epoch': 0.04}        \n",
      "{'loss': 1.8756, 'learning_rate': 0.00016333496590438158, 'epoch': 0.04}        \n",
      "{'loss': 2.4728, 'learning_rate': 0.00016337264062087935, 'epoch': 0.04}        \n",
      "{'loss': 2.3988, 'learning_rate': 0.0001634103153373771, 'epoch': 0.04}         \n",
      "{'loss': 1.9611, 'learning_rate': 0.00016344799005387484, 'epoch': 0.04}        \n",
      "{'loss': 2.1488, 'learning_rate': 0.00016348566477037262, 'epoch': 0.04}        \n",
      "{'loss': 2.275, 'learning_rate': 0.00016352333948687036, 'epoch': 0.04}         \n",
      "{'loss': 2.4188, 'learning_rate': 0.00016356101420336814, 'epoch': 0.04}        \n",
      "{'loss': 2.1976, 'learning_rate': 0.00016359868891986588, 'epoch': 0.04}        \n",
      "{'loss': 2.648, 'learning_rate': 0.00016363636363636366, 'epoch': 0.04}         \n",
      "{'loss': 1.9306, 'learning_rate': 0.00016367403835286138, 'epoch': 0.04}        \n",
      "{'loss': 2.4255, 'learning_rate': 0.00016371171306935915, 'epoch': 0.04}        \n",
      "{'loss': 2.0485, 'learning_rate': 0.0001637493877858569, 'epoch': 0.04}         \n",
      "{'loss': 2.23, 'learning_rate': 0.00016378706250235467, 'epoch': 0.04}          \n",
      "{'loss': 2.2848, 'learning_rate': 0.00016382473721885242, 'epoch': 0.04}        \n",
      "{'loss': 1.8989, 'learning_rate': 0.0001638624119353502, 'epoch': 0.04}         \n",
      "{'loss': 2.1818, 'learning_rate': 0.00016390008665184794, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 43999/1061708 [6:34:54<150:10:40,  1.88it/s][2024-03-01 00:41:55,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=486, lr=[0.00016393776136834572], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 00:41:55,618] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=1.8920703213985335, CurrSamplesPerSec=1.8971780504176745, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.2556, 'learning_rate': 0.00016393776136834572, 'epoch': 0.04}        \n",
      "{'loss': 1.8502, 'learning_rate': 0.00016397543608484346, 'epoch': 0.04}        \n",
      "{'loss': 2.2052, 'learning_rate': 0.00016401311080134124, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44025/1061708 [6:35:08<151:19:52,  1.87it/s][2024-03-01 00:42:09,401] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 44026/1061708 [6:35:09<141:45:44,  1.99it/s][2024-03-01 00:42:09,824] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0441, 'learning_rate': 0.00016404325057453944, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44035/1061708 [6:35:14<150:35:29,  1.88it/s][2024-03-01 00:42:14,535] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.8376, 'learning_rate': 0.0001640771578193874, 'epoch': 0.04}         \n",
      "{'loss': 2.3009, 'learning_rate': 0.00016411483253588518, 'epoch': 0.04}        \n",
      "{'loss': 2.0829, 'learning_rate': 0.00016415250725238293, 'epoch': 0.04}        \n",
      "{'loss': 1.8729, 'learning_rate': 0.0001641901819688807, 'epoch': 0.04}         \n",
      "{'loss': 2.2644, 'learning_rate': 0.00016422785668537845, 'epoch': 0.04}        \n",
      "{'loss': 1.8661, 'learning_rate': 0.00016426553140187622, 'epoch': 0.04}        \n",
      "{'loss': 1.6952, 'learning_rate': 0.00016430320611837397, 'epoch': 0.04}        \n",
      "{'loss': 2.1328, 'learning_rate': 0.00016434088083487175, 'epoch': 0.04}        \n",
      "{'loss': 2.308, 'learning_rate': 0.00016437855555136947, 'epoch': 0.04}         \n",
      "{'loss': 2.3692, 'learning_rate': 0.00016441623026786724, 'epoch': 0.04}        \n",
      "{'loss': 2.1221, 'learning_rate': 0.00016445390498436499, 'epoch': 0.04}        \n",
      "{'loss': 2.1543, 'learning_rate': 0.00016449157970086276, 'epoch': 0.04}        \n",
      "{'loss': 2.4049, 'learning_rate': 0.0001645292544173605, 'epoch': 0.04}         \n",
      "{'loss': 2.3847, 'learning_rate': 0.00016456692913385828, 'epoch': 0.04}        \n",
      "{'loss': 2.3301, 'learning_rate': 0.00016460460385035603, 'epoch': 0.04}        \n",
      "{'loss': 2.269, 'learning_rate': 0.0001646422785668538, 'epoch': 0.04}          \n",
      "{'loss': 2.1621, 'learning_rate': 0.00016467995328335155, 'epoch': 0.04}        \n",
      "{'loss': 2.2278, 'learning_rate': 0.00016471762799984932, 'epoch': 0.04}        \n",
      "{'loss': 2.5333, 'learning_rate': 0.00016475530271634707, 'epoch': 0.04}        \n",
      "{'loss': 2.3359, 'learning_rate': 0.00016479297743284484, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44236/1061708 [6:37:01<150:44:33,  1.87it/s][2024-03-01 00:44:01,631] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 44237/1061708 [6:37:01<141:17:32,  2.00it/s][2024-03-01 00:44:02,053] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1058, 'learning_rate': 0.00016482311720604305, 'epoch': 0.04}        \n",
      "{'loss': 2.254, 'learning_rate': 0.00016486079192254077, 'epoch': 0.04}         \n",
      "{'loss': 2.0292, 'learning_rate': 0.00016489846663903855, 'epoch': 0.04}        \n",
      "{'loss': 2.0884, 'learning_rate': 0.0001649361413555363, 'epoch': 0.04}         \n",
      "{'loss': 1.9757, 'learning_rate': 0.00016497381607203407, 'epoch': 0.04}        \n",
      "{'loss': 2.2, 'learning_rate': 0.00016501149078853181, 'epoch': 0.04}           \n",
      "  4%|█▏                           | 44291/1061708 [6:37:30<149:29:42,  1.89it/s][2024-03-01 00:44:30,696] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0655, 'learning_rate': 0.0001650453980333798, 'epoch': 0.04}         \n",
      "{'loss': 2.2188, 'learning_rate': 0.00016508307274987755, 'epoch': 0.04}        \n",
      "{'loss': 1.9611, 'learning_rate': 0.00016512074746637533, 'epoch': 0.04}        \n",
      "{'loss': 2.1435, 'learning_rate': 0.00016515842218287307, 'epoch': 0.04}        \n",
      "{'loss': 2.2896, 'learning_rate': 0.00016519609689937085, 'epoch': 0.04}        \n",
      "{'loss': 1.9242, 'learning_rate': 0.0001652337716158686, 'epoch': 0.04}         \n",
      "{'loss': 2.0553, 'learning_rate': 0.00016527144633236634, 'epoch': 0.04}        \n",
      "{'loss': 2.3903, 'learning_rate': 0.00016530912104886411, 'epoch': 0.04}        \n",
      "{'loss': 2.352, 'learning_rate': 0.00016534679576536186, 'epoch': 0.04}         \n",
      "{'loss': 2.2527, 'learning_rate': 0.00016538447048185964, 'epoch': 0.04}        \n",
      "{'loss': 2.5523, 'learning_rate': 0.00016542214519835738, 'epoch': 0.04}        \n",
      "{'loss': 2.0364, 'learning_rate': 0.00016545981991485516, 'epoch': 0.04}        \n",
      "{'loss': 2.2374, 'learning_rate': 0.0001654974946313529, 'epoch': 0.04}         \n",
      "{'loss': 2.121, 'learning_rate': 0.00016553516934785068, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 44439/1061708 [6:38:48<149:47:39,  1.89it/s][2024-03-01 00:45:49,398] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1933, 'learning_rate': 0.00016556907659269864, 'epoch': 0.04}        \n",
      "{'loss': 2.0486, 'learning_rate': 0.00016560675130919642, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44451/1061708 [6:38:55<149:13:03,  1.89it/s][2024-03-01 00:45:55,684] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9816, 'learning_rate': 0.00016564065855404438, 'epoch': 0.04}        \n",
      "{'loss': 2.4179, 'learning_rate': 0.00016567833327054215, 'epoch': 0.04}        \n",
      "{'loss': 2.2866, 'learning_rate': 0.0001657160079870399, 'epoch': 0.04}         \n",
      "{'loss': 2.3976, 'learning_rate': 0.00016575368270353765, 'epoch': 0.04}        \n",
      "{'loss': 2.3, 'learning_rate': 0.00016579135742003542, 'epoch': 0.04}           \n",
      "{'loss': 2.3534, 'learning_rate': 0.00016582903213653317, 'epoch': 0.04}        \n",
      "{'loss': 1.5838, 'learning_rate': 0.00016586670685303094, 'epoch': 0.04}        \n",
      "{'loss': 2.1003, 'learning_rate': 0.0001659043815695287, 'epoch': 0.04}         \n",
      "{'loss': 2.0287, 'learning_rate': 0.00016594205628602646, 'epoch': 0.04}        \n",
      "{'loss': 2.4265, 'learning_rate': 0.0001659797310025242, 'epoch': 0.04}         \n",
      "{'loss': 2.1856, 'learning_rate': 0.00016601740571902198, 'epoch': 0.04}        \n",
      "{'loss': 1.8973, 'learning_rate': 0.00016605508043551973, 'epoch': 0.04}        \n",
      "{'loss': 1.8626, 'learning_rate': 0.0001660927551520175, 'epoch': 0.04}         \n",
      "{'loss': 1.9522, 'learning_rate': 0.00016613042986851525, 'epoch': 0.04}        \n",
      "{'loss': 2.3346, 'learning_rate': 0.000166168104585013, 'epoch': 0.04}          \n",
      "{'loss': 2.1386, 'learning_rate': 0.00016620577930151075, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44617/1061708 [6:40:23<150:02:21,  1.88it/s][2024-03-01 00:47:23,936] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3793, 'learning_rate': 0.00016623968654635876, 'epoch': 0.04}        \n",
      "{'loss': 2.5513, 'learning_rate': 0.0001662773612628565, 'epoch': 0.04}         \n",
      "{'loss': 2.1019, 'learning_rate': 0.00016631503597935428, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44640/1061708 [6:40:35<149:27:35,  1.89it/s][2024-03-01 00:47:36,076] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0945, 'learning_rate': 0.00016634894322420225, 'epoch': 0.04}        \n",
      "{'loss': 2.1974, 'learning_rate': 0.0001663866179407, 'epoch': 0.04}            \n",
      "{'loss': 2.3168, 'learning_rate': 0.00016642429265719777, 'epoch': 0.04}        \n",
      "{'loss': 2.4266, 'learning_rate': 0.00016646196737369552, 'epoch': 0.04}        \n",
      "{'loss': 2.2288, 'learning_rate': 0.0001664996420901933, 'epoch': 0.04}         \n",
      "{'loss': 2.6984, 'learning_rate': 0.00016653731680669104, 'epoch': 0.04}        \n",
      "{'loss': 2.4165, 'learning_rate': 0.0001665749915231888, 'epoch': 0.04}         \n",
      "{'loss': 2.2704, 'learning_rate': 0.00016661266623968653, 'epoch': 0.04}        \n",
      "{'loss': 2.1109, 'learning_rate': 0.0001666503409561843, 'epoch': 0.04}         \n",
      "{'loss': 1.8674, 'learning_rate': 0.00016668801567268205, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44743/1061708 [6:41:30<152:28:25,  1.85it/s][2024-03-01 00:48:30,792] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3545, 'learning_rate': 0.00016672192291753007, 'epoch': 0.04}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7706, 'learning_rate': 0.00016675959763402782, 'epoch': 0.04}        \n",
      "{'loss': 2.1138, 'learning_rate': 0.0001667972723505256, 'epoch': 0.04}         \n",
      "{'loss': 2.4365, 'learning_rate': 0.00016683494706702334, 'epoch': 0.04}        \n",
      "{'loss': 2.0197, 'learning_rate': 0.00016687262178352108, 'epoch': 0.04}        \n",
      "{'loss': 2.2289, 'learning_rate': 0.00016691029650001883, 'epoch': 0.04}        \n",
      "{'loss': 2.3134, 'learning_rate': 0.0001669479712165166, 'epoch': 0.04}         \n",
      "{'loss': 2.354, 'learning_rate': 0.00016698564593301435, 'epoch': 0.04}         \n",
      "{'loss': 2.0172, 'learning_rate': 0.00016702332064951213, 'epoch': 0.04}        \n",
      "{'loss': 2.2063, 'learning_rate': 0.00016706099536600987, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44844/1061708 [6:42:23<151:47:18,  1.86it/s][2024-03-01 00:49:24,514] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 44845/1061708 [6:42:24<141:59:15,  1.99it/s][2024-03-01 00:49:24,937] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0391, 'learning_rate': 0.00016709113513920808, 'epoch': 0.04}        \n",
      "{'loss': 1.9055, 'learning_rate': 0.00016712880985570586, 'epoch': 0.04}        \n",
      "{'loss': 2.2297, 'learning_rate': 0.0001671664845722036, 'epoch': 0.04}         \n",
      "{'loss': 2.5008, 'learning_rate': 0.00016720415928870138, 'epoch': 0.04}        \n",
      "{'loss': 1.9565, 'learning_rate': 0.00016724183400519912, 'epoch': 0.04}        \n",
      "{'loss': 2.2554, 'learning_rate': 0.0001672795087216969, 'epoch': 0.04}         \n",
      "{'loss': 2.2745, 'learning_rate': 0.00016731718343819462, 'epoch': 0.04}        \n",
      "{'loss': 2.1908, 'learning_rate': 0.0001673548581546924, 'epoch': 0.04}         \n",
      "{'loss': 2.0447, 'learning_rate': 0.00016739253287119014, 'epoch': 0.04}        \n",
      "{'loss': 2.5392, 'learning_rate': 0.0001674302075876879, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 44946/1061708 [6:43:18<150:25:54,  1.88it/s][2024-03-01 00:50:18,634] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 44947/1061708 [6:43:18<140:59:02,  2.00it/s][2024-03-01 00:50:19,056] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7277, 'learning_rate': 0.00016746034736088612, 'epoch': 0.04}        \n",
      "{'loss': 2.0055, 'learning_rate': 0.00016749802207738387, 'epoch': 0.04}        \n",
      "{'loss': 2.0878, 'learning_rate': 0.00016753569679388164, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 44978/1061708 [6:43:34<149:56:34,  1.88it/s][2024-03-01 00:50:35,473] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4451, 'learning_rate': 0.0001675696040387296, 'epoch': 0.04}         \n",
      "{'loss': 2.0815, 'learning_rate': 0.00016760727875522738, 'epoch': 0.04}        \n",
      "{'loss': 2.3781, 'learning_rate': 0.00016764495347172513, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 45000/1061708 [6:43:46<149:21:07,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 00:50:46,655 >> Saving model checkpoint to output_model/checkpoint-45000\n",
      "[INFO|trainer.py:2880] 2024-03-01 00:50:46,658 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 00:50:47,863 >> tokenizer config file saved in output_model/checkpoint-45000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 00:50:47,863 >> Special tokens file saved in output_model/checkpoint-45000/special_tokens_map.json\n",
      "[2024-03-01 00:50:47,864] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step45000 is about to be saved!\n",
      "[2024-03-01 00:50:53,093] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-45000/global_step45000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 00:50:53,094] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-45000/global_step45000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 00:51:06,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-45000/global_step45000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 00:51:07,667] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-45000/global_step45000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 00:51:14,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-45000/global_step45000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 00:51:14,811] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-45000/global_step45000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 00:51:14,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 00:51:14,851 >> Deleting older checkpoint [output_model/checkpoint-30000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 00:51:18,603 >> tokenizer config file saved in output_model/checkpoint-45000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 00:51:18,603 >> Special tokens file saved in output_model/checkpoint-45000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.22, 'learning_rate': 0.0001676826281882229, 'epoch': 0.04}           \n",
      "{'loss': 2.4492, 'learning_rate': 0.00016772030290472065, 'epoch': 0.04}        \n",
      "{'loss': 2.0304, 'learning_rate': 0.00016775797762121842, 'epoch': 0.04}        \n",
      "{'loss': 2.177, 'learning_rate': 0.00016779565233771617, 'epoch': 0.04}         \n",
      "{'loss': 2.2446, 'learning_rate': 0.00016783332705421394, 'epoch': 0.04}        \n",
      "{'loss': 1.9886, 'learning_rate': 0.0001678710017707117, 'epoch': 0.04}         \n",
      "{'loss': 1.9651, 'learning_rate': 0.00016790867648720944, 'epoch': 0.04}        \n",
      "{'loss': 2.1878, 'learning_rate': 0.0001679463512037072, 'epoch': 0.04}         \n",
      "{'loss': 2.3311, 'learning_rate': 0.00016798402592020496, 'epoch': 0.04}        \n",
      "{'loss': 2.0149, 'learning_rate': 0.0001680217006367027, 'epoch': 0.04}         \n",
      "{'loss': 2.059, 'learning_rate': 0.00016805937535320048, 'epoch': 0.04}         \n",
      "{'loss': 1.986, 'learning_rate': 0.00016809705006969822, 'epoch': 0.04}         \n",
      "{'loss': 1.8679, 'learning_rate': 0.00016813472478619597, 'epoch': 0.04}        \n",
      "{'loss': 2.5439, 'learning_rate': 0.00016817239950269375, 'epoch': 0.04}        \n",
      "{'loss': 2.1813, 'learning_rate': 0.0001682100742191915, 'epoch': 0.04}         \n",
      "{'loss': 2.1652, 'learning_rate': 0.00016824774893568927, 'epoch': 0.04}        \n",
      "{'loss': 2.102, 'learning_rate': 0.000168285423652187, 'epoch': 0.04}           \n",
      "  4%|█▏                           | 45179/1061708 [6:45:53<150:41:40,  1.87it/s][2024-03-01 00:52:54,324] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5714, 'learning_rate': 0.000168319330897035, 'epoch': 0.04}          \n",
      "  4%|█▏                           | 45180/1061708 [6:45:54<142:26:49,  1.98it/s][2024-03-01 00:52:54,748] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3247, 'learning_rate': 0.000168353238141883, 'epoch': 0.04}          \n",
      "{'loss': 2.2709, 'learning_rate': 0.00016839091285838074, 'epoch': 0.04}        \n",
      "{'loss': 2.1678, 'learning_rate': 0.00016842858757487852, 'epoch': 0.04}        \n",
      "{'loss': 2.423, 'learning_rate': 0.00016846626229137626, 'epoch': 0.04}         \n",
      "{'loss': 2.1437, 'learning_rate': 0.000168503937007874, 'epoch': 0.04}          \n",
      "{'loss': 2.1548, 'learning_rate': 0.00016854161172437178, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 45244/1061708 [6:46:28<152:04:47,  1.86it/s][2024-03-01 00:53:28,873] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8477, 'learning_rate': 0.00016857551896921978, 'epoch': 0.04}        \n",
      "{'loss': 2.2847, 'learning_rate': 0.00016861319368571752, 'epoch': 0.04}        \n",
      "{'loss': 2.3325, 'learning_rate': 0.0001686508684022153, 'epoch': 0.04}         \n",
      "{'loss': 2.3353, 'learning_rate': 0.00016868854311871304, 'epoch': 0.04}        \n",
      "{'loss': 2.1213, 'learning_rate': 0.00016872621783521082, 'epoch': 0.04}        \n",
      "{'loss': 2.0938, 'learning_rate': 0.00016876389255170854, 'epoch': 0.04}        \n",
      "{'loss': 2.168, 'learning_rate': 0.0001688015672682063, 'epoch': 0.04}          \n",
      "{'loss': 1.8596, 'learning_rate': 0.00016883924198470406, 'epoch': 0.04}        \n",
      "{'loss': 2.0616, 'learning_rate': 0.00016887691670120183, 'epoch': 0.04}        \n",
      "{'loss': 2.3865, 'learning_rate': 0.00016891459141769958, 'epoch': 0.04}        \n",
      "{'loss': 2.2375, 'learning_rate': 0.00016895226613419735, 'epoch': 0.04}        \n",
      "{'loss': 2.2596, 'learning_rate': 0.0001689899408506951, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 45369/1061708 [6:47:35<149:55:56,  1.88it/s][2024-03-01 00:54:35,608] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0185, 'learning_rate': 0.0001690238480955431, 'epoch': 0.04}         \n",
      "{'loss': 2.3581, 'learning_rate': 0.00016906152281204084, 'epoch': 0.04}        \n",
      "{'loss': 2.1802, 'learning_rate': 0.0001690991975285386, 'epoch': 0.04}         \n",
      "{'loss': 2.1724, 'learning_rate': 0.00016913687224503636, 'epoch': 0.04}        \n",
      "{'loss': 2.1769, 'learning_rate': 0.0001691745469615341, 'epoch': 0.04}         \n",
      "{'loss': 2.0526, 'learning_rate': 0.00016921222167803188, 'epoch': 0.04}        \n",
      "{'loss': 1.9229, 'learning_rate': 0.00016924989639452963, 'epoch': 0.04}        \n",
      "{'loss': 2.2691, 'learning_rate': 0.0001692875711110274, 'epoch': 0.04}         \n",
      "{'loss': 1.9146, 'learning_rate': 0.00016932524582752515, 'epoch': 0.04}        \n",
      "{'loss': 2.3972, 'learning_rate': 0.00016936292054402292, 'epoch': 0.04}        \n",
      "{'loss': 2.2607, 'learning_rate': 0.00016940059526052067, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 45470/1061708 [6:48:28<149:38:15,  1.89it/s][2024-03-01 00:55:29,466] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 45471/1061708 [6:48:29<140:38:34,  2.01it/s][2024-03-01 00:55:29,890] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2939, 'learning_rate': 0.00016943073503371888, 'epoch': 0.04}        \n",
      "{'loss': 2.0577, 'learning_rate': 0.00016946840975021662, 'epoch': 0.04}        \n",
      "{'loss': 2.4382, 'learning_rate': 0.0001695060844667144, 'epoch': 0.04}         \n",
      "{'loss': 1.8551, 'learning_rate': 0.00016954375918321214, 'epoch': 0.04}        \n",
      "{'loss': 2.3057, 'learning_rate': 0.00016958143389970992, 'epoch': 0.04}        \n",
      "{'loss': 2.1527, 'learning_rate': 0.00016961910861620767, 'epoch': 0.04}        \n",
      "{'loss': 2.163, 'learning_rate': 0.00016965678333270544, 'epoch': 0.04}         \n",
      "{'loss': 2.1438, 'learning_rate': 0.0001696944580492032, 'epoch': 0.04}         \n",
      "{'loss': 2.1715, 'learning_rate': 0.00016973213276570093, 'epoch': 0.04}        \n",
      "{'loss': 2.5266, 'learning_rate': 0.0001697698074821987, 'epoch': 0.04}         \n",
      "  4%|█▏                           | 45572/1061708 [6:49:23<149:58:31,  1.88it/s][2024-03-01 00:56:23,709] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 45573/1061708 [6:49:23<143:58:34,  1.96it/s][2024-03-01 00:56:24,134] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1229, 'learning_rate': 0.00016979994725539692, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 45582/1061708 [6:49:28<149:12:01,  1.89it/s][2024-03-01 00:56:28,886] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8264, 'learning_rate': 0.00016983385450024488, 'epoch': 0.04}        \n",
      "{'loss': 2.4477, 'learning_rate': 0.00016987152921674265, 'epoch': 0.04}        \n",
      "{'loss': 2.305, 'learning_rate': 0.0001699092039332404, 'epoch': 0.04}          \n",
      "{'loss': 2.3815, 'learning_rate': 0.00016994687864973817, 'epoch': 0.04}        \n",
      "{'loss': 2.0327, 'learning_rate': 0.00016998455336623592, 'epoch': 0.04}        \n",
      "{'loss': 2.2224, 'learning_rate': 0.0001700222280827337, 'epoch': 0.04}         \n",
      "{'loss': 1.9492, 'learning_rate': 0.00017005990279923144, 'epoch': 0.04}        \n",
      "{'loss': 2.6341, 'learning_rate': 0.00017009757751572922, 'epoch': 0.04}        \n",
      "{'loss': 1.8431, 'learning_rate': 0.00017013525223222696, 'epoch': 0.04}        \n",
      "{'loss': 2.056, 'learning_rate': 0.0001701729269487247, 'epoch': 0.04}          \n",
      "{'loss': 1.8199, 'learning_rate': 0.00017021060166522246, 'epoch': 0.04}        \n",
      "  4%|█▏                           | 45692/1061708 [6:50:27<149:45:39,  1.88it/s][2024-03-01 00:57:27,593] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▏                           | 45695/1061708 [6:50:28<147:35:31,  1.91it/s][2024-03-01 00:57:29,127] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9694, 'learning_rate': 0.0001702407414384207, 'epoch': 0.04}         \n",
      "{'loss': 2.4312, 'learning_rate': 0.00017027841615491844, 'epoch': 0.04}        \n",
      "{'loss': 2.2964, 'learning_rate': 0.00017031609087141621, 'epoch': 0.04}        \n",
      "{'loss': 2.2156, 'learning_rate': 0.00017035376558791396, 'epoch': 0.04}        \n",
      "{'loss': 2.2566, 'learning_rate': 0.0001703914403044117, 'epoch': 0.04}         \n",
      "{'loss': 2.0956, 'learning_rate': 0.00017042911502090948, 'epoch': 0.04}        \n",
      "{'loss': 2.4215, 'learning_rate': 0.00017046678973740723, 'epoch': 0.04}        \n",
      "{'loss': 2.1929, 'learning_rate': 0.000170504464453905, 'epoch': 0.04}          \n",
      "{'loss': 2.3779, 'learning_rate': 0.00017054213917040275, 'epoch': 0.04}        \n",
      "{'loss': 2.6627, 'learning_rate': 0.00017057981388690052, 'epoch': 0.04}        \n",
      "{'loss': 2.0277, 'learning_rate': 0.00017061748860339827, 'epoch': 0.04}        \n",
      "{'loss': 2.1814, 'learning_rate': 0.00017065516331989602, 'epoch': 0.04}        \n",
      "{'loss': 2.3925, 'learning_rate': 0.00017069283803639376, 'epoch': 0.04}        \n",
      "{'loss': 1.887, 'learning_rate': 0.00017073051275289154, 'epoch': 0.04}         \n",
      "{'loss': 2.4633, 'learning_rate': 0.00017076818746938928, 'epoch': 0.04}        \n",
      "{'loss': 2.365, 'learning_rate': 0.00017080586218588706, 'epoch': 0.04}         \n",
      "{'loss': 1.8943, 'learning_rate': 0.0001708435369023848, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 45867/1061708 [6:52:00<150:26:26,  1.88it/s][2024-03-01 00:59:00,916] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.206, 'learning_rate': 0.0001708774441472328, 'epoch': 0.04}          \n",
      "{'loss': 2.2467, 'learning_rate': 0.00017091511886373054, 'epoch': 0.04}        \n",
      "{'loss': 2.3134, 'learning_rate': 0.00017095279358022832, 'epoch': 0.04}        \n",
      "{'loss': 2.4498, 'learning_rate': 0.00017099046829672606, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 45900/1061708 [6:52:17<149:38:42,  1.89it/s][2024-03-01 00:59:18,436] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2411, 'learning_rate': 0.00017102437554157406, 'epoch': 0.04}        \n",
      "{'loss': 2.3635, 'learning_rate': 0.00017106205025807183, 'epoch': 0.04}        \n",
      "{'loss': 2.5055, 'learning_rate': 0.00017109972497456958, 'epoch': 0.04}        \n",
      "{'loss': 2.507, 'learning_rate': 0.00017113739969106732, 'epoch': 0.04}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2686, 'learning_rate': 0.00017117507440756507, 'epoch': 0.04}        \n",
      "{'loss': 2.5238, 'learning_rate': 0.00017121274912406284, 'epoch': 0.04}        \n",
      "{'loss': 2.1722, 'learning_rate': 0.0001712504238405606, 'epoch': 0.04}         \n",
      "{'loss': 2.3613, 'learning_rate': 0.00017128809855705837, 'epoch': 0.04}        \n",
      "{'loss': 2.4139, 'learning_rate': 0.0001713257732735561, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 45999/1061708 [6:53:10<149:52:41,  1.88it/s][2024-03-01 01:00:11,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=515, lr=[0.00017136344799005389], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 01:00:11,307] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=1.8922257206551671, CurrSamplesPerSec=1.8968760345229934, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3733, 'learning_rate': 0.00017136344799005389, 'epoch': 0.04}        \n",
      "{'loss': 2.1582, 'learning_rate': 0.00017140112270655163, 'epoch': 0.04}        \n",
      "{'loss': 2.3227, 'learning_rate': 0.0001714387974230494, 'epoch': 0.04}         \n",
      "{'loss': 2.2803, 'learning_rate': 0.00017147647213954715, 'epoch': 0.04}        \n",
      "{'loss': 2.6002, 'learning_rate': 0.00017151414685604493, 'epoch': 0.04}        \n",
      "{'loss': 2.3244, 'learning_rate': 0.00017155182157254267, 'epoch': 0.04}        \n",
      "{'loss': 2.1309, 'learning_rate': 0.00017158949628904045, 'epoch': 0.04}        \n",
      "{'loss': 2.6185, 'learning_rate': 0.0001716271710055382, 'epoch': 0.04}         \n",
      "{'loss': 2.2227, 'learning_rate': 0.00017166484572203597, 'epoch': 0.04}        \n",
      "{'loss': 1.851, 'learning_rate': 0.0001717025204385337, 'epoch': 0.04}          \n",
      "{'loss': 2.1229, 'learning_rate': 0.00017174019515503146, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46101/1061708 [6:54:05<149:37:06,  1.89it/s][2024-03-01 01:01:05,671] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▎                           | 46102/1061708 [6:54:05<140:30:22,  2.01it/s][2024-03-01 01:01:06,094] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  4%|█▎                           | 46105/1061708 [6:54:07<141:52:57,  1.99it/s][2024-03-01 01:01:07,628] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1253, 'learning_rate': 0.00017176656745657992, 'epoch': 0.04}        \n",
      "{'loss': 1.8052, 'learning_rate': 0.00017180424217307764, 'epoch': 0.04}        \n",
      "{'loss': 2.5477, 'learning_rate': 0.0001718419168895754, 'epoch': 0.04}         \n",
      "{'loss': 2.3217, 'learning_rate': 0.00017187959160607316, 'epoch': 0.04}        \n",
      "{'loss': 2.0342, 'learning_rate': 0.00017191726632257093, 'epoch': 0.04}        \n",
      "{'loss': 2.5984, 'learning_rate': 0.00017195494103906868, 'epoch': 0.04}        \n",
      "{'loss': 2.4947, 'learning_rate': 0.00017199261575556645, 'epoch': 0.04}        \n",
      "{'loss': 2.4737, 'learning_rate': 0.0001720302904720642, 'epoch': 0.04}         \n",
      "{'loss': 2.2929, 'learning_rate': 0.00017206796518856197, 'epoch': 0.04}        \n",
      "{'loss': 2.0665, 'learning_rate': 0.00017210563990505972, 'epoch': 0.04}        \n",
      "{'loss': 1.9524, 'learning_rate': 0.0001721433146215575, 'epoch': 0.04}         \n",
      "{'loss': 2.4028, 'learning_rate': 0.00017218098933805524, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46224/1061708 [6:55:10<152:04:07,  1.85it/s][2024-03-01 01:02:11,145] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2978, 'learning_rate': 0.0001722148965829032, 'epoch': 0.04}         \n",
      "{'loss': 2.4605, 'learning_rate': 0.00017225257129940098, 'epoch': 0.04}        \n",
      "{'loss': 2.0689, 'learning_rate': 0.00017229024601589873, 'epoch': 0.04}        \n",
      "{'loss': 2.1637, 'learning_rate': 0.0001723279207323965, 'epoch': 0.04}         \n",
      "{'loss': 2.3133, 'learning_rate': 0.00017236559544889425, 'epoch': 0.04}        \n",
      "{'loss': 2.0013, 'learning_rate': 0.00017240327016539202, 'epoch': 0.04}        \n",
      "{'loss': 2.4562, 'learning_rate': 0.00017244094488188977, 'epoch': 0.04}        \n",
      "{'loss': 2.2872, 'learning_rate': 0.00017247861959838754, 'epoch': 0.04}        \n",
      "{'loss': 2.0095, 'learning_rate': 0.0001725162943148853, 'epoch': 0.04}         \n",
      "{'loss': 2.1755, 'learning_rate': 0.00017255396903138306, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46325/1061708 [6:56:04<151:20:43,  1.86it/s][2024-03-01 01:03:05,080] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  4%|█▎                           | 46326/1061708 [6:56:04<141:43:16,  1.99it/s][2024-03-01 01:03:05,503] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3569, 'learning_rate': 0.00017258410880458124, 'epoch': 0.04}        \n",
      "{'loss': 2.2698, 'learning_rate': 0.00017262178352107902, 'epoch': 0.04}        \n",
      "{'loss': 2.0637, 'learning_rate': 0.00017265945823757676, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46351/1061708 [6:56:18<149:40:55,  1.88it/s][2024-03-01 01:03:18,786] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  4%|█▎                           | 46356/1061708 [6:56:20<150:02:13,  1.88it/s][2024-03-01 01:03:21,380] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2327, 'learning_rate': 0.00017268959801077497, 'epoch': 0.04}        \n",
      "{'loss': 2.0552, 'learning_rate': 0.00017272727272727275, 'epoch': 0.04}        \n",
      "{'loss': 2.1361, 'learning_rate': 0.0001727649474437705, 'epoch': 0.04}         \n",
      "{'loss': 2.4178, 'learning_rate': 0.00017280262216026827, 'epoch': 0.04}        \n",
      "{'loss': 2.0945, 'learning_rate': 0.00017284029687676601, 'epoch': 0.04}        \n",
      "{'loss': 2.2762, 'learning_rate': 0.0001728779715932638, 'epoch': 0.04}         \n",
      "{'loss': 2.1674, 'learning_rate': 0.00017291564630976154, 'epoch': 0.04}        \n",
      "{'loss': 2.2589, 'learning_rate': 0.0001729533210262593, 'epoch': 0.04}         \n",
      "{'loss': 2.2829, 'learning_rate': 0.00017299099574275703, 'epoch': 0.04}        \n",
      "{'loss': 2.1581, 'learning_rate': 0.0001730286704592548, 'epoch': 0.04}         \n",
      "{'loss': 2.0882, 'learning_rate': 0.00017306634517575255, 'epoch': 0.04}        \n",
      "{'loss': 2.2067, 'learning_rate': 0.00017310401989225032, 'epoch': 0.04}        \n",
      "{'loss': 2.2317, 'learning_rate': 0.00017314169460874807, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46481/1061708 [6:57:27<149:36:14,  1.89it/s][2024-03-01 01:04:28,044] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3848, 'learning_rate': 0.00017317560185359606, 'epoch': 0.04}        \n",
      "{'loss': 2.2191, 'learning_rate': 0.0001732132765700938, 'epoch': 0.04}         \n",
      "{'loss': 2.1049, 'learning_rate': 0.00017325095128659156, 'epoch': 0.04}        \n",
      "{'loss': 2.2467, 'learning_rate': 0.00017328862600308933, 'epoch': 0.04}        \n",
      "{'loss': 2.4138, 'learning_rate': 0.00017332630071958708, 'epoch': 0.04}        \n",
      "{'loss': 2.2435, 'learning_rate': 0.00017336397543608485, 'epoch': 0.04}        \n",
      "{'loss': 2.2853, 'learning_rate': 0.0001734016501525826, 'epoch': 0.04}         \n",
      "{'loss': 2.039, 'learning_rate': 0.00017343932486908037, 'epoch': 0.04}         \n",
      "{'loss': 2.1003, 'learning_rate': 0.00017347699958557812, 'epoch': 0.04}        \n",
      "{'loss': 2.6203, 'learning_rate': 0.0001735146743020759, 'epoch': 0.04}         \n",
      "{'loss': 1.9723, 'learning_rate': 0.00017355234901857364, 'epoch': 0.04}        \n",
      "{'loss': 1.9648, 'learning_rate': 0.0001735900237350714, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 46606/1061708 [6:58:34<150:44:10,  1.87it/s][2024-03-01 01:05:34,745] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8832, 'learning_rate': 0.00017362393097991938, 'epoch': 0.04}        \n",
      "{'loss': 2.4731, 'learning_rate': 0.00017366160569641715, 'epoch': 0.04}        \n",
      "{'loss': 2.2608, 'learning_rate': 0.0001736992804129149, 'epoch': 0.04}         \n",
      "{'loss': 2.5711, 'learning_rate': 0.00017373695512941267, 'epoch': 0.04}        \n",
      "{'loss': 2.3537, 'learning_rate': 0.00017377462984591042, 'epoch': 0.04}        \n",
      "{'loss': 2.0806, 'learning_rate': 0.00017381230456240817, 'epoch': 0.04}        \n",
      "{'loss': 2.6587, 'learning_rate': 0.00017384997927890594, 'epoch': 0.04}        \n",
      "{'loss': 2.2757, 'learning_rate': 0.0001738876539954037, 'epoch': 0.04}         \n",
      "{'loss': 2.0535, 'learning_rate': 0.00017392532871190146, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46692/1061708 [6:59:20<149:58:39,  1.88it/s][2024-03-01 01:06:20,583] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9277, 'learning_rate': 0.00017395923595674943, 'epoch': 0.04}        \n",
      "{'loss': 2.2504, 'learning_rate': 0.0001739969106732472, 'epoch': 0.04}         \n",
      "{'loss': 1.7833, 'learning_rate': 0.00017403458538974495, 'epoch': 0.04}        \n",
      "{'loss': 2.1048, 'learning_rate': 0.00017407226010624272, 'epoch': 0.04}        \n",
      "{'loss': 2.1076, 'learning_rate': 0.00017410993482274047, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 46743/1061708 [6:59:47<152:45:32,  1.85it/s][2024-03-01 01:06:47,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9671, 'learning_rate': 0.00017414384206758846, 'epoch': 0.04}        \n",
      "{'loss': 2.1466, 'learning_rate': 0.0001741815167840862, 'epoch': 0.04}         \n",
      "{'loss': 2.495, 'learning_rate': 0.00017421919150058398, 'epoch': 0.04}         \n",
      "{'loss': 1.8806, 'learning_rate': 0.00017425686621708173, 'epoch': 0.04}        \n",
      "{'loss': 2.2593, 'learning_rate': 0.0001742945409335795, 'epoch': 0.04}         \n",
      "{'loss': 2.1599, 'learning_rate': 0.00017433221565007725, 'epoch': 0.04}        \n",
      "{'loss': 2.0593, 'learning_rate': 0.000174369890366575, 'epoch': 0.04}          \n",
      "{'loss': 2.2801, 'learning_rate': 0.00017440756508307277, 'epoch': 0.04}        \n",
      "{'loss': 2.5461, 'learning_rate': 0.00017444523979957051, 'epoch': 0.04}        \n",
      "{'loss': 1.9381, 'learning_rate': 0.0001744829145160683, 'epoch': 0.04}         \n",
      "{'loss': 2.2261, 'learning_rate': 0.00017452058923256604, 'epoch': 0.04}        \n",
      "{'loss': 2.3401, 'learning_rate': 0.00017455826394906378, 'epoch': 0.04}        \n",
      "{'loss': 2.274, 'learning_rate': 0.00017459593866556153, 'epoch': 0.04}         \n",
      "{'loss': 2.0472, 'learning_rate': 0.0001746336133820593, 'epoch': 0.04}         \n",
      "{'loss': 2.4586, 'learning_rate': 0.00017467128809855705, 'epoch': 0.04}        \n",
      "{'loss': 2.0166, 'learning_rate': 0.00017470896281505482, 'epoch': 0.04}        \n",
      "{'loss': 1.9812, 'learning_rate': 0.00017474663753155257, 'epoch': 0.04}        \n",
      "{'loss': 1.9294, 'learning_rate': 0.00017478431224805035, 'epoch': 0.04}        \n",
      "{'loss': 1.7413, 'learning_rate': 0.0001748219869645481, 'epoch': 0.04}         \n",
      "{'loss': 1.9872, 'learning_rate': 0.00017485966168104587, 'epoch': 0.04}        \n",
      "{'loss': 2.1795, 'learning_rate': 0.0001748973363975436, 'epoch': 0.04}         \n",
      "{'loss': 2.5296, 'learning_rate': 0.0001749350111140414, 'epoch': 0.04}         \n",
      "{'loss': 2.3346, 'learning_rate': 0.00017497268583053913, 'epoch': 0.04}        \n",
      "{'loss': 2.306, 'learning_rate': 0.0001750103605470369, 'epoch': 0.04}          \n",
      "{'loss': 2.3089, 'learning_rate': 0.00017504803526353465, 'epoch': 0.04}        \n",
      "{'loss': 2.2376, 'learning_rate': 0.00017508570998003243, 'epoch': 0.04}        \n",
      "{'loss': 2.222, 'learning_rate': 0.00017512338469653015, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 47016/1061708 [7:02:13<150:30:55,  1.87it/s][2024-03-01 01:09:13,535] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1715, 'learning_rate': 0.00017515729194137817, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 47024/1061708 [7:02:17<152:16:13,  1.85it/s][2024-03-01 01:09:17,767] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0527, 'learning_rate': 0.00017519119918622613, 'epoch': 0.04}        \n",
      "{'loss': 2.4336, 'learning_rate': 0.00017522887390272388, 'epoch': 0.04}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00017526654861922165, 'epoch': 0.04}        \n",
      "{'loss': 2.3177, 'learning_rate': 0.0001753042233357194, 'epoch': 0.04}         \n",
      "{'loss': 2.2983, 'learning_rate': 0.00017534189805221717, 'epoch': 0.04}        \n",
      "{'loss': 2.6483, 'learning_rate': 0.00017537957276871492, 'epoch': 0.04}        \n",
      "{'loss': 2.0295, 'learning_rate': 0.0001754172474852127, 'epoch': 0.04}         \n",
      "{'loss': 2.0818, 'learning_rate': 0.00017545492220171044, 'epoch': 0.04}        \n",
      "{'loss': 2.3163, 'learning_rate': 0.00017549259691820821, 'epoch': 0.04}        \n",
      "{'loss': 2.2462, 'learning_rate': 0.00017553027163470596, 'epoch': 0.04}        \n",
      "{'loss': 2.0163, 'learning_rate': 0.0001755679463512037, 'epoch': 0.04}         \n",
      "{'loss': 1.7129, 'learning_rate': 0.00017560562106770146, 'epoch': 0.04}        \n",
      "{'loss': 2.2634, 'learning_rate': 0.00017564329578419923, 'epoch': 0.04}        \n",
      "{'loss': 2.1488, 'learning_rate': 0.00017568097050069698, 'epoch': 0.04}        \n",
      "{'loss': 2.1592, 'learning_rate': 0.00017571864521719475, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 47177/1061708 [7:03:38<150:03:49,  1.88it/s][2024-03-01 01:10:39,380] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1277, 'learning_rate': 0.00017575255246204274, 'epoch': 0.04}        \n",
      "{'loss': 2.464, 'learning_rate': 0.0001757902271785405, 'epoch': 0.04}          \n",
      "{'loss': 2.339, 'learning_rate': 0.00017582790189503823, 'epoch': 0.04}         \n",
      "{'loss': 2.07, 'learning_rate': 0.000175865576611536, 'epoch': 0.04}            \n",
      "{'loss': 2.1763, 'learning_rate': 0.00017590325132803376, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 47224/1061708 [7:04:03<152:05:24,  1.85it/s][2024-03-01 01:11:04,399] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3252, 'learning_rate': 0.00017593715857288175, 'epoch': 0.04}        \n",
      "{'loss': 2.2756, 'learning_rate': 0.00017597483328937952, 'epoch': 0.04}        \n",
      "{'loss': 2.1487, 'learning_rate': 0.00017601250800587727, 'epoch': 0.04}        \n",
      "{'loss': 2.8912, 'learning_rate': 0.00017605018272237501, 'epoch': 0.04}        \n",
      "{'loss': 2.1171, 'learning_rate': 0.00017608785743887276, 'epoch': 0.04}        \n",
      "{'loss': 2.1594, 'learning_rate': 0.00017612553215537054, 'epoch': 0.04}        \n",
      "{'loss': 2.0973, 'learning_rate': 0.00017616320687186828, 'epoch': 0.04}        \n",
      "{'loss': 2.0184, 'learning_rate': 0.00017620088158836606, 'epoch': 0.04}        \n",
      "{'loss': 2.5585, 'learning_rate': 0.0001762385563048638, 'epoch': 0.04}         \n",
      "{'loss': 2.0812, 'learning_rate': 0.00017627623102136158, 'epoch': 0.04}        \n",
      "{'loss': 2.3181, 'learning_rate': 0.00017631390573785932, 'epoch': 0.04}        \n",
      "{'loss': 2.0017, 'learning_rate': 0.0001763515804543571, 'epoch': 0.04}         \n",
      "{'loss': 2.0327, 'learning_rate': 0.00017638925517085485, 'epoch': 0.04}        \n",
      "{'loss': 2.4018, 'learning_rate': 0.00017642692988735262, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 47360/1061708 [7:05:16<149:26:08,  1.89it/s][2024-03-01 01:12:16,906] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2413, 'learning_rate': 0.00017646083713220058, 'epoch': 0.04}        \n",
      "{'loss': 2.2541, 'learning_rate': 0.00017649851184869836, 'epoch': 0.04}        \n",
      "  4%|█▎                           | 47389/1061708 [7:05:31<149:28:56,  1.88it/s][2024-03-01 01:12:32,295] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0882, 'learning_rate': 0.00017653241909354632, 'epoch': 0.04}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9016, 'learning_rate': 0.00017657009381004407, 'epoch': 0.04}        \n",
      "{'loss': 2.3801, 'learning_rate': 0.00017660776852654184, 'epoch': 0.04}        \n",
      "{'loss': 1.8987, 'learning_rate': 0.0001766454432430396, 'epoch': 0.04}         \n",
      "{'loss': 2.1219, 'learning_rate': 0.00017668311795953736, 'epoch': 0.04}        \n",
      "{'loss': 2.3275, 'learning_rate': 0.0001767207926760351, 'epoch': 0.04}         \n",
      "{'loss': 2.2411, 'learning_rate': 0.00017675846739253288, 'epoch': 0.04}        \n",
      "{'loss': 2.2471, 'learning_rate': 0.00017679614210903063, 'epoch': 0.04}        \n",
      "{'loss': 2.0685, 'learning_rate': 0.0001768338168255284, 'epoch': 0.04}         \n",
      "{'loss': 1.9648, 'learning_rate': 0.00017687149154202615, 'epoch': 0.04}        \n",
      "{'loss': 1.9067, 'learning_rate': 0.00017690916625852393, 'epoch': 0.04}        \n",
      "{'loss': 2.7017, 'learning_rate': 0.00017694684097502167, 'epoch': 0.04}        \n",
      "{'loss': 2.3771, 'learning_rate': 0.00017698451569151945, 'epoch': 0.04}        \n",
      "{'loss': 2.0486, 'learning_rate': 0.0001770221904080172, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 47524/1061708 [7:06:43<151:21:50,  1.86it/s][2024-03-01 01:13:44,248] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1032, 'learning_rate': 0.00017705609765286518, 'epoch': 0.04}        \n",
      "{'loss': 2.4737, 'learning_rate': 0.00017709377236936293, 'epoch': 0.04}        \n",
      "{'loss': 2.5021, 'learning_rate': 0.00017713144708586068, 'epoch': 0.04}        \n",
      "{'loss': 2.1951, 'learning_rate': 0.00017716912180235845, 'epoch': 0.04}        \n",
      "{'loss': 2.3503, 'learning_rate': 0.0001772067965188562, 'epoch': 0.04}         \n",
      "{'loss': 2.7602, 'learning_rate': 0.00017724447123535397, 'epoch': 0.04}        \n",
      "{'loss': 1.8957, 'learning_rate': 0.00017728214595185172, 'epoch': 0.04}        \n",
      "{'loss': 2.3471, 'learning_rate': 0.0001773198206683495, 'epoch': 0.04}         \n",
      "{'loss': 2.1501, 'learning_rate': 0.00017735749538484721, 'epoch': 0.04}        \n",
      "{'loss': 2.0065, 'learning_rate': 0.000177395170101345, 'epoch': 0.04}          \n",
      "  4%|█▎                           | 47623/1061708 [7:07:36<152:36:38,  1.85it/s][2024-03-01 01:14:36,931] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3474, 'learning_rate': 0.00017742907734619298, 'epoch': 0.04}        \n",
      "{'loss': 2.2976, 'learning_rate': 0.00017746675206269075, 'epoch': 0.04}        \n",
      "{'loss': 2.6391, 'learning_rate': 0.0001775044267791885, 'epoch': 0.04}         \n",
      "{'loss': 2.138, 'learning_rate': 0.00017754210149568627, 'epoch': 0.04}         \n",
      "{'loss': 2.3292, 'learning_rate': 0.000177579776212184, 'epoch': 0.04}          \n",
      "{'loss': 2.1912, 'learning_rate': 0.00017761745092868177, 'epoch': 0.04}        \n",
      "{'loss': 1.9197, 'learning_rate': 0.00017765512564517951, 'epoch': 0.04}        \n",
      "{'loss': 2.207, 'learning_rate': 0.0001776928003616773, 'epoch': 0.04}          \n",
      "{'loss': 2.1683, 'learning_rate': 0.00017773047507817504, 'epoch': 0.04}        \n",
      "{'loss': 2.6351, 'learning_rate': 0.0001777681497946728, 'epoch': 0.04}         \n",
      "  4%|█▎                           | 47721/1061708 [7:08:28<149:10:05,  1.89it/s][2024-03-01 01:15:29,058] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1795, 'learning_rate': 0.00017780205703952077, 'epoch': 0.04}        \n",
      "{'loss': 2.3391, 'learning_rate': 0.00017783973175601855, 'epoch': 0.04}        \n",
      "{'loss': 2.1442, 'learning_rate': 0.0001778774064725163, 'epoch': 0.04}         \n",
      "{'loss': 1.8552, 'learning_rate': 0.00017791508118901404, 'epoch': 0.04}        \n",
      "{'loss': 2.0894, 'learning_rate': 0.00017795275590551182, 'epoch': 0.04}        \n",
      "{'loss': 2.5212, 'learning_rate': 0.00017799043062200956, 'epoch': 0.05}        \n",
      "{'loss': 2.1326, 'learning_rate': 0.00017802810533850734, 'epoch': 0.05}        \n",
      "{'loss': 2.1271, 'learning_rate': 0.00017806578005500508, 'epoch': 0.05}        \n",
      "{'loss': 2.1133, 'learning_rate': 0.00017810345477150286, 'epoch': 0.05}        \n",
      "{'loss': 2.162, 'learning_rate': 0.0001781411294880006, 'epoch': 0.05}          \n",
      "{'loss': 2.4021, 'learning_rate': 0.00017817880420449838, 'epoch': 0.05}        \n",
      "{'loss': 2.4146, 'learning_rate': 0.00017821647892099613, 'epoch': 0.05}        \n",
      "{'loss': 2.1056, 'learning_rate': 0.0001782541536374939, 'epoch': 0.05}         \n",
      "{'loss': 2.3381, 'learning_rate': 0.00017829182835399165, 'epoch': 0.05}        \n",
      "{'loss': 2.1706, 'learning_rate': 0.00017832950307048942, 'epoch': 0.05}        \n",
      "{'loss': 2.1968, 'learning_rate': 0.00017836717778698717, 'epoch': 0.05}        \n",
      "{'loss': 2.0713, 'learning_rate': 0.00017840485250348491, 'epoch': 0.05}        \n",
      "{'loss': 2.3164, 'learning_rate': 0.00017844252721998266, 'epoch': 0.05}        \n",
      "{'loss': 2.1748, 'learning_rate': 0.00017848020193648043, 'epoch': 0.05}        \n",
      "{'loss': 2.0676, 'learning_rate': 0.00017851787665297818, 'epoch': 0.05}        \n",
      "{'loss': 2.3104, 'learning_rate': 0.00017855555136947596, 'epoch': 0.05}        \n",
      "{'loss': 2.0784, 'learning_rate': 0.0001785932260859737, 'epoch': 0.05}         \n",
      "{'loss': 2.2981, 'learning_rate': 0.00017863090080247148, 'epoch': 0.05}        \n",
      "{'loss': 2.21, 'learning_rate': 0.00017866857551896922, 'epoch': 0.05}          \n",
      "{'loss': 2.1259, 'learning_rate': 0.000178706250235467, 'epoch': 0.05}          \n",
      "{'loss': 2.1905, 'learning_rate': 0.00017874392495196474, 'epoch': 0.05}        \n",
      "{'loss': 2.0656, 'learning_rate': 0.00017878159966846252, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 47999/1061708 [7:10:56<149:13:32,  1.89it/s][2024-03-01 01:17:57,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=48000, skipped=536, lr=[0.00017881927438496027], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 01:17:57,087] [INFO] [timer.py:260:stop] epoch=0/micro_step=48000/global_step=48000, RunningAvgSamplesPerSec=1.892218022084024, CurrSamplesPerSec=1.9069230748247112, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3478, 'learning_rate': 0.00017881927438496027, 'epoch': 0.05}        \n",
      "{'loss': 2.2531, 'learning_rate': 0.00017885694910145804, 'epoch': 0.05}        \n",
      "{'loss': 1.7961, 'learning_rate': 0.00017889462381795579, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48022/1061708 [7:11:08<149:43:31,  1.88it/s][2024-03-01 01:18:09,272] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▎                           | 48023/1061708 [7:11:09<143:48:50,  1.96it/s][2024-03-01 01:18:09,695] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3663, 'learning_rate': 0.00017892476359115397, 'epoch': 0.05}        \n",
      "{'loss': 2.1831, 'learning_rate': 0.00017896243830765174, 'epoch': 0.05}        \n",
      "{'loss': 2.5589, 'learning_rate': 0.0001790001130241495, 'epoch': 0.05}         \n",
      "{'loss': 2.1836, 'learning_rate': 0.00017903778774064726, 'epoch': 0.05}        \n",
      "{'loss': 2.024, 'learning_rate': 0.000179075462457145, 'epoch': 0.05}           \n",
      "{'loss': 2.2232, 'learning_rate': 0.00017911313717364278, 'epoch': 0.05}        \n",
      "{'loss': 2.3498, 'learning_rate': 0.00017915081189014053, 'epoch': 0.05}        \n",
      "{'loss': 1.8702, 'learning_rate': 0.0001791884866066383, 'epoch': 0.05}         \n",
      "{'loss': 2.2468, 'learning_rate': 0.00017922616132313605, 'epoch': 0.05}        \n",
      "{'loss': 2.0802, 'learning_rate': 0.00017926383603963383, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48124/1061708 [7:12:02<151:22:29,  1.86it/s][2024-03-01 01:19:03,420] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▎                           | 48125/1061708 [7:12:03<141:38:20,  1.99it/s][2024-03-01 01:19:03,843] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2483, 'learning_rate': 0.00017929397581283203, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.33, 'learning_rate': 0.00017933165052932975, 'epoch': 0.05}          \n",
      "{'loss': 2.5537, 'learning_rate': 0.00017936932524582753, 'epoch': 0.05}        \n",
      "{'loss': 2.2178, 'learning_rate': 0.00017940699996232527, 'epoch': 0.05}        \n",
      "{'loss': 2.3487, 'learning_rate': 0.00017944467467882305, 'epoch': 0.05}        \n",
      "{'loss': 2.1515, 'learning_rate': 0.0001794823493953208, 'epoch': 0.05}         \n",
      "{'loss': 2.285, 'learning_rate': 0.00017952002411181857, 'epoch': 0.05}         \n",
      "{'loss': 2.44, 'learning_rate': 0.00017955769882831632, 'epoch': 0.05}          \n",
      "{'loss': 2.333, 'learning_rate': 0.0001795953735448141, 'epoch': 0.05}          \n",
      "{'loss': 2.0434, 'learning_rate': 0.00017963304826131184, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48226/1061708 [7:12:57<149:56:11,  1.88it/s][2024-03-01 01:19:57,579] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▎                           | 48227/1061708 [7:12:57<140:33:01,  2.00it/s][2024-03-01 01:19:58,001] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0708, 'learning_rate': 0.00017966318803451005, 'epoch': 0.05}        \n",
      "{'loss': 2.4023, 'learning_rate': 0.00017970086275100782, 'epoch': 0.05}        \n",
      "{'loss': 2.2383, 'learning_rate': 0.00017973853746750557, 'epoch': 0.05}        \n",
      "{'loss': 1.971, 'learning_rate': 0.00017977621218400334, 'epoch': 0.05}         \n",
      "{'loss': 1.8051, 'learning_rate': 0.00017981388690050106, 'epoch': 0.05}        \n",
      "{'loss': 2.3357, 'learning_rate': 0.00017985156161699883, 'epoch': 0.05}        \n",
      "{'loss': 2.7943, 'learning_rate': 0.00017988923633349658, 'epoch': 0.05}        \n",
      "{'loss': 1.9246, 'learning_rate': 0.00017992691104999435, 'epoch': 0.05}        \n",
      "{'loss': 2.5939, 'learning_rate': 0.0001799645857664921, 'epoch': 0.05}         \n",
      "{'loss': 2.1333, 'learning_rate': 0.00018000226048298988, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48321/1061708 [7:13:47<148:52:57,  1.89it/s][2024-03-01 01:20:47,959] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1508, 'learning_rate': 0.00018003616772783784, 'epoch': 0.05}        \n",
      "{'loss': 2.1992, 'learning_rate': 0.00018007384244433561, 'epoch': 0.05}        \n",
      "{'loss': 2.2255, 'learning_rate': 0.00018011151716083336, 'epoch': 0.05}        \n",
      "{'loss': 2.2172, 'learning_rate': 0.00018014919187733113, 'epoch': 0.05}        \n",
      "{'loss': 2.4223, 'learning_rate': 0.00018018686659382888, 'epoch': 0.05}        \n",
      "{'loss': 1.9666, 'learning_rate': 0.00018022454131032666, 'epoch': 0.05}        \n",
      "{'loss': 2.1922, 'learning_rate': 0.0001802622160268244, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 48397/1061708 [7:14:27<149:33:34,  1.88it/s][2024-03-01 01:21:28,400] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.246, 'learning_rate': 0.00018029612327167237, 'epoch': 0.05}         \n",
      "{'loss': 2.1084, 'learning_rate': 0.00018033379798817014, 'epoch': 0.05}        \n",
      "{'loss': 2.2734, 'learning_rate': 0.0001803714727046679, 'epoch': 0.05}         \n",
      "{'loss': 2.2771, 'learning_rate': 0.00018040914742116566, 'epoch': 0.05}        \n",
      "{'loss': 2.0535, 'learning_rate': 0.0001804468221376634, 'epoch': 0.05}         \n",
      "{'loss': 2.2287, 'learning_rate': 0.00018048449685416118, 'epoch': 0.05}        \n",
      "{'loss': 2.2793, 'learning_rate': 0.00018052217157065893, 'epoch': 0.05}        \n",
      "{'loss': 1.6058, 'learning_rate': 0.0001805598462871567, 'epoch': 0.05}         \n",
      "{'loss': 2.2004, 'learning_rate': 0.00018059752100365445, 'epoch': 0.05}        \n",
      "{'loss': 2.2529, 'learning_rate': 0.00018063519572015222, 'epoch': 0.05}        \n",
      "{'loss': 2.4377, 'learning_rate': 0.00018067287043664997, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48509/1061708 [7:15:27<149:02:40,  1.89it/s][2024-03-01 01:22:27,903] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.109, 'learning_rate': 0.00018070677768149796, 'epoch': 0.05}         \n",
      "{'loss': 2.7218, 'learning_rate': 0.0001807444523979957, 'epoch': 0.05}         \n",
      "{'loss': 1.992, 'learning_rate': 0.00018078212711449348, 'epoch': 0.05}         \n",
      "{'loss': 2.176, 'learning_rate': 0.00018081980183099123, 'epoch': 0.05}         \n",
      "{'loss': 2.2088, 'learning_rate': 0.000180857476547489, 'epoch': 0.05}          \n",
      "{'loss': 2.3039, 'learning_rate': 0.00018089515126398675, 'epoch': 0.05}        \n",
      "{'loss': 1.7681, 'learning_rate': 0.0001809328259804845, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 48572/1061708 [7:16:00<149:05:35,  1.89it/s][2024-03-01 01:23:01,414] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0404, 'learning_rate': 0.0001809667332253325, 'epoch': 0.05}         \n",
      "{'loss': 1.943, 'learning_rate': 0.00018100440794183024, 'epoch': 0.05}         \n",
      "{'loss': 2.2973, 'learning_rate': 0.000181042082658328, 'epoch': 0.05}          \n",
      "{'loss': 2.6039, 'learning_rate': 0.00018107975737482576, 'epoch': 0.05}        \n",
      "{'loss': 2.1817, 'learning_rate': 0.00018111743209132353, 'epoch': 0.05}        \n",
      "{'loss': 2.3022, 'learning_rate': 0.00018115510680782128, 'epoch': 0.05}        \n",
      "{'loss': 2.2788, 'learning_rate': 0.00018119278152431905, 'epoch': 0.05}        \n",
      "{'loss': 2.2436, 'learning_rate': 0.0001812304562408168, 'epoch': 0.05}         \n",
      "{'loss': 1.9659, 'learning_rate': 0.00018126813095731457, 'epoch': 0.05}        \n",
      "{'loss': 2.2204, 'learning_rate': 0.00018130580567381232, 'epoch': 0.05}        \n",
      "{'loss': 2.2273, 'learning_rate': 0.00018134348039031007, 'epoch': 0.05}        \n",
      "{'loss': 2.2214, 'learning_rate': 0.0001813811551068078, 'epoch': 0.05}         \n",
      "{'loss': 1.8468, 'learning_rate': 0.0001814188298233056, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 48709/1061708 [7:17:13<149:00:45,  1.89it/s][2024-03-01 01:24:14,308] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8682, 'learning_rate': 0.00018145273706815358, 'epoch': 0.05}        \n",
      "{'loss': 2.3223, 'learning_rate': 0.00018149041178465133, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48729/1061708 [7:17:24<148:48:37,  1.89it/s][2024-03-01 01:24:24,856] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0125, 'learning_rate': 0.00018152431902949932, 'epoch': 0.05}        \n",
      "{'loss': 2.1923, 'learning_rate': 0.00018156199374599706, 'epoch': 0.05}        \n",
      "{'loss': 2.1165, 'learning_rate': 0.00018159966846249484, 'epoch': 0.05}        \n",
      "{'loss': 2.0485, 'learning_rate': 0.00018163734317899258, 'epoch': 0.05}        \n",
      "{'loss': 2.3263, 'learning_rate': 0.00018167501789549036, 'epoch': 0.05}        \n",
      "{'loss': 2.1291, 'learning_rate': 0.0001817126926119881, 'epoch': 0.05}         \n",
      "{'loss': 2.5194, 'learning_rate': 0.00018175036732848588, 'epoch': 0.05}        \n",
      "{'loss': 1.9591, 'learning_rate': 0.00018178804204498363, 'epoch': 0.05}        \n",
      "{'loss': 2.1766, 'learning_rate': 0.00018182571676148137, 'epoch': 0.05}        \n",
      "{'loss': 2.3537, 'learning_rate': 0.00018186339147797912, 'epoch': 0.05}        \n",
      "{'loss': 2.454, 'learning_rate': 0.0001819010661944769, 'epoch': 0.05}          \n",
      "{'loss': 2.3128, 'learning_rate': 0.00018193874091097464, 'epoch': 0.05}        \n",
      "{'loss': 2.145, 'learning_rate': 0.00018197641562747241, 'epoch': 0.05}         \n",
      "{'loss': 2.1587, 'learning_rate': 0.00018201409034397016, 'epoch': 0.05}        \n",
      "{'loss': 2.0093, 'learning_rate': 0.00018205176506046794, 'epoch': 0.05}        \n",
      "{'loss': 1.9501, 'learning_rate': 0.00018208943977696568, 'epoch': 0.05}        \n",
      "{'loss': 2.2848, 'learning_rate': 0.00018212711449346346, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|█▎                           | 48891/1061708 [7:18:50<148:41:03,  1.89it/s][2024-03-01 01:25:50,991] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3812, 'learning_rate': 0.00018216102173831142, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 48900/1061708 [7:18:55<148:41:02,  1.89it/s][2024-03-01 01:25:55,687] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1977, 'learning_rate': 0.0001821949289831594, 'epoch': 0.05}         \n",
      "{'loss': 2.2965, 'learning_rate': 0.00018223260369965719, 'epoch': 0.05}        \n",
      "{'loss': 2.0994, 'learning_rate': 0.00018227027841615493, 'epoch': 0.05}        \n",
      "{'loss': 2.2354, 'learning_rate': 0.00018230795313265268, 'epoch': 0.05}        \n",
      "{'loss': 2.4448, 'learning_rate': 0.00018234562784915043, 'epoch': 0.05}        \n",
      "{'loss': 2.1923, 'learning_rate': 0.0001823833025656482, 'epoch': 0.05}         \n",
      "{'loss': 2.6588, 'learning_rate': 0.00018242097728214595, 'epoch': 0.05}        \n",
      "{'loss': 2.5082, 'learning_rate': 0.00018245865199864372, 'epoch': 0.05}        \n",
      "{'loss': 2.2115, 'learning_rate': 0.00018249632671514147, 'epoch': 0.05}        \n",
      "{'loss': 2.087, 'learning_rate': 0.00018253400143163924, 'epoch': 0.05}         \n",
      "{'loss': 2.1388, 'learning_rate': 0.000182571676148137, 'epoch': 0.05}          \n",
      "  5%|█▎                           | 49016/1061708 [7:19:56<149:49:29,  1.88it/s][2024-03-01 01:26:57,372] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2619, 'learning_rate': 0.00018260558339298498, 'epoch': 0.05}        \n",
      "{'loss': 2.3914, 'learning_rate': 0.00018264325810948273, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49033/1061708 [7:20:05<152:03:03,  1.85it/s][2024-03-01 01:27:06,381] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4843, 'learning_rate': 0.00018267716535433072, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49043/1061708 [7:20:11<152:48:56,  1.84it/s][2024-03-01 01:27:11,665] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 1.9938, 'learning_rate': 0.0001827110725991787, 'epoch': 0.05}         \n",
      "{'loss': 2.3617, 'learning_rate': 0.00018274874731567646, 'epoch': 0.05}        \n",
      "{'loss': 1.8865, 'learning_rate': 0.00018278642203217423, 'epoch': 0.05}        \n",
      "{'loss': 1.84, 'learning_rate': 0.00018282409674867198, 'epoch': 0.05}          \n",
      "{'loss': 2.0137, 'learning_rate': 0.00018286177146516975, 'epoch': 0.05}        \n",
      "{'loss': 2.3789, 'learning_rate': 0.0001828994461816675, 'epoch': 0.05}         \n",
      "{'loss': 2.086, 'learning_rate': 0.00018293712089816527, 'epoch': 0.05}         \n",
      "{'loss': 2.144, 'learning_rate': 0.00018297479561466302, 'epoch': 0.05}         \n",
      "{'loss': 2.4789, 'learning_rate': 0.00018301247033116077, 'epoch': 0.05}        \n",
      "{'loss': 2.1825, 'learning_rate': 0.0001830501450476585, 'epoch': 0.05}         \n",
      "{'loss': 1.8564, 'learning_rate': 0.0001830878197641563, 'epoch': 0.05}         \n",
      "{'loss': 2.0436, 'learning_rate': 0.00018312549448065403, 'epoch': 0.05}        \n",
      "{'loss': 1.9122, 'learning_rate': 0.0001831631691971518, 'epoch': 0.05}         \n",
      "{'loss': 2.46, 'learning_rate': 0.00018320084391364955, 'epoch': 0.05}          \n",
      "{'loss': 2.4092, 'learning_rate': 0.00018323851863014733, 'epoch': 0.05}        \n",
      "{'loss': 2.4284, 'learning_rate': 0.00018327619334664508, 'epoch': 0.05}        \n",
      "{'loss': 2.2588, 'learning_rate': 0.00018331386806314282, 'epoch': 0.05}        \n",
      "{'loss': 2.0962, 'learning_rate': 0.0001833515427796406, 'epoch': 0.05}         \n",
      "{'loss': 2.2633, 'learning_rate': 0.00018338921749613834, 'epoch': 0.05}        \n",
      "{'loss': 2.213, 'learning_rate': 0.00018342689221263612, 'epoch': 0.05}         \n",
      "{'loss': 2.2914, 'learning_rate': 0.00018346456692913386, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49256/1061708 [7:22:04<149:56:57,  1.88it/s][2024-03-01 01:29:05,084] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.191, 'learning_rate': 0.00018349847417398186, 'epoch': 0.05}         \n",
      "{'loss': 2.0418, 'learning_rate': 0.0001835361488904796, 'epoch': 0.05}         \n",
      "{'loss': 2.0387, 'learning_rate': 0.00018357382360697738, 'epoch': 0.05}        \n",
      "{'loss': 2.4367, 'learning_rate': 0.00018361149832347512, 'epoch': 0.05}        \n",
      "{'loss': 2.0753, 'learning_rate': 0.0001836491730399729, 'epoch': 0.05}         \n",
      "{'loss': 2.2188, 'learning_rate': 0.00018368684775647064, 'epoch': 0.05}        \n",
      "{'loss': 2.6274, 'learning_rate': 0.00018372452247296842, 'epoch': 0.05}        \n",
      "{'loss': 2.3001, 'learning_rate': 0.00018376219718946616, 'epoch': 0.05}        \n",
      "{'loss': 2.0109, 'learning_rate': 0.00018379987190596394, 'epoch': 0.05}        \n",
      "{'loss': 2.2444, 'learning_rate': 0.00018383754662246166, 'epoch': 0.05}        \n",
      "{'loss': 2.2159, 'learning_rate': 0.00018387522133895943, 'epoch': 0.05}        \n",
      "{'loss': 2.1453, 'learning_rate': 0.00018391289605545718, 'epoch': 0.05}        \n",
      "{'loss': 2.0179, 'learning_rate': 0.00018395057077195495, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49389/1061708 [7:23:15<148:44:46,  1.89it/s][2024-03-01 01:30:15,849] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0319, 'learning_rate': 0.00018398447801680294, 'epoch': 0.05}        \n",
      "{'loss': 2.2875, 'learning_rate': 0.0001840221527333007, 'epoch': 0.05}         \n",
      "{'loss': 2.2598, 'learning_rate': 0.00018405982744979844, 'epoch': 0.05}        \n",
      "{'loss': 2.2584, 'learning_rate': 0.00018409750216629619, 'epoch': 0.05}        \n",
      "{'loss': 2.316, 'learning_rate': 0.00018413517688279396, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 49439/1061708 [7:23:41<148:55:13,  1.89it/s][2024-03-01 01:30:42,377] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1827, 'learning_rate': 0.00018416908412764195, 'epoch': 0.05}        \n",
      "{'loss': 1.8897, 'learning_rate': 0.00018420675884413972, 'epoch': 0.05}        \n",
      "{'loss': 2.3071, 'learning_rate': 0.00018424443356063747, 'epoch': 0.05}        \n",
      "{'loss': 2.4032, 'learning_rate': 0.00018428210827713522, 'epoch': 0.05}        \n",
      "{'loss': 2.7475, 'learning_rate': 0.00018431978299363297, 'epoch': 0.05}        \n",
      "{'loss': 2.2124, 'learning_rate': 0.00018435745771013074, 'epoch': 0.05}        \n",
      "{'loss': 2.0064, 'learning_rate': 0.00018439513242662849, 'epoch': 0.05}        \n",
      "{'loss': 2.245, 'learning_rate': 0.00018443280714312626, 'epoch': 0.05}         \n",
      "{'loss': 2.528, 'learning_rate': 0.000184470481859624, 'epoch': 0.05}           \n",
      "{'loss': 2.1855, 'learning_rate': 0.00018450815657612178, 'epoch': 0.05}        \n",
      "{'loss': 2.1897, 'learning_rate': 0.00018454583129261953, 'epoch': 0.05}        \n",
      "{'loss': 2.274, 'learning_rate': 0.0001845835060091173, 'epoch': 0.05}          \n",
      "{'loss': 2.2583, 'learning_rate': 0.00018462118072561505, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49561/1061708 [7:24:46<149:06:51,  1.89it/s][2024-03-01 01:31:47,278] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0678, 'learning_rate': 0.000184655087970463, 'epoch': 0.05}          \n",
      "{'loss': 2.2456, 'learning_rate': 0.0001846927626869608, 'epoch': 0.05}         \n",
      "{'loss': 2.1762, 'learning_rate': 0.00018473043740345853, 'epoch': 0.05}        \n",
      "{'loss': 2.0803, 'learning_rate': 0.0001847681121199563, 'epoch': 0.05}         \n",
      "{'loss': 2.4052, 'learning_rate': 0.00018480578683645405, 'epoch': 0.05}        \n",
      "{'loss': 2.1663, 'learning_rate': 0.00018484346155295183, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|█▎                           | 49626/1061708 [7:25:21<150:22:46,  1.87it/s][2024-03-01 01:32:21,834] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3636, 'learning_rate': 0.0001848773687977998, 'epoch': 0.05}         \n",
      "{'loss': 2.738, 'learning_rate': 0.00018491504351429757, 'epoch': 0.05}         \n",
      "{'loss': 2.033, 'learning_rate': 0.00018495271823079531, 'epoch': 0.05}         \n",
      "{'loss': 1.9722, 'learning_rate': 0.0001849903929472931, 'epoch': 0.05}         \n",
      "{'loss': 2.1322, 'learning_rate': 0.00018502806766379083, 'epoch': 0.05}        \n",
      "{'loss': 2.3492, 'learning_rate': 0.0001850657423802886, 'epoch': 0.05}         \n",
      "{'loss': 2.0785, 'learning_rate': 0.00018510341709678636, 'epoch': 0.05}        \n",
      "{'loss': 2.3114, 'learning_rate': 0.00018514109181328413, 'epoch': 0.05}        \n",
      "{'loss': 2.4967, 'learning_rate': 0.00018517876652978188, 'epoch': 0.05}        \n",
      "{'loss': 2.2548, 'learning_rate': 0.00018521644124627965, 'epoch': 0.05}        \n",
      "{'loss': 2.5268, 'learning_rate': 0.0001852541159627774, 'epoch': 0.05}         \n",
      "{'loss': 2.0298, 'learning_rate': 0.00018529179067927514, 'epoch': 0.05}        \n",
      "{'loss': 2.0267, 'learning_rate': 0.00018532946539577292, 'epoch': 0.05}        \n",
      "{'loss': 1.9911, 'learning_rate': 0.00018536714011227067, 'epoch': 0.05}        \n",
      "{'loss': 1.943, 'learning_rate': 0.0001854048148287684, 'epoch': 0.05}          \n",
      "{'loss': 2.1518, 'learning_rate': 0.00018544248954526619, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49787/1061708 [7:26:47<149:22:47,  1.88it/s][2024-03-01 01:33:47,580] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.434, 'learning_rate': 0.00018547639679011418, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 49794/1061708 [7:26:50<150:43:22,  1.86it/s][2024-03-01 01:33:51,236] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1997, 'learning_rate': 0.00018551030403496214, 'epoch': 0.05}        \n",
      "{'loss': 2.1686, 'learning_rate': 0.00018554797875145992, 'epoch': 0.05}        \n",
      "{'loss': 2.068, 'learning_rate': 0.00018558565346795766, 'epoch': 0.05}         \n",
      "{'loss': 2.3891, 'learning_rate': 0.00018562332818445544, 'epoch': 0.05}        \n",
      "{'loss': 1.9825, 'learning_rate': 0.00018566100290095318, 'epoch': 0.05}        \n",
      "{'loss': 2.046, 'learning_rate': 0.00018569867761745096, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 49851/1061708 [7:27:20<148:33:33,  1.89it/s][2024-03-01 01:34:21,499] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.7214, 'learning_rate': 0.00018573258486229892, 'epoch': 0.05}        \n",
      "{'loss': 2.4635, 'learning_rate': 0.00018577025957879667, 'epoch': 0.05}        \n",
      "{'loss': 2.3398, 'learning_rate': 0.00018580793429529444, 'epoch': 0.05}        \n",
      "{'loss': 2.0223, 'learning_rate': 0.0001858456090117922, 'epoch': 0.05}         \n",
      "{'loss': 1.9433, 'learning_rate': 0.00018588328372828996, 'epoch': 0.05}        \n",
      "{'loss': 2.1539, 'learning_rate': 0.0001859209584447877, 'epoch': 0.05}         \n",
      "{'loss': 2.1021, 'learning_rate': 0.00018595863316128548, 'epoch': 0.05}        \n",
      "{'loss': 2.4509, 'learning_rate': 0.00018599630787778323, 'epoch': 0.05}        \n",
      "{'loss': 2.4809, 'learning_rate': 0.000186033982594281, 'epoch': 0.05}          \n",
      "{'loss': 1.7702, 'learning_rate': 0.00018607165731077872, 'epoch': 0.05}        \n",
      "{'loss': 1.9199, 'learning_rate': 0.0001861093320272765, 'epoch': 0.05}         \n",
      "{'loss': 2.247, 'learning_rate': 0.00018614700674377425, 'epoch': 0.05}         \n",
      "{'loss': 1.9241, 'learning_rate': 0.00018618468146027202, 'epoch': 0.05}        \n",
      "{'loss': 2.2975, 'learning_rate': 0.00018622235617676977, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 49999/1061708 [7:28:39<148:55:16,  1.89it/s][2024-03-01 01:35:40,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=50000, skipped=561, lr=[0.00018626003089326754], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 01:35:40,380] [INFO] [timer.py:260:stop] epoch=0/micro_step=50000/global_step=50000, RunningAvgSamplesPerSec=1.8923750905370018, CurrSamplesPerSec=1.9038848419263703, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3813, 'learning_rate': 0.00018626003089326754, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 50000/1061708 [7:28:40<148:54:40,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 01:35:40,382 >> Saving model checkpoint to output_model/checkpoint-50000\n",
      "[INFO|trainer.py:2880] 2024-03-01 01:35:40,384 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 01:35:41,581 >> tokenizer config file saved in output_model/checkpoint-50000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 01:35:41,581 >> Special tokens file saved in output_model/checkpoint-50000/special_tokens_map.json\n",
      "[2024-03-01 01:35:41,582] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step50000 is about to be saved!\n",
      "[2024-03-01 01:35:46,793] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-50000/global_step50000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 01:35:46,793] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-50000/global_step50000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 01:36:00,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-50000/global_step50000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 01:36:01,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-50000/global_step50000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 01:36:08,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-50000/global_step50000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 01:36:08,480] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-50000/global_step50000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 01:36:08,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 01:36:08,525 >> Deleting older checkpoint [output_model/checkpoint-35000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 01:36:12,289 >> tokenizer config file saved in output_model/checkpoint-50000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 01:36:12,290 >> Special tokens file saved in output_model/checkpoint-50000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.2136, 'learning_rate': 0.0001862977056097653, 'epoch': 0.05}         \n",
      "{'loss': 2.3127, 'learning_rate': 0.00018633538032626306, 'epoch': 0.05}        \n",
      "{'loss': 2.2397, 'learning_rate': 0.0001863730550427608, 'epoch': 0.05}         \n",
      "{'loss': 2.3813, 'learning_rate': 0.00018641072975925858, 'epoch': 0.05}        \n",
      "{'loss': 2.1967, 'learning_rate': 0.00018644840447575633, 'epoch': 0.05}        \n",
      "{'loss': 2.0856, 'learning_rate': 0.0001864860791922541, 'epoch': 0.05}         \n",
      "  5%|█▎                           | 50065/1061708 [7:29:46<149:45:05,  1.88it/s][2024-03-01 01:36:47,158] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0283, 'learning_rate': 0.00018651998643710207, 'epoch': 0.05}        \n",
      "{'loss': 2.0848, 'learning_rate': 0.00018655766115359984, 'epoch': 0.05}        \n",
      "{'loss': 2.144, 'learning_rate': 0.0001865953358700976, 'epoch': 0.05}          \n",
      "  5%|█▎                           | 50098/1061708 [7:30:04<149:11:55,  1.88it/s][2024-03-01 01:37:04,707] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3421, 'learning_rate': 0.00018662924311494555, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4881, 'learning_rate': 0.00018666691783144333, 'epoch': 0.05}        \n",
      "{'loss': 2.326, 'learning_rate': 0.00018670459254794107, 'epoch': 0.05}         \n",
      "{'loss': 2.2829, 'learning_rate': 0.00018674226726443885, 'epoch': 0.05}        \n",
      "{'loss': 1.985, 'learning_rate': 0.0001867799419809366, 'epoch': 0.05}          \n",
      "{'loss': 2.0468, 'learning_rate': 0.00018681761669743437, 'epoch': 0.05}        \n",
      "{'loss': 2.1782, 'learning_rate': 0.00018685529141393211, 'epoch': 0.05}        \n",
      "{'loss': 2.3779, 'learning_rate': 0.0001868929661304299, 'epoch': 0.05}         \n",
      "{'loss': 2.2646, 'learning_rate': 0.00018693064084692764, 'epoch': 0.05}        \n",
      "{'loss': 2.149, 'learning_rate': 0.0001869683155634254, 'epoch': 0.05}          \n",
      "{'loss': 2.1553, 'learning_rate': 0.00018700599027992316, 'epoch': 0.05}        \n",
      "{'loss': 2.0673, 'learning_rate': 0.00018704366499642093, 'epoch': 0.05}        \n",
      "{'loss': 2.1638, 'learning_rate': 0.00018708133971291868, 'epoch': 0.05}        \n",
      "{'loss': 2.238, 'learning_rate': 0.00018711901442941642, 'epoch': 0.05}         \n",
      "{'loss': 2.2696, 'learning_rate': 0.00018715668914591417, 'epoch': 0.05}        \n",
      "{'loss': 2.1891, 'learning_rate': 0.00018719436386241195, 'epoch': 0.05}        \n",
      "{'loss': 2.3044, 'learning_rate': 0.0001872320385789097, 'epoch': 0.05}         \n",
      "{'loss': 2.6136, 'learning_rate': 0.00018726971329540747, 'epoch': 0.05}        \n",
      "{'loss': 2.3782, 'learning_rate': 0.0001873073880119052, 'epoch': 0.05}         \n",
      "{'loss': 2.2356, 'learning_rate': 0.000187345062728403, 'epoch': 0.05}          \n",
      "  5%|█▎                           | 50299/1061708 [7:31:51<149:18:29,  1.88it/s][2024-03-01 01:38:52,071] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0794, 'learning_rate': 0.00018737896997325095, 'epoch': 0.05}        \n",
      "  5%|█▎                           | 50300/1061708 [7:31:51<140:12:34,  2.00it/s][2024-03-01 01:38:52,512] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2616, 'learning_rate': 0.00018741287721809894, 'epoch': 0.05}        \n",
      "{'loss': 2.0327, 'learning_rate': 0.00018745055193459672, 'epoch': 0.05}        \n",
      "{'loss': 2.2578, 'learning_rate': 0.00018748822665109446, 'epoch': 0.05}        \n",
      "{'loss': 2.1912, 'learning_rate': 0.00018752590136759224, 'epoch': 0.05}        \n",
      "{'loss': 2.0298, 'learning_rate': 0.00018756357608408998, 'epoch': 0.05}        \n",
      "{'loss': 1.6892, 'learning_rate': 0.00018760125080058773, 'epoch': 0.05}        \n",
      "{'loss': 2.0781, 'learning_rate': 0.00018763892551708548, 'epoch': 0.05}        \n",
      "{'loss': 2.1346, 'learning_rate': 0.00018767660023358325, 'epoch': 0.05}        \n",
      "{'loss': 2.4694, 'learning_rate': 0.000187714274950081, 'epoch': 0.05}          \n",
      "{'loss': 1.7722, 'learning_rate': 0.00018775194966657877, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 50401/1061708 [7:32:45<148:59:15,  1.89it/s][2024-03-01 01:39:46,408] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 50402/1061708 [7:32:46<139:53:42,  2.01it/s][2024-03-01 01:39:46,831] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2558, 'learning_rate': 0.00018778208943977698, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 50417/1061708 [7:32:54<149:33:42,  1.88it/s][2024-03-01 01:39:54,762] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9375, 'learning_rate': 0.00018781599668462495, 'epoch': 0.05}        \n",
      "{'loss': 2.25, 'learning_rate': 0.00018785367140112272, 'epoch': 0.05}          \n",
      "{'loss': 2.0859, 'learning_rate': 0.00018789134611762047, 'epoch': 0.05}        \n",
      "{'loss': 2.3004, 'learning_rate': 0.00018792902083411824, 'epoch': 0.05}        \n",
      "{'loss': 2.0451, 'learning_rate': 0.000187966695550616, 'epoch': 0.05}          \n",
      "{'loss': 2.3483, 'learning_rate': 0.00018800437026711376, 'epoch': 0.05}        \n",
      "{'loss': 2.2179, 'learning_rate': 0.0001880420449836115, 'epoch': 0.05}         \n",
      "{'loss': 2.1019, 'learning_rate': 0.00018807971970010928, 'epoch': 0.05}        \n",
      "{'loss': 2.4563, 'learning_rate': 0.00018811739441660703, 'epoch': 0.05}        \n",
      "{'loss': 2.4181, 'learning_rate': 0.00018815506913310478, 'epoch': 0.05}        \n",
      "{'loss': 1.9674, 'learning_rate': 0.00018819274384960255, 'epoch': 0.05}        \n",
      "{'loss': 2.3615, 'learning_rate': 0.0001882304185661003, 'epoch': 0.05}         \n",
      "{'loss': 2.013, 'learning_rate': 0.00018826809328259807, 'epoch': 0.05}         \n",
      "{'loss': 2.2432, 'learning_rate': 0.00018830576799909582, 'epoch': 0.05}        \n",
      "{'loss': 2.3359, 'learning_rate': 0.00018834344271559356, 'epoch': 0.05}        \n",
      "{'loss': 2.4534, 'learning_rate': 0.00018838111743209134, 'epoch': 0.05}        \n",
      "{'loss': 2.1425, 'learning_rate': 0.00018841879214858909, 'epoch': 0.05}        \n",
      "{'loss': 2.4211, 'learning_rate': 0.00018845646686508683, 'epoch': 0.05}        \n",
      "{'loss': 2.1687, 'learning_rate': 0.0001884941415815846, 'epoch': 0.05}         \n",
      "{'loss': 2.3434, 'learning_rate': 0.00018853181629808235, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 50618/1061708 [7:34:41<149:21:38,  1.88it/s][2024-03-01 01:41:41,936] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 50619/1061708 [7:34:41<140:11:44,  2.00it/s][2024-03-01 01:41:42,361] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1662, 'learning_rate': 0.0001885619560712806, 'epoch': 0.05}         \n",
      "{'loss': 2.1396, 'learning_rate': 0.00018859963078777834, 'epoch': 0.05}        \n",
      "{'loss': 2.2746, 'learning_rate': 0.0001886373055042761, 'epoch': 0.05}         \n",
      "{'loss': 2.2003, 'learning_rate': 0.00018867498022077386, 'epoch': 0.05}        \n",
      "{'loss': 2.3292, 'learning_rate': 0.0001887126549372716, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 50660/1061708 [7:35:03<149:01:06,  1.88it/s][2024-03-01 01:42:04,146] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1762, 'learning_rate': 0.0001887465621821196, 'epoch': 0.05}         \n",
      "{'loss': 2.2657, 'learning_rate': 0.00018878423689861734, 'epoch': 0.05}        \n",
      "{'loss': 2.11, 'learning_rate': 0.00018882191161511512, 'epoch': 0.05}          \n",
      "{'loss': 1.7907, 'learning_rate': 0.00018885958633161286, 'epoch': 0.05}        \n",
      "{'loss': 2.4369, 'learning_rate': 0.00018889726104811064, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 50710/1061708 [7:35:30<148:56:53,  1.89it/s][2024-03-01 01:42:30,717] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4752, 'learning_rate': 0.0001889311682929586, 'epoch': 0.05}         \n",
      "{'loss': 1.8639, 'learning_rate': 0.00018896884300945637, 'epoch': 0.05}        \n",
      "{'loss': 2.5503, 'learning_rate': 0.00018900651772595412, 'epoch': 0.05}        \n",
      "{'loss': 2.0748, 'learning_rate': 0.0001890441924424519, 'epoch': 0.05}         \n",
      "{'loss': 1.9202, 'learning_rate': 0.00018908186715894964, 'epoch': 0.05}        \n",
      "{'loss': 2.2551, 'learning_rate': 0.00018911954187544742, 'epoch': 0.05}        \n",
      "{'loss': 1.9439, 'learning_rate': 0.00018915721659194516, 'epoch': 0.05}        \n",
      "{'loss': 2.2007, 'learning_rate': 0.0001891948913084429, 'epoch': 0.05}         \n",
      "{'loss': 1.9347, 'learning_rate': 0.00018923256602494066, 'epoch': 0.05}        \n",
      "{'loss': 2.0705, 'learning_rate': 0.00018927024074143843, 'epoch': 0.05}        \n",
      "{'loss': 2.27, 'learning_rate': 0.00018930791545793618, 'epoch': 0.05}          \n",
      "{'loss': 2.4066, 'learning_rate': 0.00018934559017443395, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2395, 'learning_rate': 0.0001893832648909317, 'epoch': 0.05}         \n",
      "{'loss': 2.3612, 'learning_rate': 0.00018942093960742947, 'epoch': 0.05}        \n",
      "{'loss': 2.1773, 'learning_rate': 0.00018945861432392722, 'epoch': 0.05}        \n",
      "{'loss': 2.2278, 'learning_rate': 0.00018949628904042497, 'epoch': 0.05}        \n",
      "{'loss': 2.1117, 'learning_rate': 0.00018953396375692274, 'epoch': 0.05}        \n",
      "{'loss': 2.1581, 'learning_rate': 0.0001895716384734205, 'epoch': 0.05}         \n",
      "{'loss': 2.0807, 'learning_rate': 0.00018960931318991826, 'epoch': 0.05}        \n",
      "{'loss': 1.8071, 'learning_rate': 0.000189646987906416, 'epoch': 0.05}          \n",
      "{'loss': 1.8919, 'learning_rate': 0.00018968466262291378, 'epoch': 0.05}        \n",
      "{'loss': 2.1855, 'learning_rate': 0.00018972233733941153, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 50936/1061708 [7:37:30<149:44:27,  1.88it/s][2024-03-01 01:44:31,298] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.925, 'learning_rate': 0.00018975624458425952, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 50940/1061708 [7:37:32<149:36:36,  1.88it/s][2024-03-01 01:44:33,359] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7403, 'learning_rate': 0.00018979015182910748, 'epoch': 0.05}        \n",
      "{'loss': 2.1423, 'learning_rate': 0.00018982782654560526, 'epoch': 0.05}        \n",
      "{'loss': 2.2059, 'learning_rate': 0.000189865501262103, 'epoch': 0.05}          \n",
      "{'loss': 2.2913, 'learning_rate': 0.00018990317597860078, 'epoch': 0.05}        \n",
      "{'loss': 2.1257, 'learning_rate': 0.00018994085069509853, 'epoch': 0.05}        \n",
      "{'loss': 2.1149, 'learning_rate': 0.0001899785254115963, 'epoch': 0.05}         \n",
      "{'loss': 2.1843, 'learning_rate': 0.00019001620012809405, 'epoch': 0.05}        \n",
      "{'loss': 2.4228, 'learning_rate': 0.0001900538748445918, 'epoch': 0.05}         \n",
      "{'loss': 2.3026, 'learning_rate': 0.00019009154956108957, 'epoch': 0.05}        \n",
      "{'loss': 2.0983, 'learning_rate': 0.00019012922427758731, 'epoch': 0.05}        \n",
      "{'loss': 2.1063, 'learning_rate': 0.0001901668989940851, 'epoch': 0.05}         \n",
      "{'loss': 2.0488, 'learning_rate': 0.00019020457371058284, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51066/1061708 [7:38:39<149:46:34,  1.87it/s][2024-03-01 01:45:40,434] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.974, 'learning_rate': 0.00019023848095543083, 'epoch': 0.05}         \n",
      "{'loss': 2.2206, 'learning_rate': 0.00019027615567192857, 'epoch': 0.05}        \n",
      "{'loss': 2.5592, 'learning_rate': 0.00019031383038842635, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51099/1061708 [7:38:57<148:49:28,  1.89it/s][2024-03-01 01:45:57,963] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0721, 'learning_rate': 0.0001903477376332743, 'epoch': 0.05}         \n",
      "{'loss': 2.3629, 'learning_rate': 0.00019038541234977209, 'epoch': 0.05}        \n",
      "{'loss': 2.2722, 'learning_rate': 0.00019042308706626983, 'epoch': 0.05}        \n",
      "{'loss': 2.2533, 'learning_rate': 0.0001904607617827676, 'epoch': 0.05}         \n",
      "{'loss': 2.1214, 'learning_rate': 0.00019049843649926535, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51140/1061708 [7:39:19<148:36:10,  1.89it/s][2024-03-01 01:46:19,709] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2755, 'learning_rate': 0.00019053234374411332, 'epoch': 0.05}        \n",
      "{'loss': 2.0634, 'learning_rate': 0.0001905700184606111, 'epoch': 0.05}         \n",
      "{'loss': 1.8352, 'learning_rate': 0.00019060769317710884, 'epoch': 0.05}        \n",
      "{'loss': 2.1631, 'learning_rate': 0.0001906453678936066, 'epoch': 0.05}         \n",
      "{'loss': 2.226, 'learning_rate': 0.00019068304261010436, 'epoch': 0.05}         \n",
      "{'loss': 2.0408, 'learning_rate': 0.00019072071732660213, 'epoch': 0.05}        \n",
      "{'loss': 1.961, 'learning_rate': 0.00019075839204309988, 'epoch': 0.05}         \n",
      "{'loss': 1.9591, 'learning_rate': 0.00019079606675959765, 'epoch': 0.05}        \n",
      "{'loss': 2.0362, 'learning_rate': 0.0001908337414760954, 'epoch': 0.05}         \n",
      "{'loss': 2.4208, 'learning_rate': 0.00019087141619259318, 'epoch': 0.05}        \n",
      "{'loss': 1.9759, 'learning_rate': 0.00019090909090909092, 'epoch': 0.05}        \n",
      "{'loss': 2.0486, 'learning_rate': 0.0001909467656255887, 'epoch': 0.05}         \n",
      "{'loss': 2.1163, 'learning_rate': 0.00019098444034208644, 'epoch': 0.05}        \n",
      "{'loss': 2.1988, 'learning_rate': 0.0001910221150585842, 'epoch': 0.05}         \n",
      "{'loss': 2.0611, 'learning_rate': 0.00019105978977508194, 'epoch': 0.05}        \n",
      "{'loss': 2.1654, 'learning_rate': 0.0001910974644915797, 'epoch': 0.05}         \n",
      "{'loss': 2.1779, 'learning_rate': 0.00019113513920807746, 'epoch': 0.05}        \n",
      "{'loss': 1.8817, 'learning_rate': 0.00019117281392457523, 'epoch': 0.05}        \n",
      "{'loss': 2.4632, 'learning_rate': 0.00019121048864107298, 'epoch': 0.05}        \n",
      "{'loss': 1.8226, 'learning_rate': 0.00019124816335757075, 'epoch': 0.05}        \n",
      "{'loss': 1.8902, 'learning_rate': 0.0001912858380740685, 'epoch': 0.05}         \n",
      "{'loss': 2.3421, 'learning_rate': 0.00019132351279056627, 'epoch': 0.05}        \n",
      "{'loss': 1.9388, 'learning_rate': 0.00019136118750706402, 'epoch': 0.05}        \n",
      "{'loss': 1.6702, 'learning_rate': 0.0001913988622235618, 'epoch': 0.05}         \n",
      "{'loss': 2.0958, 'learning_rate': 0.00019143653694005954, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51391/1061708 [7:41:32<148:30:31,  1.89it/s][2024-03-01 01:48:33,289] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 51399/1061708 [7:41:36<148:20:36,  1.89it/s][2024-03-01 01:48:37,457] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9837, 'learning_rate': 0.00019146667671325772, 'epoch': 0.05}        \n",
      "{'loss': 1.9626, 'learning_rate': 0.0001915043514297555, 'epoch': 0.05}         \n",
      "{'loss': 2.258, 'learning_rate': 0.00019154202614625324, 'epoch': 0.05}         \n",
      "{'loss': 2.0071, 'learning_rate': 0.00019157970086275102, 'epoch': 0.05}        \n",
      "{'loss': 2.1697, 'learning_rate': 0.00019161737557924876, 'epoch': 0.05}        \n",
      "{'loss': 2.5311, 'learning_rate': 0.00019165505029574654, 'epoch': 0.05}        \n",
      "{'loss': 1.8219, 'learning_rate': 0.00019169272501224429, 'epoch': 0.05}        \n",
      "{'loss': 2.5796, 'learning_rate': 0.00019173039972874206, 'epoch': 0.05}        \n",
      "{'loss': 2.7214, 'learning_rate': 0.0001917680744452398, 'epoch': 0.05}         \n",
      "{'loss': 2.368, 'learning_rate': 0.00019180574916173758, 'epoch': 0.05}         \n",
      "{'loss': 2.1797, 'learning_rate': 0.00019184342387823533, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51508/1061708 [7:42:34<148:49:47,  1.89it/s][2024-03-01 01:49:35,452] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8784, 'learning_rate': 0.0001918773311230833, 'epoch': 0.05}         \n",
      "{'loss': 2.08, 'learning_rate': 0.00019191500583958106, 'epoch': 0.05}          \n",
      "{'loss': 2.3263, 'learning_rate': 0.0001919526805560788, 'epoch': 0.05}         \n",
      "{'loss': 2.1064, 'learning_rate': 0.00019199035527257659, 'epoch': 0.05}        \n",
      "{'loss': 2.4708, 'learning_rate': 0.00019202802998907433, 'epoch': 0.05}        \n",
      "{'loss': 2.5621, 'learning_rate': 0.0001920657047055721, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 51568/1061708 [7:43:06<148:53:40,  1.88it/s][2024-03-01 01:50:07,299] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0734, 'learning_rate': 0.00019209961195042007, 'epoch': 0.05}        \n",
      "{'loss': 2.3598, 'learning_rate': 0.00019213728666691784, 'epoch': 0.05}        \n",
      "{'loss': 2.7113, 'learning_rate': 0.0001921749613834156, 'epoch': 0.05}         \n",
      "{'loss': 2.3083, 'learning_rate': 0.00019221263609991337, 'epoch': 0.05}        \n",
      "{'loss': 2.0373, 'learning_rate': 0.0001922503108164111, 'epoch': 0.05}         \n",
      "{'loss': 2.1138, 'learning_rate': 0.0001922879855329089, 'epoch': 0.05}         \n",
      "{'loss': 2.2096, 'learning_rate': 0.00019232566024940663, 'epoch': 0.05}        \n",
      "{'loss': 2.3779, 'learning_rate': 0.0001923633349659044, 'epoch': 0.05}         \n",
      "{'loss': 1.9964, 'learning_rate': 0.00019240100968240215, 'epoch': 0.05}        \n",
      "{'loss': 2.2751, 'learning_rate': 0.00019243868439889993, 'epoch': 0.05}        \n",
      "{'loss': 2.3069, 'learning_rate': 0.00019247635911539768, 'epoch': 0.05}        \n",
      "{'loss': 2.2935, 'learning_rate': 0.00019251403383189542, 'epoch': 0.05}        \n",
      "{'loss': 2.2188, 'learning_rate': 0.00019255170854839317, 'epoch': 0.05}        \n",
      "{'loss': 2.0823, 'learning_rate': 0.00019258938326489094, 'epoch': 0.05}        \n",
      "{'loss': 2.1509, 'learning_rate': 0.0001926270579813887, 'epoch': 0.05}         \n",
      "{'loss': 2.4469, 'learning_rate': 0.00019266473269788646, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51728/1061708 [7:44:31<149:07:44,  1.88it/s][2024-03-01 01:51:32,445] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9758, 'learning_rate': 0.00019269863994273446, 'epoch': 0.05}        \n",
      "{'loss': 2.4659, 'learning_rate': 0.0001927363146592322, 'epoch': 0.05}         \n",
      "{'loss': 2.0832, 'learning_rate': 0.00019277398937572995, 'epoch': 0.05}        \n",
      "{'loss': 2.3189, 'learning_rate': 0.0001928116640922277, 'epoch': 0.05}         \n",
      "{'loss': 2.3008, 'learning_rate': 0.00019284933880872547, 'epoch': 0.05}        \n",
      "{'loss': 2.5362, 'learning_rate': 0.00019288701352522322, 'epoch': 0.05}        \n",
      "{'loss': 2.11, 'learning_rate': 0.000192924688241721, 'epoch': 0.05}            \n",
      "{'loss': 2.0799, 'learning_rate': 0.00019296236295821874, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51804/1061708 [7:45:12<151:03:45,  1.86it/s][2024-03-01 01:52:12,841] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0689, 'learning_rate': 0.00019299627020306673, 'epoch': 0.05}        \n",
      "{'loss': 2.5695, 'learning_rate': 0.00019303394491956448, 'epoch': 0.05}        \n",
      "{'loss': 2.2016, 'learning_rate': 0.00019307161963606225, 'epoch': 0.05}        \n",
      "{'loss': 2.0344, 'learning_rate': 0.00019310929435256, 'epoch': 0.05}           \n",
      "{'loss': 2.3081, 'learning_rate': 0.00019314696906905777, 'epoch': 0.05}        \n",
      "{'loss': 2.2347, 'learning_rate': 0.00019318464378555552, 'epoch': 0.05}        \n",
      "{'loss': 2.0342, 'learning_rate': 0.0001932223185020533, 'epoch': 0.05}         \n",
      "{'loss': 1.8532, 'learning_rate': 0.00019325999321855104, 'epoch': 0.05}        \n",
      "{'loss': 2.224, 'learning_rate': 0.00019329766793504879, 'epoch': 0.05}         \n",
      "{'loss': 2.2229, 'learning_rate': 0.00019333534265154656, 'epoch': 0.05}        \n",
      "{'loss': 2.3219, 'learning_rate': 0.0001933730173680443, 'epoch': 0.05}         \n",
      "{'loss': 2.2496, 'learning_rate': 0.00019341069208454208, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51929/1061708 [7:46:18<148:29:12,  1.89it/s][2024-03-01 01:53:19,362] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2127, 'learning_rate': 0.00019344459932939004, 'epoch': 0.05}        \n",
      "{'loss': 2.2435, 'learning_rate': 0.00019348227404588782, 'epoch': 0.05}        \n",
      "{'loss': 2.0288, 'learning_rate': 0.00019351994876238557, 'epoch': 0.05}        \n",
      "{'loss': 1.9844, 'learning_rate': 0.00019355762347888334, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51962/1061708 [7:46:36<148:41:45,  1.89it/s][2024-03-01 01:53:36,873] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1309, 'learning_rate': 0.0001935915307237313, 'epoch': 0.05}         \n",
      "{'loss': 2.2376, 'learning_rate': 0.00019362920544022908, 'epoch': 0.05}        \n",
      "{'loss': 1.6662, 'learning_rate': 0.00019366688015672682, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 51999/1061708 [7:46:55<148:27:05,  1.89it/s][2024-03-01 01:53:56,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=52000, skipped=585, lr=[0.0001937045548732246], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 01:53:56,592] [INFO] [timer.py:260:stop] epoch=0/micro_step=52000/global_step=52000, RunningAvgSamplesPerSec=1.8924589386049004, CurrSamplesPerSec=1.902882880921012, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1953, 'learning_rate': 0.0001937045548732246, 'epoch': 0.05}         \n",
      "{'loss': 2.174, 'learning_rate': 0.00019374222958972234, 'epoch': 0.05}         \n",
      "{'loss': 2.3051, 'learning_rate': 0.00019377990430622012, 'epoch': 0.05}        \n",
      "{'loss': 2.4687, 'learning_rate': 0.00019381757902271787, 'epoch': 0.05}        \n",
      "{'loss': 2.7756, 'learning_rate': 0.0001938552537392156, 'epoch': 0.05}         \n",
      "{'loss': 2.3642, 'learning_rate': 0.0001938929284557134, 'epoch': 0.05}         \n",
      "{'loss': 2.0911, 'learning_rate': 0.00019393060317221113, 'epoch': 0.05}        \n",
      "{'loss': 2.4001, 'learning_rate': 0.0001939682778887089, 'epoch': 0.05}         \n",
      "{'loss': 2.1488, 'learning_rate': 0.00019400595260520665, 'epoch': 0.05}        \n",
      "{'loss': 1.8332, 'learning_rate': 0.00019404362732170443, 'epoch': 0.05}        \n",
      "{'loss': 1.9713, 'learning_rate': 0.00019408130203820215, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52103/1061708 [7:47:51<151:35:22,  1.85it/s][2024-03-01 01:54:51,978] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3233, 'learning_rate': 0.00019411520928305017, 'epoch': 0.05}        \n",
      "{'loss': 2.1372, 'learning_rate': 0.0001941528839995479, 'epoch': 0.05}         \n",
      "{'loss': 2.2447, 'learning_rate': 0.0001941905587160457, 'epoch': 0.05}         \n",
      "{'loss': 2.4176, 'learning_rate': 0.00019422823343254343, 'epoch': 0.05}        \n",
      "{'loss': 2.2031, 'learning_rate': 0.0001942659081490412, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 52156/1061708 [7:48:19<149:40:00,  1.87it/s][2024-03-01 01:55:20,167] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3558, 'learning_rate': 0.00019429981539388917, 'epoch': 0.05}        \n",
      "{'loss': 1.8974, 'learning_rate': 0.00019433749011038692, 'epoch': 0.05}        \n",
      "{'loss': 1.9176, 'learning_rate': 0.0001943751648268847, 'epoch': 0.05}         \n",
      "{'loss': 2.1942, 'learning_rate': 0.00019441283954338244, 'epoch': 0.05}        \n",
      "{'loss': 2.0288, 'learning_rate': 0.00019445051425988021, 'epoch': 0.05}        \n",
      "{'loss': 1.7818, 'learning_rate': 0.00019448818897637796, 'epoch': 0.05}        \n",
      "{'loss': 2.1172, 'learning_rate': 0.0001945258636928757, 'epoch': 0.05}         \n",
      "{'loss': 2.2159, 'learning_rate': 0.00019456353840937348, 'epoch': 0.05}        \n",
      "{'loss': 2.4142, 'learning_rate': 0.00019460121312587123, 'epoch': 0.05}        \n",
      "{'loss': 2.0529, 'learning_rate': 0.00019463888784236898, 'epoch': 0.05}        \n",
      "{'loss': 2.3283, 'learning_rate': 0.00019467656255886675, 'epoch': 0.05}        \n",
      "{'loss': 2.6693, 'learning_rate': 0.0001947142372753645, 'epoch': 0.05}         \n",
      "{'loss': 2.4494, 'learning_rate': 0.00019475191199186227, 'epoch': 0.05}        \n",
      "{'loss': 2.279, 'learning_rate': 0.00019478958670836002, 'epoch': 0.05}         \n",
      "{'loss': 2.499, 'learning_rate': 0.0001948272614248578, 'epoch': 0.05}          \n",
      "{'loss': 2.1086, 'learning_rate': 0.00019486493614135554, 'epoch': 0.05}        \n",
      "{'loss': 2.2208, 'learning_rate': 0.0001949026108578533, 'epoch': 0.05}         \n",
      "{'loss': 1.9704, 'learning_rate': 0.00019494028557435106, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3796, 'learning_rate': 0.00019497796029084883, 'epoch': 0.05}        \n",
      "{'loss': 2.5077, 'learning_rate': 0.00019501563500734658, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52357/1061708 [7:50:06<149:07:14,  1.88it/s][2024-03-01 01:57:07,363] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 52358/1061708 [7:50:07<139:58:42,  2.00it/s][2024-03-01 01:57:07,788] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8753, 'learning_rate': 0.0001950457747805448, 'epoch': 0.05}         \n",
      "{'loss': 2.3021, 'learning_rate': 0.00019508344949704254, 'epoch': 0.05}        \n",
      "{'loss': 2.1878, 'learning_rate': 0.0001951211242135403, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 52380/1061708 [7:50:18<148:38:31,  1.89it/s][2024-03-01 01:57:19,414] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5164, 'learning_rate': 0.0001951550314583883, 'epoch': 0.05}         \n",
      "{'loss': 2.0447, 'learning_rate': 0.00019519270617488605, 'epoch': 0.05}        \n",
      "{'loss': 2.0841, 'learning_rate': 0.0001952303808913838, 'epoch': 0.05}         \n",
      "{'loss': 2.3632, 'learning_rate': 0.00019526805560788154, 'epoch': 0.05}        \n",
      "{'loss': 2.1494, 'learning_rate': 0.00019530573032437932, 'epoch': 0.05}        \n",
      "{'loss': 2.0299, 'learning_rate': 0.00019534340504087706, 'epoch': 0.05}        \n",
      "{'loss': 2.2898, 'learning_rate': 0.00019538107975737484, 'epoch': 0.05}        \n",
      "{'loss': 1.7661, 'learning_rate': 0.00019541875447387258, 'epoch': 0.05}        \n",
      "{'loss': 2.6709, 'learning_rate': 0.00019545642919037036, 'epoch': 0.05}        \n",
      "{'loss': 1.8944, 'learning_rate': 0.0001954941039068681, 'epoch': 0.05}         \n",
      "{'loss': 2.0062, 'learning_rate': 0.00019553177862336588, 'epoch': 0.05}        \n",
      "{'loss': 2.3941, 'learning_rate': 0.00019556945333986362, 'epoch': 0.05}        \n",
      "{'loss': 2.2853, 'learning_rate': 0.0001956071280563614, 'epoch': 0.05}         \n",
      "{'loss': 2.2291, 'learning_rate': 0.00019564480277285915, 'epoch': 0.05}        \n",
      "{'loss': 2.074, 'learning_rate': 0.00019568247748935692, 'epoch': 0.05}         \n",
      "{'loss': 2.1888, 'learning_rate': 0.00019572015220585467, 'epoch': 0.05}        \n",
      "{'loss': 2.1097, 'learning_rate': 0.00019575782692235244, 'epoch': 0.05}        \n",
      "{'loss': 2.2759, 'learning_rate': 0.0001957955016388502, 'epoch': 0.05}         \n",
      "{'loss': 2.4294, 'learning_rate': 0.00019583317635534793, 'epoch': 0.05}        \n",
      "{'loss': 1.951, 'learning_rate': 0.00019587085107184568, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 52581/1061708 [7:52:06<148:34:37,  1.89it/s][2024-03-01 01:59:06,595] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 52582/1061708 [7:52:06<139:29:23,  2.01it/s][2024-03-01 01:59:07,017] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.153, 'learning_rate': 0.0001959009908450439, 'epoch': 0.05}          \n",
      "{'loss': 1.9801, 'learning_rate': 0.00019593866556154166, 'epoch': 0.05}        \n",
      "{'loss': 2.0112, 'learning_rate': 0.0001959763402780394, 'epoch': 0.05}         \n",
      "{'loss': 2.2689, 'learning_rate': 0.00019601401499453718, 'epoch': 0.05}        \n",
      "{'loss': 2.3231, 'learning_rate': 0.00019605168971103493, 'epoch': 0.05}        \n",
      "{'loss': 1.8754, 'learning_rate': 0.0001960893644275327, 'epoch': 0.05}         \n",
      "{'loss': 1.9319, 'learning_rate': 0.00019612703914403045, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52651/1061708 [7:52:43<148:36:02,  1.89it/s][2024-03-01 01:59:43,780] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.6519, 'learning_rate': 0.00019616094638887844, 'epoch': 0.05}        \n",
      "{'loss': 1.7878, 'learning_rate': 0.0001961986211053762, 'epoch': 0.05}         \n",
      "{'loss': 2.0079, 'learning_rate': 0.00019623629582187394, 'epoch': 0.05}        \n",
      "{'loss': 1.905, 'learning_rate': 0.0001962739705383717, 'epoch': 0.05}          \n",
      "{'loss': 2.6462, 'learning_rate': 0.00019631164525486946, 'epoch': 0.05}        \n",
      "{'loss': 2.4634, 'learning_rate': 0.00019634931997136723, 'epoch': 0.05}        \n",
      "{'loss': 2.2602, 'learning_rate': 0.00019638699468786498, 'epoch': 0.05}        \n",
      "{'loss': 2.1245, 'learning_rate': 0.00019642466940436275, 'epoch': 0.05}        \n",
      "{'loss': 2.4511, 'learning_rate': 0.0001964623441208605, 'epoch': 0.05}         \n",
      "{'loss': 2.23, 'learning_rate': 0.00019650001883735827, 'epoch': 0.05}          \n",
      "{'loss': 2.6097, 'learning_rate': 0.000196537693553856, 'epoch': 0.05}          \n",
      "{'loss': 2.4699, 'learning_rate': 0.00019657536827035377, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52773/1061708 [7:53:48<151:35:06,  1.85it/s][2024-03-01 02:00:48,760] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0374, 'learning_rate': 0.00019660927551520176, 'epoch': 0.05}        \n",
      "{'loss': 2.3135, 'learning_rate': 0.00019664695023169953, 'epoch': 0.05}        \n",
      "{'loss': 1.9727, 'learning_rate': 0.00019668462494819728, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52801/1061708 [7:54:03<148:29:41,  1.89it/s][2024-03-01 02:01:03,614] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4214, 'learning_rate': 0.00019671853219304524, 'epoch': 0.05}        \n",
      "{'loss': 2.416, 'learning_rate': 0.00019675620690954302, 'epoch': 0.05}         \n",
      "{'loss': 2.1605, 'learning_rate': 0.00019679388162604076, 'epoch': 0.05}        \n",
      "{'loss': 2.2872, 'learning_rate': 0.00019683155634253854, 'epoch': 0.05}        \n",
      "{'loss': 2.3188, 'learning_rate': 0.00019686923105903629, 'epoch': 0.05}        \n",
      "{'loss': 2.2203, 'learning_rate': 0.00019690690577553406, 'epoch': 0.05}        \n",
      "{'loss': 2.0406, 'learning_rate': 0.0001969445804920318, 'epoch': 0.05}         \n",
      "{'loss': 1.9185, 'learning_rate': 0.00019698225520852958, 'epoch': 0.05}        \n",
      "{'loss': 1.8167, 'learning_rate': 0.0001970199299250273, 'epoch': 0.05}         \n",
      "{'loss': 1.9345, 'learning_rate': 0.00019705760464152507, 'epoch': 0.05}        \n",
      "{'loss': 2.6507, 'learning_rate': 0.00019709527935802282, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 52912/1061708 [7:55:02<148:17:17,  1.89it/s][2024-03-01 02:02:02,653] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1197, 'learning_rate': 0.00019712918660287084, 'epoch': 0.05}        \n",
      "{'loss': 1.7064, 'learning_rate': 0.0001971668613193686, 'epoch': 0.05}         \n",
      "{'loss': 2.1758, 'learning_rate': 0.00019720453603586636, 'epoch': 0.05}        \n",
      "{'loss': 2.3707, 'learning_rate': 0.00019724221075236408, 'epoch': 0.05}        \n",
      "{'loss': 2.371, 'learning_rate': 0.00019727988546886185, 'epoch': 0.05}         \n",
      "{'loss': 1.9494, 'learning_rate': 0.0001973175601853596, 'epoch': 0.05}         \n",
      "{'loss': 2.2378, 'learning_rate': 0.00019735523490185738, 'epoch': 0.05}        \n",
      "{'loss': 2.5, 'learning_rate': 0.00019739290961835512, 'epoch': 0.05}           \n",
      "  5%|█▍                           | 52994/1061708 [7:55:45<150:35:01,  1.86it/s][2024-03-01 02:02:46,280] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1723, 'learning_rate': 0.0001974268168632031, 'epoch': 0.05}         \n",
      "{'loss': 2.4416, 'learning_rate': 0.0001974644915797009, 'epoch': 0.05}         \n",
      "{'loss': 2.2924, 'learning_rate': 0.00019750216629619863, 'epoch': 0.05}        \n",
      "{'loss': 2.3263, 'learning_rate': 0.00019753984101269638, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6221, 'learning_rate': 0.00019757751572919413, 'epoch': 0.05}        \n",
      "{'loss': 2.0076, 'learning_rate': 0.0001976151904456919, 'epoch': 0.05}         \n",
      "{'loss': 1.7763, 'learning_rate': 0.00019765286516218965, 'epoch': 0.05}        \n",
      "{'loss': 2.4582, 'learning_rate': 0.00019769053987868742, 'epoch': 0.05}        \n",
      "{'loss': 2.4298, 'learning_rate': 0.00019772821459518517, 'epoch': 0.05}        \n",
      "{'loss': 2.4043, 'learning_rate': 0.00019776588931168294, 'epoch': 0.05}        \n",
      "{'loss': 2.8379, 'learning_rate': 0.0001978035640281807, 'epoch': 0.05}         \n",
      "{'loss': 2.1921, 'learning_rate': 0.00019784123874467846, 'epoch': 0.05}        \n",
      "{'loss': 2.2522, 'learning_rate': 0.0001978789134611762, 'epoch': 0.05}         \n",
      "{'loss': 1.9249, 'learning_rate': 0.00019791658817767399, 'epoch': 0.05}        \n",
      "{'loss': 2.101, 'learning_rate': 0.00019795426289417173, 'epoch': 0.05}         \n",
      "{'loss': 2.1809, 'learning_rate': 0.0001979919376106695, 'epoch': 0.05}         \n",
      "{'loss': 1.8662, 'learning_rate': 0.00019802961232716725, 'epoch': 0.05}        \n",
      "{'loss': 2.3066, 'learning_rate': 0.000198067287043665, 'epoch': 0.05}          \n",
      "{'loss': 1.4389, 'learning_rate': 0.00019810496176016275, 'epoch': 0.05}        \n",
      "{'loss': 2.0898, 'learning_rate': 0.00019814263647666052, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53195/1061708 [7:57:32<149:37:45,  1.87it/s][2024-03-01 02:04:33,312] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 53196/1061708 [7:57:33<140:14:29,  2.00it/s][2024-03-01 02:04:33,736] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8596, 'learning_rate': 0.00019817277624985873, 'epoch': 0.05}        \n",
      "{'loss': 1.9584, 'learning_rate': 0.00019821045096635648, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53211/1061708 [7:57:41<148:09:25,  1.89it/s][2024-03-01 02:04:41,641] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0235, 'learning_rate': 0.00019824435821120447, 'epoch': 0.05}        \n",
      "{'loss': 2.17, 'learning_rate': 0.00019828203292770221, 'epoch': 0.05}          \n",
      "{'loss': 2.2837, 'learning_rate': 0.0001983197076442, 'epoch': 0.05}            \n",
      "{'loss': 2.0482, 'learning_rate': 0.00019835738236069774, 'epoch': 0.05}        \n",
      "{'loss': 2.0813, 'learning_rate': 0.0001983950570771955, 'epoch': 0.05}         \n",
      "{'loss': 2.1136, 'learning_rate': 0.00019843273179369326, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53276/1061708 [7:58:15<149:17:30,  1.88it/s][2024-03-01 02:05:16,188] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0923, 'learning_rate': 0.00019846663903854125, 'epoch': 0.05}        \n",
      "{'loss': 2.3255, 'learning_rate': 0.000198504313755039, 'epoch': 0.05}          \n",
      "{'loss': 2.1594, 'learning_rate': 0.00019854198847153677, 'epoch': 0.05}        \n",
      "{'loss': 2.4847, 'learning_rate': 0.00019857966318803452, 'epoch': 0.05}        \n",
      "{'loss': 1.7713, 'learning_rate': 0.00019861733790453226, 'epoch': 0.05}        \n",
      "{'loss': 2.1721, 'learning_rate': 0.00019865501262103004, 'epoch': 0.05}        \n",
      "{'loss': 1.7865, 'learning_rate': 0.00019869268733752778, 'epoch': 0.05}        \n",
      "{'loss': 2.2215, 'learning_rate': 0.00019873036205402556, 'epoch': 0.05}        \n",
      "{'loss': 2.317, 'learning_rate': 0.0001987680367705233, 'epoch': 0.05}          \n",
      "{'loss': 2.3545, 'learning_rate': 0.00019880571148702108, 'epoch': 0.05}        \n",
      "{'loss': 2.398, 'learning_rate': 0.00019884338620351882, 'epoch': 0.05}         \n",
      "{'loss': 2.3301, 'learning_rate': 0.0001988810609200166, 'epoch': 0.05}         \n",
      "{'loss': 2.1722, 'learning_rate': 0.00019891873563651435, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53403/1061708 [7:59:23<151:51:42,  1.84it/s][2024-03-01 02:06:23,825] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.057, 'learning_rate': 0.00019895264288136234, 'epoch': 0.05}         \n",
      "{'loss': 1.9299, 'learning_rate': 0.00019899031759786008, 'epoch': 0.05}        \n",
      "{'loss': 1.9406, 'learning_rate': 0.00019902799231435786, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53437/1061708 [7:59:41<148:54:47,  1.88it/s][2024-03-01 02:06:41,861] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3323, 'learning_rate': 0.00019906189955920582, 'epoch': 0.05}        \n",
      "{'loss': 2.0242, 'learning_rate': 0.00019909957427570357, 'epoch': 0.05}        \n",
      "{'loss': 1.9055, 'learning_rate': 0.00019913724899220134, 'epoch': 0.05}        \n",
      "{'loss': 2.0997, 'learning_rate': 0.0001991749237086991, 'epoch': 0.05}         \n",
      "{'loss': 2.1387, 'learning_rate': 0.00019921259842519686, 'epoch': 0.05}        \n",
      "{'loss': 2.3401, 'learning_rate': 0.0001992502731416946, 'epoch': 0.05}         \n",
      "{'loss': 2.1451, 'learning_rate': 0.00019928794785819238, 'epoch': 0.05}        \n",
      "{'loss': 2.5084, 'learning_rate': 0.00019932562257469013, 'epoch': 0.05}        \n",
      "{'loss': 1.9767, 'learning_rate': 0.0001993632972911879, 'epoch': 0.05}         \n",
      "{'loss': 2.0268, 'learning_rate': 0.00019940097200768565, 'epoch': 0.05}        \n",
      "{'loss': 2.3977, 'learning_rate': 0.00019943864672418343, 'epoch': 0.05}        \n",
      "{'loss': 2.0303, 'learning_rate': 0.00019947632144068115, 'epoch': 0.05}        \n",
      "{'loss': 2.2369, 'learning_rate': 0.00019951399615717892, 'epoch': 0.05}        \n",
      "{'loss': 2.5574, 'learning_rate': 0.00019955167087367667, 'epoch': 0.05}        \n",
      "{'loss': 2.2777, 'learning_rate': 0.00019958934559017444, 'epoch': 0.05}        \n",
      "{'loss': 2.1061, 'learning_rate': 0.0001996270203066722, 'epoch': 0.05}         \n",
      "{'loss': 1.992, 'learning_rate': 0.00019966469502316996, 'epoch': 0.05}         \n",
      "{'loss': 2.6642, 'learning_rate': 0.0001997023697396677, 'epoch': 0.05}         \n",
      "{'loss': 2.1079, 'learning_rate': 0.00019974004445616548, 'epoch': 0.05}        \n",
      "{'loss': 2.3628, 'learning_rate': 0.00019977771917266323, 'epoch': 0.05}        \n",
      "{'loss': 2.0953, 'learning_rate': 0.000199815393889161, 'epoch': 0.05}          \n",
      "{'loss': 2.3136, 'learning_rate': 0.00019985306860565875, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53656/1061708 [8:01:38<149:22:11,  1.87it/s][2024-03-01 02:08:38,534] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9733, 'learning_rate': 0.00019988697585050674, 'epoch': 0.05}        \n",
      "{'loss': 2.1119, 'learning_rate': 0.0001999246505670045, 'epoch': 0.05}         \n",
      "{'loss': 2.3527, 'learning_rate': 0.00019996232528350226, 'epoch': 0.05}        \n",
      "{'loss': 2.1197, 'learning_rate': 0.0002, 'epoch': 0.05}                        \n",
      "{'loss': 2.2574, 'learning_rate': 0.00019999999995149205, 'epoch': 0.05}        \n",
      "{'loss': 2.325, 'learning_rate': 0.00019999999980596823, 'epoch': 0.05}         \n",
      "{'loss': 2.0071, 'learning_rate': 0.00019999999956342852, 'epoch': 0.05}        \n",
      "{'loss': 2.2934, 'learning_rate': 0.00019999999922387288, 'epoch': 0.05}        \n",
      "{'loss': 2.4225, 'learning_rate': 0.0001999999987873014, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 53744/1061708 [8:02:24<150:39:29,  1.86it/s][2024-03-01 02:09:25,492] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0749, 'learning_rate': 0.00019999999831143845, 'epoch': 0.05}        \n",
      "{'loss': 2.4532, 'learning_rate': 0.00019999999769053676, 'epoch': 0.05}        \n",
      "{'loss': 2.4186, 'learning_rate': 0.00019999999697261917, 'epoch': 0.05}        \n",
      "{'loss': 2.5402, 'learning_rate': 0.0001999999961576857, 'epoch': 0.05}         \n",
      "{'loss': 2.0629, 'learning_rate': 0.00019999999524573635, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4284, 'learning_rate': 0.00019999999423677114, 'epoch': 0.05}        \n",
      "{'loss': 2.5899, 'learning_rate': 0.00019999999313079, 'epoch': 0.05}           \n",
      "{'loss': 2.1927, 'learning_rate': 0.000199999991927793, 'epoch': 0.05}          \n",
      "{'loss': 1.9193, 'learning_rate': 0.00019999999062778013, 'epoch': 0.05}        \n",
      "{'loss': 2.2235, 'learning_rate': 0.00019999998923075134, 'epoch': 0.05}        \n",
      "{'loss': 1.8084, 'learning_rate': 0.00019999998773670668, 'epoch': 0.05}        \n",
      "{'loss': 1.8311, 'learning_rate': 0.00019999998614564617, 'epoch': 0.05}        \n",
      "{'loss': 2.1396, 'learning_rate': 0.00019999998445756977, 'epoch': 0.05}        \n",
      "{'loss': 2.2097, 'learning_rate': 0.00019999998267247748, 'epoch': 0.05}        \n",
      "{'loss': 2.1397, 'learning_rate': 0.00019999998079036932, 'epoch': 0.05}        \n",
      "{'loss': 2.5881, 'learning_rate': 0.0001999999788112453, 'epoch': 0.05}         \n",
      "{'loss': 1.9628, 'learning_rate': 0.00019999997673510541, 'epoch': 0.05}        \n",
      "{'loss': 2.2706, 'learning_rate': 0.0001999999745619497, 'epoch': 0.05}         \n",
      "{'loss': 2.6161, 'learning_rate': 0.00019999997229177804, 'epoch': 0.05}        \n",
      "{'loss': 2.5243, 'learning_rate': 0.00019999996992459056, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53945/1061708 [8:04:12<149:57:45,  1.87it/s][2024-03-01 02:11:12,605] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▍                           | 53946/1061708 [8:04:12<140:24:31,  1.99it/s][2024-03-01 02:11:13,029] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3748, 'learning_rate': 0.00019999996796098913, 'epoch': 0.05}        \n",
      "{'loss': 2.4607, 'learning_rate': 0.0001999999654191731, 'epoch': 0.05}         \n",
      "{'loss': 1.6958, 'learning_rate': 0.00019999996278034123, 'epoch': 0.05}        \n",
      "{'loss': 2.3214, 'learning_rate': 0.00019999996004449347, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53988/1061708 [8:04:34<148:33:12,  1.88it/s][2024-03-01 02:11:35,361] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2229, 'learning_rate': 0.00019999995749928195, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 53999/1061708 [8:04:40<148:05:11,  1.89it/s][2024-03-01 02:11:41,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=54000, skipped=608, lr=[0.00019999995457910407], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 02:11:41,220] [INFO] [timer.py:260:stop] epoch=0/micro_step=54000/global_step=54000, RunningAvgSamplesPerSec=1.8925153826022196, CurrSamplesPerSec=1.9024651330721905, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.7821, 'learning_rate': 0.00019999995457910407, 'epoch': 0.05}        \n",
      "{'loss': 2.1897, 'learning_rate': 0.00019999995156191037, 'epoch': 0.05}        \n",
      "{'loss': 1.8617, 'learning_rate': 0.00019999994844770082, 'epoch': 0.05}        \n",
      "{'loss': 1.9171, 'learning_rate': 0.00019999994523647546, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 54038/1061708 [8:05:01<148:33:52,  1.88it/s][2024-03-01 02:12:01,912] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0861, 'learning_rate': 0.00019999994226342405, 'epoch': 0.05}        \n",
      "{'loss': 2.0299, 'learning_rate': 0.0001999999388678686, 'epoch': 0.05}         \n",
      "{'loss': 2.3978, 'learning_rate': 0.00019999993537529726, 'epoch': 0.05}        \n",
      "{'loss': 1.7861, 'learning_rate': 0.00019999993178571016, 'epoch': 0.05}        \n",
      "{'loss': 2.7053, 'learning_rate': 0.00019999992809910716, 'epoch': 0.05}        \n",
      "{'loss': 2.1805, 'learning_rate': 0.00019999992431548843, 'epoch': 0.05}        \n",
      "{'loss': 2.0679, 'learning_rate': 0.00019999992043485382, 'epoch': 0.05}        \n",
      "{'loss': 2.1132, 'learning_rate': 0.00019999991645720341, 'epoch': 0.05}        \n",
      "{'loss': 2.42, 'learning_rate': 0.0001999999123825372, 'epoch': 0.05}           \n",
      "{'loss': 2.0738, 'learning_rate': 0.00019999990821085518, 'epoch': 0.05}        \n",
      "{'loss': 2.1748, 'learning_rate': 0.00019999990394215734, 'epoch': 0.05}        \n",
      "{'loss': 1.776, 'learning_rate': 0.00019999989957644374, 'epoch': 0.05}         \n",
      "{'loss': 2.0574, 'learning_rate': 0.00019999989511371432, 'epoch': 0.05}        \n",
      "{'loss': 2.2594, 'learning_rate': 0.00019999989055396913, 'epoch': 0.05}        \n",
      "{'loss': 2.328, 'learning_rate': 0.00019999988589720812, 'epoch': 0.05}         \n",
      "{'loss': 2.6755, 'learning_rate': 0.00019999988114343138, 'epoch': 0.05}        \n",
      "{'loss': 2.1391, 'learning_rate': 0.00019999987629263884, 'epoch': 0.05}        \n",
      "{'loss': 2.0084, 'learning_rate': 0.00019999987134483054, 'epoch': 0.05}        \n",
      "{'loss': 2.127, 'learning_rate': 0.00019999986630000647, 'epoch': 0.05}         \n",
      "{'loss': 2.3007, 'learning_rate': 0.00019999986115816663, 'epoch': 0.05}        \n",
      "{'loss': 2.3108, 'learning_rate': 0.00019999985591931103, 'epoch': 0.05}        \n",
      "{'loss': 2.1151, 'learning_rate': 0.0001999998505834397, 'epoch': 0.05}         \n",
      "{'loss': 2.5612, 'learning_rate': 0.00019999984515055262, 'epoch': 0.05}        \n",
      "{'loss': 2.2969, 'learning_rate': 0.0001999998396206498, 'epoch': 0.05}         \n",
      "{'loss': 1.9841, 'learning_rate': 0.00019999983399373125, 'epoch': 0.05}        \n",
      "{'loss': 2.0508, 'learning_rate': 0.00019999982826979698, 'epoch': 0.05}        \n",
      "{'loss': 2.1799, 'learning_rate': 0.00019999982244884695, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 54304/1061708 [8:07:23<150:43:56,  1.86it/s][2024-03-01 02:14:23,597] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0884, 'learning_rate': 0.00019999981712704349, 'epoch': 0.05}        \n",
      "{'loss': 2.5574, 'learning_rate': 0.00019999981112176363, 'epoch': 0.05}        \n",
      "{'loss': 1.887, 'learning_rate': 0.00019999980501946807, 'epoch': 0.05}         \n",
      "{'loss': 2.2946, 'learning_rate': 0.00019999979882015677, 'epoch': 0.05}        \n",
      "{'loss': 2.016, 'learning_rate': 0.0001999997925238298, 'epoch': 0.05}          \n",
      "  5%|█▍                           | 54359/1061708 [8:07:52<149:50:55,  1.87it/s][2024-03-01 02:14:52,839] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8375, 'learning_rate': 0.00019999978677418714, 'epoch': 0.05}        \n",
      "{'loss': 2.3085, 'learning_rate': 0.00019999978029353036, 'epoch': 0.05}        \n",
      "{'loss': 2.3385, 'learning_rate': 0.00019999977371585792, 'epoch': 0.05}        \n",
      "{'loss': 2.5762, 'learning_rate': 0.00019999976704116978, 'epoch': 0.05}        \n",
      "{'loss': 2.2922, 'learning_rate': 0.000199999760269466, 'epoch': 0.05}          \n",
      "{'loss': 2.3021, 'learning_rate': 0.00019999975340074657, 'epoch': 0.05}        \n",
      "{'loss': 1.8028, 'learning_rate': 0.0001999997464350115, 'epoch': 0.05}         \n",
      "{'loss': 2.2763, 'learning_rate': 0.00019999973937226076, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 54436/1061708 [8:08:33<149:10:55,  1.88it/s][2024-03-01 02:15:33,795] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0999, 'learning_rate': 0.0001999997329328367, 'epoch': 0.05}         \n",
      "{'loss': 2.1111, 'learning_rate': 0.00019999972568575627, 'epoch': 0.05}        \n",
      "{'loss': 2.3375, 'learning_rate': 0.0001999997183416602, 'epoch': 0.05}         \n",
      "{'loss': 2.4476, 'learning_rate': 0.00019999971090054852, 'epoch': 0.05}        \n",
      "{'loss': 2.0597, 'learning_rate': 0.00019999970336242125, 'epoch': 0.05}        \n",
      "{'loss': 2.4886, 'learning_rate': 0.00019999969572727833, 'epoch': 0.05}        \n",
      "{'loss': 2.435, 'learning_rate': 0.00019999968799511985, 'epoch': 0.05}         \n",
      "{'loss': 2.4433, 'learning_rate': 0.0001999996801659458, 'epoch': 0.05}         \n",
      "{'loss': 2.0856, 'learning_rate': 0.00019999967223975612, 'epoch': 0.05}        \n",
      "{'loss': 1.9679, 'learning_rate': 0.0001999996642165509, 'epoch': 0.05}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1461, 'learning_rate': 0.00019999965609633012, 'epoch': 0.05}        \n",
      "{'loss': 1.8142, 'learning_rate': 0.00019999964787909376, 'epoch': 0.05}        \n",
      "{'loss': 2.4479, 'learning_rate': 0.00019999963956484186, 'epoch': 0.05}        \n",
      "{'loss': 2.1295, 'learning_rate': 0.00019999963115357444, 'epoch': 0.05}        \n",
      "{'loss': 2.3335, 'learning_rate': 0.00019999962264529147, 'epoch': 0.05}        \n",
      "{'loss': 2.3874, 'learning_rate': 0.000199999614039993, 'epoch': 0.05}          \n",
      "{'loss': 2.3703, 'learning_rate': 0.00019999960533767896, 'epoch': 0.05}        \n",
      "{'loss': 2.5985, 'learning_rate': 0.00019999959653834945, 'epoch': 0.05}        \n",
      "{'loss': 2.0167, 'learning_rate': 0.00019999958764200444, 'epoch': 0.05}        \n",
      "{'loss': 2.4172, 'learning_rate': 0.00019999957864864394, 'epoch': 0.05}        \n",
      "{'loss': 1.892, 'learning_rate': 0.00019999956955826795, 'epoch': 0.05}         \n",
      "{'loss': 1.9918, 'learning_rate': 0.0001999995603708765, 'epoch': 0.05}         \n",
      "{'loss': 2.1332, 'learning_rate': 0.00019999955108646958, 'epoch': 0.05}        \n",
      "{'loss': 2.2187, 'learning_rate': 0.00019999954170504721, 'epoch': 0.05}        \n",
      "{'loss': 2.0636, 'learning_rate': 0.00019999953222660938, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 54685/1061708 [8:10:45<149:44:28,  1.87it/s][2024-03-01 02:17:46,444] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.4176, 'learning_rate': 0.00019999952361306715, 'epoch': 0.05}        \n",
      "{'loss': 2.1172, 'learning_rate': 0.0001999995139503, 'epoch': 0.05}            \n",
      "{'loss': 2.2546, 'learning_rate': 0.00019999950419051746, 'epoch': 0.05}        \n",
      "  5%|█▍                           | 54715/1061708 [8:11:01<149:51:14,  1.87it/s][2024-03-01 02:18:02,378] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1774, 'learning_rate': 0.000199999495323765, 'epoch': 0.05}          \n",
      "{'loss': 2.265, 'learning_rate': 0.00019999948537965313, 'epoch': 0.05}         \n",
      "{'loss': 2.255, 'learning_rate': 0.0001999994753385259, 'epoch': 0.05}          \n",
      "{'loss': 1.9328, 'learning_rate': 0.00019999946520038334, 'epoch': 0.05}        \n",
      "{'loss': 2.1876, 'learning_rate': 0.00019999945496522537, 'epoch': 0.05}        \n",
      "{'loss': 2.3434, 'learning_rate': 0.00019999944463305202, 'epoch': 0.05}        \n",
      "{'loss': 2.2177, 'learning_rate': 0.00019999943420386337, 'epoch': 0.05}        \n",
      "{'loss': 2.3091, 'learning_rate': 0.00019999942367765933, 'epoch': 0.05}        \n",
      "{'loss': 2.6789, 'learning_rate': 0.00019999941305443998, 'epoch': 0.05}        \n",
      "{'loss': 2.0062, 'learning_rate': 0.00019999940233420528, 'epoch': 0.05}        \n",
      "{'loss': 2.4855, 'learning_rate': 0.00019999939151695532, 'epoch': 0.05}        \n",
      "{'loss': 1.9332, 'learning_rate': 0.00019999938060269004, 'epoch': 0.05}        \n",
      "{'loss': 2.1872, 'learning_rate': 0.00019999936959140947, 'epoch': 0.05}        \n",
      "{'loss': 2.1057, 'learning_rate': 0.0001999993584831136, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 54859/1061708 [8:12:18<148:12:34,  1.89it/s][2024-03-01 02:19:19,078] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.183, 'learning_rate': 0.0001999993484026993, 'epoch': 0.05}          \n",
      "{'loss': 2.0975, 'learning_rate': 0.00019999933711007444, 'epoch': 0.05}        \n",
      "{'loss': 2.4102, 'learning_rate': 0.00019999932572043437, 'epoch': 0.05}        \n",
      "{'loss': 2.1277, 'learning_rate': 0.000199999314233779, 'epoch': 0.05}          \n",
      "{'loss': 2.204, 'learning_rate': 0.00019999930265010845, 'epoch': 0.05}         \n",
      "{'loss': 2.2518, 'learning_rate': 0.0001999992909694227, 'epoch': 0.05}         \n",
      "  5%|█▍                           | 54915/1061708 [8:12:48<149:44:55,  1.87it/s][2024-03-01 02:19:48,834] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9198, 'learning_rate': 0.00019999928037385748, 'epoch': 0.05}        \n",
      "{'loss': 2.0211, 'learning_rate': 0.00019999926850884284, 'epoch': 0.05}        \n",
      "{'loss': 2.1874, 'learning_rate': 0.00019999925654681303, 'epoch': 0.05}        \n",
      "{'loss': 2.0213, 'learning_rate': 0.00019999924448776802, 'epoch': 0.05}        \n",
      "{'loss': 2.2627, 'learning_rate': 0.00019999923233170788, 'epoch': 0.05}        \n",
      "{'loss': 2.4073, 'learning_rate': 0.00019999922007863256, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 54971/1061708 [8:13:18<148:01:27,  1.89it/s][2024-03-01 02:20:18,599] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2287, 'learning_rate': 0.00019999920896791688, 'epoch': 0.05}        \n",
      "{'loss': 2.3427, 'learning_rate': 0.00019999919653051283, 'epoch': 0.05}        \n",
      "{'loss': 2.1746, 'learning_rate': 0.0001999991839960937, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 55000/1061708 [8:13:33<147:57:51,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 02:20:33,531 >> Saving model checkpoint to output_model/checkpoint-55000\n",
      "[INFO|trainer.py:2880] 2024-03-01 02:20:33,534 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 02:20:34,743 >> tokenizer config file saved in output_model/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 02:20:34,744 >> Special tokens file saved in output_model/checkpoint-55000/special_tokens_map.json\n",
      "[2024-03-01 02:20:34,745] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step55000 is about to be saved!\n",
      "[2024-03-01 02:20:40,019] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-55000/global_step55000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 02:20:40,019] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-55000/global_step55000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 02:20:54,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-55000/global_step55000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 02:20:54,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-55000/global_step55000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 02:21:02,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-55000/global_step55000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 02:21:02,015] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-55000/global_step55000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 02:21:02,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 02:21:02,064 >> Deleting older checkpoint [output_model/checkpoint-40000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 02:21:05,847 >> tokenizer config file saved in output_model/checkpoint-55000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 02:21:05,847 >> Special tokens file saved in output_model/checkpoint-55000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 1.9637, 'learning_rate': 0.00019999917136465944, 'epoch': 0.05}        \n",
      "{'loss': 2.1313, 'learning_rate': 0.0001999991586362101, 'epoch': 0.05}         \n",
      "{'loss': 1.7696, 'learning_rate': 0.00019999914581074571, 'epoch': 0.05}        \n",
      "{'loss': 2.2804, 'learning_rate': 0.00019999913288826622, 'epoch': 0.05}        \n",
      "{'loss': 1.9866, 'learning_rate': 0.00019999911986877172, 'epoch': 0.05}        \n",
      "{'loss': 1.8417, 'learning_rate': 0.00019999910675226213, 'epoch': 0.05}        \n",
      "{'loss': 1.9304, 'learning_rate': 0.00019999909353873757, 'epoch': 0.05}        \n",
      "{'loss': 2.2194, 'learning_rate': 0.000199999080228198, 'epoch': 0.05}          \n",
      "{'loss': 2.3041, 'learning_rate': 0.00019999906682064339, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3472, 'learning_rate': 0.00019999905331607383, 'epoch': 0.05}        \n",
      "{'loss': 2.4033, 'learning_rate': 0.00019999903971448931, 'epoch': 0.05}        \n",
      "{'loss': 1.9356, 'learning_rate': 0.0001999990260158898, 'epoch': 0.05}         \n",
      "{'loss': 2.1544, 'learning_rate': 0.00019999901222027535, 'epoch': 0.05}        \n",
      "{'loss': 2.1776, 'learning_rate': 0.00019999899832764598, 'epoch': 0.05}        \n",
      "{'loss': 2.2787, 'learning_rate': 0.00019999898433800168, 'epoch': 0.05}        \n",
      "{'loss': 1.8687, 'learning_rate': 0.00019999897025134246, 'epoch': 0.05}        \n",
      "{'loss': 2.4191, 'learning_rate': 0.0001999989560676684, 'epoch': 0.05}         \n",
      "{'loss': 2.2092, 'learning_rate': 0.0001999989417869794, 'epoch': 0.05}         \n",
      "{'loss': 2.1683, 'learning_rate': 0.0001999989274092756, 'epoch': 0.05}         \n",
      "{'loss': 2.1352, 'learning_rate': 0.00019999891293455692, 'epoch': 0.05}        \n",
      "{'loss': 2.3601, 'learning_rate': 0.0001999988983628234, 'epoch': 0.05}         \n",
      "{'loss': 2.6272, 'learning_rate': 0.00019999888369407506, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55221/1061708 [8:16:03<148:45:42,  1.88it/s][2024-03-01 02:23:04,171] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6266, 'learning_rate': 0.0001999988704092539, 'epoch': 0.05}         \n",
      "{'loss': 2.3033, 'learning_rate': 0.00019999885555617745, 'epoch': 0.05}        \n",
      "{'loss': 1.9395, 'learning_rate': 0.0001999988406060862, 'epoch': 0.05}         \n",
      "{'loss': 2.4906, 'learning_rate': 0.0001999988255589802, 'epoch': 0.05}         \n",
      "{'loss': 2.4365, 'learning_rate': 0.00019999881041485944, 'epoch': 0.05}        \n",
      "{'loss': 2.5354, 'learning_rate': 0.00019999879517372396, 'epoch': 0.05}        \n",
      "{'loss': 2.0097, 'learning_rate': 0.00019999877983557375, 'epoch': 0.05}        \n",
      "{'loss': 1.9365, 'learning_rate': 0.00019999876440040883, 'epoch': 0.05}        \n",
      "{'loss': 2.6684, 'learning_rate': 0.00019999874886822923, 'epoch': 0.05}        \n",
      "{'loss': 2.2956, 'learning_rate': 0.00019999873323903495, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55322/1061708 [8:16:57<148:46:27,  1.88it/s][2024-03-01 02:23:58,123] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▌                           | 55323/1061708 [8:16:58<142:32:30,  1.96it/s][2024-03-01 02:23:58,548] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.678, 'learning_rate': 0.00019999872066582897, 'epoch': 0.05}         \n",
      "{'loss': 2.1775, 'learning_rate': 0.0001999987048620083, 'epoch': 0.05}         \n",
      "{'loss': 2.1621, 'learning_rate': 0.000199998688961173, 'epoch': 0.05}          \n",
      "{'loss': 2.0736, 'learning_rate': 0.0001999986729633231, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 55360/1061708 [8:17:17<154:17:32,  1.81it/s][2024-03-01 02:24:18,316] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4201, 'learning_rate': 0.00019999865848231068, 'epoch': 0.05}        \n",
      "{'loss': 2.4466, 'learning_rate': 0.00019999864230013303, 'epoch': 0.05}        \n",
      "{'loss': 2.2111, 'learning_rate': 0.00019999862602094082, 'epoch': 0.05}        \n",
      "{'loss': 1.9062, 'learning_rate': 0.00019999860964473406, 'epoch': 0.05}        \n",
      "{'loss': 1.9452, 'learning_rate': 0.00019999859317151273, 'epoch': 0.05}        \n",
      "{'loss': 2.3436, 'learning_rate': 0.0001999985766012769, 'epoch': 0.05}         \n",
      "{'loss': 2.2512, 'learning_rate': 0.00019999855993402653, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55431/1061708 [8:17:55<149:11:23,  1.87it/s][2024-03-01 02:24:56,212] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0476, 'learning_rate': 0.00019999854485055384, 'epoch': 0.05}        \n",
      "{'loss': 2.0429, 'learning_rate': 0.00019999852799897595, 'epoch': 0.05}        \n",
      "{'loss': 2.5439, 'learning_rate': 0.00019999851105038363, 'epoch': 0.05}        \n",
      "{'loss': 2.1036, 'learning_rate': 0.00019999849400477687, 'epoch': 0.05}        \n",
      "{'loss': 2.2496, 'learning_rate': 0.00019999847686215562, 'epoch': 0.05}        \n",
      "{'loss': 2.199, 'learning_rate': 0.00019999845962252, 'epoch': 0.05}            \n",
      "{'loss': 2.0288, 'learning_rate': 0.00019999844228587002, 'epoch': 0.05}        \n",
      "{'loss': 2.0713, 'learning_rate': 0.00019999842485220559, 'epoch': 0.05}        \n",
      "{'loss': 2.2322, 'learning_rate': 0.00019999840732152685, 'epoch': 0.05}        \n",
      "{'loss': 2.0851, 'learning_rate': 0.00019999838969383374, 'epoch': 0.05}        \n",
      "{'loss': 2.2631, 'learning_rate': 0.00019999837196912632, 'epoch': 0.05}        \n",
      "{'loss': 2.123, 'learning_rate': 0.00019999835414740458, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 55559/1061708 [8:19:03<148:39:32,  1.88it/s][2024-03-01 02:26:04,525] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4557, 'learning_rate': 0.0001999983380249078, 'epoch': 0.05}         \n",
      "{'loss': 2.4202, 'learning_rate': 0.0001999983200188589, 'epoch': 0.05}         \n",
      "{'loss': 2.1619, 'learning_rate': 0.00019999830191579578, 'epoch': 0.05}        \n",
      "{'loss': 1.9555, 'learning_rate': 0.00019999828371571838, 'epoch': 0.05}        \n",
      "{'loss': 1.7838, 'learning_rate': 0.00019999826541862675, 'epoch': 0.05}        \n",
      "{'loss': 1.8581, 'learning_rate': 0.00019999824702452096, 'epoch': 0.05}        \n",
      "{'loss': 2.3032, 'learning_rate': 0.00019999822853340095, 'epoch': 0.05}        \n",
      "{'loss': 2.3129, 'learning_rate': 0.00019999820994526677, 'epoch': 0.05}        \n",
      "{'loss': 2.1665, 'learning_rate': 0.00019999819126011845, 'epoch': 0.05}        \n",
      "{'loss': 1.9218, 'learning_rate': 0.00019999817247795597, 'epoch': 0.05}        \n",
      "{'loss': 2.3181, 'learning_rate': 0.00019999815359877937, 'epoch': 0.05}        \n",
      "{'loss': 1.8025, 'learning_rate': 0.00019999813462258872, 'epoch': 0.05}        \n",
      "{'loss': 1.8915, 'learning_rate': 0.00019999811554938398, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55684/1061708 [8:20:10<150:17:20,  1.86it/s][2024-03-01 02:27:11,253] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3056, 'learning_rate': 0.00019999809830055263, 'epoch': 0.05}        \n",
      "{'loss': 2.4233, 'learning_rate': 0.00019999807904302118, 'epoch': 0.05}        \n",
      "{'loss': 2.3991, 'learning_rate': 0.00019999805968847568, 'epoch': 0.05}        \n",
      "{'loss': 2.3133, 'learning_rate': 0.0001999980402369162, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 55722/1061708 [8:20:30<148:11:00,  1.89it/s][2024-03-01 02:27:31,497] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1089, 'learning_rate': 0.00019999802264756569, 'epoch': 0.05}        \n",
      "{'loss': 2.3869, 'learning_rate': 0.00019999800301167962, 'epoch': 0.05}        \n",
      "{'loss': 2.0882, 'learning_rate': 0.00019999798327877962, 'epoch': 0.05}        \n",
      "{'loss': 2.6697, 'learning_rate': 0.00019999796344886565, 'epoch': 0.05}        \n",
      "{'loss': 2.3722, 'learning_rate': 0.00019999794352193782, 'epoch': 0.05}        \n",
      "{'loss': 2.0325, 'learning_rate': 0.00019999792349799605, 'epoch': 0.05}        \n",
      "{'loss': 2.0992, 'learning_rate': 0.00019999790337704043, 'epoch': 0.05}        \n",
      "{'loss': 2.0063, 'learning_rate': 0.00019999788315907093, 'epoch': 0.05}        \n",
      "{'loss': 2.4559, 'learning_rate': 0.00019999786284408765, 'epoch': 0.05}        \n",
      "{'loss': 2.2668, 'learning_rate': 0.00019999784243209047, 'epoch': 0.05}        \n",
      "{'loss': 2.6181, 'learning_rate': 0.00019999782192307955, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55834/1061708 [8:21:30<150:49:58,  1.85it/s][2024-03-01 02:28:31,280] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0232, 'learning_rate': 0.00019999780338202295, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55849/1061708 [8:21:38<148:27:57,  1.88it/s][2024-03-01 02:28:39,208] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1018, 'learning_rate': 0.00019999778476238516, 'epoch': 0.05}        \n",
      "{'loss': 2.5165, 'learning_rate': 0.00019999776398173572, 'epoch': 0.05}        \n",
      "{'loss': 2.0986, 'learning_rate': 0.00019999774310407254, 'epoch': 0.05}        \n",
      "{'loss': 2.2789, 'learning_rate': 0.00019999772212939566, 'epoch': 0.05}        \n",
      "{'loss': 2.3484, 'learning_rate': 0.00019999770105770507, 'epoch': 0.05}        \n",
      "{'loss': 2.2239, 'learning_rate': 0.00019999767988900086, 'epoch': 0.05}        \n",
      "{'loss': 2.3953, 'learning_rate': 0.00019999765862328302, 'epoch': 0.05}        \n",
      "{'loss': 2.7066, 'learning_rate': 0.00019999763726055153, 'epoch': 0.05}        \n",
      "{'loss': 2.2957, 'learning_rate': 0.00019999761580080644, 'epoch': 0.05}        \n",
      "{'loss': 1.9587, 'learning_rate': 0.00019999759424404778, 'epoch': 0.05}        \n",
      "{'loss': 2.3725, 'learning_rate': 0.00019999757259027558, 'epoch': 0.05}        \n",
      "{'loss': 1.8786, 'learning_rate': 0.00019999755083948984, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55964/1061708 [8:22:40<150:49:40,  1.85it/s][2024-03-01 02:29:40,597] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9879, 'learning_rate': 0.0001999975311808361, 'epoch': 0.05}         \n",
      "{'loss': 2.4905, 'learning_rate': 0.00019999750924572473, 'epoch': 0.05}        \n",
      "{'loss': 2.4728, 'learning_rate': 0.00019999748721359986, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 55999/1061708 [8:22:58<148:12:59,  1.88it/s][2024-03-01 02:29:59,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=56000, skipped=628, lr=[0.00019999746508446152], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 02:29:59,329] [INFO] [timer.py:260:stop] epoch=0/micro_step=56000/global_step=56000, RunningAvgSamplesPerSec=1.8925003527741584, CurrSamplesPerSec=1.8977308512294089, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3688, 'learning_rate': 0.00019999746508446152, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56007/1061708 [8:23:02<149:01:18,  1.87it/s][2024-03-01 02:30:03,517] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2895, 'learning_rate': 0.00019999744508529053, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56014/1061708 [8:23:06<149:51:04,  1.86it/s][2024-03-01 02:30:07,180] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1478, 'learning_rate': 0.0001999974250075387, 'epoch': 0.05}         \n",
      "{'loss': 1.9851, 'learning_rate': 0.0001999974026067628, 'epoch': 0.05}         \n",
      "{'loss': 1.9702, 'learning_rate': 0.00019999738010897358, 'epoch': 0.05}        \n",
      "{'loss': 2.2482, 'learning_rate': 0.00019999735751417098, 'epoch': 0.05}        \n",
      "{'loss': 2.1463, 'learning_rate': 0.00019999733482235505, 'epoch': 0.05}        \n",
      "{'loss': 2.0838, 'learning_rate': 0.0001999973120335258, 'epoch': 0.05}         \n",
      "{'loss': 2.5286, 'learning_rate': 0.0001999972891476833, 'epoch': 0.05}         \n",
      "{'loss': 1.9674, 'learning_rate': 0.00019999726616482755, 'epoch': 0.05}        \n",
      "{'loss': 2.4141, 'learning_rate': 0.00019999724308495854, 'epoch': 0.05}        \n",
      "{'loss': 2.1806, 'learning_rate': 0.0001999972199080763, 'epoch': 0.05}         \n",
      "{'loss': 2.6182, 'learning_rate': 0.00019999719663418088, 'epoch': 0.05}        \n",
      "{'loss': 2.2233, 'learning_rate': 0.0001999971732632723, 'epoch': 0.05}         \n",
      "{'loss': 2.1445, 'learning_rate': 0.0001999971497953506, 'epoch': 0.05}         \n",
      "{'loss': 2.3845, 'learning_rate': 0.0001999971262304157, 'epoch': 0.05}         \n",
      "{'loss': 2.1939, 'learning_rate': 0.00019999710256846776, 'epoch': 0.05}        \n",
      "{'loss': 2.4072, 'learning_rate': 0.00019999707880950673, 'epoch': 0.05}        \n",
      "{'loss': 2.2298, 'learning_rate': 0.00019999705495353263, 'epoch': 0.05}        \n",
      "{'loss': 2.2276, 'learning_rate': 0.0001999970310005455, 'epoch': 0.05}         \n",
      "{'loss': 2.1002, 'learning_rate': 0.00019999700695054538, 'epoch': 0.05}        \n",
      "{'loss': 2.1605, 'learning_rate': 0.00019999698280353224, 'epoch': 0.05}        \n",
      "{'loss': 2.0306, 'learning_rate': 0.00019999695855950616, 'epoch': 0.05}        \n",
      "{'loss': 2.0847, 'learning_rate': 0.00019999693421846713, 'epoch': 0.05}        \n",
      "{'loss': 2.4415, 'learning_rate': 0.00019999690978041518, 'epoch': 0.05}        \n",
      "{'loss': 2.3207, 'learning_rate': 0.00019999688524535033, 'epoch': 0.05}        \n",
      "{'loss': 2.1708, 'learning_rate': 0.00019999686061327265, 'epoch': 0.05}        \n",
      "{'loss': 2.2237, 'learning_rate': 0.0001999968358841821, 'epoch': 0.05}         \n",
      "{'loss': 2.3471, 'learning_rate': 0.00019999681105807872, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56280/1061708 [8:25:28<148:12:16,  1.88it/s][2024-03-01 02:32:29,258] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3549, 'learning_rate': 0.00019999678863163976, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56290/1061708 [8:25:33<148:10:07,  1.88it/s][2024-03-01 02:32:34,514] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  5%|█▌                           | 56296/1061708 [8:25:37<148:24:55,  1.88it/s][2024-03-01 02:32:37,638] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0421, 'learning_rate': 0.00019999676863105865, 'epoch': 0.05}        \n",
      "{'loss': 2.0829, 'learning_rate': 0.0001999967435430208, 'epoch': 0.05}         \n",
      "{'loss': 2.3115, 'learning_rate': 0.0001999967183579702, 'epoch': 0.05}         \n",
      "{'loss': 2.2588, 'learning_rate': 0.00019999669307590691, 'epoch': 0.05}        \n",
      "{'loss': 2.0159, 'learning_rate': 0.00019999666769683093, 'epoch': 0.05}        \n",
      "{'loss': 2.4519, 'learning_rate': 0.0001999966422207423, 'epoch': 0.05}         \n",
      "{'loss': 2.1711, 'learning_rate': 0.00019999661664764102, 'epoch': 0.05}        \n",
      "{'loss': 1.9838, 'learning_rate': 0.00019999659097752717, 'epoch': 0.05}        \n",
      "{'loss': 2.3727, 'learning_rate': 0.0001999965652104007, 'epoch': 0.05}         \n",
      "{'loss': 2.2162, 'learning_rate': 0.0001999965393462617, 'epoch': 0.05}         \n",
      "{'loss': 1.8813, 'learning_rate': 0.00019999651338511014, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56402/1061708 [8:26:33<148:11:07,  1.88it/s][2024-03-01 02:33:34,262] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3263, 'learning_rate': 0.00019999648993712806, 'epoch': 0.05}        \n",
      "{'loss': 1.9686, 'learning_rate': 0.00019999646379165276, 'epoch': 0.05}        \n",
      "{'loss': 2.2499, 'learning_rate': 0.000199996437549165, 'epoch': 0.05}          \n",
      "{'loss': 2.302, 'learning_rate': 0.00019999641120966484, 'epoch': 0.05}         \n",
      "{'loss': 2.2373, 'learning_rate': 0.00019999638477315222, 'epoch': 0.05}        \n",
      "{'loss': 1.8208, 'learning_rate': 0.00019999635823962725, 'epoch': 0.05}        \n",
      "{'loss': 2.1831, 'learning_rate': 0.00019999633160908993, 'epoch': 0.05}        \n",
      "{'loss': 2.2578, 'learning_rate': 0.00019999630488154026, 'epoch': 0.05}        \n",
      "{'loss': 1.9144, 'learning_rate': 0.0001999962780569783, 'epoch': 0.05}         \n",
      "{'loss': 1.9253, 'learning_rate': 0.00019999625113540406, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56504/1061708 [8:27:28<150:07:01,  1.86it/s][2024-03-01 02:34:28,674] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.884, 'learning_rate': 0.00019999622682304177, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 56519/1061708 [8:27:36<148:18:59,  1.88it/s][2024-03-01 02:34:36,603] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3575, 'learning_rate': 0.00019999620243209957, 'epoch': 0.05}        \n",
      "{'loss': 2.1329, 'learning_rate': 0.00019999617523889105, 'epoch': 0.05}        \n",
      "{'loss': 2.1192, 'learning_rate': 0.00019999614794867038, 'epoch': 0.05}        \n",
      "{'loss': 2.4891, 'learning_rate': 0.00019999612056143757, 'epoch': 0.05}        \n",
      "{'loss': 1.7884, 'learning_rate': 0.0001999960930771926, 'epoch': 0.05}         \n",
      "{'loss': 2.6424, 'learning_rate': 0.00019999606549593558, 'epoch': 0.05}        \n",
      "{'loss': 2.1976, 'learning_rate': 0.00019999603781766645, 'epoch': 0.05}        \n",
      "{'loss': 2.4347, 'learning_rate': 0.00019999601004238527, 'epoch': 0.05}        \n",
      "{'loss': 2.4919, 'learning_rate': 0.0001999959821700921, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 56608/1061708 [8:28:23<148:45:03,  1.88it/s][2024-03-01 02:35:24,046] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.454, 'learning_rate': 0.00019999595700208298, 'epoch': 0.05}         \n",
      "{'loss': 2.5561, 'learning_rate': 0.000199995928945467, 'epoch': 0.05}          \n",
      "{'loss': 2.2574, 'learning_rate': 0.00019999590079183914, 'epoch': 0.05}        \n",
      "{'loss': 2.4554, 'learning_rate': 0.00019999587254119935, 'epoch': 0.05}        \n",
      "{'loss': 2.4931, 'learning_rate': 0.00019999584419354765, 'epoch': 0.05}        \n",
      "{'loss': 2.2756, 'learning_rate': 0.00019999581574888412, 'epoch': 0.05}        \n",
      "{'loss': 2.2547, 'learning_rate': 0.00019999578720720872, 'epoch': 0.05}        \n",
      "{'loss': 2.325, 'learning_rate': 0.00019999575856852157, 'epoch': 0.05}         \n",
      "{'loss': 1.9604, 'learning_rate': 0.0001999957298328226, 'epoch': 0.05}         \n",
      "{'loss': 2.1672, 'learning_rate': 0.00019999570100011192, 'epoch': 0.05}        \n",
      "{'loss': 2.2826, 'learning_rate': 0.0001999956720703895, 'epoch': 0.05}         \n",
      "{'loss': 2.2544, 'learning_rate': 0.0001999956430436554, 'epoch': 0.05}         \n",
      "{'loss': 2.3949, 'learning_rate': 0.0001999956139199096, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 56733/1061708 [8:29:30<151:41:04,  1.84it/s][2024-03-01 02:36:30,711] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0729, 'learning_rate': 0.00019999558762559346, 'epoch': 0.05}        \n",
      "{'loss': 2.1943, 'learning_rate': 0.0001999955583175256, 'epoch': 0.05}         \n",
      "{'loss': 2.4357, 'learning_rate': 0.00019999552891244616, 'epoch': 0.05}        \n",
      "{'loss': 2.5645, 'learning_rate': 0.00019999549941035518, 'epoch': 0.05}        \n",
      "{'loss': 2.1851, 'learning_rate': 0.00019999546981125268, 'epoch': 0.05}        \n",
      "{'loss': 2.1463, 'learning_rate': 0.00019999544011513865, 'epoch': 0.05}        \n",
      "{'loss': 2.0077, 'learning_rate': 0.0001999954103220132, 'epoch': 0.05}         \n",
      "{'loss': 2.1522, 'learning_rate': 0.00019999538043187628, 'epoch': 0.05}        \n",
      "{'loss': 2.4144, 'learning_rate': 0.00019999535044472796, 'epoch': 0.05}        \n",
      "{'loss': 2.1636, 'learning_rate': 0.00019999532036056827, 'epoch': 0.05}        \n",
      "{'loss': 1.853, 'learning_rate': 0.00019999529017939723, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 56844/1061708 [8:30:29<150:28:22,  1.86it/s][2024-03-01 02:37:29,986] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▌                           | 56849/1061708 [8:30:32<149:22:27,  1.87it/s][2024-03-01 02:37:32,589] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.172, 'learning_rate': 0.00019999526596461224, 'epoch': 0.05}         \n",
      "{'loss': 2.2747, 'learning_rate': 0.00019999523560882082, 'epoch': 0.05}        \n",
      "{'loss': 2.3733, 'learning_rate': 0.00019999520515601816, 'epoch': 0.05}        \n",
      "{'loss': 2.2261, 'learning_rate': 0.00019999517460620426, 'epoch': 0.05}        \n",
      "{'loss': 2.2717, 'learning_rate': 0.00019999514395937915, 'epoch': 0.05}        \n",
      "{'loss': 2.2146, 'learning_rate': 0.00019999511321554283, 'epoch': 0.05}        \n",
      "{'loss': 2.2635, 'learning_rate': 0.0001999950823746954, 'epoch': 0.05}         \n",
      "{'loss': 2.1661, 'learning_rate': 0.00019999505143683684, 'epoch': 0.05}        \n",
      "{'loss': 2.278, 'learning_rate': 0.00019999502040196718, 'epoch': 0.05}         \n",
      "{'loss': 1.9669, 'learning_rate': 0.00019999498927008647, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 56944/1061708 [8:31:22<150:45:21,  1.85it/s][2024-03-01 02:38:23,264] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3842, 'learning_rate': 0.0001999949611684494, 'epoch': 0.05}         \n",
      "{'loss': 2.1083, 'learning_rate': 0.00019999492985224774, 'epoch': 0.05}        \n",
      "{'loss': 1.8415, 'learning_rate': 0.00019999489843903513, 'epoch': 0.05}        \n",
      "{'loss': 1.9475, 'learning_rate': 0.00019999486692881159, 'epoch': 0.05}        \n",
      "{'loss': 1.866, 'learning_rate': 0.00019999483532157712, 'epoch': 0.05}         \n",
      "{'loss': 1.9025, 'learning_rate': 0.00019999480361733177, 'epoch': 0.05}        \n",
      "{'loss': 2.129, 'learning_rate': 0.0001999947718160756, 'epoch': 0.05}          \n",
      "{'loss': 2.6104, 'learning_rate': 0.0001999947399178086, 'epoch': 0.05}         \n",
      "{'loss': 2.1675, 'learning_rate': 0.0001999947079225308, 'epoch': 0.05}         \n",
      "{'loss': 2.4821, 'learning_rate': 0.00019999467583024222, 'epoch': 0.05}        \n",
      "{'loss': 2.3817, 'learning_rate': 0.00019999464364094295, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57050/1061708 [8:32:19<147:39:16,  1.89it/s][2024-03-01 02:39:19,722] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6091, 'learning_rate': 0.00019999461458762948, 'epoch': 0.05}        \n",
      "{'loss': 2.4121, 'learning_rate': 0.0001999945822140099, 'epoch': 0.05}         \n",
      "{'loss': 1.5538, 'learning_rate': 0.0001999945497433797, 'epoch': 0.05}         \n",
      "{'loss': 1.828, 'learning_rate': 0.00019999451717573892, 'epoch': 0.05}         \n",
      "{'loss': 2.0765, 'learning_rate': 0.00019999448451108752, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57107/1061708 [8:32:49<148:23:22,  1.88it/s][2024-03-01 02:39:50,063] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2656, 'learning_rate': 0.0001999944550299573, 'epoch': 0.05}         \n",
      "{'loss': 2.1769, 'learning_rate': 0.00019999442218098591, 'epoch': 0.05}        \n",
      "{'loss': 2.137, 'learning_rate': 0.00019999438923500406, 'epoch': 0.05}         \n",
      "{'loss': 2.3733, 'learning_rate': 0.00019999435619201175, 'epoch': 0.05}        \n",
      "{'loss': 1.8928, 'learning_rate': 0.00019999432305200908, 'epoch': 0.05}        \n",
      "{'loss': 2.1412, 'learning_rate': 0.00019999428981499597, 'epoch': 0.05}        \n",
      "{'loss': 2.1739, 'learning_rate': 0.00019999425648097252, 'epoch': 0.05}        \n",
      "{'loss': 2.193, 'learning_rate': 0.00019999422304993876, 'epoch': 0.05}         \n",
      "{'loss': 1.9272, 'learning_rate': 0.00019999418952189473, 'epoch': 0.05}        \n",
      "{'loss': 2.4706, 'learning_rate': 0.00019999415589684044, 'epoch': 0.05}        \n",
      "{'loss': 2.186, 'learning_rate': 0.0001999941221747759, 'epoch': 0.05}          \n",
      "{'loss': 2.2785, 'learning_rate': 0.0001999940883557012, 'epoch': 0.05}         \n",
      "{'loss': 2.3941, 'learning_rate': 0.00019999405443961633, 'epoch': 0.05}        \n",
      "{'loss': 2.1211, 'learning_rate': 0.00019999402042652134, 'epoch': 0.05}        \n",
      "{'loss': 2.0622, 'learning_rate': 0.00019999398631641627, 'epoch': 0.05}        \n",
      "{'loss': 2.2197, 'learning_rate': 0.00019999395210930113, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9524, 'learning_rate': 0.000199993917805176, 'epoch': 0.05}          \n",
      "{'loss': 2.4476, 'learning_rate': 0.00019999388340404087, 'epoch': 0.05}        \n",
      "{'loss': 2.0526, 'learning_rate': 0.00019999384890589575, 'epoch': 0.05}        \n",
      "{'loss': 2.4567, 'learning_rate': 0.00019999381431074075, 'epoch': 0.05}        \n",
      "{'loss': 2.3028, 'learning_rate': 0.00019999377961857584, 'epoch': 0.05}        \n",
      "{'loss': 2.1192, 'learning_rate': 0.00019999374482940107, 'epoch': 0.05}        \n",
      "{'loss': 2.2717, 'learning_rate': 0.0001999937099432165, 'epoch': 0.05}         \n",
      "{'loss': 2.1329, 'learning_rate': 0.00019999367496002213, 'epoch': 0.05}        \n",
      "{'loss': 2.2272, 'learning_rate': 0.00019999363987981798, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57353/1061708 [8:35:00<151:12:50,  1.84it/s][2024-03-01 02:42:01,046] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1375, 'learning_rate': 0.00019999360822469097, 'epoch': 0.05}        \n",
      "{'loss': 2.3491, 'learning_rate': 0.0001999935729601684, 'epoch': 0.05}         \n",
      "{'loss': 2.4332, 'learning_rate': 0.0001999935375986362, 'epoch': 0.05}         \n",
      "{'loss': 2.4138, 'learning_rate': 0.00019999350214009435, 'epoch': 0.05}        \n",
      "{'loss': 2.0112, 'learning_rate': 0.00019999346658454293, 'epoch': 0.05}        \n",
      "{'loss': 1.9435, 'learning_rate': 0.00019999343093198197, 'epoch': 0.05}        \n",
      "{'loss': 2.3322, 'learning_rate': 0.00019999339518241146, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57427/1061708 [8:35:39<148:17:54,  1.88it/s][2024-03-01 02:42:40,398] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9563, 'learning_rate': 0.00019999336292485494, 'epoch': 0.05}        \n",
      "{'loss': 1.8944, 'learning_rate': 0.00019999332699096646, 'epoch': 0.05}        \n",
      "{'loss': 2.0169, 'learning_rate': 0.00019999329096006855, 'epoch': 0.05}        \n",
      "{'loss': 2.8115, 'learning_rate': 0.00019999325483216127, 'epoch': 0.05}        \n",
      "{'loss': 2.0669, 'learning_rate': 0.00019999321860724465, 'epoch': 0.05}        \n",
      "{'loss': 2.1372, 'learning_rate': 0.0001999931822853187, 'epoch': 0.05}         \n",
      "{'loss': 2.5049, 'learning_rate': 0.00019999314586638351, 'epoch': 0.05}        \n",
      "{'loss': 2.1177, 'learning_rate': 0.00019999310935043906, 'epoch': 0.05}        \n",
      "{'loss': 2.2118, 'learning_rate': 0.0001999930727374854, 'epoch': 0.05}         \n",
      "{'loss': 2.3213, 'learning_rate': 0.00019999303602752258, 'epoch': 0.05}        \n",
      "{'loss': 1.7404, 'learning_rate': 0.0001999929992205506, 'epoch': 0.05}         \n",
      "{'loss': 2.0554, 'learning_rate': 0.00019999296231656958, 'epoch': 0.05}        \n",
      "{'loss': 2.3396, 'learning_rate': 0.00019999292531557944, 'epoch': 0.05}        \n",
      "{'loss': 2.2468, 'learning_rate': 0.00019999288821758032, 'epoch': 0.05}        \n",
      "{'loss': 2.1243, 'learning_rate': 0.0001999928510225722, 'epoch': 0.05}         \n",
      "{'loss': 2.2493, 'learning_rate': 0.00019999281373055507, 'epoch': 0.05}        \n",
      "{'loss': 2.0765, 'learning_rate': 0.00019999277634152906, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57592/1061708 [8:37:07<147:52:45,  1.89it/s][2024-03-01 02:44:08,380] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▌                           | 57597/1061708 [8:37:10<148:43:02,  1.88it/s][2024-03-01 02:44:10,965] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0431, 'learning_rate': 0.00019999274636046187, 'epoch': 0.05}        \n",
      "{'loss': 2.5972, 'learning_rate': 0.0001999927087968199, 'epoch': 0.05}         \n",
      "{'loss': 1.9112, 'learning_rate': 0.00019999267113616912, 'epoch': 0.05}        \n",
      "{'loss': 2.3274, 'learning_rate': 0.00019999263337850954, 'epoch': 0.05}        \n",
      "{'loss': 2.4487, 'learning_rate': 0.00019999259552384125, 'epoch': 0.05}        \n",
      "{'loss': 2.2024, 'learning_rate': 0.00019999255757216424, 'epoch': 0.05}        \n",
      "{'loss': 2.3389, 'learning_rate': 0.00019999251952347854, 'epoch': 0.05}        \n",
      "{'loss': 2.2116, 'learning_rate': 0.00019999248137778425, 'epoch': 0.05}        \n",
      "{'loss': 2.0481, 'learning_rate': 0.00019999244313508133, 'epoch': 0.05}        \n",
      "{'loss': 2.4803, 'learning_rate': 0.00019999240479536988, 'epoch': 0.05}        \n",
      "{'loss': 2.2375, 'learning_rate': 0.00019999236635864987, 'epoch': 0.05}        \n",
      "{'loss': 1.9679, 'learning_rate': 0.0001999923278249214, 'epoch': 0.05}         \n",
      "{'loss': 2.0361, 'learning_rate': 0.00019999228919418446, 'epoch': 0.05}        \n",
      "{'loss': 2.1636, 'learning_rate': 0.00019999225046643914, 'epoch': 0.05}        \n",
      "{'loss': 1.8893, 'learning_rate': 0.00019999221164168545, 'epoch': 0.05}        \n",
      "{'loss': 2.3031, 'learning_rate': 0.0001999921727199234, 'epoch': 0.05}         \n",
      "{'loss': 2.3018, 'learning_rate': 0.0001999921337011531, 'epoch': 0.05}         \n",
      "{'loss': 2.2076, 'learning_rate': 0.0001999920945853745, 'epoch': 0.05}         \n",
      "{'loss': 2.2634, 'learning_rate': 0.00019999205537258766, 'epoch': 0.05}        \n",
      "{'loss': 2.1425, 'learning_rate': 0.00019999201606279266, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57798/1061708 [8:38:57<147:58:21,  1.88it/s][2024-03-01 02:45:58,032] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▌                           | 57799/1061708 [8:38:57<138:48:29,  2.01it/s][2024-03-01 02:45:58,454] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4635, 'learning_rate': 0.0001999919845451108, 'epoch': 0.05}         \n",
      "{'loss': 1.8267, 'learning_rate': 0.0001999919450607012, 'epoch': 0.05}         \n",
      "{'loss': 2.2219, 'learning_rate': 0.00019999190547928352, 'epoch': 0.05}        \n",
      "{'loss': 2.226, 'learning_rate': 0.00019999186580085777, 'epoch': 0.05}         \n",
      "{'loss': 2.0187, 'learning_rate': 0.000199991826025424, 'epoch': 0.05}          \n",
      "  5%|█▌                           | 57849/1061708 [8:39:24<147:43:06,  1.89it/s][2024-03-01 02:46:25,018] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0513, 'learning_rate': 0.00019999179014459185, 'epoch': 0.05}        \n",
      "{'loss': 2.0448, 'learning_rate': 0.00019999175018484301, 'epoch': 0.05}        \n",
      "{'loss': 2.2484, 'learning_rate': 0.0001999917101280863, 'epoch': 0.05}         \n",
      "{'loss': 1.9594, 'learning_rate': 0.00019999166997432174, 'epoch': 0.05}        \n",
      "{'loss': 2.376, 'learning_rate': 0.00019999162972354938, 'epoch': 0.05}         \n",
      "{'loss': 2.339, 'learning_rate': 0.00019999158937576922, 'epoch': 0.05}         \n",
      "{'loss': 2.013, 'learning_rate': 0.00019999154893098138, 'epoch': 0.05}         \n",
      "{'loss': 2.568, 'learning_rate': 0.00019999150838918581, 'epoch': 0.05}         \n",
      "{'loss': 1.9537, 'learning_rate': 0.0001999914677503826, 'epoch': 0.05}         \n",
      "{'loss': 2.2199, 'learning_rate': 0.00019999142701457176, 'epoch': 0.05}        \n",
      "{'loss': 2.248, 'learning_rate': 0.00019999138618175338, 'epoch': 0.05}         \n",
      "{'loss': 2.2204, 'learning_rate': 0.00019999134525192745, 'epoch': 0.05}        \n",
      "{'loss': 2.3025, 'learning_rate': 0.000199991304225094, 'epoch': 0.05}          \n",
      "{'loss': 2.2049, 'learning_rate': 0.00019999126310125314, 'epoch': 0.05}        \n",
      "{'loss': 2.1642, 'learning_rate': 0.00019999122188040484, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 57999/1061708 [8:40:44<147:43:05,  1.89it/s][2024-03-01 02:47:45,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=58000, skipped=650, lr=[0.00019999118056254918], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 02:47:45,072] [INFO] [timer.py:260:stop] epoch=0/micro_step=58000/global_step=58000, RunningAvgSamplesPerSec=1.8924894653549824, CurrSamplesPerSec=1.8868018911636235, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.5208, 'learning_rate': 0.00019999118056254918, 'epoch': 0.05}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3804, 'learning_rate': 0.00019999113914768616, 'epoch': 0.05}        \n",
      "{'loss': 2.2645, 'learning_rate': 0.00019999109763581586, 'epoch': 0.05}        \n",
      "{'loss': 2.4926, 'learning_rate': 0.00019999105602693833, 'epoch': 0.05}        \n",
      "{'loss': 1.6866, 'learning_rate': 0.00019999101432105357, 'epoch': 0.05}        \n",
      "{'loss': 2.5289, 'learning_rate': 0.00019999097251816164, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 58050/1061708 [8:41:11<147:40:48,  1.89it/s][2024-03-01 02:48:12,137] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  5%|█▌                           | 58051/1061708 [8:41:12<138:44:40,  2.01it/s][2024-03-01 02:48:12,559] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.5983, 'learning_rate': 0.00019999093900600296, 'epoch': 0.05}        \n",
      "{'loss': 2.007, 'learning_rate': 0.00019999089702849824, 'epoch': 0.05}         \n",
      "  5%|█▌                           | 58074/1061708 [8:41:24<149:59:35,  1.86it/s][2024-03-01 02:48:24,763] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3808, 'learning_rate': 0.00019999085916580294, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 58085/1061708 [8:41:30<149:46:10,  1.86it/s][2024-03-01 02:48:30,587] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2012, 'learning_rate': 0.00019999082122453191, 'epoch': 0.05}        \n",
      "  5%|█▌                           | 58093/1061708 [8:41:34<151:18:55,  1.84it/s][2024-03-01 02:48:34,813] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.0589, 'learning_rate': 0.00019999078320468528, 'epoch': 0.05}        \n",
      "{'loss': 2.1566, 'learning_rate': 0.00019999074086825465, 'epoch': 0.05}        \n",
      "{'loss': 2.1713, 'learning_rate': 0.00019999069843481708, 'epoch': 0.05}        \n",
      "{'loss': 2.0178, 'learning_rate': 0.00019999065590437266, 'epoch': 0.05}        \n",
      "{'loss': 2.0896, 'learning_rate': 0.00019999061327692142, 'epoch': 0.05}        \n",
      "{'loss': 2.2234, 'learning_rate': 0.00019999057055246336, 'epoch': 0.05}        \n",
      "{'loss': 2.0263, 'learning_rate': 0.0001999905277309986, 'epoch': 0.05}         \n",
      "{'loss': 2.2959, 'learning_rate': 0.0001999904848125271, 'epoch': 0.05}         \n",
      "{'loss': 1.7139, 'learning_rate': 0.00019999044179704896, 'epoch': 0.05}        \n",
      "{'loss': 2.2488, 'learning_rate': 0.00019999039868456425, 'epoch': 0.05}        \n",
      "{'loss': 2.4253, 'learning_rate': 0.00019999035547507292, 'epoch': 0.05}        \n",
      "{'loss': 2.3228, 'learning_rate': 0.00019999031216857505, 'epoch': 0.05}        \n",
      "{'loss': 1.9967, 'learning_rate': 0.0001999902687650707, 'epoch': 0.05}         \n",
      "{'loss': 2.2956, 'learning_rate': 0.0001999902252645599, 'epoch': 0.05}         \n",
      "{'loss': 2.4292, 'learning_rate': 0.0001999901816670427, 'epoch': 0.05}         \n",
      "{'loss': 2.3035, 'learning_rate': 0.00019999013797251914, 'epoch': 0.05}        \n",
      "{'loss': 2.1808, 'learning_rate': 0.00019999009418098922, 'epoch': 0.05}        \n",
      "{'loss': 2.5109, 'learning_rate': 0.00019999005029245305, 'epoch': 0.05}        \n",
      "{'loss': 2.2217, 'learning_rate': 0.00019999000630691063, 'epoch': 0.05}        \n",
      "{'loss': 2.0173, 'learning_rate': 0.00019998996222436204, 'epoch': 0.05}        \n",
      "{'loss': 2.3033, 'learning_rate': 0.00019998991804480727, 'epoch': 0.05}        \n",
      "{'loss': 1.9765, 'learning_rate': 0.0001999898737682464, 'epoch': 0.05}         \n",
      "{'loss': 2.3699, 'learning_rate': 0.0001999898293946795, 'epoch': 0.05}         \n",
      "{'loss': 2.1233, 'learning_rate': 0.00019998978492410653, 'epoch': 0.05}        \n",
      "{'loss': 2.0135, 'learning_rate': 0.0001999897403565276, 'epoch': 0.05}         \n",
      "{'loss': 2.1248, 'learning_rate': 0.00019998969569194272, 'epoch': 0.05}        \n",
      "{'loss': 2.1083, 'learning_rate': 0.00019998965093035196, 'epoch': 0.05}        \n",
      "{'loss': 2.1821, 'learning_rate': 0.00019998960607175534, 'epoch': 0.05}        \n",
      "{'loss': 1.8453, 'learning_rate': 0.00019998956111615294, 'epoch': 0.05}        \n",
      "{'loss': 2.3957, 'learning_rate': 0.00019998951606354473, 'epoch': 0.05}        \n",
      "  6%|█▌                           | 58398/1061708 [8:44:17<148:13:53,  1.88it/s][2024-03-01 02:51:17,596] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9248, 'learning_rate': 0.0001999894754332575, 'epoch': 0.06}         \n",
      "{'loss': 1.6837, 'learning_rate': 0.00019998943019633849, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 58418/1061708 [8:44:27<148:08:14,  1.88it/s][2024-03-01 02:51:28,172] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0354, 'learning_rate': 0.00019998938940017156, 'epoch': 0.06}        \n",
      "{'loss': 2.3323, 'learning_rate': 0.0001999893439789419, 'epoch': 0.06}         \n",
      "{'loss': 2.5668, 'learning_rate': 0.00019998929846070664, 'epoch': 0.06}        \n",
      "{'loss': 2.3854, 'learning_rate': 0.0001999892528454659, 'epoch': 0.06}         \n",
      "{'loss': 2.0577, 'learning_rate': 0.0001999892071332197, 'epoch': 0.06}         \n",
      "{'loss': 2.2536, 'learning_rate': 0.00019998916132396807, 'epoch': 0.06}        \n",
      "{'loss': 2.2725, 'learning_rate': 0.00019998911541771109, 'epoch': 0.06}        \n",
      "{'loss': 2.2188, 'learning_rate': 0.00019998906941444874, 'epoch': 0.06}        \n",
      "{'loss': 2.2569, 'learning_rate': 0.00019998902331418115, 'epoch': 0.06}        \n",
      "{'loss': 2.1198, 'learning_rate': 0.00019998897711690828, 'epoch': 0.06}        \n",
      "{'loss': 2.1197, 'learning_rate': 0.00019998893082263025, 'epoch': 0.06}        \n",
      "{'loss': 2.184, 'learning_rate': 0.00019998888443134702, 'epoch': 0.06}         \n",
      "  6%|█▌                           | 58531/1061708 [8:45:27<148:02:33,  1.88it/s][2024-03-01 02:52:28,409] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1545, 'learning_rate': 0.0001999888425962528, 'epoch': 0.06}         \n",
      "{'loss': 2.2216, 'learning_rate': 0.00019998879602065993, 'epoch': 0.06}        \n",
      "{'loss': 2.1047, 'learning_rate': 0.00019998874934806205, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 58560/1061708 [8:45:43<147:55:51,  1.88it/s][2024-03-01 02:52:43,806] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4874, 'learning_rate': 0.00019998870725978468, 'epoch': 0.06}        \n",
      "{'loss': 2.6816, 'learning_rate': 0.0001999886604028774, 'epoch': 0.06}         \n",
      "{'loss': 2.0125, 'learning_rate': 0.0001999886134489652, 'epoch': 0.06}         \n",
      "{'loss': 2.3189, 'learning_rate': 0.00019998856639804818, 'epoch': 0.06}        \n",
      "{'loss': 1.9979, 'learning_rate': 0.00019998851925012633, 'epoch': 0.06}        \n",
      "{'loss': 2.3023, 'learning_rate': 0.0001999884720051998, 'epoch': 0.06}         \n",
      "{'loss': 2.0361, 'learning_rate': 0.00019998842466326848, 'epoch': 0.06}        \n",
      "{'loss': 2.373, 'learning_rate': 0.00019998837722433254, 'epoch': 0.06}         \n",
      "  6%|█▌                           | 58644/1061708 [8:46:28<150:04:36,  1.86it/s][2024-03-01 02:53:28,618] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.214, 'learning_rate': 0.00019998833444635127, 'epoch': 0.06}         \n",
      "{'loss': 1.9206, 'learning_rate': 0.00019998828682310656, 'epoch': 0.06}        \n",
      "{'loss': 2.3817, 'learning_rate': 0.00019998823910285737, 'epoch': 0.06}        \n",
      "{'loss': 2.0221, 'learning_rate': 0.0001999881912856037, 'epoch': 0.06}         \n",
      "{'loss': 2.3634, 'learning_rate': 0.00019998814337134557, 'epoch': 0.06}        \n",
      "{'loss': 2.1166, 'learning_rate': 0.0001999880953600831, 'epoch': 0.06}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9437, 'learning_rate': 0.00019998804725181627, 'epoch': 0.06}        \n",
      "{'loss': 2.0126, 'learning_rate': 0.0001999879990465451, 'epoch': 0.06}         \n",
      "{'loss': 2.2148, 'learning_rate': 0.00019998795074426974, 'epoch': 0.06}        \n",
      "{'loss': 2.3012, 'learning_rate': 0.00019998790234499017, 'epoch': 0.06}        \n",
      "{'loss': 2.3118, 'learning_rate': 0.00019998785384870641, 'epoch': 0.06}        \n",
      "{'loss': 2.0215, 'learning_rate': 0.00019998780525541857, 'epoch': 0.06}        \n",
      "{'loss': 2.0677, 'learning_rate': 0.00019998775656512668, 'epoch': 0.06}        \n",
      "{'loss': 2.1034, 'learning_rate': 0.00019998770777783075, 'epoch': 0.06}        \n",
      "{'loss': 2.1595, 'learning_rate': 0.0001999876588935309, 'epoch': 0.06}         \n",
      "{'loss': 2.3102, 'learning_rate': 0.00019998760991222708, 'epoch': 0.06}        \n",
      "{'loss': 2.2075, 'learning_rate': 0.00019998756083391942, 'epoch': 0.06}        \n",
      "{'loss': 2.3261, 'learning_rate': 0.00019998751165860795, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 58824/1061708 [8:48:04<150:07:08,  1.86it/s][2024-03-01 02:55:04,743] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2334, 'learning_rate': 0.00019998746731788938, 'epoch': 0.06}        \n",
      "{'loss': 1.8155, 'learning_rate': 0.00019998741795827077, 'epoch': 0.06}        \n",
      "{'loss': 2.1489, 'learning_rate': 0.0001999873685016485, 'epoch': 0.06}         \n",
      "{'loss': 1.9278, 'learning_rate': 0.00019998731894802253, 'epoch': 0.06}        \n",
      "{'loss': 2.1766, 'learning_rate': 0.00019998726929739303, 'epoch': 0.06}        \n",
      "{'loss': 2.0938, 'learning_rate': 0.00019998721954975993, 'epoch': 0.06}        \n",
      "{'loss': 1.9003, 'learning_rate': 0.0001999871697051234, 'epoch': 0.06}         \n",
      "{'loss': 2.0727, 'learning_rate': 0.0001999871197634834, 'epoch': 0.06}         \n",
      "{'loss': 2.1998, 'learning_rate': 0.00019998706972484002, 'epoch': 0.06}        \n",
      "{'loss': 2.1778, 'learning_rate': 0.0001999870195891933, 'epoch': 0.06}         \n",
      "  6%|█▌                           | 58928/1061708 [8:48:59<148:09:57,  1.88it/s][2024-03-01 02:56:00,209] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.6535, 'learning_rate': 0.0001999869743841734, 'epoch': 0.06}         \n",
      "{'loss': 2.5224, 'learning_rate': 0.00019998692406422047, 'epoch': 0.06}        \n",
      "{'loss': 1.9307, 'learning_rate': 0.0001999868736472643, 'epoch': 0.06}         \n",
      "{'loss': 2.2063, 'learning_rate': 0.00019998682313330499, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 58961/1061708 [8:49:17<147:38:41,  1.89it/s][2024-03-01 02:56:17,723] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2213, 'learning_rate': 0.00019998677758780394, 'epoch': 0.06}        \n",
      "{'loss': 2.1352, 'learning_rate': 0.00019998672688953877, 'epoch': 0.06}        \n",
      "{'loss': 2.1383, 'learning_rate': 0.00019998667609427058, 'epoch': 0.06}        \n",
      "{'loss': 2.0941, 'learning_rate': 0.00019998662520199942, 'epoch': 0.06}        \n",
      "{'loss': 2.0115, 'learning_rate': 0.00019998657421272536, 'epoch': 0.06}        \n",
      "{'loss': 2.0513, 'learning_rate': 0.00019998652312644844, 'epoch': 0.06}        \n",
      "{'loss': 2.1275, 'learning_rate': 0.0001999864719431687, 'epoch': 0.06}         \n",
      "{'loss': 2.369, 'learning_rate': 0.00019998642066288618, 'epoch': 0.06}         \n",
      "{'loss': 2.2211, 'learning_rate': 0.00019998636928560094, 'epoch': 0.06}        \n",
      "{'loss': 1.7513, 'learning_rate': 0.00019998631781131305, 'epoch': 0.06}        \n",
      "{'loss': 2.0766, 'learning_rate': 0.00019998626624002255, 'epoch': 0.06}        \n",
      "{'loss': 2.0774, 'learning_rate': 0.00019998621457172945, 'epoch': 0.06}        \n",
      "{'loss': 2.3076, 'learning_rate': 0.00019998616280643387, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 59096/1061708 [8:50:29<148:38:46,  1.87it/s][2024-03-01 02:57:29,786] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5875, 'learning_rate': 0.00019998611613473072, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 59107/1061708 [8:50:35<148:46:04,  1.87it/s][2024-03-01 02:57:35,609] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0878, 'learning_rate': 0.00019998606938445563, 'epoch': 0.06}        \n",
      "{'loss': 2.2675, 'learning_rate': 0.00019998601734755328, 'epoch': 0.06}        \n",
      "{'loss': 2.0969, 'learning_rate': 0.00019998596521364855, 'epoch': 0.06}        \n",
      "{'loss': 2.3329, 'learning_rate': 0.00019998591298274162, 'epoch': 0.06}        \n",
      "{'loss': 2.1005, 'learning_rate': 0.0001999858606548324, 'epoch': 0.06}         \n",
      "{'loss': 2.3893, 'learning_rate': 0.00019998580822992105, 'epoch': 0.06}        \n",
      "{'loss': 1.7998, 'learning_rate': 0.00019998575570800758, 'epoch': 0.06}        \n",
      "{'loss': 2.4934, 'learning_rate': 0.000199985703089092, 'epoch': 0.06}          \n",
      "{'loss': 2.292, 'learning_rate': 0.00019998565037317441, 'epoch': 0.06}         \n",
      "{'loss': 2.2757, 'learning_rate': 0.00019998559756025488, 'epoch': 0.06}        \n",
      "{'loss': 2.2692, 'learning_rate': 0.00019998554465033342, 'epoch': 0.06}        \n",
      "{'loss': 1.9272, 'learning_rate': 0.0001999854916434101, 'epoch': 0.06}         \n",
      "{'loss': 2.8129, 'learning_rate': 0.00019998543853948496, 'epoch': 0.06}        \n",
      "{'loss': 2.4383, 'learning_rate': 0.00019998538533855806, 'epoch': 0.06}        \n",
      "{'loss': 1.9493, 'learning_rate': 0.00019998533204062944, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 59256/1061708 [8:51:54<150:14:20,  1.85it/s][2024-03-01 02:58:55,127] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.917, 'learning_rate': 0.00019998528398955728, 'epoch': 0.06}         \n",
      "{'loss': 2.2662, 'learning_rate': 0.00019998523050732554, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 59277/1061708 [8:52:05<148:22:05,  1.88it/s][2024-03-01 02:59:06,285] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0561, 'learning_rate': 0.00019998518229038067, 'epoch': 0.06}        \n",
      "{'loss': 1.9362, 'learning_rate': 0.00019998512862384602, 'epoch': 0.06}        \n",
      "{'loss': 2.2303, 'learning_rate': 0.00019998507486030992, 'epoch': 0.06}        \n",
      "{'loss': 2.2678, 'learning_rate': 0.0001999850209997724, 'epoch': 0.06}         \n",
      "{'loss': 1.8283, 'learning_rate': 0.0001999849670422335, 'epoch': 0.06}         \n",
      "{'loss': 1.8495, 'learning_rate': 0.00019998491298769334, 'epoch': 0.06}        \n",
      "{'loss': 2.3721, 'learning_rate': 0.00019998485883615188, 'epoch': 0.06}        \n",
      "{'loss': 2.1883, 'learning_rate': 0.00019998480458760924, 'epoch': 0.06}        \n",
      "{'loss': 2.4296, 'learning_rate': 0.0001999847502420655, 'epoch': 0.06}         \n",
      "{'loss': 2.8752, 'learning_rate': 0.0001999846957995206, 'epoch': 0.06}         \n",
      "{'loss': 2.4629, 'learning_rate': 0.00019998464125997467, 'epoch': 0.06}        \n",
      "{'loss': 2.2624, 'learning_rate': 0.00019998458662342776, 'epoch': 0.06}        \n",
      "{'loss': 2.3745, 'learning_rate': 0.0001999845318898799, 'epoch': 0.06}         \n",
      "{'loss': 1.983, 'learning_rate': 0.0001999844770593312, 'epoch': 0.06}          \n",
      "{'loss': 2.0587, 'learning_rate': 0.00019998442213178163, 'epoch': 0.06}        \n",
      "{'loss': 2.0857, 'learning_rate': 0.00019998436710723128, 'epoch': 0.06}        \n",
      "{'loss': 2.426, 'learning_rate': 0.0001999843119856802, 'epoch': 0.06}          \n",
      "{'loss': 2.3177, 'learning_rate': 0.00019998425676712848, 'epoch': 0.06}        \n",
      "{'loss': 2.4065, 'learning_rate': 0.00019998420145157613, 'epoch': 0.06}        \n",
      "{'loss': 2.3749, 'learning_rate': 0.00019998414603902322, 'epoch': 0.06}        \n",
      "  6%|█▌                           | 59478/1061708 [8:53:53<149:01:45,  1.87it/s][2024-03-01 03:00:53,559] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|█▌                           | 59479/1061708 [8:53:53<139:44:07,  1.99it/s][2024-03-01 03:00:53,985] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4751, 'learning_rate': 0.00019998410163914052, 'epoch': 0.06}        \n",
      "{'loss': 2.1867, 'learning_rate': 0.00019998404605198674, 'epoch': 0.06}        \n",
      "{'loss': 1.9937, 'learning_rate': 0.00019998399036783254, 'epoch': 0.06}        \n",
      "{'loss': 1.8846, 'learning_rate': 0.000199983934586678, 'epoch': 0.06}          \n",
      "{'loss': 2.2727, 'learning_rate': 0.00019998387870852314, 'epoch': 0.06}        \n",
      "{'loss': 2.4619, 'learning_rate': 0.000199983822733368, 'epoch': 0.06}          \n",
      "{'loss': 2.1136, 'learning_rate': 0.0001999837666612127, 'epoch': 0.06}         \n",
      "{'loss': 2.1417, 'learning_rate': 0.00019998371049205726, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 59559/1061708 [8:54:36<147:44:36,  1.88it/s][2024-03-01 03:01:36,655] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2203, 'learning_rate': 0.00019998365985688228, 'epoch': 0.06}        \n",
      "{'loss': 2.043, 'learning_rate': 0.0001999836035034267, 'epoch': 0.06}          \n",
      "{'loss': 2.0319, 'learning_rate': 0.00019998354705297116, 'epoch': 0.06}        \n",
      "{'loss': 1.928, 'learning_rate': 0.00019998349050551567, 'epoch': 0.06}         \n",
      "{'loss': 2.4365, 'learning_rate': 0.00019998343386106032, 'epoch': 0.06}        \n",
      "{'loss': 2.0079, 'learning_rate': 0.00019998337711960514, 'epoch': 0.06}        \n",
      "{'loss': 2.3018, 'learning_rate': 0.00019998332028115022, 'epoch': 0.06}        \n",
      "{'loss': 2.5113, 'learning_rate': 0.0001999832633456956, 'epoch': 0.06}         \n",
      "{'loss': 1.9301, 'learning_rate': 0.00019998320631324127, 'epoch': 0.06}        \n",
      "{'loss': 2.0469, 'learning_rate': 0.0001999831491837874, 'epoch': 0.06}         \n",
      "{'loss': 2.1479, 'learning_rate': 0.00019998309195733396, 'epoch': 0.06}        \n",
      "{'loss': 2.2309, 'learning_rate': 0.00019998303463388104, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 59676/1061708 [8:55:38<148:39:52,  1.87it/s][2024-03-01 03:02:39,031] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.953, 'learning_rate': 0.0001999829829598389, 'epoch': 0.06}          \n",
      "{'loss': 2.3573, 'learning_rate': 0.0001999829254520871, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 59696/1061708 [8:55:49<148:45:51,  1.87it/s][2024-03-01 03:02:49,667] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0282, 'learning_rate': 0.0001999828736121761, 'epoch': 0.06}         \n",
      "{'loss': 2.3703, 'learning_rate': 0.00019998281592012562, 'epoch': 0.06}        \n",
      "{'loss': 2.4843, 'learning_rate': 0.00019998275813107595, 'epoch': 0.06}        \n",
      "{'loss': 2.0824, 'learning_rate': 0.0001999827002450271, 'epoch': 0.06}         \n",
      "{'loss': 2.2054, 'learning_rate': 0.00019998264226197917, 'epoch': 0.06}        \n",
      "{'loss': 1.963, 'learning_rate': 0.00019998258418193216, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 59753/1061708 [8:56:19<150:55:25,  1.84it/s][2024-03-01 03:03:20,063] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2224, 'learning_rate': 0.0001999825318269557, 'epoch': 0.06}         \n",
      "{'loss': 2.0705, 'learning_rate': 0.00019998247356261067, 'epoch': 0.06}        \n",
      "{'loss': 1.8358, 'learning_rate': 0.00019998241520126676, 'epoch': 0.06}        \n",
      "{'loss': 2.2998, 'learning_rate': 0.00019998235674292402, 'epoch': 0.06}        \n",
      "{'loss': 1.9672, 'learning_rate': 0.00019998229818758248, 'epoch': 0.06}        \n",
      "{'loss': 2.2745, 'learning_rate': 0.00019998223953524225, 'epoch': 0.06}        \n",
      "{'loss': 1.9228, 'learning_rate': 0.00019998218078590333, 'epoch': 0.06}        \n",
      "{'loss': 2.2894, 'learning_rate': 0.00019998212193956584, 'epoch': 0.06}        \n",
      "{'loss': 2.4843, 'learning_rate': 0.00019998206299622976, 'epoch': 0.06}        \n",
      "{'loss': 2.2691, 'learning_rate': 0.00019998200395589523, 'epoch': 0.06}        \n",
      "{'loss': 2.3261, 'learning_rate': 0.00019998194481856226, 'epoch': 0.06}        \n",
      "{'loss': 2.4147, 'learning_rate': 0.00019998188558423096, 'epoch': 0.06}        \n",
      "{'loss': 2.3378, 'learning_rate': 0.00019998182625290128, 'epoch': 0.06}        \n",
      "{'loss': 2.2603, 'learning_rate': 0.00019998176682457336, 'epoch': 0.06}        \n",
      "{'loss': 1.9473, 'learning_rate': 0.00019998170729924723, 'epoch': 0.06}        \n",
      "{'loss': 2.318, 'learning_rate': 0.00019998164767692298, 'epoch': 0.06}         \n",
      "{'loss': 2.0671, 'learning_rate': 0.00019998158795760064, 'epoch': 0.06}        \n",
      "{'loss': 1.9939, 'learning_rate': 0.00019998152814128023, 'epoch': 0.06}        \n",
      "{'loss': 2.0787, 'learning_rate': 0.00019998146822796192, 'epoch': 0.06}        \n",
      "{'loss': 1.8337, 'learning_rate': 0.00019998140821764565, 'epoch': 0.06}        \n",
      "{'loss': 1.886, 'learning_rate': 0.00019998134811033156, 'epoch': 0.06}         \n",
      "{'loss': 2.108, 'learning_rate': 0.00019998128790601964, 'epoch': 0.06}         \n",
      "{'loss': 2.4148, 'learning_rate': 0.00019998122760471002, 'epoch': 0.06}        \n",
      "{'loss': 2.2505, 'learning_rate': 0.0001999811672064027, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 59999/1061708 [8:58:30<147:14:04,  1.89it/s][2024-03-01 03:05:31,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=60000, skipped=673, lr=[0.00019998110671109776], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 03:05:31,236] [INFO] [timer.py:260:stop] epoch=0/micro_step=60000/global_step=60000, RunningAvgSamplesPerSec=1.8924545998103715, CurrSamplesPerSec=1.896142845065978, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3354, 'learning_rate': 0.00019998110671109776, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60000/1061708 [8:58:31<147:27:20,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 03:05:31,238 >> Saving model checkpoint to output_model/checkpoint-60000\n",
      "[INFO|trainer.py:2880] 2024-03-01 03:05:31,241 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 03:05:32,450 >> tokenizer config file saved in output_model/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 03:05:32,450 >> Special tokens file saved in output_model/checkpoint-60000/special_tokens_map.json\n",
      "[2024-03-01 03:05:32,451] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60000 is about to be saved!\n",
      "[2024-03-01 03:05:37,663] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-60000/global_step60000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 03:05:37,663] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-60000/global_step60000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 03:05:51,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-60000/global_step60000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 03:05:52,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-60000/global_step60000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 03:05:59,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-60000/global_step60000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 03:05:59,359] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-60000/global_step60000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 03:05:59,359] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 03:05:59,412 >> Deleting older checkpoint [output_model/checkpoint-45000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 03:06:03,168 >> tokenizer config file saved in output_model/checkpoint-60000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 03:06:03,168 >> Special tokens file saved in output_model/checkpoint-60000/pt_lora_model/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.412, 'learning_rate': 0.00019998104611879523, 'epoch': 0.06}         \n",
      "{'loss': 2.3196, 'learning_rate': 0.00019998098542949527, 'epoch': 0.06}        \n",
      "{'loss': 2.4727, 'learning_rate': 0.00019998092464319783, 'epoch': 0.06}        \n",
      "{'loss': 2.0918, 'learning_rate': 0.000199980863759903, 'epoch': 0.06}          \n",
      "{'loss': 2.2337, 'learning_rate': 0.00019998080277961085, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60054/1061708 [8:59:31<149:33:13,  1.86it/s][2024-03-01 03:06:32,131] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 60055/1061708 [8:59:32<139:53:57,  1.99it/s][2024-03-01 03:06:32,554] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4578, 'learning_rate': 0.0001999807539255391, 'epoch': 0.06}         \n",
      "{'loss': 2.334, 'learning_rate': 0.00019998069277065193, 'epoch': 0.06}         \n",
      "{'loss': 1.883, 'learning_rate': 0.00019998063151876756, 'epoch': 0.06}         \n",
      "{'loss': 2.3454, 'learning_rate': 0.00019998057016988612, 'epoch': 0.06}        \n",
      "{'loss': 2.0849, 'learning_rate': 0.00019998050872400766, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60103/1061708 [8:59:57<151:22:41,  1.84it/s][2024-03-01 03:06:58,157] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8917, 'learning_rate': 0.0001999804533397846, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 60118/1061708 [9:00:05<147:59:50,  1.88it/s][2024-03-01 03:07:06,090] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2194, 'learning_rate': 0.00019998039787699404, 'epoch': 0.06}        \n",
      "{'loss': 2.0417, 'learning_rate': 0.00019998033615952415, 'epoch': 0.06}        \n",
      "{'loss': 2.0664, 'learning_rate': 0.00019998027434505746, 'epoch': 0.06}        \n",
      "{'loss': 2.0699, 'learning_rate': 0.000199980212433594, 'epoch': 0.06}          \n",
      "{'loss': 1.8789, 'learning_rate': 0.00019998015042513388, 'epoch': 0.06}        \n",
      "{'loss': 1.9974, 'learning_rate': 0.0001999800883196771, 'epoch': 0.06}         \n",
      "{'loss': 2.034, 'learning_rate': 0.00019998002611722377, 'epoch': 0.06}         \n",
      "{'loss': 2.1487, 'learning_rate': 0.00019997996381777391, 'epoch': 0.06}        \n",
      "{'loss': 1.9409, 'learning_rate': 0.00019997990142132764, 'epoch': 0.06}        \n",
      "{'loss': 2.1194, 'learning_rate': 0.00019997983892788493, 'epoch': 0.06}        \n",
      "{'loss': 2.2924, 'learning_rate': 0.00019997977633744588, 'epoch': 0.06}        \n",
      "{'loss': 1.9825, 'learning_rate': 0.0001999797136500106, 'epoch': 0.06}         \n",
      "{'loss': 1.9947, 'learning_rate': 0.00019997965086557907, 'epoch': 0.06}        \n",
      "{'loss': 2.3947, 'learning_rate': 0.00019997958798415144, 'epoch': 0.06}        \n",
      "{'loss': 2.1156, 'learning_rate': 0.0001999795250057277, 'epoch': 0.06}         \n",
      "{'loss': 2.2454, 'learning_rate': 0.00019997946193030792, 'epoch': 0.06}        \n",
      "{'loss': 2.1347, 'learning_rate': 0.0001999793987578922, 'epoch': 0.06}         \n",
      "{'loss': 2.5294, 'learning_rate': 0.00019997933548848056, 'epoch': 0.06}        \n",
      "{'loss': 2.2254, 'learning_rate': 0.00019997927212207312, 'epoch': 0.06}        \n",
      "{'loss': 1.6995, 'learning_rate': 0.00019997920865866983, 'epoch': 0.06}        \n",
      "{'loss': 2.231, 'learning_rate': 0.00019997914509827086, 'epoch': 0.06}         \n",
      "{'loss': 2.0562, 'learning_rate': 0.00019997908144087624, 'epoch': 0.06}        \n",
      "{'loss': 2.0559, 'learning_rate': 0.00019997901768648601, 'epoch': 0.06}        \n",
      "{'loss': 2.031, 'learning_rate': 0.00019997895383510025, 'epoch': 0.06}         \n",
      "{'loss': 2.355, 'learning_rate': 0.000199978889886719, 'epoch': 0.06}           \n",
      "{'loss': 2.2441, 'learning_rate': 0.00019997882584134235, 'epoch': 0.06}        \n",
      "{'loss': 2.5741, 'learning_rate': 0.00019997876169897035, 'epoch': 0.06}        \n",
      "{'loss': 1.9864, 'learning_rate': 0.00019997869745960308, 'epoch': 0.06}        \n",
      "{'loss': 2.3324, 'learning_rate': 0.0001999786331232406, 'epoch': 0.06}         \n",
      "{'loss': 2.323, 'learning_rate': 0.0001999785686898829, 'epoch': 0.06}          \n",
      "  6%|█▋                           | 60410/1061708 [9:02:41<147:33:18,  1.88it/s][2024-03-01 03:09:42,092] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2207, 'learning_rate': 0.00019997851061693024, 'epoch': 0.06}        \n",
      "{'loss': 2.1945, 'learning_rate': 0.00019997844599928193, 'epoch': 0.06}        \n",
      "{'loss': 2.2301, 'learning_rate': 0.00019997838128463867, 'epoch': 0.06}        \n",
      "{'loss': 2.3466, 'learning_rate': 0.00019997831647300046, 'epoch': 0.06}        \n",
      "{'loss': 2.0099, 'learning_rate': 0.00019997825156436742, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60461/1061708 [9:03:08<147:18:27,  1.89it/s][2024-03-01 03:10:09,288] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4165, 'learning_rate': 0.00019997819306366715, 'epoch': 0.06}        \n",
      "{'loss': 1.962, 'learning_rate': 0.00019997812797074404, 'epoch': 0.06}         \n",
      "{'loss': 1.7819, 'learning_rate': 0.00019997806278082626, 'epoch': 0.06}        \n",
      "{'loss': 2.221, 'learning_rate': 0.0001999779974939139, 'epoch': 0.06}          \n",
      "{'loss': 2.0906, 'learning_rate': 0.000199977932110007, 'epoch': 0.06}          \n",
      "{'loss': 2.0616, 'learning_rate': 0.00019997786662910564, 'epoch': 0.06}        \n",
      "{'loss': 2.1326, 'learning_rate': 0.0001999778010512098, 'epoch': 0.06}         \n",
      "{'loss': 2.4861, 'learning_rate': 0.00019997773537631963, 'epoch': 0.06}        \n",
      "{'loss': 2.2986, 'learning_rate': 0.00019997766960443519, 'epoch': 0.06}        \n",
      "{'loss': 2.2491, 'learning_rate': 0.00019997760373555653, 'epoch': 0.06}        \n",
      "{'loss': 2.1032, 'learning_rate': 0.00019997753776968366, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60570/1061708 [9:04:06<147:51:15,  1.88it/s][2024-03-01 03:11:07,435] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 60576/1061708 [9:04:10<147:40:09,  1.88it/s][2024-03-01 03:11:10,556] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3937, 'learning_rate': 0.00019997748492714962, 'epoch': 0.06}        \n",
      "{'loss': 2.278, 'learning_rate': 0.00019997741878668746, 'epoch': 0.06}         \n",
      "{'loss': 2.1578, 'learning_rate': 0.00019997735254923132, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60608/1061708 [9:04:27<147:50:10,  1.88it/s][2024-03-01 03:11:27,588] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.5621, 'learning_rate': 0.00019997729285259097, 'epoch': 0.06}        \n",
      "{'loss': 2.2631, 'learning_rate': 0.00019997722643084643, 'epoch': 0.06}        \n",
      "{'loss': 2.0647, 'learning_rate': 0.00019997715991210808, 'epoch': 0.06}        \n",
      "{'loss': 2.1044, 'learning_rate': 0.000199977093296376, 'epoch': 0.06}          \n",
      "{'loss': 1.9201, 'learning_rate': 0.00019997702658365027, 'epoch': 0.06}        \n",
      "{'loss': 2.1669, 'learning_rate': 0.00019997695977393094, 'epoch': 0.06}        \n",
      "{'loss': 2.0726, 'learning_rate': 0.00019997689286721805, 'epoch': 0.06}        \n",
      "{'loss': 2.3143, 'learning_rate': 0.0001999768258635117, 'epoch': 0.06}         \n",
      "{'loss': 2.3293, 'learning_rate': 0.00019997675876281195, 'epoch': 0.06}        \n",
      "{'loss': 2.4865, 'learning_rate': 0.00019997669156511885, 'epoch': 0.06}        \n",
      "{'loss': 2.0452, 'learning_rate': 0.00019997662427043248, 'epoch': 0.06}        \n",
      "{'loss': 2.3532, 'learning_rate': 0.00019997655687875287, 'epoch': 0.06}        \n",
      "{'loss': 1.8655, 'learning_rate': 0.00019997648939008015, 'epoch': 0.06}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5123, 'learning_rate': 0.00019997642180441432, 'epoch': 0.06}        \n",
      "{'loss': 1.9595, 'learning_rate': 0.0001999763541217555, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 60754/1061708 [9:05:44<149:50:09,  1.86it/s][2024-03-01 03:12:45,525] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.967, 'learning_rate': 0.0001999762931244336, 'epoch': 0.06}          \n",
      "{'loss': 2.2873, 'learning_rate': 0.0001999762252574882, 'epoch': 0.06}         \n",
      "{'loss': 2.3236, 'learning_rate': 0.00019997615729355, 'epoch': 0.06}           \n",
      "{'loss': 2.2425, 'learning_rate': 0.00019997608923261903, 'epoch': 0.06}        \n",
      "{'loss': 2.1895, 'learning_rate': 0.00019997602107469539, 'epoch': 0.06}        \n",
      "{'loss': 2.3854, 'learning_rate': 0.0001999759528197791, 'epoch': 0.06}         \n",
      "{'loss': 2.3186, 'learning_rate': 0.00019997588446787025, 'epoch': 0.06}        \n",
      "{'loss': 1.7833, 'learning_rate': 0.00019997581601896892, 'epoch': 0.06}        \n",
      "{'loss': 2.0668, 'learning_rate': 0.00019997574747307516, 'epoch': 0.06}        \n",
      "{'loss': 2.139, 'learning_rate': 0.00019997567883018902, 'epoch': 0.06}         \n",
      "{'loss': 2.0544, 'learning_rate': 0.0001999756100903106, 'epoch': 0.06}         \n",
      "{'loss': 2.0109, 'learning_rate': 0.00019997554125343996, 'epoch': 0.06}        \n",
      "{'loss': 2.2467, 'learning_rate': 0.00019997547231957716, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60885/1061708 [9:06:54<149:15:57,  1.86it/s][2024-03-01 03:13:55,492] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0792, 'learning_rate': 0.00019997541019617238, 'epoch': 0.06}        \n",
      "{'loss': 2.1792, 'learning_rate': 0.00019997534107802465, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 60909/1061708 [9:07:07<147:31:19,  1.88it/s][2024-03-01 03:14:08,243] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.056, 'learning_rate': 0.00019997527878876355, 'epoch': 0.06}         \n",
      "{'loss': 2.2829, 'learning_rate': 0.00019997520948633114, 'epoch': 0.06}        \n",
      "{'loss': 1.9029, 'learning_rate': 0.0001999751400869069, 'epoch': 0.06}         \n",
      "{'loss': 2.3958, 'learning_rate': 0.00019997507059049089, 'epoch': 0.06}        \n",
      "{'loss': 2.4284, 'learning_rate': 0.00019997500099708316, 'epoch': 0.06}        \n",
      "{'loss': 2.2413, 'learning_rate': 0.0001999749313066838, 'epoch': 0.06}         \n",
      "{'loss': 1.9446, 'learning_rate': 0.00019997486151929284, 'epoch': 0.06}        \n",
      "{'loss': 2.2102, 'learning_rate': 0.0001999747916349104, 'epoch': 0.06}         \n",
      "{'loss': 1.9771, 'learning_rate': 0.00019997472165353655, 'epoch': 0.06}        \n",
      "{'loss': 2.4178, 'learning_rate': 0.00019997465157517132, 'epoch': 0.06}        \n",
      "{'loss': 2.0592, 'learning_rate': 0.0001999745813998148, 'epoch': 0.06}         \n",
      "{'loss': 1.919, 'learning_rate': 0.00019997451112746705, 'epoch': 0.06}         \n",
      "{'loss': 2.4053, 'learning_rate': 0.0001999744407581281, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 61037/1061708 [9:08:16<148:07:07,  1.88it/s][2024-03-01 03:15:16,578] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1797, 'learning_rate': 0.0001999743773427957, 'epoch': 0.06}         \n",
      "{'loss': 2.2103, 'learning_rate': 0.00019997430678917374, 'epoch': 0.06}        \n",
      "{'loss': 2.3336, 'learning_rate': 0.00019997423613856086, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61061/1061708 [9:08:28<147:33:01,  1.88it/s][2024-03-01 03:15:29,298] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8812, 'learning_rate': 0.000199974172470082, 'epoch': 0.06}          \n",
      "{'loss': 2.1969, 'learning_rate': 0.00019997410163518647, 'epoch': 0.06}        \n",
      "{'loss': 2.5215, 'learning_rate': 0.00019997403070330015, 'epoch': 0.06}        \n",
      "{'loss': 2.3228, 'learning_rate': 0.00019997395967442314, 'epoch': 0.06}        \n",
      "{'loss': 2.2576, 'learning_rate': 0.0001999738885485555, 'epoch': 0.06}         \n",
      "{'loss': 2.2422, 'learning_rate': 0.00019997381732569732, 'epoch': 0.06}        \n",
      "{'loss': 2.0786, 'learning_rate': 0.00019997374600584864, 'epoch': 0.06}        \n",
      "{'loss': 2.1958, 'learning_rate': 0.00019997367458900956, 'epoch': 0.06}        \n",
      "{'loss': 2.284, 'learning_rate': 0.0001999736030751801, 'epoch': 0.06}          \n",
      "{'loss': 2.2047, 'learning_rate': 0.00019997353146436036, 'epoch': 0.06}        \n",
      "{'loss': 2.1415, 'learning_rate': 0.00019997345975655042, 'epoch': 0.06}        \n",
      "{'loss': 2.3574, 'learning_rate': 0.00019997338795175033, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61180/1061708 [9:09:32<147:21:13,  1.89it/s][2024-03-01 03:16:32,789] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1282, 'learning_rate': 0.00019997332324450375, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61192/1061708 [9:09:38<147:32:04,  1.88it/s][2024-03-01 03:16:39,135] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0157, 'learning_rate': 0.00019997325845869525, 'epoch': 0.06}        \n",
      "{'loss': 2.2095, 'learning_rate': 0.00019997318638232314, 'epoch': 0.06}        \n",
      "{'loss': 2.0992, 'learning_rate': 0.00019997311420896115, 'epoch': 0.06}        \n",
      "{'loss': 2.4634, 'learning_rate': 0.00019997304193860934, 'epoch': 0.06}        \n",
      "{'loss': 2.1227, 'learning_rate': 0.00019997296957126787, 'epoch': 0.06}        \n",
      "{'loss': 2.0574, 'learning_rate': 0.00019997289710693665, 'epoch': 0.06}        \n",
      "{'loss': 2.24, 'learning_rate': 0.00019997282454561584, 'epoch': 0.06}          \n",
      "{'loss': 2.1867, 'learning_rate': 0.00019997275188730554, 'epoch': 0.06}        \n",
      "{'loss': 2.2547, 'learning_rate': 0.00019997267913200576, 'epoch': 0.06}        \n",
      "{'loss': 1.9745, 'learning_rate': 0.00019997260627971662, 'epoch': 0.06}        \n",
      "{'loss': 1.7736, 'learning_rate': 0.00019997253333043814, 'epoch': 0.06}        \n",
      "{'loss': 2.0553, 'learning_rate': 0.00019997246028417043, 'epoch': 0.06}        \n",
      "{'loss': 2.2351, 'learning_rate': 0.00019997238714091354, 'epoch': 0.06}        \n",
      "{'loss': 2.1863, 'learning_rate': 0.00019997231390066756, 'epoch': 0.06}        \n",
      "{'loss': 1.9114, 'learning_rate': 0.00019997224056343257, 'epoch': 0.06}        \n",
      "{'loss': 2.2434, 'learning_rate': 0.00019997216712920856, 'epoch': 0.06}        \n",
      "{'loss': 2.1723, 'learning_rate': 0.0001999720935979957, 'epoch': 0.06}         \n",
      "{'loss': 2.044, 'learning_rate': 0.00019997201996979405, 'epoch': 0.06}         \n",
      "{'loss': 2.4564, 'learning_rate': 0.0001999719462446036, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 61383/1061708 [9:11:20<151:15:16,  1.84it/s][2024-03-01 03:18:21,189] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0584, 'learning_rate': 0.0001999718798090069, 'epoch': 0.06}         \n",
      "{'loss': 2.1746, 'learning_rate': 0.0001999718058995381, 'epoch': 0.06}         \n",
      "{'loss': 2.1642, 'learning_rate': 0.0001999717318930807, 'epoch': 0.06}         \n",
      "{'loss': 2.3038, 'learning_rate': 0.00019997165778963484, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61426/1061708 [9:11:43<148:35:30,  1.87it/s][2024-03-01 03:18:44,098] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0502, 'learning_rate': 0.0001999715910136085, 'epoch': 0.06}         \n",
      "{'loss': 2.137, 'learning_rate': 0.00019997151672588478, 'epoch': 0.06}         \n",
      "{'loss': 2.16, 'learning_rate': 0.0001999714423411728, 'epoch': 0.06}           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8589, 'learning_rate': 0.00019997136785947263, 'epoch': 0.06}        \n",
      "{'loss': 2.2693, 'learning_rate': 0.00019997129328078436, 'epoch': 0.06}        \n",
      "{'loss': 2.5115, 'learning_rate': 0.00019997121860510801, 'epoch': 0.06}        \n",
      "{'loss': 2.3608, 'learning_rate': 0.00019997114383244372, 'epoch': 0.06}        \n",
      "{'loss': 1.947, 'learning_rate': 0.00019997106896279157, 'epoch': 0.06}         \n",
      "{'loss': 1.8719, 'learning_rate': 0.00019997099399615155, 'epoch': 0.06}        \n",
      "{'loss': 2.3101, 'learning_rate': 0.0001999709189325238, 'epoch': 0.06}         \n",
      "{'loss': 1.93, 'learning_rate': 0.00019997084377190835, 'epoch': 0.06}          \n",
      "{'loss': 1.8652, 'learning_rate': 0.0001999707685143053, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 61547/1061708 [9:12:48<148:12:42,  1.87it/s][2024-03-01 03:19:48,679] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.08, 'learning_rate': 0.00019997070069953824, 'epoch': 0.06}          \n",
      "{'loss': 2.0733, 'learning_rate': 0.00019997062525765896, 'epoch': 0.06}        \n",
      "{'loss': 2.2961, 'learning_rate': 0.0001999705497187923, 'epoch': 0.06}         \n",
      "{'loss': 2.312, 'learning_rate': 0.0001999704740829383, 'epoch': 0.06}          \n",
      "{'loss': 2.3075, 'learning_rate': 0.00019997039835009705, 'epoch': 0.06}        \n",
      "{'loss': 2.4003, 'learning_rate': 0.00019997032252026866, 'epoch': 0.06}        \n",
      "{'loss': 2.3968, 'learning_rate': 0.00019997024659345315, 'epoch': 0.06}        \n",
      "{'loss': 2.2603, 'learning_rate': 0.0001999701705696506, 'epoch': 0.06}         \n",
      "{'loss': 2.3371, 'learning_rate': 0.0001999700944488611, 'epoch': 0.06}         \n",
      "{'loss': 2.3645, 'learning_rate': 0.00019997001823108477, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61648/1061708 [9:13:42<147:56:23,  1.88it/s][2024-03-01 03:20:42,537] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 61649/1061708 [9:13:42<138:50:21,  2.00it/s][2024-03-01 03:20:42,962] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4408, 'learning_rate': 0.00019996995718703318, 'epoch': 0.06}        \n",
      "{'loss': 2.322, 'learning_rate': 0.00019996988079468064, 'epoch': 0.06}         \n",
      "{'loss': 2.2381, 'learning_rate': 0.0001999698043053414, 'epoch': 0.06}         \n",
      "{'loss': 2.0435, 'learning_rate': 0.0001999697277190156, 'epoch': 0.06}         \n",
      "{'loss': 2.2358, 'learning_rate': 0.00019996965103570325, 'epoch': 0.06}        \n",
      "{'loss': 2.5073, 'learning_rate': 0.00019996957425540447, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61704/1061708 [9:14:11<149:46:25,  1.85it/s][2024-03-01 03:21:12,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1789, 'learning_rate': 0.00019996950507021223, 'epoch': 0.06}        \n",
      "{'loss': 2.2145, 'learning_rate': 0.00019996942810563935, 'epoch': 0.06}        \n",
      "{'loss': 2.1332, 'learning_rate': 0.00019996935104408032, 'epoch': 0.06}        \n",
      "{'loss': 2.2637, 'learning_rate': 0.00019996927388553514, 'epoch': 0.06}        \n",
      "{'loss': 2.2867, 'learning_rate': 0.00019996919663000382, 'epoch': 0.06}        \n",
      "{'loss': 2.1546, 'learning_rate': 0.00019996911927748652, 'epoch': 0.06}        \n",
      "{'loss': 2.0707, 'learning_rate': 0.00019996904182798328, 'epoch': 0.06}        \n",
      "{'loss': 2.2183, 'learning_rate': 0.0001999689642814942, 'epoch': 0.06}         \n",
      "{'loss': 2.4296, 'learning_rate': 0.00019996888663801935, 'epoch': 0.06}        \n",
      "{'loss': 1.7541, 'learning_rate': 0.0001999688088975588, 'epoch': 0.06}         \n",
      "{'loss': 1.8945, 'learning_rate': 0.00019996873106011256, 'epoch': 0.06}        \n",
      "{'loss': 2.4145, 'learning_rate': 0.0001999686531256808, 'epoch': 0.06}         \n",
      "{'loss': 2.1533, 'learning_rate': 0.00019996857509426357, 'epoch': 0.06}        \n",
      "{'loss': 1.9677, 'learning_rate': 0.00019996849696586093, 'epoch': 0.06}        \n",
      "{'loss': 2.0825, 'learning_rate': 0.00019996841874047298, 'epoch': 0.06}        \n",
      "{'loss': 2.1091, 'learning_rate': 0.00019996834041809976, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61860/1061708 [9:15:35<147:19:59,  1.89it/s][2024-03-01 03:22:35,585] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.131, 'learning_rate': 0.00019996826984504154, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 61871/1061708 [9:15:40<147:45:46,  1.88it/s][2024-03-01 03:22:41,371] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0303, 'learning_rate': 0.00019996819919342537, 'epoch': 0.06}        \n",
      "{'loss': 2.0566, 'learning_rate': 0.00019996812059949387, 'epoch': 0.06}        \n",
      "{'loss': 2.2856, 'learning_rate': 0.00019996804190857738, 'epoch': 0.06}        \n",
      "{'loss': 2.3689, 'learning_rate': 0.00019996796312067602, 'epoch': 0.06}        \n",
      "{'loss': 2.0422, 'learning_rate': 0.00019996788423578985, 'epoch': 0.06}        \n",
      "{'loss': 2.2247, 'learning_rate': 0.00019996780525391897, 'epoch': 0.06}        \n",
      "{'loss': 1.7565, 'learning_rate': 0.00019996772617506342, 'epoch': 0.06}        \n",
      "{'loss': 2.221, 'learning_rate': 0.00019996764699922327, 'epoch': 0.06}         \n",
      "{'loss': 2.1812, 'learning_rate': 0.0001999675677263986, 'epoch': 0.06}         \n",
      "{'loss': 2.5846, 'learning_rate': 0.00019996748835658955, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 61979/1061708 [9:16:38<147:13:48,  1.89it/s][2024-03-01 03:23:39,033] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1623, 'learning_rate': 0.00019996741684083976, 'epoch': 0.06}        \n",
      "{'loss': 1.971, 'learning_rate': 0.00019996733728676054, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 61999/1061708 [9:16:49<147:07:11,  1.89it/s][2024-03-01 03:23:49,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=62000, skipped=698, lr=[0.00019996725763569707], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 03:23:49,689] [INFO] [timer.py:260:stop] epoch=0/micro_step=62000/global_step=62000, RunningAvgSamplesPerSec=1.8924108359231229, CurrSamplesPerSec=1.9000941823912905, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0351, 'learning_rate': 0.00019996725763569707, 'epoch': 0.06}        \n",
      "{'loss': 2.3629, 'learning_rate': 0.00019996717788764947, 'epoch': 0.06}        \n",
      "{'loss': 2.1215, 'learning_rate': 0.00019996709804261784, 'epoch': 0.06}        \n",
      "{'loss': 1.6502, 'learning_rate': 0.00019996701810060224, 'epoch': 0.06}        \n",
      "{'loss': 2.0492, 'learning_rate': 0.00019996693806160275, 'epoch': 0.06}        \n",
      "{'loss': 2.2838, 'learning_rate': 0.00019996685792561942, 'epoch': 0.06}        \n",
      "{'loss': 2.2964, 'learning_rate': 0.00019996677769265237, 'epoch': 0.06}        \n",
      "{'loss': 2.3421, 'learning_rate': 0.00019996669736270167, 'epoch': 0.06}        \n",
      "{'loss': 2.2189, 'learning_rate': 0.00019996661693576738, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62080/1061708 [9:17:32<146:50:51,  1.89it/s][2024-03-01 03:24:32,737] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 62081/1061708 [9:17:32<138:09:07,  2.01it/s][2024-03-01 03:24:33,159] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.16, 'learning_rate': 0.00019996655252439182, 'epoch': 0.06}          \n",
      "{'loss': 2.1414, 'learning_rate': 0.00019996647192288726, 'epoch': 0.06}        \n",
      "{'loss': 2.2078, 'learning_rate': 0.00019996639122439938, 'epoch': 0.06}        \n",
      "{'loss': 2.003, 'learning_rate': 0.00019996631042892818, 'epoch': 0.06}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9945, 'learning_rate': 0.0001999662295364738, 'epoch': 0.06}         \n",
      "{'loss': 2.2915, 'learning_rate': 0.0001999661485470363, 'epoch': 0.06}         \n",
      "{'loss': 1.5097, 'learning_rate': 0.00019996606746061572, 'epoch': 0.06}        \n",
      "{'loss': 2.2343, 'learning_rate': 0.00019996598627721216, 'epoch': 0.06}        \n",
      "{'loss': 2.5735, 'learning_rate': 0.00019996590499682574, 'epoch': 0.06}        \n",
      "{'loss': 2.1771, 'learning_rate': 0.00019996582361945652, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62182/1061708 [9:18:26<146:56:03,  1.89it/s][2024-03-01 03:25:27,043] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 62183/1061708 [9:18:26<141:45:59,  1.96it/s][2024-03-01 03:25:27,468] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1022, 'learning_rate': 0.00019996575844773355, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62199/1061708 [9:18:35<147:15:05,  1.89it/s][2024-03-01 03:25:35,921] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3815, 'learning_rate': 0.00019996568505535348, 'epoch': 0.06}        \n",
      "{'loss': 2.1307, 'learning_rate': 0.00019996560341613105, 'epoch': 0.06}        \n",
      "{'loss': 1.9797, 'learning_rate': 0.0001999655216799261, 'epoch': 0.06}         \n",
      "{'loss': 2.2251, 'learning_rate': 0.00019996543984673868, 'epoch': 0.06}        \n",
      "{'loss': 2.0744, 'learning_rate': 0.00019996535791656891, 'epoch': 0.06}        \n",
      "{'loss': 1.9407, 'learning_rate': 0.00019996527588941686, 'epoch': 0.06}        \n",
      "{'loss': 2.1053, 'learning_rate': 0.00019996519376528263, 'epoch': 0.06}        \n",
      "{'loss': 2.3218, 'learning_rate': 0.00019996511154416627, 'epoch': 0.06}        \n",
      "{'loss': 1.9911, 'learning_rate': 0.00019996502922606786, 'epoch': 0.06}        \n",
      "{'loss': 2.1617, 'learning_rate': 0.00019996494681098748, 'epoch': 0.06}        \n",
      "{'loss': 2.0916, 'learning_rate': 0.00019996486429892522, 'epoch': 0.06}        \n",
      "{'loss': 2.2917, 'learning_rate': 0.00019996478168988118, 'epoch': 0.06}        \n",
      "{'loss': 2.145, 'learning_rate': 0.00019996469898385543, 'epoch': 0.06}         \n",
      "{'loss': 2.3461, 'learning_rate': 0.00019996461618084798, 'epoch': 0.06}        \n",
      "{'loss': 2.4067, 'learning_rate': 0.000199964533280859, 'epoch': 0.06}          \n",
      "  6%|█▋                           | 62345/1061708 [9:19:53<149:21:40,  1.86it/s][2024-03-01 03:26:53,680] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1209, 'learning_rate': 0.00019996445858794974, 'epoch': 0.06}        \n",
      "{'loss': 2.2057, 'learning_rate': 0.000199964375503696, 'epoch': 0.06}          \n",
      "{'loss': 2.5096, 'learning_rate': 0.00019996429232246095, 'epoch': 0.06}        \n",
      "{'loss': 2.3636, 'learning_rate': 0.00019996420904424465, 'epoch': 0.06}        \n",
      "{'loss': 1.9299, 'learning_rate': 0.00019996412566904717, 'epoch': 0.06}        \n",
      "{'loss': 2.1775, 'learning_rate': 0.00019996404219686862, 'epoch': 0.06}        \n",
      "{'loss': 1.788, 'learning_rate': 0.00019996395862770906, 'epoch': 0.06}         \n",
      "{'loss': 1.9006, 'learning_rate': 0.00019996387496156855, 'epoch': 0.06}        \n",
      "{'loss': 2.3147, 'learning_rate': 0.00019996379119844725, 'epoch': 0.06}        \n",
      "{'loss': 2.2946, 'learning_rate': 0.00019996370733834512, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62446/1061708 [9:20:46<147:51:33,  1.88it/s][2024-03-01 03:27:47,456] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 62447/1061708 [9:20:47<138:36:46,  2.00it/s][2024-03-01 03:27:47,877] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8075, 'learning_rate': 0.00019996364018043736, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62453/1061708 [9:20:50<147:43:19,  1.88it/s][2024-03-01 03:27:50,996] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0921, 'learning_rate': 0.00019996356455360097, 'epoch': 0.06}        \n",
      "{'loss': 2.0296, 'learning_rate': 0.00019996348043165122, 'epoch': 0.06}        \n",
      "{'loss': 2.099, 'learning_rate': 0.00019996339621272102, 'epoch': 0.06}         \n",
      "{'loss': 1.7175, 'learning_rate': 0.0001999633118968104, 'epoch': 0.06}         \n",
      "{'loss': 2.4529, 'learning_rate': 0.00019996322748391956, 'epoch': 0.06}        \n",
      "{'loss': 2.27, 'learning_rate': 0.00019996314297404845, 'epoch': 0.06}          \n",
      "{'loss': 1.9932, 'learning_rate': 0.00019996305836719725, 'epoch': 0.06}        \n",
      "{'loss': 1.9168, 'learning_rate': 0.00019996297366336595, 'epoch': 0.06}        \n",
      "{'loss': 2.1916, 'learning_rate': 0.0001999628888625547, 'epoch': 0.06}         \n",
      "{'loss': 2.1379, 'learning_rate': 0.00019996280396476359, 'epoch': 0.06}        \n",
      "{'loss': 2.2284, 'learning_rate': 0.00019996271896999263, 'epoch': 0.06}        \n",
      "{'loss': 2.3067, 'learning_rate': 0.00019996263387824198, 'epoch': 0.06}        \n",
      "{'loss': 1.9282, 'learning_rate': 0.00019996254868951167, 'epoch': 0.06}        \n",
      "{'loss': 2.0396, 'learning_rate': 0.00019996246340380186, 'epoch': 0.06}        \n",
      "{'loss': 2.0913, 'learning_rate': 0.00019996237802111252, 'epoch': 0.06}        \n",
      "{'loss': 2.0358, 'learning_rate': 0.0001999622925414438, 'epoch': 0.06}         \n",
      "{'loss': 2.0869, 'learning_rate': 0.00019996220696479579, 'epoch': 0.06}        \n",
      "{'loss': 2.1172, 'learning_rate': 0.00019996212129116852, 'epoch': 0.06}        \n",
      "{'loss': 2.1984, 'learning_rate': 0.0001999620355205621, 'epoch': 0.06}         \n",
      "{'loss': 2.5942, 'learning_rate': 0.00019996194965297666, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62654/1061708 [9:22:37<149:32:05,  1.86it/s][2024-03-01 03:29:38,069] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 62655/1061708 [9:22:37<139:44:55,  1.99it/s][2024-03-01 03:29:38,490] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2461, 'learning_rate': 0.00019996188088908348, 'epoch': 0.06}        \n",
      "{'loss': 2.6617, 'learning_rate': 0.0001999617948469359, 'epoch': 0.06}         \n",
      "{'loss': 2.1398, 'learning_rate': 0.00019996170870780955, 'epoch': 0.06}        \n",
      "{'loss': 1.8666, 'learning_rate': 0.00019996162247170444, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62697/1061708 [9:23:00<147:37:24,  1.88it/s][2024-03-01 03:30:00,821] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3053, 'learning_rate': 0.00019996154477629308, 'epoch': 0.06}        \n",
      "{'loss': 2.096, 'learning_rate': 0.00019996145835592862, 'epoch': 0.06}         \n",
      "{'loss': 2.2587, 'learning_rate': 0.0001999613718385856, 'epoch': 0.06}         \n",
      "{'loss': 2.3042, 'learning_rate': 0.00019996128522426423, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62738/1061708 [9:23:22<146:54:25,  1.89it/s][2024-03-01 03:30:22,555] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2666, 'learning_rate': 0.00019996120718845847, 'epoch': 0.06}        \n",
      "{'loss': 2.0173, 'learning_rate': 0.00019996112038987832, 'epoch': 0.06}        \n",
      "{'loss': 2.1241, 'learning_rate': 0.00019996103349432, 'epoch': 0.06}           \n",
      "{'loss': 2.1005, 'learning_rate': 0.00019996094650178358, 'epoch': 0.06}        \n",
      "{'loss': 2.4316, 'learning_rate': 0.00019996085941226916, 'epoch': 0.06}        \n",
      "{'loss': 2.2529, 'learning_rate': 0.00019996077222577684, 'epoch': 0.06}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1443, 'learning_rate': 0.00019996068494230665, 'epoch': 0.06}        \n",
      "{'loss': 2.0621, 'learning_rate': 0.00019996059756185878, 'epoch': 0.06}        \n",
      "{'loss': 2.106, 'learning_rate': 0.0001999605100844332, 'epoch': 0.06}          \n",
      "{'loss': 2.282, 'learning_rate': 0.00019996042251003003, 'epoch': 0.06}         \n",
      "{'loss': 2.1728, 'learning_rate': 0.00019996033483864938, 'epoch': 0.06}        \n",
      "{'loss': 2.1272, 'learning_rate': 0.0001999602470702913, 'epoch': 0.06}         \n",
      "{'loss': 1.7425, 'learning_rate': 0.00019996015920495594, 'epoch': 0.06}        \n",
      "{'loss': 2.1254, 'learning_rate': 0.0001999600712426433, 'epoch': 0.06}         \n",
      "{'loss': 2.1982, 'learning_rate': 0.00019995998318335354, 'epoch': 0.06}        \n",
      "{'loss': 2.1247, 'learning_rate': 0.00019995989502708672, 'epoch': 0.06}        \n",
      "{'loss': 2.2813, 'learning_rate': 0.0001999598067738429, 'epoch': 0.06}         \n",
      "{'loss': 2.6599, 'learning_rate': 0.00019995971842362217, 'epoch': 0.06}        \n",
      "{'loss': 2.4218, 'learning_rate': 0.00019995962997642464, 'epoch': 0.06}        \n",
      "{'loss': 1.9813, 'learning_rate': 0.0001999595414322504, 'epoch': 0.06}         \n",
      "{'loss': 2.2781, 'learning_rate': 0.00019995945279109952, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 62940/1061708 [9:25:09<147:17:24,  1.88it/s][2024-03-01 03:32:10,309] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▋                           | 62946/1061708 [9:25:12<147:41:39,  1.88it/s][2024-03-01 03:32:13,439] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0192, 'learning_rate': 0.00019995938180835567, 'epoch': 0.06}        \n",
      "{'loss': 2.0935, 'learning_rate': 0.00019995929299264703, 'epoch': 0.06}        \n",
      "{'loss': 2.1217, 'learning_rate': 0.00019995920407996204, 'epoch': 0.06}        \n",
      "{'loss': 1.9488, 'learning_rate': 0.0001999591150703007, 'epoch': 0.06}         \n",
      "{'loss': 1.9091, 'learning_rate': 0.00019995902596366315, 'epoch': 0.06}        \n",
      "{'loss': 2.1218, 'learning_rate': 0.00019995893676004944, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63007/1061708 [9:25:45<147:51:46,  1.88it/s][2024-03-01 03:32:45,997] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1491, 'learning_rate': 0.00019995885639388262, 'epoch': 0.06}        \n",
      "{'loss': 2.1402, 'learning_rate': 0.00019995876700601447, 'epoch': 0.06}        \n",
      "{'loss': 2.3584, 'learning_rate': 0.00019995867752117046, 'epoch': 0.06}        \n",
      "{'loss': 1.8401, 'learning_rate': 0.00019995858793935066, 'epoch': 0.06}        \n",
      "{'loss': 2.3456, 'learning_rate': 0.00019995849826055513, 'epoch': 0.06}        \n",
      "{'loss': 2.0371, 'learning_rate': 0.000199958408484784, 'epoch': 0.06}          \n",
      "{'loss': 2.1612, 'learning_rate': 0.00019995831861203728, 'epoch': 0.06}        \n",
      "{'loss': 2.3066, 'learning_rate': 0.00019995822864231518, 'epoch': 0.06}        \n",
      "{'loss': 1.85, 'learning_rate': 0.00019995813857561764, 'epoch': 0.06}          \n",
      "{'loss': 2.112, 'learning_rate': 0.00019995804841194486, 'epoch': 0.06}         \n",
      "{'loss': 2.284, 'learning_rate': 0.0001999579581512969, 'epoch': 0.06}          \n",
      "{'loss': 2.2989, 'learning_rate': 0.00019995786779367382, 'epoch': 0.06}        \n",
      "{'loss': 2.1079, 'learning_rate': 0.00019995777733907571, 'epoch': 0.06}        \n",
      "{'loss': 2.1232, 'learning_rate': 0.00019995768678750272, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63145/1061708 [9:26:59<148:41:29,  1.87it/s][2024-03-01 03:33:59,635] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7269, 'learning_rate': 0.00019995760520817352, 'epoch': 0.06}        \n",
      "{'loss': 2.0427, 'learning_rate': 0.00019995751447234836, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63165/1061708 [9:27:09<148:43:30,  1.87it/s][2024-03-01 03:34:10,251] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1935, 'learning_rate': 0.00019995743272719239, 'epoch': 0.06}        \n",
      "{'loss': 2.3193, 'learning_rate': 0.00019995734180711545, 'epoch': 0.06}        \n",
      "{'loss': 2.1721, 'learning_rate': 0.000199957250790064, 'epoch': 0.06}          \n",
      "{'loss': 2.4979, 'learning_rate': 0.00019995715967603816, 'epoch': 0.06}        \n",
      "{'loss': 2.2051, 'learning_rate': 0.00019995706846503796, 'epoch': 0.06}        \n",
      "{'loss': 2.1157, 'learning_rate': 0.0001999569771570635, 'epoch': 0.06}         \n",
      "{'loss': 1.9986, 'learning_rate': 0.00019995688575211496, 'epoch': 0.06}        \n",
      "{'loss': 2.2833, 'learning_rate': 0.0001999567942501923, 'epoch': 0.06}         \n",
      "{'loss': 2.5052, 'learning_rate': 0.0001999567026512957, 'epoch': 0.06}         \n",
      "{'loss': 2.3103, 'learning_rate': 0.0001999566109554252, 'epoch': 0.06}         \n",
      "{'loss': 2.274, 'learning_rate': 0.0001999565191625809, 'epoch': 0.06}          \n",
      "{'loss': 2.1968, 'learning_rate': 0.0001999564272727629, 'epoch': 0.06}         \n",
      "{'loss': 2.0898, 'learning_rate': 0.0001999563352859713, 'epoch': 0.06}         \n",
      "{'loss': 2.3141, 'learning_rate': 0.00019995624320220614, 'epoch': 0.06}        \n",
      "{'loss': 2.0021, 'learning_rate': 0.00019995615102146754, 'epoch': 0.06}        \n",
      "{'loss': 2.1557, 'learning_rate': 0.0001999560587437556, 'epoch': 0.06}         \n",
      "{'loss': 2.2794, 'learning_rate': 0.00019995596636907038, 'epoch': 0.06}        \n",
      "{'loss': 2.177, 'learning_rate': 0.00019995587389741204, 'epoch': 0.06}         \n",
      "{'loss': 2.3825, 'learning_rate': 0.0001999557813287806, 'epoch': 0.06}         \n",
      "{'loss': 2.5284, 'learning_rate': 0.00019995568866317616, 'epoch': 0.06}        \n",
      "{'loss': 2.0319, 'learning_rate': 0.0001999555959005988, 'epoch': 0.06}         \n",
      "{'loss': 2.1863, 'learning_rate': 0.00019995550304104865, 'epoch': 0.06}        \n",
      "{'loss': 2.4677, 'learning_rate': 0.00019995541008452578, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63391/1061708 [9:29:10<147:02:31,  1.89it/s][2024-03-01 03:36:10,847] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7977, 'learning_rate': 0.0001999553263407436, 'epoch': 0.06}         \n",
      "{'loss': 2.5568, 'learning_rate': 0.00019995523319997282, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63412/1061708 [9:29:21<146:50:36,  1.89it/s][2024-03-01 03:36:21,970] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2311, 'learning_rate': 0.00019995514929036764, 'epoch': 0.06}        \n",
      "{'loss': 1.8608, 'learning_rate': 0.00019995505596534928, 'epoch': 0.06}        \n",
      "{'loss': 2.2783, 'learning_rate': 0.0001999549625433586, 'epoch': 0.06}         \n",
      "{'loss': 2.3512, 'learning_rate': 0.0001999548690243957, 'epoch': 0.06}         \n",
      "{'loss': 1.9159, 'learning_rate': 0.00019995477540846074, 'epoch': 0.06}        \n",
      "{'loss': 2.079, 'learning_rate': 0.00019995468169555376, 'epoch': 0.06}         \n",
      "{'loss': 2.176, 'learning_rate': 0.00019995458788567484, 'epoch': 0.06}         \n",
      "{'loss': 1.9122, 'learning_rate': 0.0001999544939788241, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 63496/1061708 [9:30:06<147:49:44,  1.88it/s][2024-03-01 03:37:06,709] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9418, 'learning_rate': 0.0001999544093797476, 'epoch': 0.06}         \n",
      "{'loss': 2.2795, 'learning_rate': 0.0001999543152886506, 'epoch': 0.06}         \n",
      "{'loss': 1.8789, 'learning_rate': 0.00019995422110058203, 'epoch': 0.06}        \n",
      "{'loss': 2.2784, 'learning_rate': 0.00019995412681554203, 'epoch': 0.06}        \n",
      "{'loss': 2.0688, 'learning_rate': 0.00019995403243353062, 'epoch': 0.06}        \n",
      "{'loss': 1.876, 'learning_rate': 0.0001999539379545479, 'epoch': 0.06}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0556, 'learning_rate': 0.000199953843378594, 'epoch': 0.06}          \n",
      "{'loss': 2.5723, 'learning_rate': 0.000199953748705669, 'epoch': 0.06}          \n",
      "{'loss': 2.3222, 'learning_rate': 0.00019995365393577294, 'epoch': 0.06}        \n",
      "{'loss': 2.2683, 'learning_rate': 0.00019995355906890597, 'epoch': 0.06}        \n",
      "{'loss': 1.86, 'learning_rate': 0.00019995346410506818, 'epoch': 0.06}          \n",
      "{'loss': 2.0321, 'learning_rate': 0.00019995336904425964, 'epoch': 0.06}        \n",
      "{'loss': 2.1357, 'learning_rate': 0.00019995327388648045, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63620/1061708 [9:31:12<146:42:03,  1.89it/s][2024-03-01 03:38:12,709] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1833, 'learning_rate': 0.00019995318816156936, 'epoch': 0.06}        \n",
      "{'loss': 2.2143, 'learning_rate': 0.00019995309281954619, 'epoch': 0.06}        \n",
      "{'loss': 1.8751, 'learning_rate': 0.00019995299738055262, 'epoch': 0.06}        \n",
      "{'loss': 2.518, 'learning_rate': 0.00019995290184458877, 'epoch': 0.06}         \n",
      "{'loss': 2.0786, 'learning_rate': 0.00019995280621165474, 'epoch': 0.06}        \n",
      "{'loss': 1.8563, 'learning_rate': 0.0001999527104817506, 'epoch': 0.06}         \n",
      "{'loss': 2.3807, 'learning_rate': 0.00019995261465487643, 'epoch': 0.06}        \n",
      "{'loss': 2.4965, 'learning_rate': 0.00019995251873103234, 'epoch': 0.06}        \n",
      "{'loss': 2.1256, 'learning_rate': 0.00019995242271021843, 'epoch': 0.06}        \n",
      "{'loss': 2.198, 'learning_rate': 0.0001999523265924348, 'epoch': 0.06}          \n",
      "{'loss': 2.0157, 'learning_rate': 0.00019995223037768152, 'epoch': 0.06}        \n",
      "{'loss': 1.967, 'learning_rate': 0.0001999521340659587, 'epoch': 0.06}          \n",
      "{'loss': 2.2061, 'learning_rate': 0.00019995203765726642, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63756/1061708 [9:32:24<147:49:06,  1.88it/s][2024-03-01 03:39:25,128] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9405, 'learning_rate': 0.00019995195080653458, 'epoch': 0.06}        \n",
      "{'loss': 2.0096, 'learning_rate': 0.00019995185421360062, 'epoch': 0.06}        \n",
      "{'loss': 2.0549, 'learning_rate': 0.00019995175752369747, 'epoch': 0.06}        \n",
      "{'loss': 1.9066, 'learning_rate': 0.0001999516607368252, 'epoch': 0.06}         \n",
      "{'loss': 2.0594, 'learning_rate': 0.00019995156385298397, 'epoch': 0.06}        \n",
      "{'loss': 2.3069, 'learning_rate': 0.00019995146687217387, 'epoch': 0.06}        \n",
      "{'loss': 2.0712, 'learning_rate': 0.00019995136979439492, 'epoch': 0.06}        \n",
      "{'loss': 2.1283, 'learning_rate': 0.00019995127261964726, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 63835/1061708 [9:33:06<148:20:00,  1.87it/s][2024-03-01 03:40:07,222] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.6967, 'learning_rate': 0.0001999511850794662, 'epoch': 0.06}         \n",
      "{'loss': 2.2991, 'learning_rate': 0.00019995108772047826, 'epoch': 0.06}        \n",
      "{'loss': 2.0877, 'learning_rate': 0.0001999509902645219, 'epoch': 0.06}         \n",
      "{'loss': 2.3032, 'learning_rate': 0.00019995089271159714, 'epoch': 0.06}        \n",
      "{'loss': 2.2587, 'learning_rate': 0.00019995079506170417, 'epoch': 0.06}        \n",
      "{'loss': 2.3406, 'learning_rate': 0.00019995069731484306, 'epoch': 0.06}        \n",
      "{'loss': 2.0789, 'learning_rate': 0.00019995059947101388, 'epoch': 0.06}        \n",
      "{'loss': 2.1674, 'learning_rate': 0.00019995050153021672, 'epoch': 0.06}        \n",
      "{'loss': 2.0521, 'learning_rate': 0.0001999504034924517, 'epoch': 0.06}         \n",
      "{'loss': 2.1486, 'learning_rate': 0.00019995030535771894, 'epoch': 0.06}        \n",
      "{'loss': 1.97, 'learning_rate': 0.00019995020712601847, 'epoch': 0.06}          \n",
      "{'loss': 2.2436, 'learning_rate': 0.00019995010879735042, 'epoch': 0.06}        \n",
      "{'loss': 2.1923, 'learning_rate': 0.0001999500103717149, 'epoch': 0.06}         \n",
      "{'loss': 2.3706, 'learning_rate': 0.00019994991184911194, 'epoch': 0.06}        \n",
      "{'loss': 2.2412, 'learning_rate': 0.00019994981322954172, 'epoch': 0.06}        \n",
      "{'loss': 2.0118, 'learning_rate': 0.0001999497145130043, 'epoch': 0.06}         \n",
      "  6%|█▋                           | 63999/1061708 [9:34:34<146:37:10,  1.89it/s][2024-03-01 03:41:34,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=64000, skipped=722, lr=[0.00019994961569949976], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 03:41:34,651] [INFO] [timer.py:260:stop] epoch=0/micro_step=64000/global_step=64000, RunningAvgSamplesPerSec=1.8924406747436804, CurrSamplesPerSec=1.9082209517847442, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.9544, 'learning_rate': 0.00019994961569949976, 'epoch': 0.06}        \n",
      "{'loss': 2.4563, 'learning_rate': 0.00019994951678902823, 'epoch': 0.06}        \n",
      "  6%|█▋                           | 64019/1061708 [9:34:44<146:50:23,  1.89it/s][2024-03-01 03:41:45,198] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0788, 'learning_rate': 0.0001999494276866971, 'epoch': 0.06}         \n",
      "{'loss': 2.2554, 'learning_rate': 0.00019994932859198855, 'epoch': 0.06}        \n",
      "{'loss': 2.3034, 'learning_rate': 0.00019994922940031322, 'epoch': 0.06}        \n",
      "{'loss': 2.0225, 'learning_rate': 0.00019994913011167124, 'epoch': 0.06}        \n",
      "{'loss': 2.0818, 'learning_rate': 0.00019994903072606275, 'epoch': 0.06}        \n",
      "{'loss': 1.9005, 'learning_rate': 0.00019994893124348782, 'epoch': 0.06}        \n",
      "{'loss': 1.979, 'learning_rate': 0.00019994883166394654, 'epoch': 0.06}         \n",
      "{'loss': 2.3659, 'learning_rate': 0.000199948731987439, 'epoch': 0.06}          \n",
      "  6%|█▊                           | 64093/1061708 [9:35:24<149:50:08,  1.85it/s][2024-03-01 03:42:24,551] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4495, 'learning_rate': 0.00019994864219567617, 'epoch': 0.06}        \n",
      "{'loss': 2.4099, 'learning_rate': 0.00019994854233493304, 'epoch': 0.06}        \n",
      "{'loss': 2.4078, 'learning_rate': 0.00019994844237722391, 'epoch': 0.06}        \n",
      "{'loss': 2.2227, 'learning_rate': 0.00019994834232254895, 'epoch': 0.06}        \n",
      "{'loss': 2.194, 'learning_rate': 0.0001999482421709082, 'epoch': 0.06}          \n",
      "{'loss': 1.796, 'learning_rate': 0.00019994814192230177, 'epoch': 0.06}         \n",
      "{'loss': 2.5512, 'learning_rate': 0.00019994804157672977, 'epoch': 0.06}        \n",
      "{'loss': 1.921, 'learning_rate': 0.0001999479411341923, 'epoch': 0.06}          \n",
      "{'loss': 2.0223, 'learning_rate': 0.0001999478405946894, 'epoch': 0.06}         \n",
      "{'loss': 2.264, 'learning_rate': 0.00019994773995822126, 'epoch': 0.06}         \n",
      "{'loss': 2.1834, 'learning_rate': 0.0001999476392247879, 'epoch': 0.06}         \n",
      "{'loss': 2.0036, 'learning_rate': 0.00019994753839438944, 'epoch': 0.06}        \n",
      "{'loss': 2.19, 'learning_rate': 0.00019994743746702603, 'epoch': 0.06}          \n",
      "{'loss': 2.442, 'learning_rate': 0.0001999473364426977, 'epoch': 0.06}          \n",
      "{'loss': 2.2643, 'learning_rate': 0.00019994723532140455, 'epoch': 0.06}        \n",
      "{'loss': 2.3254, 'learning_rate': 0.00019994713410314673, 'epoch': 0.06}        \n",
      "{'loss': 2.3225, 'learning_rate': 0.0001999470327879243, 'epoch': 0.06}         \n",
      "{'loss': 1.7085, 'learning_rate': 0.00019994693137573738, 'epoch': 0.06}        \n",
      "{'loss': 2.0513, 'learning_rate': 0.00019994682986658604, 'epoch': 0.06}        \n",
      "{'loss': 2.5116, 'learning_rate': 0.00019994672826047037, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64294/1061708 [9:37:11<149:28:16,  1.85it/s][2024-03-01 03:44:11,714] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                           | 64295/1061708 [9:37:11<139:39:26,  1.98it/s][2024-03-01 03:44:12,138] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6435, 'learning_rate': 0.00019994664690576363, 'epoch': 0.06}        \n",
      "{'loss': 1.9576, 'learning_rate': 0.00019994654512511248, 'epoch': 0.06}        \n",
      "{'loss': 2.0444, 'learning_rate': 0.00019994644324749733, 'epoch': 0.06}        \n",
      "{'loss': 2.1607, 'learning_rate': 0.0001999463412729182, 'epoch': 0.06}         \n",
      "{'loss': 2.3665, 'learning_rate': 0.0001999462392013753, 'epoch': 0.06}         \n",
      "{'loss': 2.1542, 'learning_rate': 0.00019994613703286859, 'epoch': 0.06}        \n",
      "{'loss': 2.3869, 'learning_rate': 0.00019994603476739827, 'epoch': 0.06}        \n",
      "{'loss': 2.1061, 'learning_rate': 0.00019994593240496445, 'epoch': 0.06}        \n",
      "{'loss': 2.1149, 'learning_rate': 0.00019994582994556717, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64381/1061708 [9:37:57<147:10:42,  1.88it/s][2024-03-01 03:44:58,025] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9457, 'learning_rate': 0.00019994573764920594, 'epoch': 0.06}        \n",
      "{'loss': 2.4184, 'learning_rate': 0.00019994563500557843, 'epoch': 0.06}        \n",
      "{'loss': 2.3503, 'learning_rate': 0.00019994553226498774, 'epoch': 0.06}        \n",
      "{'loss': 2.365, 'learning_rate': 0.00019994542942743403, 'epoch': 0.06}         \n",
      "{'loss': 1.6896, 'learning_rate': 0.00019994532649291732, 'epoch': 0.06}        \n",
      "{'loss': 2.3539, 'learning_rate': 0.00019994522346143781, 'epoch': 0.06}        \n",
      "{'loss': 2.2059, 'learning_rate': 0.00019994512033299553, 'epoch': 0.06}        \n",
      "{'loss': 2.1385, 'learning_rate': 0.0001999450171075906, 'epoch': 0.06}         \n",
      "{'loss': 2.0326, 'learning_rate': 0.00019994491378522316, 'epoch': 0.06}        \n",
      "{'loss': 2.2902, 'learning_rate': 0.00019994481036589323, 'epoch': 0.06}        \n",
      "{'loss': 2.2505, 'learning_rate': 0.00019994470684960098, 'epoch': 0.06}        \n",
      "{'loss': 2.7111, 'learning_rate': 0.00019994460323634648, 'epoch': 0.06}        \n",
      "{'loss': 2.088, 'learning_rate': 0.0001999444995261298, 'epoch': 0.06}          \n",
      "  6%|█▊                           | 64515/1061708 [9:39:09<148:31:01,  1.87it/s][2024-03-01 03:46:09,543] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.02, 'learning_rate': 0.00019994440610403226, 'epoch': 0.06}          \n",
      "{'loss': 2.3475, 'learning_rate': 0.0001999443022095878, 'epoch': 0.06}         \n",
      "{'loss': 2.1847, 'learning_rate': 0.0001999441982181815, 'epoch': 0.06}         \n",
      "{'loss': 1.8678, 'learning_rate': 0.00019994409412981342, 'epoch': 0.06}        \n",
      "{'loss': 2.2145, 'learning_rate': 0.0001999439899444837, 'epoch': 0.06}         \n",
      "{'loss': 2.2596, 'learning_rate': 0.00019994388566219244, 'epoch': 0.06}        \n",
      "{'loss': 2.1087, 'learning_rate': 0.00019994378128293972, 'epoch': 0.06}        \n",
      "{'loss': 1.7111, 'learning_rate': 0.00019994367680672566, 'epoch': 0.06}        \n",
      "{'loss': 2.105, 'learning_rate': 0.00019994357223355035, 'epoch': 0.06}         \n",
      "{'loss': 2.199, 'learning_rate': 0.00019994346756341388, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 64616/1061708 [9:40:02<147:55:27,  1.87it/s][2024-03-01 03:47:03,462] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                           | 64617/1061708 [9:40:03<138:43:02,  2.00it/s][2024-03-01 03:47:03,886] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9305, 'learning_rate': 0.00019994338375749275, 'epoch': 0.06}        \n",
      "{'loss': 2.1972, 'learning_rate': 0.0001999432789128265, 'epoch': 0.06}         \n",
      "{'loss': 2.0462, 'learning_rate': 0.00019994317397119935, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64642/1061708 [9:40:16<147:01:29,  1.88it/s][2024-03-01 03:47:17,180] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4302, 'learning_rate': 0.0001999430794408335, 'epoch': 0.06}         \n",
      "{'loss': 2.1011, 'learning_rate': 0.000199942974314981, 'epoch': 0.06}          \n",
      "{'loss': 2.1169, 'learning_rate': 0.00019994286909216796, 'epoch': 0.06}        \n",
      "{'loss': 2.0038, 'learning_rate': 0.00019994276377239441, 'epoch': 0.06}        \n",
      "{'loss': 2.0791, 'learning_rate': 0.00019994265835566056, 'epoch': 0.06}        \n",
      "{'loss': 1.9044, 'learning_rate': 0.00019994255284196642, 'epoch': 0.06}        \n",
      "{'loss': 2.0796, 'learning_rate': 0.00019994244723131214, 'epoch': 0.06}        \n",
      "{'loss': 2.2588, 'learning_rate': 0.0001999423415236978, 'epoch': 0.06}         \n",
      "{'loss': 2.2032, 'learning_rate': 0.0001999422357191235, 'epoch': 0.06}         \n",
      "{'loss': 2.0477, 'learning_rate': 0.00019994212981758934, 'epoch': 0.06}        \n",
      "{'loss': 2.156, 'learning_rate': 0.00019994202381909545, 'epoch': 0.06}         \n",
      "{'loss': 2.2224, 'learning_rate': 0.0001999419177236419, 'epoch': 0.06}         \n",
      "{'loss': 1.9882, 'learning_rate': 0.00019994181153122883, 'epoch': 0.06}        \n",
      "{'loss': 2.2492, 'learning_rate': 0.00019994170524185632, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64782/1061708 [9:41:31<146:37:22,  1.89it/s][2024-03-01 03:48:31,814] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5071, 'learning_rate': 0.00019994160949852077, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64797/1061708 [9:41:39<147:16:14,  1.88it/s][2024-03-01 03:48:39,732] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2316, 'learning_rate': 0.0001999415136766483, 'epoch': 0.06}         \n",
      "{'loss': 1.9178, 'learning_rate': 0.0001999414071157899, 'epoch': 0.06}         \n",
      "{'loss': 1.9779, 'learning_rate': 0.00019994130045797247, 'epoch': 0.06}        \n",
      "{'loss': 1.8474, 'learning_rate': 0.0001999411937031961, 'epoch': 0.06}         \n",
      "{'loss': 2.0274, 'learning_rate': 0.00019994108685146087, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 64842/1061708 [9:42:03<146:43:08,  1.89it/s][2024-03-01 03:49:03,670] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0418, 'learning_rate': 0.00019994099060199948, 'epoch': 0.06}        \n",
      "{'loss': 2.031, 'learning_rate': 0.00019994088356604272, 'epoch': 0.06}         \n",
      "{'loss': 2.2221, 'learning_rate': 0.00019994077643312744, 'epoch': 0.06}        \n",
      "{'loss': 1.9513, 'learning_rate': 0.00019994066920325375, 'epoch': 0.06}        \n",
      "{'loss': 2.3999, 'learning_rate': 0.0001999405618764217, 'epoch': 0.06}         \n",
      "{'loss': 2.1212, 'learning_rate': 0.00019994045445263146, 'epoch': 0.06}        \n",
      "{'loss': 1.9999, 'learning_rate': 0.0001999403469318831, 'epoch': 0.06}         \n",
      "{'loss': 2.0647, 'learning_rate': 0.0001999402393141767, 'epoch': 0.06}         \n",
      "{'loss': 1.9588, 'learning_rate': 0.0001999401315995124, 'epoch': 0.06}         \n",
      "{'loss': 2.0199, 'learning_rate': 0.00019994002378789026, 'epoch': 0.06}        \n",
      "{'loss': 1.982, 'learning_rate': 0.00019993991587931046, 'epoch': 0.06}         \n",
      "{'loss': 1.9631, 'learning_rate': 0.00019993980787377303, 'epoch': 0.06}        \n",
      "{'loss': 2.5391, 'learning_rate': 0.00019993969977127814, 'epoch': 0.06}        \n",
      "{'loss': 2.3456, 'learning_rate': 0.0001999395915718258, 'epoch': 0.06}         \n",
      "{'loss': 2.4184, 'learning_rate': 0.00019993948327541624, 'epoch': 0.06}        \n",
      "{'loss': 2.1686, 'learning_rate': 0.00019993937488204947, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65000/1061708 [9:43:27<146:51:08,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 03:50:27,528 >> Saving model checkpoint to output_model/checkpoint-65000\n",
      "[INFO|trainer.py:2880] 2024-03-01 03:50:27,531 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 03:50:28,759 >> tokenizer config file saved in output_model/checkpoint-65000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 03:50:28,759 >> Special tokens file saved in output_model/checkpoint-65000/special_tokens_map.json\n",
      "[2024-03-01 03:50:28,761] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step65000 is about to be saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-01 03:50:33,994] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-65000/global_step65000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 03:50:33,994] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-65000/global_step65000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 03:50:47,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-65000/global_step65000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 03:50:48,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-65000/global_step65000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 03:50:55,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-65000/global_step65000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 03:50:55,778] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-65000/global_step65000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 03:50:55,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 03:50:55,836 >> Deleting older checkpoint [output_model/checkpoint-50000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 03:50:59,595 >> tokenizer config file saved in output_model/checkpoint-65000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 03:50:59,595 >> Special tokens file saved in output_model/checkpoint-65000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.0778, 'learning_rate': 0.00019993926639172565, 'epoch': 0.06}        \n",
      "{'loss': 2.1628, 'learning_rate': 0.00019993915780444483, 'epoch': 0.06}        \n",
      "{'loss': 2.1616, 'learning_rate': 0.00019993904912020717, 'epoch': 0.06}        \n",
      "{'loss': 2.2001, 'learning_rate': 0.00019993894033901275, 'epoch': 0.06}        \n",
      "{'loss': 2.1498, 'learning_rate': 0.00019993883146086168, 'epoch': 0.06}        \n",
      "{'loss': 2.0794, 'learning_rate': 0.00019993872248575404, 'epoch': 0.06}        \n",
      "{'loss': 2.1954, 'learning_rate': 0.00019993861341368997, 'epoch': 0.06}        \n",
      "{'loss': 2.1735, 'learning_rate': 0.00019993850424466957, 'epoch': 0.06}        \n",
      "{'loss': 1.9191, 'learning_rate': 0.00019993839497869294, 'epoch': 0.06}        \n",
      "{'loss': 1.9702, 'learning_rate': 0.0001999382856157602, 'epoch': 0.06}         \n",
      "{'loss': 2.4146, 'learning_rate': 0.0001999381761558714, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 65115/1061708 [9:45:00<148:56:16,  1.86it/s][2024-03-01 03:52:01,194] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1784, 'learning_rate': 0.0001999380775590742, 'epoch': 0.06}         \n",
      "{'loss': 1.6304, 'learning_rate': 0.00019993796791496928, 'epoch': 0.06}        \n",
      "{'loss': 1.7496, 'learning_rate': 0.00019993785817390867, 'epoch': 0.06}        \n",
      "{'loss': 2.2324, 'learning_rate': 0.00019993774833589246, 'epoch': 0.06}        \n",
      "{'loss': 2.1368, 'learning_rate': 0.00019993763840092072, 'epoch': 0.06}        \n",
      "{'loss': 1.898, 'learning_rate': 0.00019993752836899362, 'epoch': 0.06}         \n",
      "{'loss': 2.0518, 'learning_rate': 0.0001999374182401112, 'epoch': 0.06}         \n",
      "{'loss': 2.1932, 'learning_rate': 0.00019993730801427364, 'epoch': 0.06}        \n",
      "{'loss': 2.033, 'learning_rate': 0.00019993719769148102, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 65202/1061708 [9:45:47<147:25:58,  1.88it/s][2024-03-01 03:52:47,715] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2583, 'learning_rate': 0.00019993709831807115, 'epoch': 0.06}        \n",
      "{'loss': 1.6235, 'learning_rate': 0.00019993698781106418, 'epoch': 0.06}        \n",
      "{'loss': 2.2342, 'learning_rate': 0.00019993687720710245, 'epoch': 0.06}        \n",
      "{'loss': 2.1202, 'learning_rate': 0.00019993676650618606, 'epoch': 0.06}        \n",
      "{'loss': 2.4792, 'learning_rate': 0.00019993665570831515, 'epoch': 0.06}        \n",
      "{'loss': 2.3017, 'learning_rate': 0.0001999365448134898, 'epoch': 0.06}         \n",
      "{'loss': 2.1876, 'learning_rate': 0.00019993643382171011, 'epoch': 0.06}        \n",
      "{'loss': 2.4047, 'learning_rate': 0.0001999363227329762, 'epoch': 0.06}         \n",
      "{'loss': 1.9517, 'learning_rate': 0.00019993621154728816, 'epoch': 0.06}        \n",
      "{'loss': 1.9227, 'learning_rate': 0.00019993610026464612, 'epoch': 0.06}        \n",
      "{'loss': 2.21, 'learning_rate': 0.00019993598888505022, 'epoch': 0.06}          \n",
      "{'loss': 2.3041, 'learning_rate': 0.0001999358774085005, 'epoch': 0.06}         \n",
      "{'loss': 2.2659, 'learning_rate': 0.00019993576583499708, 'epoch': 0.06}        \n",
      "{'loss': 1.7985, 'learning_rate': 0.00019993565416454015, 'epoch': 0.06}        \n",
      "{'loss': 2.2875, 'learning_rate': 0.0001999355423971297, 'epoch': 0.06}         \n",
      "{'loss': 1.8349, 'learning_rate': 0.00019993543053276592, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65362/1061708 [9:47:12<146:50:51,  1.88it/s][2024-03-01 03:54:13,272] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1103, 'learning_rate': 0.00019993532977194347, 'epoch': 0.06}        \n",
      "{'loss': 2.1855, 'learning_rate': 0.0001999352177233686, 'epoch': 0.06}         \n",
      "{'loss': 2.2316, 'learning_rate': 0.00019993510557784073, 'epoch': 0.06}        \n",
      "{'loss': 2.4893, 'learning_rate': 0.00019993499333535988, 'epoch': 0.06}        \n",
      "{'loss': 2.2811, 'learning_rate': 0.00019993488099592622, 'epoch': 0.06}        \n",
      "{'loss': 2.3153, 'learning_rate': 0.00019993476855953985, 'epoch': 0.06}        \n",
      "{'loss': 2.0795, 'learning_rate': 0.00019993465602620087, 'epoch': 0.06}        \n",
      "{'loss': 2.1549, 'learning_rate': 0.0001999345433959094, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 65443/1061708 [9:47:56<150:43:33,  1.84it/s][2024-03-01 03:54:56,563] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2775, 'learning_rate': 0.00019993444194575278, 'epoch': 0.06}        \n",
      "{'loss': 2.2823, 'learning_rate': 0.00019993432913125188, 'epoch': 0.06}        \n",
      "{'loss': 2.2604, 'learning_rate': 0.00019993421621979877, 'epoch': 0.06}        \n",
      "{'loss': 2.121, 'learning_rate': 0.0001999341032113936, 'epoch': 0.06}          \n",
      "{'loss': 1.972, 'learning_rate': 0.00019993399010603645, 'epoch': 0.06}         \n",
      "{'loss': 2.8316, 'learning_rate': 0.0001999338769037275, 'epoch': 0.06}         \n",
      "{'loss': 2.3424, 'learning_rate': 0.00019993376360446678, 'epoch': 0.06}        \n",
      "{'loss': 2.4169, 'learning_rate': 0.00019993365020825446, 'epoch': 0.06}        \n",
      "{'loss': 2.0817, 'learning_rate': 0.0001999335367150906, 'epoch': 0.06}         \n",
      "{'loss': 2.3616, 'learning_rate': 0.00019993342312497528, 'epoch': 0.06}        \n",
      "{'loss': 1.9714, 'learning_rate': 0.00019993330943790874, 'epoch': 0.06}        \n",
      "{'loss': 2.4129, 'learning_rate': 0.00019993319565389097, 'epoch': 0.06}        \n",
      "{'loss': 2.1469, 'learning_rate': 0.00019993308177292213, 'epoch': 0.06}        \n",
      "{'loss': 1.9385, 'learning_rate': 0.00019993296779500234, 'epoch': 0.06}        \n",
      "{'loss': 2.4901, 'learning_rate': 0.00019993285372013167, 'epoch': 0.06}        \n",
      "{'loss': 2.0659, 'learning_rate': 0.00019993273954831026, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65604/1061708 [9:49:22<149:07:19,  1.86it/s][2024-03-01 03:56:22,602] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0234, 'learning_rate': 0.00019993263671077815, 'epoch': 0.06}        \n",
      "{'loss': 2.3691, 'learning_rate': 0.00019993252235475062, 'epoch': 0.06}        \n",
      "{'loss': 1.9387, 'learning_rate': 0.00019993240790177268, 'epoch': 0.06}        \n",
      "{'loss': 2.0178, 'learning_rate': 0.0001999322933518444, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 65641/1061708 [9:49:41<146:57:52,  1.88it/s][2024-03-01 03:56:42,296] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0861, 'learning_rate': 0.00019993219017401653, 'epoch': 0.06}        \n",
      "{'loss': 2.1523, 'learning_rate': 0.00019993207543988296, 'epoch': 0.06}        \n",
      "{'loss': 2.3355, 'learning_rate': 0.0001999319606087994, 'epoch': 0.06}         \n",
      "{'loss': 2.1597, 'learning_rate': 0.00019993184568076598, 'epoch': 0.06}        \n",
      "{'loss': 2.0785, 'learning_rate': 0.00019993173065578276, 'epoch': 0.06}        \n",
      "{'loss': 1.9959, 'learning_rate': 0.0001999316155338499, 'epoch': 0.06}         \n",
      "{'loss': 2.2062, 'learning_rate': 0.0001999315003149675, 'epoch': 0.06}         \n",
      "{'loss': 2.0456, 'learning_rate': 0.00019993138499913563, 'epoch': 0.06}        \n",
      "{'loss': 2.3416, 'learning_rate': 0.0001999312695863545, 'epoch': 0.06}         \n",
      "{'loss': 2.0045, 'learning_rate': 0.0001999311540766241, 'epoch': 0.06}         \n",
      "{'loss': 2.1123, 'learning_rate': 0.00019993103846994463, 'epoch': 0.06}        \n",
      "{'loss': 2.3416, 'learning_rate': 0.00019993092276631615, 'epoch': 0.06}        \n",
      "{'loss': 1.9706, 'learning_rate': 0.00019993080696573885, 'epoch': 0.06}        \n",
      "{'loss': 2.251, 'learning_rate': 0.00019993069106821274, 'epoch': 0.06}         \n",
      "{'loss': 1.9472, 'learning_rate': 0.000199930575073738, 'epoch': 0.06}          \n",
      "{'loss': 2.1831, 'learning_rate': 0.0001999304589823147, 'epoch': 0.06}         \n",
      "{'loss': 2.3041, 'learning_rate': 0.00019993034279394302, 'epoch': 0.06}        \n",
      "{'loss': 1.9088, 'learning_rate': 0.00019993022650862297, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65827/1061708 [9:51:21<147:23:44,  1.88it/s][2024-03-01 03:58:21,584] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1446, 'learning_rate': 0.00019993012176894426, 'epoch': 0.06}        \n",
      "{'loss': 2.2774, 'learning_rate': 0.00019993000529942273, 'epoch': 0.06}        \n",
      "{'loss': 2.2077, 'learning_rate': 0.00019992988873295325, 'epoch': 0.06}        \n",
      "{'loss': 2.367, 'learning_rate': 0.00019992977206953588, 'epoch': 0.06}         \n",
      "{'loss': 1.9801, 'learning_rate': 0.00019992965530917078, 'epoch': 0.06}        \n",
      "{'loss': 2.4209, 'learning_rate': 0.00019992953845185803, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65889/1061708 [9:51:54<147:34:06,  1.87it/s][2024-03-01 03:58:54,660] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1839, 'learning_rate': 0.00019992943319738639, 'epoch': 0.06}        \n",
      "{'loss': 1.9853, 'learning_rate': 0.00019992931615587343, 'epoch': 0.06}        \n",
      "{'loss': 2.0147, 'learning_rate': 0.00019992919901741315, 'epoch': 0.06}        \n",
      "{'loss': 2.4334, 'learning_rate': 0.00019992908178200568, 'epoch': 0.06}        \n",
      "{'loss': 2.2774, 'learning_rate': 0.0001999289644496511, 'epoch': 0.06}         \n",
      "{'loss': 2.0422, 'learning_rate': 0.00019992884702034954, 'epoch': 0.06}        \n",
      "{'loss': 1.7882, 'learning_rate': 0.00019992872949410115, 'epoch': 0.06}        \n",
      "{'loss': 2.3975, 'learning_rate': 0.00019992861187090598, 'epoch': 0.06}        \n",
      "{'loss': 2.1485, 'learning_rate': 0.0001999284941507642, 'epoch': 0.06}         \n",
      "{'loss': 2.2648, 'learning_rate': 0.0001999283763336759, 'epoch': 0.06}         \n",
      "{'loss': 1.8432, 'learning_rate': 0.00019992825841964117, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 65999/1061708 [9:52:52<147:05:13,  1.88it/s][2024-03-01 03:59:53,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=66000, skipped=742, lr=[0.0001999281404086602], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 03:59:53,449] [INFO] [timer.py:260:stop] epoch=0/micro_step=66000/global_step=66000, RunningAvgSamplesPerSec=1.8923792418501002, CurrSamplesPerSec=1.9008071726930817, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1917, 'learning_rate': 0.0001999281404086602, 'epoch': 0.06}         \n",
      "{'loss': 2.3001, 'learning_rate': 0.00019992802230073302, 'epoch': 0.06}        \n",
      "{'loss': 2.2235, 'learning_rate': 0.00019992790409585978, 'epoch': 0.06}        \n",
      "{'loss': 1.9007, 'learning_rate': 0.00019992778579404062, 'epoch': 0.06}        \n",
      "{'loss': 2.4858, 'learning_rate': 0.0001999276673952756, 'epoch': 0.06}         \n",
      "{'loss': 2.0997, 'learning_rate': 0.00019992754889956488, 'epoch': 0.06}        \n",
      "{'loss': 2.0919, 'learning_rate': 0.0001999274303069085, 'epoch': 0.06}         \n",
      "{'loss': 2.2725, 'learning_rate': 0.00019992731161730675, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66076/1061708 [9:53:33<147:55:40,  1.87it/s][2024-03-01 04:00:34,512] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3451, 'learning_rate': 0.0001999272047137768, 'epoch': 0.06}         \n",
      "{'loss': 2.2255, 'learning_rate': 0.00019992708583997887, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66090/1061708 [9:53:41<146:41:00,  1.89it/s][2024-03-01 04:00:41,917] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0085, 'learning_rate': 0.00019992697877067265, 'epoch': 0.06}        \n",
      "{'loss': 2.1187, 'learning_rate': 0.00019992685971267907, 'epoch': 0.06}        \n",
      "{'loss': 2.2924, 'learning_rate': 0.00019992674055774052, 'epoch': 0.06}        \n",
      "{'loss': 2.4066, 'learning_rate': 0.00019992662130585714, 'epoch': 0.06}        \n",
      "{'loss': 2.1684, 'learning_rate': 0.00019992650195702912, 'epoch': 0.06}        \n",
      "{'loss': 2.4639, 'learning_rate': 0.00019992638251125646, 'epoch': 0.06}        \n",
      "{'loss': 2.1404, 'learning_rate': 0.00019992626296853937, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66165/1061708 [9:54:21<148:26:20,  1.86it/s][2024-03-01 04:01:21,960] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3286, 'learning_rate': 0.00019992615529720653, 'epoch': 0.06}        \n",
      "{'loss': 2.6256, 'learning_rate': 0.00019992603557029523, 'epoch': 0.06}        \n",
      "{'loss': 2.1049, 'learning_rate': 0.00019992591574643983, 'epoch': 0.06}        \n",
      "{'loss': 1.9751, 'learning_rate': 0.0001999257958256404, 'epoch': 0.06}         \n",
      "{'loss': 2.0054, 'learning_rate': 0.00019992567580789708, 'epoch': 0.06}        \n",
      "{'loss': 1.7665, 'learning_rate': 0.00019992555569320995, 'epoch': 0.06}        \n",
      "{'loss': 1.9259, 'learning_rate': 0.00019992543548157915, 'epoch': 0.06}        \n",
      "{'loss': 2.091, 'learning_rate': 0.00019992531517300482, 'epoch': 0.06}         \n",
      "{'loss': 2.2966, 'learning_rate': 0.00019992519476748704, 'epoch': 0.06}        \n",
      "{'loss': 1.7798, 'learning_rate': 0.00019992507426502595, 'epoch': 0.06}        \n",
      "{'loss': 1.8888, 'learning_rate': 0.00019992495366562168, 'epoch': 0.06}        \n",
      "{'loss': 2.1045, 'learning_rate': 0.00019992483296927432, 'epoch': 0.06}        \n",
      "{'loss': 2.2329, 'learning_rate': 0.000199924712175984, 'epoch': 0.06}          \n",
      "{'loss': 2.1248, 'learning_rate': 0.0001999245912857508, 'epoch': 0.06}         \n",
      "{'loss': 2.211, 'learning_rate': 0.00019992447029857488, 'epoch': 0.06}         \n",
      "{'loss': 1.9948, 'learning_rate': 0.00019992434921445638, 'epoch': 0.06}        \n",
      "{'loss': 2.2696, 'learning_rate': 0.00019992422803339534, 'epoch': 0.06}        \n",
      "{'loss': 1.7124, 'learning_rate': 0.00019992410675539194, 'epoch': 0.06}        \n",
      "{'loss': 2.2409, 'learning_rate': 0.00019992398538044628, 'epoch': 0.06}        \n",
      "{'loss': 2.1396, 'learning_rate': 0.00019992386390855845, 'epoch': 0.06}        \n",
      "{'loss': 2.1201, 'learning_rate': 0.0001999237423397286, 'epoch': 0.06}         \n",
      "{'loss': 2.2666, 'learning_rate': 0.00019992362067395688, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66389/1061708 [9:56:21<146:42:11,  1.88it/s][2024-03-01 04:03:21,644] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9601, 'learning_rate': 0.0001999235110918771, 'epoch': 0.06}         \n",
      "{'loss': 2.3199, 'learning_rate': 0.00019992338924191606, 'epoch': 0.06}        \n",
      "{'loss': 2.1884, 'learning_rate': 0.00019992326729501345, 'epoch': 0.06}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8444, 'learning_rate': 0.0001999231452511694, 'epoch': 0.06}         \n",
      "  6%|█▊                           | 66427/1061708 [9:56:41<147:21:46,  1.88it/s][2024-03-01 04:03:41,862] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0128, 'learning_rate': 0.00019992303532882487, 'epoch': 0.06}        \n",
      "{'loss': 2.139, 'learning_rate': 0.00019992291310079243, 'epoch': 0.06}         \n",
      "{'loss': 2.5223, 'learning_rate': 0.00019992279077581882, 'epoch': 0.06}        \n",
      "{'loss': 2.1358, 'learning_rate': 0.00019992266835390429, 'epoch': 0.06}        \n",
      "{'loss': 1.9969, 'learning_rate': 0.00019992254583504885, 'epoch': 0.06}        \n",
      "{'loss': 2.1962, 'learning_rate': 0.00019992242321925264, 'epoch': 0.06}        \n",
      "{'loss': 2.3943, 'learning_rate': 0.00019992230050651583, 'epoch': 0.06}        \n",
      "{'loss': 2.0756, 'learning_rate': 0.00019992217769683853, 'epoch': 0.06}        \n",
      "{'loss': 2.2506, 'learning_rate': 0.00019992205479022084, 'epoch': 0.06}        \n",
      "{'loss': 2.1521, 'learning_rate': 0.00019992193178666284, 'epoch': 0.06}        \n",
      "{'loss': 2.1206, 'learning_rate': 0.00019992180868616473, 'epoch': 0.06}        \n",
      "{'loss': 2.1677, 'learning_rate': 0.00019992168548872656, 'epoch': 0.06}        \n",
      "{'loss': 2.2257, 'learning_rate': 0.0001999215621943485, 'epoch': 0.06}         \n",
      "{'loss': 2.092, 'learning_rate': 0.00019992143880303063, 'epoch': 0.06}         \n",
      "{'loss': 2.6021, 'learning_rate': 0.00019992131531477308, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66572/1061708 [9:57:58<147:12:50,  1.88it/s][2024-03-01 04:04:59,256] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3762, 'learning_rate': 0.00019992120409245798, 'epoch': 0.06}        \n",
      "{'loss': 2.3854, 'learning_rate': 0.00019992108042001539, 'epoch': 0.06}        \n",
      "{'loss': 2.1834, 'learning_rate': 0.0001999209566506335, 'epoch': 0.06}         \n",
      "{'loss': 1.8991, 'learning_rate': 0.00019992083278431237, 'epoch': 0.06}        \n",
      "{'loss': 2.3078, 'learning_rate': 0.00019992070882105217, 'epoch': 0.06}        \n",
      "{'loss': 1.8523, 'learning_rate': 0.000199920584760853, 'epoch': 0.06}          \n",
      "{'loss': 1.8484, 'learning_rate': 0.000199920460603715, 'epoch': 0.06}          \n",
      "{'loss': 1.9779, 'learning_rate': 0.00019992033634963824, 'epoch': 0.06}        \n",
      "{'loss': 2.4359, 'learning_rate': 0.0001999202119986229, 'epoch': 0.06}         \n",
      "{'loss': 2.2098, 'learning_rate': 0.00019992008755066908, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66673/1061708 [9:58:52<149:55:01,  1.84it/s][2024-03-01 04:05:53,013] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                           | 66674/1061708 [9:58:52<140:03:18,  1.97it/s][2024-03-01 04:05:53,437] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2768, 'learning_rate': 0.00019991998792251038, 'epoch': 0.06}        \n",
      "{'loss': 1.5376, 'learning_rate': 0.00019991986330006762, 'epoch': 0.06}        \n",
      "{'loss': 2.2572, 'learning_rate': 0.00019991973858068666, 'epoch': 0.06}        \n",
      "  6%|█▊                           | 66702/1061708 [9:59:07<146:20:24,  1.89it/s][2024-03-01 04:06:08,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2953, 'learning_rate': 0.00019991962625036182, 'epoch': 0.06}        \n",
      "{'loss': 2.9258, 'learning_rate': 0.00019991950134679875, 'epoch': 0.06}        \n",
      "{'loss': 2.0488, 'learning_rate': 0.00019991937634629788, 'epoch': 0.06}        \n",
      "{'loss': 2.1927, 'learning_rate': 0.00019991925124885932, 'epoch': 0.06}        \n",
      "{'loss': 2.1778, 'learning_rate': 0.00019991912605448327, 'epoch': 0.06}        \n",
      "{'loss': 2.25, 'learning_rate': 0.00019991900076316973, 'epoch': 0.06}          \n",
      "{'loss': 2.0838, 'learning_rate': 0.00019991887537491892, 'epoch': 0.06}        \n",
      "{'loss': 2.3695, 'learning_rate': 0.0001999187498897309, 'epoch': 0.06}         \n",
      "{'loss': 1.9262, 'learning_rate': 0.00019991862430760582, 'epoch': 0.06}        \n",
      "{'loss': 1.7754, 'learning_rate': 0.0001999184986285438, 'epoch': 0.06}         \n",
      "{'loss': 1.9343, 'learning_rate': 0.00019991837285254495, 'epoch': 0.06}        \n",
      "{'loss': 2.3469, 'learning_rate': 0.00019991824697960942, 'epoch': 0.06}        \n",
      "{'loss': 2.1902, 'learning_rate': 0.0001999181210097373, 'epoch': 0.06}         \n",
      "{'loss': 2.1354, 'learning_rate': 0.00019991799494292872, 'epoch': 0.06}        \n",
      "{'loss': 1.9658, 'learning_rate': 0.00019991786877918382, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 66858/1061708 [10:00:30<146:29:26,  1.89it/s][2024-03-01 04:07:31,423] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2962, 'learning_rate': 0.00019991775514893294, 'epoch': 0.06}        \n",
      "{'loss': 1.7902, 'learning_rate': 0.00019991762880100934, 'epoch': 0.06}        \n",
      "{'loss': 2.0912, 'learning_rate': 0.0001999175023561498, 'epoch': 0.06}         \n",
      "{'loss': 1.939, 'learning_rate': 0.00019991737581435436, 'epoch': 0.06}         \n",
      "{'loss': 2.4901, 'learning_rate': 0.0001999172491756232, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 66908/1061708 [10:00:57<146:34:20,  1.89it/s][2024-03-01 04:07:57,979] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1429, 'learning_rate': 0.0001999171351178852, 'epoch': 0.06}         \n",
      "{'loss': 2.1024, 'learning_rate': 0.00019991700829497647, 'epoch': 0.06}        \n",
      "{'loss': 2.1105, 'learning_rate': 0.00019991688137513243, 'epoch': 0.06}        \n",
      "{'loss': 2.139, 'learning_rate': 0.00019991675435835306, 'epoch': 0.06}         \n",
      "{'loss': 1.9706, 'learning_rate': 0.00019991662724463862, 'epoch': 0.06}        \n",
      "{'loss': 2.1543, 'learning_rate': 0.00019991650003398914, 'epoch': 0.06}        \n",
      "{'loss': 1.958, 'learning_rate': 0.0001999163727264048, 'epoch': 0.06}          \n",
      "{'loss': 2.2916, 'learning_rate': 0.00019991624532188568, 'epoch': 0.06}        \n",
      "{'loss': 2.0086, 'learning_rate': 0.00019991611782043195, 'epoch': 0.06}        \n",
      "{'loss': 2.0967, 'learning_rate': 0.00019991599022204368, 'epoch': 0.06}        \n",
      "{'loss': 2.3324, 'learning_rate': 0.00019991586252672105, 'epoch': 0.06}        \n",
      "{'loss': 2.2974, 'learning_rate': 0.00019991573473446413, 'epoch': 0.06}        \n",
      "{'loss': 1.971, 'learning_rate': 0.0001999156068452731, 'epoch': 0.06}          \n",
      "{'loss': 1.8556, 'learning_rate': 0.00019991547885914803, 'epoch': 0.06}        \n",
      "{'loss': 2.2453, 'learning_rate': 0.00019991535077608908, 'epoch': 0.06}        \n",
      "{'loss': 2.267, 'learning_rate': 0.00019991522259609637, 'epoch': 0.06}         \n",
      "{'loss': 2.3444, 'learning_rate': 0.00019991509431917001, 'epoch': 0.06}        \n",
      "{'loss': 2.5249, 'learning_rate': 0.0001999149659453101, 'epoch': 0.06}         \n",
      "{'loss': 2.2083, 'learning_rate': 0.00019991483747451686, 'epoch': 0.06}        \n",
      "{'loss': 2.2231, 'learning_rate': 0.00019991470890679032, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67109/1061708 [10:02:44<146:34:59,  1.88it/s][2024-03-01 04:09:45,039] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8784, 'learning_rate': 0.0001999145931129586, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 67110/1061708 [10:02:44<137:38:10,  2.01it/s][2024-03-01 04:09:45,462] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8909, 'learning_rate': 0.0001999144772406111, 'epoch': 0.06}         \n",
      "{'loss': 2.4829, 'learning_rate': 0.00019991434840147207, 'epoch': 0.06}        \n",
      "{'loss': 2.4527, 'learning_rate': 0.00019991421946540024, 'epoch': 0.06}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1944, 'learning_rate': 0.00019991409043239577, 'epoch': 0.06}        \n",
      "{'loss': 2.28, 'learning_rate': 0.00019991396130245872, 'epoch': 0.06}          \n",
      "{'loss': 1.9247, 'learning_rate': 0.00019991383207558928, 'epoch': 0.06}        \n",
      "{'loss': 2.4168, 'learning_rate': 0.0001999137027517875, 'epoch': 0.06}         \n",
      "{'loss': 2.0865, 'learning_rate': 0.0001999135733310536, 'epoch': 0.06}         \n",
      "{'loss': 2.0388, 'learning_rate': 0.00019991344381338765, 'epoch': 0.06}        \n",
      "{'loss': 1.744, 'learning_rate': 0.00019991331419878976, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 67211/1061708 [10:03:38<145:53:44,  1.89it/s][2024-03-01 04:10:39,234] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                          | 67212/1061708 [10:03:39<137:04:38,  2.02it/s][2024-03-01 04:10:39,657] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9792, 'learning_rate': 0.00019991321043732056, 'epoch': 0.06}        \n",
      "{'loss': 2.1846, 'learning_rate': 0.00019991308064824556, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67233/1061708 [10:03:50<149:39:03,  1.85it/s][2024-03-01 04:10:50,791] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3986, 'learning_rate': 0.00019991296375520157, 'epoch': 0.06}        \n",
      "{'loss': 2.1939, 'learning_rate': 0.00019991283378195668, 'epoch': 0.06}        \n",
      "{'loss': 2.0161, 'learning_rate': 0.0001999127037117805, 'epoch': 0.06}         \n",
      "{'loss': 2.0666, 'learning_rate': 0.0001999125735446731, 'epoch': 0.06}         \n",
      "{'loss': 2.1867, 'learning_rate': 0.00019991244328063464, 'epoch': 0.06}        \n",
      "{'loss': 2.2622, 'learning_rate': 0.0001999123129196652, 'epoch': 0.06}         \n",
      "{'loss': 2.153, 'learning_rate': 0.000199912182461765, 'epoch': 0.06}           \n",
      "{'loss': 1.8958, 'learning_rate': 0.00019991205190693405, 'epoch': 0.06}        \n",
      "{'loss': 2.4839, 'learning_rate': 0.00019991192125517256, 'epoch': 0.06}        \n",
      "{'loss': 2.3227, 'learning_rate': 0.00019991179050648063, 'epoch': 0.06}        \n",
      "{'loss': 1.8913, 'learning_rate': 0.0001999116596608584, 'epoch': 0.06}         \n",
      "{'loss': 1.8689, 'learning_rate': 0.00019991152871830594, 'epoch': 0.06}        \n",
      "{'loss': 1.9438, 'learning_rate': 0.00019991139767882346, 'epoch': 0.06}        \n",
      "{'loss': 1.8944, 'learning_rate': 0.00019991126654241104, 'epoch': 0.06}        \n",
      "{'loss': 2.1073, 'learning_rate': 0.0001999111353090688, 'epoch': 0.06}         \n",
      "{'loss': 2.5931, 'learning_rate': 0.00019991100397879688, 'epoch': 0.06}        \n",
      "{'loss': 2.3962, 'learning_rate': 0.00019991087255159546, 'epoch': 0.06}        \n",
      "{'loss': 1.912, 'learning_rate': 0.0001999107410274646, 'epoch': 0.06}          \n",
      "{'loss': 2.2851, 'learning_rate': 0.0001999106094064044, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 67421/1061708 [10:05:30<146:02:08,  1.89it/s][2024-03-01 04:12:30,979] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4288, 'learning_rate': 0.00019991049086457582, 'epoch': 0.06}        \n",
      "{'loss': 2.5372, 'learning_rate': 0.00019991035905935034, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67443/1061708 [10:05:42<149:53:07,  1.84it/s][2024-03-01 04:12:42,650] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8935, 'learning_rate': 0.00019991024035177318, 'epoch': 0.06}        \n",
      "{'loss': 2.3207, 'learning_rate': 0.00019991010836238284, 'epoch': 0.06}        \n",
      "{'loss': 1.8417, 'learning_rate': 0.00019990997627606386, 'epoch': 0.06}        \n",
      "{'loss': 2.4507, 'learning_rate': 0.0001999098440928163, 'epoch': 0.06}         \n",
      "{'loss': 2.1713, 'learning_rate': 0.0001999097118126403, 'epoch': 0.06}         \n",
      "{'loss': 2.3533, 'learning_rate': 0.00019990957943553602, 'epoch': 0.06}        \n",
      "{'loss': 2.3806, 'learning_rate': 0.00019990944696150357, 'epoch': 0.06}        \n",
      "{'loss': 1.9555, 'learning_rate': 0.0001999093143905431, 'epoch': 0.06}         \n",
      "{'loss': 2.1534, 'learning_rate': 0.0001999091817226547, 'epoch': 0.06}         \n",
      "{'loss': 2.1345, 'learning_rate': 0.00019990904895783853, 'epoch': 0.06}        \n",
      "{'loss': 2.1507, 'learning_rate': 0.0001999089160960947, 'epoch': 0.06}         \n",
      "{'loss': 2.7154, 'learning_rate': 0.00019990878313742334, 'epoch': 0.06}        \n",
      "{'loss': 2.0644, 'learning_rate': 0.0001999086500818246, 'epoch': 0.06}         \n",
      "{'loss': 2.2437, 'learning_rate': 0.0001999085169292986, 'epoch': 0.06}         \n",
      "{'loss': 2.4979, 'learning_rate': 0.00019990838367984543, 'epoch': 0.06}        \n",
      "{'loss': 1.9381, 'learning_rate': 0.00019990825033346527, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67604/1061708 [10:07:07<148:51:47,  1.85it/s][2024-03-01 04:14:08,445] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1534, 'learning_rate': 0.00019990813023885065, 'epoch': 0.06}        \n",
      "{'loss': 2.1644, 'learning_rate': 0.0001999079967083095, 'epoch': 0.06}         \n",
      "{'loss': 2.0666, 'learning_rate': 0.00019990786308084178, 'epoch': 0.06}        \n",
      "{'loss': 2.1551, 'learning_rate': 0.00019990772935644753, 'epoch': 0.06}        \n",
      "{'loss': 2.3223, 'learning_rate': 0.0001999075955351269, 'epoch': 0.06}         \n",
      "{'loss': 2.1751, 'learning_rate': 0.00019990746161688004, 'epoch': 0.06}        \n",
      "{'loss': 2.1333, 'learning_rate': 0.00019990732760170706, 'epoch': 0.06}        \n",
      "{'loss': 2.0341, 'learning_rate': 0.00019990719348960808, 'epoch': 0.06}        \n",
      "{'loss': 2.0274, 'learning_rate': 0.00019990705928058327, 'epoch': 0.06}        \n",
      "{'loss': 2.1927, 'learning_rate': 0.00019990692497463272, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67705/1061708 [10:08:01<147:50:35,  1.87it/s][2024-03-01 04:15:02,258] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                          | 67706/1061708 [10:08:02<138:30:04,  1.99it/s][2024-03-01 04:15:02,681] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4931, 'learning_rate': 0.00019990681746008586, 'epoch': 0.06}        \n",
      "{'loss': 2.0078, 'learning_rate': 0.00019990668297966937, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 67721/1061708 [10:08:10<146:09:06,  1.89it/s][2024-03-01 04:15:10,606] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3479, 'learning_rate': 0.0001999065618644233, 'epoch': 0.06}         \n",
      "{'loss': 2.1594, 'learning_rate': 0.00019990642719984873, 'epoch': 0.06}        \n",
      "{'loss': 2.2856, 'learning_rate': 0.00019990629243834903, 'epoch': 0.06}        \n",
      "{'loss': 2.3447, 'learning_rate': 0.00019990615757992436, 'epoch': 0.06}        \n",
      "{'loss': 2.1312, 'learning_rate': 0.00019990602262457484, 'epoch': 0.06}        \n",
      "{'loss': 2.0537, 'learning_rate': 0.00019990588757230065, 'epoch': 0.06}        \n",
      "{'loss': 2.376, 'learning_rate': 0.0001999057524231018, 'epoch': 0.06}          \n",
      "{'loss': 2.1237, 'learning_rate': 0.00019990561717697855, 'epoch': 0.06}        \n",
      "{'loss': 2.2832, 'learning_rate': 0.00019990548183393094, 'epoch': 0.06}        \n",
      "{'loss': 1.5369, 'learning_rate': 0.00019990534639395915, 'epoch': 0.06}        \n",
      "{'loss': 2.4398, 'learning_rate': 0.00019990521085706331, 'epoch': 0.06}        \n",
      "{'loss': 1.8299, 'learning_rate': 0.00019990507522324352, 'epoch': 0.06}        \n",
      "{'loss': 2.2138, 'learning_rate': 0.00019990493949249993, 'epoch': 0.06}        \n",
      "{'loss': 2.2433, 'learning_rate': 0.0001999048036648327, 'epoch': 0.06}         \n",
      "{'loss': 1.8522, 'learning_rate': 0.0001999046677402419, 'epoch': 0.06}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|█▊                          | 67877/1061708 [10:09:33<147:31:05,  1.87it/s][2024-03-01 04:16:33,697] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8769, 'learning_rate': 0.0001999045453252407, 'epoch': 0.06}         \n",
      "{'loss': 2.3669, 'learning_rate': 0.00019990440921649558, 'epoch': 0.06}        \n",
      "{'loss': 2.2431, 'learning_rate': 0.0001999042730108273, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 67907/1061708 [10:09:49<146:39:00,  1.88it/s][2024-03-01 04:16:49,594] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2143, 'learning_rate': 0.00019990415034285664, 'epoch': 0.06}        \n",
      "{'loss': 2.1994, 'learning_rate': 0.00019990401395303476, 'epoch': 0.06}        \n",
      "{'loss': 2.0694, 'learning_rate': 0.00019990387746629008, 'epoch': 0.06}        \n",
      "{'loss': 2.1249, 'learning_rate': 0.00019990374088262278, 'epoch': 0.06}        \n",
      "{'loss': 1.621, 'learning_rate': 0.00019990360420203297, 'epoch': 0.06}         \n",
      "{'loss': 2.097, 'learning_rate': 0.00019990346742452077, 'epoch': 0.06}         \n",
      "{'loss': 2.2359, 'learning_rate': 0.00019990333055008635, 'epoch': 0.06}        \n",
      "{'loss': 2.2201, 'learning_rate': 0.00019990319357872984, 'epoch': 0.06}        \n",
      "{'loss': 2.046, 'learning_rate': 0.00019990305651045135, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 67999/1061708 [10:10:38<146:25:07,  1.89it/s][2024-03-01 04:17:38,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=68000, skipped=766, lr=[0.00019990291934525098], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 04:17:38,641] [INFO] [timer.py:260:stop] epoch=0/micro_step=68000/global_step=68000, RunningAvgSamplesPerSec=1.8923939184721865, CurrSamplesPerSec=1.9159666936484414, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1154, 'learning_rate': 0.00019990291934525098, 'epoch': 0.06}        \n",
      "{'loss': 2.4248, 'learning_rate': 0.00019990278208312896, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68015/1061708 [10:10:46<147:45:02,  1.87it/s][2024-03-01 04:17:47,082] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2162, 'learning_rate': 0.00019990265846435117, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68026/1061708 [10:10:52<147:07:14,  1.88it/s][2024-03-01 04:17:52,885] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0215, 'learning_rate': 0.00019990253476706698, 'epoch': 0.06}        \n",
      "{'loss': 1.9247, 'learning_rate': 0.00019990239723356487, 'epoch': 0.06}        \n",
      "{'loss': 2.3241, 'learning_rate': 0.00019990225960314157, 'epoch': 0.06}        \n",
      "{'loss': 2.3013, 'learning_rate': 0.00019990212187579717, 'epoch': 0.06}        \n",
      "{'loss': 2.1552, 'learning_rate': 0.00019990198405153188, 'epoch': 0.06}        \n",
      "{'loss': 2.3811, 'learning_rate': 0.00019990184613034577, 'epoch': 0.06}        \n",
      "{'loss': 2.2626, 'learning_rate': 0.000199901708112239, 'epoch': 0.06}          \n",
      "{'loss': 1.9687, 'learning_rate': 0.00019990156999721167, 'epoch': 0.06}        \n",
      "{'loss': 2.0739, 'learning_rate': 0.00019990143178526398, 'epoch': 0.06}        \n",
      "{'loss': 2.289, 'learning_rate': 0.000199901293476396, 'epoch': 0.06}           \n",
      "{'loss': 2.0181, 'learning_rate': 0.0001999011550706079, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 68136/1061708 [10:11:50<147:11:40,  1.88it/s][2024-03-01 04:18:51,413] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8045, 'learning_rate': 0.00019990103042253203, 'epoch': 0.06}        \n",
      "{'loss': 2.1116, 'learning_rate': 0.00019990089183259606, 'epoch': 0.06}        \n",
      "{'loss': 1.8515, 'learning_rate': 0.00019990075314574036, 'epoch': 0.06}        \n",
      "{'loss': 2.1377, 'learning_rate': 0.00019990061436196504, 'epoch': 0.06}        \n",
      "{'loss': 1.9918, 'learning_rate': 0.00019990047548127025, 'epoch': 0.06}        \n",
      "{'loss': 1.9916, 'learning_rate': 0.0001999003365036561, 'epoch': 0.06}         \n",
      "{'loss': 1.7531, 'learning_rate': 0.0001999001974291228, 'epoch': 0.06}         \n",
      "{'loss': 2.0651, 'learning_rate': 0.00019990005825767042, 'epoch': 0.06}        \n",
      "{'loss': 1.9193, 'learning_rate': 0.00019989991898929912, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68229/1061708 [10:12:40<146:07:19,  1.89it/s][2024-03-01 04:19:40,938] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9934, 'learning_rate': 0.00019989979356489933, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68232/1061708 [10:12:41<141:55:12,  1.94it/s][2024-03-01 04:19:42,468] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2194, 'learning_rate': 0.00019989966806199547, 'epoch': 0.06}        \n",
      "{'loss': 2.3461, 'learning_rate': 0.00019989952852225183, 'epoch': 0.06}        \n",
      "{'loss': 2.2612, 'learning_rate': 0.00019989938888558985, 'epoch': 0.06}        \n",
      "{'loss': 2.0005, 'learning_rate': 0.00019989924915200953, 'epoch': 0.06}        \n",
      "{'loss': 1.9729, 'learning_rate': 0.0001998991093215111, 'epoch': 0.06}         \n",
      "{'loss': 2.115, 'learning_rate': 0.00019989896939409462, 'epoch': 0.06}         \n",
      "{'loss': 1.9865, 'learning_rate': 0.0001998988293697603, 'epoch': 0.06}         \n",
      "{'loss': 2.4367, 'learning_rate': 0.00019989868924850823, 'epoch': 0.06}        \n",
      "{'loss': 2.2546, 'learning_rate': 0.00019989854903033856, 'epoch': 0.06}        \n",
      "{'loss': 2.4439, 'learning_rate': 0.00019989840871525142, 'epoch': 0.06}        \n",
      "{'loss': 1.86, 'learning_rate': 0.00019989826830324697, 'epoch': 0.06}          \n",
      "{'loss': 2.2672, 'learning_rate': 0.00019989812779432528, 'epoch': 0.06}        \n",
      "{'loss': 2.0889, 'learning_rate': 0.00019989798718848657, 'epoch': 0.06}        \n",
      "{'loss': 2.0113, 'learning_rate': 0.00019989784648573094, 'epoch': 0.06}        \n",
      "{'loss': 2.3202, 'learning_rate': 0.00019989770568605849, 'epoch': 0.06}        \n",
      "{'loss': 2.1479, 'learning_rate': 0.0001998975647894694, 'epoch': 0.06}         \n",
      "{'loss': 2.3504, 'learning_rate': 0.00019989742379596383, 'epoch': 0.06}        \n",
      "{'loss': 2.4755, 'learning_rate': 0.00019989728270554188, 'epoch': 0.06}        \n",
      "{'loss': 1.9886, 'learning_rate': 0.00019989714151820366, 'epoch': 0.06}        \n",
      "{'loss': 2.068, 'learning_rate': 0.00019989700023394938, 'epoch': 0.06}         \n",
      "{'loss': 2.3388, 'learning_rate': 0.00019989685885277912, 'epoch': 0.06}        \n",
      "{'loss': 2.0286, 'learning_rate': 0.000199896717374693, 'epoch': 0.06}          \n",
      "{'loss': 1.9804, 'learning_rate': 0.00019989657579969124, 'epoch': 0.06}        \n",
      "{'loss': 2.1051, 'learning_rate': 0.0001998964341277739, 'epoch': 0.06}         \n",
      "{'loss': 2.0744, 'learning_rate': 0.00019989629235894117, 'epoch': 0.06}        \n",
      "{'loss': 2.1499, 'learning_rate': 0.00019989615049319316, 'epoch': 0.06}        \n",
      "{'loss': 2.2526, 'learning_rate': 0.00019989600853052996, 'epoch': 0.06}        \n",
      "{'loss': 2.1276, 'learning_rate': 0.00019989586647095183, 'epoch': 0.06}        \n",
      "{'loss': 2.0954, 'learning_rate': 0.0001998957243144588, 'epoch': 0.06}         \n",
      "{'loss': 1.9541, 'learning_rate': 0.00019989558206105103, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68533/1061708 [10:15:22<149:23:38,  1.85it/s][2024-03-01 04:22:22,927] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  6%|█▊                          | 68534/1061708 [10:15:22<139:36:15,  1.98it/s][2024-03-01 04:22:23,349] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4741, 'learning_rate': 0.00019989546818854632, 'epoch': 0.06}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0809, 'learning_rate': 0.0001998953257606924, 'epoch': 0.06}         \n",
      "{'loss': 1.9644, 'learning_rate': 0.00019989518323592416, 'epoch': 0.06}        \n",
      "{'loss': 2.0814, 'learning_rate': 0.0001998950406142417, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 68576/1061708 [10:15:45<147:04:05,  1.88it/s][2024-03-01 04:22:45,683] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8665, 'learning_rate': 0.00019989491217186595, 'epoch': 0.06}        \n",
      "{'loss': 2.3746, 'learning_rate': 0.00019989476936604689, 'epoch': 0.06}        \n",
      "{'loss': 2.1073, 'learning_rate': 0.00019989462646331405, 'epoch': 0.06}        \n",
      "{'loss': 2.3426, 'learning_rate': 0.00019989448346366753, 'epoch': 0.06}        \n",
      "{'loss': 2.2642, 'learning_rate': 0.0001998943403671075, 'epoch': 0.06}         \n",
      "{'loss': 2.2102, 'learning_rate': 0.00019989419717363408, 'epoch': 0.06}        \n",
      "{'loss': 2.2187, 'learning_rate': 0.0001998940538832474, 'epoch': 0.06}         \n",
      "{'loss': 1.9183, 'learning_rate': 0.00019989391049594763, 'epoch': 0.06}        \n",
      "{'loss': 1.9447, 'learning_rate': 0.0001998937670117349, 'epoch': 0.06}         \n",
      "{'loss': 2.069, 'learning_rate': 0.00019989362343060933, 'epoch': 0.06}         \n",
      "{'loss': 2.4101, 'learning_rate': 0.0001998934797525711, 'epoch': 0.06}         \n",
      "{'loss': 2.2733, 'learning_rate': 0.00019989333597762027, 'epoch': 0.06}        \n",
      "{'loss': 2.0125, 'learning_rate': 0.0001998931921057571, 'epoch': 0.06}         \n",
      "{'loss': 2.1045, 'learning_rate': 0.00019989304813698163, 'epoch': 0.06}        \n",
      "{'loss': 2.2081, 'learning_rate': 0.000199892904071294, 'epoch': 0.06}          \n",
      "{'loss': 2.1153, 'learning_rate': 0.00019989275990869442, 'epoch': 0.06}        \n",
      "{'loss': 2.5924, 'learning_rate': 0.00019989261564918296, 'epoch': 0.06}        \n",
      "{'loss': 1.6888, 'learning_rate': 0.0001998924712927598, 'epoch': 0.06}         \n",
      "{'loss': 2.1166, 'learning_rate': 0.00019989232683942506, 'epoch': 0.06}        \n",
      "{'loss': 1.9913, 'learning_rate': 0.0001998921822891789, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 68774/1061708 [10:17:30<148:23:07,  1.86it/s][2024-03-01 04:24:31,283] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5569, 'learning_rate': 0.0001998920521110982, 'epoch': 0.06}         \n",
      "{'loss': 2.2465, 'learning_rate': 0.00019989190737672074, 'epoch': 0.06}        \n",
      "{'loss': 2.1248, 'learning_rate': 0.00019989176254543224, 'epoch': 0.06}        \n",
      "{'loss': 2.1294, 'learning_rate': 0.00019989161761723283, 'epoch': 0.06}        \n",
      "{'loss': 2.3715, 'learning_rate': 0.0001998914725921227, 'epoch': 0.06}         \n",
      "  6%|█▊                          | 68828/1061708 [10:17:59<146:48:03,  1.88it/s][2024-03-01 04:25:00,087] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2813, 'learning_rate': 0.00019989134198666498, 'epoch': 0.06}        \n",
      "{'loss': 1.975, 'learning_rate': 0.00019989119677742484, 'epoch': 0.06}         \n",
      "{'loss': 1.7908, 'learning_rate': 0.00019989105147127435, 'epoch': 0.06}        \n",
      "{'loss': 1.9583, 'learning_rate': 0.00019989090606821366, 'epoch': 0.06}        \n",
      "{'loss': 2.4635, 'learning_rate': 0.00019989076056824295, 'epoch': 0.06}        \n",
      "{'loss': 2.4271, 'learning_rate': 0.0001998906149713623, 'epoch': 0.06}         \n",
      "{'loss': 1.7009, 'learning_rate': 0.00019989046927757187, 'epoch': 0.06}        \n",
      "{'loss': 1.9292, 'learning_rate': 0.00019989032348687185, 'epoch': 0.06}        \n",
      "{'loss': 2.1611, 'learning_rate': 0.00019989017759926232, 'epoch': 0.06}        \n",
      "{'loss': 2.5568, 'learning_rate': 0.00019989003161474344, 'epoch': 0.06}        \n",
      "{'loss': 1.8772, 'learning_rate': 0.00019988988553331537, 'epoch': 0.06}        \n",
      "{'loss': 1.8285, 'learning_rate': 0.00019988973935497822, 'epoch': 0.06}        \n",
      "{'loss': 2.2646, 'learning_rate': 0.00019988959307973216, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68956/1061708 [10:19:07<147:28:38,  1.87it/s][2024-03-01 04:26:08,455] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2653, 'learning_rate': 0.00019988946134915373, 'epoch': 0.06}        \n",
      "{'loss': 2.1829, 'learning_rate': 0.00019988931488978108, 'epoch': 0.06}        \n",
      "{'loss': 2.2191, 'learning_rate': 0.00019988916833349997, 'epoch': 0.06}        \n",
      "{'loss': 2.1378, 'learning_rate': 0.00019988902168031048, 'epoch': 0.06}        \n",
      "  6%|█▊                          | 68993/1061708 [10:19:27<149:52:23,  1.84it/s][2024-03-01 04:26:28,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0599, 'learning_rate': 0.0001998888896095834, 'epoch': 0.06}         \n",
      "{'loss': 2.3232, 'learning_rate': 0.00019988874277226842, 'epoch': 0.06}        \n",
      "{'loss': 2.0574, 'learning_rate': 0.0001998885958380455, 'epoch': 0.07}         \n",
      "{'loss': 2.2103, 'learning_rate': 0.00019988844880691474, 'epoch': 0.07}        \n",
      "{'loss': 2.199, 'learning_rate': 0.0001998883016788763, 'epoch': 0.07}          \n",
      "{'loss': 1.9167, 'learning_rate': 0.0001998881544539304, 'epoch': 0.07}         \n",
      "{'loss': 1.918, 'learning_rate': 0.00019988800713207705, 'epoch': 0.07}         \n",
      "{'loss': 2.1788, 'learning_rate': 0.0001998878597133165, 'epoch': 0.07}         \n",
      "{'loss': 2.3098, 'learning_rate': 0.00019988771219764883, 'epoch': 0.07}        \n",
      "{'loss': 1.9377, 'learning_rate': 0.00019988756458507424, 'epoch': 0.07}        \n",
      "{'loss': 2.3635, 'learning_rate': 0.0001998874168755928, 'epoch': 0.07}         \n",
      "{'loss': 2.1264, 'learning_rate': 0.0001998872690692047, 'epoch': 0.07}         \n",
      "{'loss': 1.9734, 'learning_rate': 0.0001998871211659101, 'epoch': 0.07}         \n",
      "{'loss': 2.2598, 'learning_rate': 0.0001998869731657091, 'epoch': 0.07}         \n",
      "{'loss': 1.8672, 'learning_rate': 0.00019988682506860185, 'epoch': 0.07}        \n",
      "{'loss': 1.8653, 'learning_rate': 0.00019988667687458853, 'epoch': 0.07}        \n",
      "{'loss': 2.162, 'learning_rate': 0.00019988652858366927, 'epoch': 0.07}         \n",
      "{'loss': 2.199, 'learning_rate': 0.0001998863801958442, 'epoch': 0.07}          \n",
      "{'loss': 2.1325, 'learning_rate': 0.00019988623171111346, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69180/1061708 [10:21:07<146:30:29,  1.88it/s][2024-03-01 04:28:08,070] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.5323, 'learning_rate': 0.0001998860979920016, 'epoch': 0.07}         \n",
      "{'loss': 2.2714, 'learning_rate': 0.00019988594932315051, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69204/1061708 [10:21:20<148:49:16,  1.85it/s][2024-03-01 04:28:20,843] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.165, 'learning_rate': 0.0001998858154383305, 'epoch': 0.07}          \n",
      "{'loss': 2.4778, 'learning_rate': 0.0001998856665853596, 'epoch': 0.07}         \n",
      "{'loss': 2.4859, 'learning_rate': 0.0001998855176354837, 'epoch': 0.07}         \n",
      "{'loss': 1.9922, 'learning_rate': 0.00019988536858870298, 'epoch': 0.07}        \n",
      "{'loss': 1.9999, 'learning_rate': 0.00019988521944501757, 'epoch': 0.07}        \n",
      "{'loss': 2.4203, 'learning_rate': 0.00019988507020442763, 'epoch': 0.07}        \n",
      "{'loss': 2.344, 'learning_rate': 0.0001998849208669333, 'epoch': 0.07}          \n",
      "{'loss': 2.0656, 'learning_rate': 0.00019988477143253474, 'epoch': 0.07}        \n",
      "{'loss': 2.3363, 'learning_rate': 0.00019988462190123206, 'epoch': 0.07}        \n",
      "{'loss': 2.0887, 'learning_rate': 0.00019988447227302545, 'epoch': 0.07}        \n",
      "{'loss': 1.9917, 'learning_rate': 0.000199884322547915, 'epoch': 0.07}          \n",
      "{'loss': 2.1965, 'learning_rate': 0.0001998841727259009, 'epoch': 0.07}         \n",
      "{'loss': 1.9086, 'learning_rate': 0.0001998840228069833, 'epoch': 0.07}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7%|█▊                          | 69338/1061708 [10:22:31<146:33:46,  1.88it/s][2024-03-01 04:29:32,354] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0489, 'learning_rate': 0.00019988388779710502, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69347/1061708 [10:22:36<146:04:55,  1.89it/s][2024-03-01 04:29:37,058] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3765, 'learning_rate': 0.00019988375270873517, 'epoch': 0.07}        \n",
      "{'loss': 2.3162, 'learning_rate': 0.00019988360251848848, 'epoch': 0.07}        \n",
      "{'loss': 2.1821, 'learning_rate': 0.00019988345223133879, 'epoch': 0.07}        \n",
      "{'loss': 2.2084, 'learning_rate': 0.00019988330184728633, 'epoch': 0.07}        \n",
      "{'loss': 1.8007, 'learning_rate': 0.00019988315136633116, 'epoch': 0.07}        \n",
      "{'loss': 2.3021, 'learning_rate': 0.00019988300078847346, 'epoch': 0.07}        \n",
      "{'loss': 2.0623, 'learning_rate': 0.00019988285011371341, 'epoch': 0.07}        \n",
      "{'loss': 1.9406, 'learning_rate': 0.00019988269934205112, 'epoch': 0.07}        \n",
      "{'loss': 2.1416, 'learning_rate': 0.0001998825484734867, 'epoch': 0.07}         \n",
      "{'loss': 2.3988, 'learning_rate': 0.00019988239750802036, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69449/1061708 [10:23:30<146:06:11,  1.89it/s][2024-03-01 04:30:31,336] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0497, 'learning_rate': 0.00019988226155624964, 'epoch': 0.07}        \n",
      "{'loss': 2.455, 'learning_rate': 0.00019988211040666997, 'epoch': 0.07}         \n",
      "{'loss': 2.2779, 'learning_rate': 0.00019988195916018886, 'epoch': 0.07}        \n",
      "{'loss': 2.2246, 'learning_rate': 0.00019988180781680632, 'epoch': 0.07}        \n",
      "{'loss': 2.0703, 'learning_rate': 0.00019988165637652258, 'epoch': 0.07}        \n",
      "{'loss': 2.1598, 'learning_rate': 0.00019988150483933777, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69502/1061708 [10:23:58<145:55:51,  1.89it/s][2024-03-01 04:30:59,573] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.097, 'learning_rate': 0.00019988136837302114, 'epoch': 0.07}         \n",
      "{'loss': 2.6669, 'learning_rate': 0.00019988121665172467, 'epoch': 0.07}        \n",
      "{'loss': 2.2973, 'learning_rate': 0.00019988106483352755, 'epoch': 0.07}        \n",
      "{'loss': 2.036, 'learning_rate': 0.00019988091291842994, 'epoch': 0.07}         \n",
      "{'loss': 2.2279, 'learning_rate': 0.00019988076090643198, 'epoch': 0.07}        \n",
      "{'loss': 2.075, 'learning_rate': 0.00019988060879753378, 'epoch': 0.07}         \n",
      "{'loss': 2.4164, 'learning_rate': 0.00019988045659173555, 'epoch': 0.07}        \n",
      "{'loss': 2.3947, 'learning_rate': 0.00019988030428903737, 'epoch': 0.07}        \n",
      "{'loss': 1.9256, 'learning_rate': 0.00019988015188943944, 'epoch': 0.07}        \n",
      "{'loss': 2.1068, 'learning_rate': 0.0001998799993929419, 'epoch': 0.07}         \n",
      "{'loss': 2.1669, 'learning_rate': 0.0001998798467995449, 'epoch': 0.07}         \n",
      "{'loss': 2.3403, 'learning_rate': 0.00019987969410924857, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69621/1061708 [10:25:02<145:55:28,  1.89it/s][2024-03-01 04:32:02,934] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.232, 'learning_rate': 0.00019987955660513308, 'epoch': 0.07}         \n",
      "{'loss': 2.2646, 'learning_rate': 0.00019987940373072843, 'epoch': 0.07}        \n",
      "{'loss': 2.4454, 'learning_rate': 0.0001998792507594249, 'epoch': 0.07}         \n",
      "  7%|█▊                          | 69657/1061708 [10:25:21<146:42:13,  1.88it/s][2024-03-01 04:32:22,053] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9649, 'learning_rate': 0.00019987911300240328, 'epoch': 0.07}        \n",
      "{'loss': 2.4342, 'learning_rate': 0.00019987895984699227, 'epoch': 0.07}        \n",
      "{'loss': 1.9017, 'learning_rate': 0.0001998788065946828, 'epoch': 0.07}         \n",
      "{'loss': 2.0079, 'learning_rate': 0.000199878653245475, 'epoch': 0.07}          \n",
      "{'loss': 2.5161, 'learning_rate': 0.00019987849979936903, 'epoch': 0.07}        \n",
      "{'loss': 2.1408, 'learning_rate': 0.00019987834625636506, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69713/1061708 [10:25:51<149:46:01,  1.84it/s][2024-03-01 04:32:51,844] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1401, 'learning_rate': 0.00019987820798481382, 'epoch': 0.07}        \n",
      "{'loss': 2.2111, 'learning_rate': 0.00019987805425770403, 'epoch': 0.07}        \n",
      "{'loss': 2.0579, 'learning_rate': 0.00019987790043369662, 'epoch': 0.07}        \n",
      "{'loss': 1.9923, 'learning_rate': 0.00019987774651279182, 'epoch': 0.07}        \n",
      "{'loss': 2.4717, 'learning_rate': 0.00019987759249498973, 'epoch': 0.07}        \n",
      "{'loss': 2.4636, 'learning_rate': 0.0001998774383802905, 'epoch': 0.07}         \n",
      "{'loss': 2.3055, 'learning_rate': 0.00019987728416869432, 'epoch': 0.07}        \n",
      "{'loss': 1.8821, 'learning_rate': 0.00019987712986020126, 'epoch': 0.07}        \n",
      "{'loss': 2.2898, 'learning_rate': 0.0001998769754548115, 'epoch': 0.07}         \n",
      "{'loss': 2.4522, 'learning_rate': 0.00019987682095252523, 'epoch': 0.07}        \n",
      "{'loss': 1.9818, 'learning_rate': 0.0001998766663533426, 'epoch': 0.07}         \n",
      "{'loss': 2.0197, 'learning_rate': 0.00019987651165726369, 'epoch': 0.07}        \n",
      "{'loss': 2.246, 'learning_rate': 0.0001998763568642887, 'epoch': 0.07}          \n",
      "  7%|█▊                          | 69845/1061708 [10:27:01<147:58:36,  1.86it/s][2024-03-01 04:34:02,342] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3644, 'learning_rate': 0.00019987621746776516, 'epoch': 0.07}        \n",
      "{'loss': 1.9542, 'learning_rate': 0.00019987606249068803, 'epoch': 0.07}        \n",
      "{'loss': 2.2544, 'learning_rate': 0.00019987590741671527, 'epoch': 0.07}        \n",
      "{'loss': 1.6906, 'learning_rate': 0.000199875752245847, 'epoch': 0.07}          \n",
      "{'loss': 2.0233, 'learning_rate': 0.00019987559697808336, 'epoch': 0.07}        \n",
      "{'loss': 2.3882, 'learning_rate': 0.0001998754416134245, 'epoch': 0.07}         \n",
      "{'loss': 2.3694, 'learning_rate': 0.00019987528615187064, 'epoch': 0.07}        \n",
      "{'loss': 2.0639, 'learning_rate': 0.00019987513059342184, 'epoch': 0.07}        \n",
      "{'loss': 2.0508, 'learning_rate': 0.00019987497493807833, 'epoch': 0.07}        \n",
      "{'loss': 2.2497, 'learning_rate': 0.00019987481918584018, 'epoch': 0.07}        \n",
      "{'loss': 1.8044, 'learning_rate': 0.00019987466333670764, 'epoch': 0.07}        \n",
      "{'loss': 2.0637, 'learning_rate': 0.00019987450739068077, 'epoch': 0.07}        \n",
      "{'loss': 2.0024, 'learning_rate': 0.00019987435134775976, 'epoch': 0.07}        \n",
      "{'loss': 2.1562, 'learning_rate': 0.00019987419520794476, 'epoch': 0.07}        \n",
      "{'loss': 1.7438, 'learning_rate': 0.00019987403897123594, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 69999/1061708 [10:28:24<146:38:39,  1.88it/s][2024-03-01 04:35:24,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=70000, skipped=788, lr=[0.0001998738826376334], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 04:35:24,680] [INFO] [timer.py:260:stop] epoch=0/micro_step=70000/global_step=70000, RunningAvgSamplesPerSec=1.8923668392170083, CurrSamplesPerSec=1.8936311515413646, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.8414, 'learning_rate': 0.0001998738826376334, 'epoch': 0.07}         \n",
      "  7%|█▊                          | 70000/1061708 [10:28:24<146:40:15,  1.88it/s][INFO|trainer.py:2868] 2024-03-01 04:35:24,682 >> Saving model checkpoint to output_model/checkpoint-70000\n",
      "[INFO|trainer.py:2880] 2024-03-01 04:35:24,685 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 04:35:25,907 >> tokenizer config file saved in output_model/checkpoint-70000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 04:35:25,907 >> Special tokens file saved in output_model/checkpoint-70000/special_tokens_map.json\n",
      "[2024-03-01 04:35:25,909] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step70000 is about to be saved!\n",
      "[2024-03-01 04:35:31,141] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-70000/global_step70000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 04:35:31,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-70000/global_step70000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 04:35:45,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-70000/global_step70000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 04:35:45,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-70000/global_step70000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 04:35:52,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-70000/global_step70000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 04:35:52,865] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-70000/global_step70000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 04:35:52,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step70000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 04:35:52,927 >> Deleting older checkpoint [output_model/checkpoint-55000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 04:35:56,687 >> tokenizer config file saved in output_model/checkpoint-70000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 04:35:56,687 >> Special tokens file saved in output_model/checkpoint-70000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.0771, 'learning_rate': 0.00019987372620713736, 'epoch': 0.07}        \n",
      "{'loss': 2.1237, 'learning_rate': 0.00019987356967974792, 'epoch': 0.07}        \n",
      "{'loss': 1.9726, 'learning_rate': 0.00019987341305546523, 'epoch': 0.07}        \n",
      "{'loss': 2.1983, 'learning_rate': 0.00019987325633428948, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70046/1061708 [10:29:20<146:00:23,  1.89it/s][2024-03-01 04:36:21,438] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▊                          | 70047/1061708 [10:29:21<136:52:45,  2.01it/s][2024-03-01 04:36:21,858] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  7%|█▊                          | 70048/1061708 [10:29:21<130:30:35,  2.11it/s][2024-03-01 04:36:22,284] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9966, 'learning_rate': 0.00019987314657181514, 'epoch': 0.07}        \n",
      "{'loss': 2.1505, 'learning_rate': 0.00019987298968592149, 'epoch': 0.07}        \n",
      "{'loss': 1.8561, 'learning_rate': 0.0001998728327031352, 'epoch': 0.07}         \n",
      "{'loss': 2.0645, 'learning_rate': 0.00019987267562345638, 'epoch': 0.07}        \n",
      "{'loss': 2.1913, 'learning_rate': 0.00019987251844688517, 'epoch': 0.07}        \n",
      "{'loss': 1.7915, 'learning_rate': 0.00019987236117342176, 'epoch': 0.07}        \n",
      "{'loss': 1.9559, 'learning_rate': 0.0001998722038030663, 'epoch': 0.07}         \n",
      "{'loss': 1.9065, 'learning_rate': 0.00019987204633581892, 'epoch': 0.07}        \n",
      "{'loss': 2.0831, 'learning_rate': 0.0001998718887716798, 'epoch': 0.07}         \n",
      "{'loss': 2.3084, 'learning_rate': 0.00019987173111064906, 'epoch': 0.07}        \n",
      "{'loss': 2.5226, 'learning_rate': 0.00019987157335272688, 'epoch': 0.07}        \n",
      "{'loss': 2.1956, 'learning_rate': 0.00019987141549791343, 'epoch': 0.07}        \n",
      "{'loss': 1.9397, 'learning_rate': 0.0001998712575462088, 'epoch': 0.07}         \n",
      "{'loss': 2.2151, 'learning_rate': 0.00019987109949761318, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70182/1061708 [10:30:33<146:35:51,  1.88it/s][2024-03-01 04:37:33,872] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9159, 'learning_rate': 0.00019987095717103546, 'epoch': 0.07}        \n",
      "{'loss': 2.2566, 'learning_rate': 0.0001998707989383474, 'epoch': 0.07}         \n",
      "{'loss': 1.9388, 'learning_rate': 0.0001998706406087688, 'epoch': 0.07}         \n",
      "{'loss': 2.415, 'learning_rate': 0.0001998704821822998, 'epoch': 0.07}          \n",
      "  7%|█▊                          | 70228/1061708 [10:30:57<146:36:05,  1.88it/s][2024-03-01 04:37:58,375] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8872, 'learning_rate': 0.00019987033951563655, 'epoch': 0.07}        \n",
      "{'loss': 1.957, 'learning_rate': 0.0001998701809050762, 'epoch': 0.07}          \n",
      "{'loss': 2.1508, 'learning_rate': 0.00019987002219762595, 'epoch': 0.07}        \n",
      "{'loss': 2.0917, 'learning_rate': 0.0001998698633932859, 'epoch': 0.07}         \n",
      "{'loss': 2.1974, 'learning_rate': 0.0001998697044920562, 'epoch': 0.07}         \n",
      "{'loss': 2.0984, 'learning_rate': 0.00019986954549393703, 'epoch': 0.07}        \n",
      "{'loss': 2.1407, 'learning_rate': 0.00019986938639892854, 'epoch': 0.07}        \n",
      "{'loss': 1.9586, 'learning_rate': 0.00019986922720703087, 'epoch': 0.07}        \n",
      "{'loss': 2.5753, 'learning_rate': 0.00019986906791824417, 'epoch': 0.07}        \n",
      "{'loss': 2.3777, 'learning_rate': 0.0001998689085325686, 'epoch': 0.07}         \n",
      "{'loss': 2.0301, 'learning_rate': 0.00019986874905000433, 'epoch': 0.07}        \n",
      "{'loss': 2.1776, 'learning_rate': 0.00019986858947055152, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70344/1061708 [10:31:59<148:36:02,  1.85it/s][2024-03-01 04:39:00,230] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3976, 'learning_rate': 0.00019986844576620438, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70359/1061708 [10:32:07<146:05:15,  1.88it/s][2024-03-01 04:39:08,162] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4833, 'learning_rate': 0.00019986830198337775, 'epoch': 0.07}        \n",
      "{'loss': 2.1296, 'learning_rate': 0.0001998681421326378, 'epoch': 0.07}         \n",
      "{'loss': 2.252, 'learning_rate': 0.0001998679821850099, 'epoch': 0.07}          \n",
      "{'loss': 2.1161, 'learning_rate': 0.00019986782214049414, 'epoch': 0.07}        \n",
      "{'loss': 2.0491, 'learning_rate': 0.00019986766199909072, 'epoch': 0.07}        \n",
      "{'loss': 2.1031, 'learning_rate': 0.00019986750176079985, 'epoch': 0.07}        \n",
      "{'loss': 2.1164, 'learning_rate': 0.0001998673414256216, 'epoch': 0.07}         \n",
      "{'loss': 2.0155, 'learning_rate': 0.00019986718099355616, 'epoch': 0.07}        \n",
      "{'loss': 2.3418, 'learning_rate': 0.00019986702046460373, 'epoch': 0.07}        \n",
      "{'loss': 2.1935, 'learning_rate': 0.00019986685983876436, 'epoch': 0.07}        \n",
      "{'loss': 2.0497, 'learning_rate': 0.0001998666991160383, 'epoch': 0.07}         \n",
      "  7%|█▊                          | 70460/1061708 [10:33:01<146:04:47,  1.88it/s][2024-03-01 04:40:01,936] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▊                          | 70469/1061708 [10:33:06<145:38:43,  1.89it/s][2024-03-01 04:40:06,638] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.759, 'learning_rate': 0.00019986657046809912, 'epoch': 0.07}         \n",
      "{'loss': 2.8004, 'learning_rate': 0.00019986640957097733, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0318, 'learning_rate': 0.0001998662485769693, 'epoch': 0.07}         \n",
      "{'loss': 2.1104, 'learning_rate': 0.0001998660874860751, 'epoch': 0.07}         \n",
      "{'loss': 2.1977, 'learning_rate': 0.00019986592629829493, 'epoch': 0.07}        \n",
      "{'loss': 1.8332, 'learning_rate': 0.00019986576501362894, 'epoch': 0.07}        \n",
      "{'loss': 2.3582, 'learning_rate': 0.00019986560363207732, 'epoch': 0.07}        \n",
      "{'loss': 2.5216, 'learning_rate': 0.00019986544215364017, 'epoch': 0.07}        \n",
      "{'loss': 2.3784, 'learning_rate': 0.00019986528057831766, 'epoch': 0.07}        \n",
      "{'loss': 1.7792, 'learning_rate': 0.00019986511890611, 'epoch': 0.07}           \n",
      "{'loss': 2.144, 'learning_rate': 0.00019986495713701725, 'epoch': 0.07}         \n",
      "{'loss': 2.0267, 'learning_rate': 0.00019986479527103968, 'epoch': 0.07}        \n",
      "{'loss': 1.9685, 'learning_rate': 0.00019986463330817734, 'epoch': 0.07}        \n",
      "{'loss': 2.4184, 'learning_rate': 0.00019986447124843045, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70600/1061708 [10:34:16<146:10:28,  1.88it/s][2024-03-01 04:41:16,673] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3017, 'learning_rate': 0.0001998643253118221, 'epoch': 0.07}         \n",
      "{'loss': 2.2577, 'learning_rate': 0.00019986416306799502, 'epoch': 0.07}        \n",
      "{'loss': 2.1154, 'learning_rate': 0.0001998640007272838, 'epoch': 0.07}         \n",
      "{'loss': 2.041, 'learning_rate': 0.0001998638382896886, 'epoch': 0.07}          \n",
      "{'loss': 2.08, 'learning_rate': 0.00019986367575520965, 'epoch': 0.07}          \n",
      "{'loss': 2.2604, 'learning_rate': 0.00019986351312384706, 'epoch': 0.07}        \n",
      "{'loss': 2.0518, 'learning_rate': 0.00019986335039560098, 'epoch': 0.07}        \n",
      "{'loss': 1.9919, 'learning_rate': 0.0001998631875704716, 'epoch': 0.07}         \n",
      "{'loss': 2.0355, 'learning_rate': 0.00019986302464845905, 'epoch': 0.07}        \n",
      "{'loss': 2.085, 'learning_rate': 0.00019986286162956352, 'epoch': 0.07}         \n",
      "  7%|█▊                          | 70701/1061708 [10:35:10<146:13:48,  1.88it/s][2024-03-01 04:42:10,574] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▊                          | 70702/1061708 [10:35:10<137:11:07,  2.01it/s][2024-03-01 04:42:10,996] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  7%|█▊                          | 70709/1061708 [10:35:14<145:31:11,  1.89it/s][2024-03-01 04:42:14,654] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.089, 'learning_rate': 0.00019986274745869133, 'epoch': 0.07}         \n",
      "{'loss': 2.2591, 'learning_rate': 0.00019986258427509503, 'epoch': 0.07}        \n",
      "{'loss': 2.3358, 'learning_rate': 0.00019986242099461615, 'epoch': 0.07}        \n",
      "{'loss': 2.1294, 'learning_rate': 0.0001998622576172549, 'epoch': 0.07}         \n",
      "{'loss': 2.429, 'learning_rate': 0.00019986209414301134, 'epoch': 0.07}         \n",
      "{'loss': 1.8259, 'learning_rate': 0.0001998619305718857, 'epoch': 0.07}         \n",
      "{'loss': 2.3446, 'learning_rate': 0.00019986176690387812, 'epoch': 0.07}        \n",
      "{'loss': 2.3502, 'learning_rate': 0.00019986160313898877, 'epoch': 0.07}        \n",
      "{'loss': 2.2652, 'learning_rate': 0.00019986143927721778, 'epoch': 0.07}        \n",
      "{'loss': 2.0115, 'learning_rate': 0.00019986127531856533, 'epoch': 0.07}        \n",
      "{'loss': 2.1464, 'learning_rate': 0.00019986111126303157, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 70816/1061708 [10:36:11<147:06:21,  1.87it/s][2024-03-01 04:43:11,821] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2648, 'learning_rate': 0.00019986096353021783, 'epoch': 0.07}        \n",
      "{'loss': 2.1101, 'learning_rate': 0.00019986079929061004, 'epoch': 0.07}        \n",
      "{'loss': 2.2612, 'learning_rate': 0.0001998606349541214, 'epoch': 0.07}         \n",
      "{'loss': 2.3082, 'learning_rate': 0.00019986047052075208, 'epoch': 0.07}        \n",
      "{'loss': 2.1621, 'learning_rate': 0.0001998603059905022, 'epoch': 0.07}         \n",
      "  7%|█▊                          | 70868/1061708 [10:36:39<146:40:42,  1.88it/s][2024-03-01 04:43:39,563] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2791, 'learning_rate': 0.00019986015783044465, 'epoch': 0.07}        \n",
      "{'loss': 2.3891, 'learning_rate': 0.00019985999311612226, 'epoch': 0.07}        \n",
      "{'loss': 2.2436, 'learning_rate': 0.00019985982830491978, 'epoch': 0.07}        \n",
      "{'loss': 1.9456, 'learning_rate': 0.00019985966339683742, 'epoch': 0.07}        \n",
      "{'loss': 2.3438, 'learning_rate': 0.0001998594983918753, 'epoch': 0.07}         \n",
      "{'loss': 2.3029, 'learning_rate': 0.00019985933329003363, 'epoch': 0.07}        \n",
      "{'loss': 2.2867, 'learning_rate': 0.0001998591680913125, 'epoch': 0.07}         \n",
      "{'loss': 2.4317, 'learning_rate': 0.00019985900279571216, 'epoch': 0.07}        \n",
      "{'loss': 2.1679, 'learning_rate': 0.0001998588374032327, 'epoch': 0.07}         \n",
      "{'loss': 1.8582, 'learning_rate': 0.00019985867191387428, 'epoch': 0.07}        \n",
      "{'loss': 2.3936, 'learning_rate': 0.00019985850632763707, 'epoch': 0.07}        \n",
      "{'loss': 2.0489, 'learning_rate': 0.00019985834064452127, 'epoch': 0.07}        \n",
      "{'loss': 2.1407, 'learning_rate': 0.00019985817486452702, 'epoch': 0.07}        \n",
      "{'loss': 2.2561, 'learning_rate': 0.00019985800898765445, 'epoch': 0.07}        \n",
      "{'loss': 2.2736, 'learning_rate': 0.00019985784301390375, 'epoch': 0.07}        \n",
      "{'loss': 2.3051, 'learning_rate': 0.0001998576769432751, 'epoch': 0.07}         \n",
      "{'loss': 2.2587, 'learning_rate': 0.00019985751077576859, 'epoch': 0.07}        \n",
      "{'loss': 2.0821, 'learning_rate': 0.00019985734451138443, 'epoch': 0.07}        \n",
      "  7%|█▊                          | 71043/1061708 [10:38:12<149:16:58,  1.84it/s][2024-03-01 04:45:13,039] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▊                          | 71045/1061708 [10:38:13<142:04:15,  1.94it/s][2024-03-01 04:45:14,001] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2496, 'learning_rate': 0.00019985721143012534, 'epoch': 0.07}        \n",
      "{'loss': 1.9197, 'learning_rate': 0.00019985704499136185, 'epoch': 0.07}        \n",
      "{'loss': 2.1086, 'learning_rate': 0.00019985687845572113, 'epoch': 0.07}        \n",
      "{'loss': 2.0447, 'learning_rate': 0.0001998567118232034, 'epoch': 0.07}         \n",
      "{'loss': 2.1351, 'learning_rate': 0.00019985654509380876, 'epoch': 0.07}        \n",
      "{'loss': 2.2928, 'learning_rate': 0.00019985637826753743, 'epoch': 0.07}        \n",
      "{'loss': 1.9512, 'learning_rate': 0.00019985621134438955, 'epoch': 0.07}        \n",
      "{'loss': 2.1871, 'learning_rate': 0.00019985604432436524, 'epoch': 0.07}        \n",
      "{'loss': 2.0537, 'learning_rate': 0.00019985587720746474, 'epoch': 0.07}        \n",
      "{'loss': 2.0215, 'learning_rate': 0.00019985570999368814, 'epoch': 0.07}        \n",
      "{'loss': 2.1549, 'learning_rate': 0.00019985554268303567, 'epoch': 0.07}        \n",
      "{'loss': 2.2493, 'learning_rate': 0.00019985537527550742, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71162/1061708 [10:39:15<145:45:03,  1.89it/s][2024-03-01 04:46:16,417] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4147, 'learning_rate': 0.00019985522452590338, 'epoch': 0.07}        \n",
      "{'loss': 2.1679, 'learning_rate': 0.00019985505693431168, 'epoch': 0.07}        \n",
      "{'loss': 2.5541, 'learning_rate': 0.00019985488924584473, 'epoch': 0.07}        \n",
      "{'loss': 2.1761, 'learning_rate': 0.00019985472146050263, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7%|█▉                          | 71207/1061708 [10:39:39<146:32:11,  1.88it/s][2024-03-01 04:46:40,351] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8, 'learning_rate': 0.00019985457037086668, 'epoch': 0.07}           \n",
      "{'loss': 2.4659, 'learning_rate': 0.00019985440240146235, 'epoch': 0.07}        \n",
      "{'loss': 1.7104, 'learning_rate': 0.00019985423433518338, 'epoch': 0.07}        \n",
      "{'loss': 2.2426, 'learning_rate': 0.00019985406617202992, 'epoch': 0.07}        \n",
      "{'loss': 1.9967, 'learning_rate': 0.00019985389791200216, 'epoch': 0.07}        \n",
      "{'loss': 2.195, 'learning_rate': 0.00019985372955510025, 'epoch': 0.07}         \n",
      "{'loss': 2.5073, 'learning_rate': 0.00019985356110132437, 'epoch': 0.07}        \n",
      "{'loss': 1.9447, 'learning_rate': 0.00019985339255067464, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71283/1061708 [10:40:20<149:19:57,  1.84it/s][2024-03-01 04:47:20,857] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6453, 'learning_rate': 0.0001998532407722629, 'epoch': 0.07}         \n",
      "{'loss': 2.2298, 'learning_rate': 0.00019985307203755336, 'epoch': 0.07}        \n",
      "{'loss': 2.4116, 'learning_rate': 0.00019985290320597053, 'epoch': 0.07}        \n",
      "{'loss': 2.1958, 'learning_rate': 0.00019985273427751445, 'epoch': 0.07}        \n",
      "{'loss': 1.9735, 'learning_rate': 0.00019985256525218537, 'epoch': 0.07}        \n",
      "{'loss': 2.0028, 'learning_rate': 0.00019985239612998345, 'epoch': 0.07}        \n",
      "{'loss': 2.3657, 'learning_rate': 0.00019985222691090883, 'epoch': 0.07}        \n",
      "{'loss': 2.1582, 'learning_rate': 0.00019985205759496171, 'epoch': 0.07}        \n",
      "{'loss': 2.1059, 'learning_rate': 0.0001998518881821422, 'epoch': 0.07}         \n",
      "{'loss': 2.0366, 'learning_rate': 0.00019985171867245048, 'epoch': 0.07}        \n",
      "{'loss': 1.9541, 'learning_rate': 0.00019985154906588677, 'epoch': 0.07}        \n",
      "{'loss': 2.1912, 'learning_rate': 0.00019985137936245114, 'epoch': 0.07}        \n",
      "{'loss': 1.9556, 'learning_rate': 0.00019985120956214384, 'epoch': 0.07}        \n",
      "{'loss': 2.1407, 'learning_rate': 0.00019985103966496496, 'epoch': 0.07}        \n",
      "{'loss': 2.3119, 'learning_rate': 0.00019985086967091474, 'epoch': 0.07}        \n",
      "{'loss': 2.1397, 'learning_rate': 0.0001998506995799933, 'epoch': 0.07}         \n",
      "{'loss': 1.9438, 'learning_rate': 0.00019985052939220085, 'epoch': 0.07}        \n",
      "{'loss': 2.1412, 'learning_rate': 0.0001998503591075375, 'epoch': 0.07}         \n",
      "{'loss': 2.1334, 'learning_rate': 0.00019985018872600342, 'epoch': 0.07}        \n",
      "{'loss': 2.2117, 'learning_rate': 0.0001998500182475988, 'epoch': 0.07}         \n",
      "{'loss': 1.904, 'learning_rate': 0.00019984984767232376, 'epoch': 0.07}         \n",
      "{'loss': 1.9593, 'learning_rate': 0.00019984967700017856, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71501/1061708 [10:42:16<145:46:36,  1.89it/s][2024-03-01 04:49:17,036] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1499, 'learning_rate': 0.00019984952331242396, 'epoch': 0.07}        \n",
      "{'loss': 1.9111, 'learning_rate': 0.00019984935245622576, 'epoch': 0.07}        \n",
      "{'loss': 2.3043, 'learning_rate': 0.00019984918150315784, 'epoch': 0.07}        \n",
      "{'loss': 2.4729, 'learning_rate': 0.00019984901045322036, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71544/1061708 [10:42:39<148:23:10,  1.85it/s][2024-03-01 04:49:39,915] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4508, 'learning_rate': 0.00019984885642545326, 'epoch': 0.07}        \n",
      "{'loss': 2.4375, 'learning_rate': 0.00019984868519146403, 'epoch': 0.07}        \n",
      "{'loss': 1.9348, 'learning_rate': 0.00019984851386060573, 'epoch': 0.07}        \n",
      "{'loss': 2.0827, 'learning_rate': 0.0001998483424328785, 'epoch': 0.07}         \n",
      "{'loss': 2.4486, 'learning_rate': 0.00019984817090828248, 'epoch': 0.07}        \n",
      "{'loss': 2.4271, 'learning_rate': 0.0001998479992868179, 'epoch': 0.07}         \n",
      "{'loss': 2.4468, 'learning_rate': 0.0001998478275684849, 'epoch': 0.07}         \n",
      "{'loss': 2.4438, 'learning_rate': 0.0001998476557532836, 'epoch': 0.07}         \n",
      "{'loss': 2.097, 'learning_rate': 0.00019984748384121426, 'epoch': 0.07}         \n",
      "{'loss': 2.2056, 'learning_rate': 0.00019984731183227697, 'epoch': 0.07}        \n",
      "{'loss': 2.454, 'learning_rate': 0.00019984713972647194, 'epoch': 0.07}         \n",
      "{'loss': 2.0487, 'learning_rate': 0.00019984696752379927, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71662/1061708 [10:43:42<146:07:06,  1.88it/s][2024-03-01 04:50:42,955] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8759, 'learning_rate': 0.00019984681245857226, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71675/1061708 [10:43:49<147:49:01,  1.86it/s][2024-03-01 04:50:49,840] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0175, 'learning_rate': 0.00019984665731488272, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71680/1061708 [10:43:51<146:28:50,  1.88it/s][2024-03-01 04:50:52,443] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.733, 'learning_rate': 0.00019984650209273083, 'epoch': 0.07}         \n",
      "{'loss': 1.9482, 'learning_rate': 0.00019984632953164958, 'epoch': 0.07}        \n",
      "{'loss': 1.8012, 'learning_rate': 0.0001998461568737015, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 71719/1061708 [10:44:12<146:05:37,  1.88it/s][2024-03-01 04:51:13,194] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "{'loss': 2.0533, 'learning_rate': 0.00019984600139872724, 'epoch': 0.07}        \n",
      "{'loss': 1.8883, 'learning_rate': 0.0001998458285567327, 'epoch': 0.07}         \n",
      "{'loss': 2.3732, 'learning_rate': 0.00019984565561787182, 'epoch': 0.07}        \n",
      "{'loss': 2.0347, 'learning_rate': 0.00019984548258214483, 'epoch': 0.07}        \n",
      "{'loss': 2.0995, 'learning_rate': 0.00019984530944955182, 'epoch': 0.07}        \n",
      "{'loss': 1.9763, 'learning_rate': 0.000199845136220093, 'epoch': 0.07}          \n",
      "{'loss': 2.1324, 'learning_rate': 0.00019984496289376856, 'epoch': 0.07}        \n",
      "{'loss': 2.1748, 'learning_rate': 0.0001998447894705786, 'epoch': 0.07}         \n",
      "{'loss': 2.0539, 'learning_rate': 0.00019984461595052335, 'epoch': 0.07}        \n",
      "{'loss': 2.1397, 'learning_rate': 0.00019984444233360295, 'epoch': 0.07}        \n",
      "{'loss': 2.0997, 'learning_rate': 0.0001998442686198176, 'epoch': 0.07}         \n",
      "{'loss': 1.6494, 'learning_rate': 0.00019984409480916743, 'epoch': 0.07}        \n",
      "{'loss': 1.9063, 'learning_rate': 0.0001998439209016526, 'epoch': 0.07}         \n",
      "{'loss': 2.1897, 'learning_rate': 0.0001998437468972733, 'epoch': 0.07}         \n",
      "{'loss': 2.571, 'learning_rate': 0.00019984357279602971, 'epoch': 0.07}         \n",
      "{'loss': 2.5963, 'learning_rate': 0.00019984339859792203, 'epoch': 0.07}        \n",
      "{'loss': 2.2755, 'learning_rate': 0.00019984322430295035, 'epoch': 0.07}        \n",
      "{'loss': 2.2083, 'learning_rate': 0.00019984304991111488, 'epoch': 0.07}        \n",
      "{'loss': 2.2238, 'learning_rate': 0.0001998428754224158, 'epoch': 0.07}         \n",
      "{'loss': 2.3691, 'learning_rate': 0.00019984270083685325, 'epoch': 0.07}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00019984252615442742, 'epoch': 0.07}        \n",
      "{'loss': 2.2377, 'learning_rate': 0.0001998423513751385, 'epoch': 0.07}         \n",
      "{'loss': 2.0606, 'learning_rate': 0.0001998421764989866, 'epoch': 0.07}         \n",
      "{'loss': 2.0862, 'learning_rate': 0.0001998420015259719, 'epoch': 0.07}         \n",
      "{'loss': 1.83, 'learning_rate': 0.00019984182645609467, 'epoch': 0.07}          \n",
      "{'loss': 2.1816, 'learning_rate': 0.00019984165128935494, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7872, 'learning_rate': 0.000199841476025753, 'epoch': 0.07}          \n",
      "{'loss': 2.0022, 'learning_rate': 0.00019984130066528892, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 71999/1061708 [10:46:42<146:01:54,  1.88it/s][2024-03-01 04:53:42,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=72000, skipped=814, lr=[0.00019984112520796292], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 04:53:42,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=72000/global_step=72000, RunningAvgSamplesPerSec=1.8923360442066426, CurrSamplesPerSec=1.9010475399671487, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0615, 'learning_rate': 0.00019984112520796292, 'epoch': 0.07}        \n",
      "{'loss': 1.9864, 'learning_rate': 0.00019984094965377514, 'epoch': 0.07}        \n",
      "{'loss': 2.2692, 'learning_rate': 0.0001998407740027258, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 72029/1061708 [10:46:58<145:57:29,  1.88it/s][2024-03-01 04:53:58,804] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0194, 'learning_rate': 0.0001998406158339649, 'epoch': 0.07}         \n",
      "{'loss': 2.2083, 'learning_rate': 0.000199840439998879, 'epoch': 0.07}          \n",
      "{'loss': 2.0487, 'learning_rate': 0.00019984026406693204, 'epoch': 0.07}        \n",
      "{'loss': 2.1029, 'learning_rate': 0.00019984008803812414, 'epoch': 0.07}        \n",
      "{'loss': 2.3612, 'learning_rate': 0.0001998399119124555, 'epoch': 0.07}         \n",
      "{'loss': 2.4051, 'learning_rate': 0.0001998397356899263, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 72088/1061708 [10:47:29<146:13:01,  1.88it/s][2024-03-01 04:54:30,246] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2271, 'learning_rate': 0.00019983957700683434, 'epoch': 0.07}        \n",
      "{'loss': 2.1163, 'learning_rate': 0.00019983940060027045, 'epoch': 0.07}        \n",
      "{'loss': 1.9652, 'learning_rate': 0.00019983922409684652, 'epoch': 0.07}        \n",
      "{'loss': 1.6599, 'learning_rate': 0.00019983904749656267, 'epoch': 0.07}        \n",
      "{'loss': 2.1789, 'learning_rate': 0.0001998388707994191, 'epoch': 0.07}         \n",
      "{'loss': 2.1023, 'learning_rate': 0.00019983869400541594, 'epoch': 0.07}        \n",
      "{'loss': 2.0118, 'learning_rate': 0.00019983851711455336, 'epoch': 0.07}        \n",
      "{'loss': 2.0548, 'learning_rate': 0.0001998383401268316, 'epoch': 0.07}         \n",
      "{'loss': 1.9229, 'learning_rate': 0.00019983816304225077, 'epoch': 0.07}        \n",
      "{'loss': 2.0797, 'learning_rate': 0.00019983798586081103, 'epoch': 0.07}        \n",
      "{'loss': 1.9454, 'learning_rate': 0.00019983780858251262, 'epoch': 0.07}        \n",
      "{'loss': 2.0646, 'learning_rate': 0.00019983763120735566, 'epoch': 0.07}        \n",
      "{'loss': 2.2374, 'learning_rate': 0.00019983745373534033, 'epoch': 0.07}        \n",
      "{'loss': 2.2704, 'learning_rate': 0.0001998372761664668, 'epoch': 0.07}         \n",
      "{'loss': 1.8953, 'learning_rate': 0.00019983709850073526, 'epoch': 0.07}        \n",
      "{'loss': 2.3784, 'learning_rate': 0.00019983692073814586, 'epoch': 0.07}        \n",
      "{'loss': 1.8026, 'learning_rate': 0.00019983674287869877, 'epoch': 0.07}        \n",
      "{'loss': 2.2755, 'learning_rate': 0.00019983656492239419, 'epoch': 0.07}        \n",
      "{'loss': 2.2035, 'learning_rate': 0.0001998363868692323, 'epoch': 0.07}         \n",
      "{'loss': 1.8457, 'learning_rate': 0.0001998362087192132, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 72289/1061708 [10:49:17<146:04:26,  1.88it/s][2024-03-01 04:56:17,697] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3401, 'learning_rate': 0.00019983604830138332, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72290/1061708 [10:49:17<137:10:03,  2.00it/s][2024-03-01 04:56:18,123] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8278, 'learning_rate': 0.0001998358878050994, 'epoch': 0.07}         \n",
      "{'loss': 2.4122, 'learning_rate': 0.00019983570938388119, 'epoch': 0.07}        \n",
      "{'loss': 2.4838, 'learning_rate': 0.00019983553086580651, 'epoch': 0.07}        \n",
      "{'loss': 1.9845, 'learning_rate': 0.00019983535225087548, 'epoch': 0.07}        \n",
      "{'loss': 2.1008, 'learning_rate': 0.0001998351735390883, 'epoch': 0.07}         \n",
      "{'loss': 2.1421, 'learning_rate': 0.00019983499473044512, 'epoch': 0.07}        \n",
      "{'loss': 2.1729, 'learning_rate': 0.00019983481582494614, 'epoch': 0.07}        \n",
      "{'loss': 2.2302, 'learning_rate': 0.00019983463682259154, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72370/1061708 [10:50:00<146:08:22,  1.88it/s][2024-03-01 04:57:00,837] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0105, 'learning_rate': 0.00019983447563766098, 'epoch': 0.07}        \n",
      "{'loss': 2.0953, 'learning_rate': 0.0001998342964512811, 'epoch': 0.07}         \n",
      "{'loss': 2.4519, 'learning_rate': 0.00019983411716804615, 'epoch': 0.07}        \n",
      "{'loss': 2.1078, 'learning_rate': 0.00019983393778795624, 'epoch': 0.07}        \n",
      "{'loss': 2.3391, 'learning_rate': 0.00019983375831101151, 'epoch': 0.07}        \n",
      "{'loss': 2.1163, 'learning_rate': 0.0001998335787372122, 'epoch': 0.07}         \n",
      "{'loss': 2.1741, 'learning_rate': 0.00019983339906655845, 'epoch': 0.07}        \n",
      "{'loss': 2.1007, 'learning_rate': 0.00019983321929905045, 'epoch': 0.07}        \n",
      "{'loss': 1.8222, 'learning_rate': 0.00019983303943468835, 'epoch': 0.07}        \n",
      "{'loss': 2.2763, 'learning_rate': 0.0001998328594734723, 'epoch': 0.07}         \n",
      "{'loss': 1.9727, 'learning_rate': 0.00019983267941540257, 'epoch': 0.07}        \n",
      "{'loss': 2.3555, 'learning_rate': 0.00019983249926047928, 'epoch': 0.07}        \n",
      "{'loss': 1.7689, 'learning_rate': 0.00019983231900870259, 'epoch': 0.07}        \n",
      "{'loss': 2.0613, 'learning_rate': 0.00019983213866007268, 'epoch': 0.07}        \n",
      "{'loss': 2.3106, 'learning_rate': 0.0001998319582145897, 'epoch': 0.07}         \n",
      "{'loss': 1.6083, 'learning_rate': 0.00019983177767225388, 'epoch': 0.07}        \n",
      "{'loss': 1.9753, 'learning_rate': 0.0001998315970330654, 'epoch': 0.07}         \n",
      "{'loss': 2.2599, 'learning_rate': 0.00019983141629702436, 'epoch': 0.07}        \n",
      "{'loss': 1.958, 'learning_rate': 0.00019983123546413105, 'epoch': 0.07}         \n",
      "{'loss': 2.0996, 'learning_rate': 0.00019983105453438553, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72571/1061708 [10:51:47<145:48:28,  1.88it/s][2024-03-01 04:58:48,125] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 72572/1061708 [10:51:48<136:58:07,  2.01it/s][2024-03-01 04:58:48,551] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1046, 'learning_rate': 0.00019983090972085568, 'epoch': 0.07}        \n",
      "{'loss': 2.4051, 'learning_rate': 0.00019983072861677674, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72597/1061708 [10:52:01<146:42:43,  1.87it/s][2024-03-01 04:59:01,875] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3162, 'learning_rate': 0.0001998305655402975, 'epoch': 0.07}         \n",
      "{'loss': 1.8301, 'learning_rate': 0.0001998303842522005, 'epoch': 0.07}         \n",
      "{'loss': 1.7205, 'learning_rate': 0.0001998302028672522, 'epoch': 0.07}         \n",
      "{'loss': 1.9931, 'learning_rate': 0.0001998300213854527, 'epoch': 0.07}         \n",
      "{'loss': 2.2954, 'learning_rate': 0.00019982983980680225, 'epoch': 0.07}        \n",
      "{'loss': 2.1393, 'learning_rate': 0.00019982965813130098, 'epoch': 0.07}        \n",
      "{'loss': 1.8624, 'learning_rate': 0.00019982947635894909, 'epoch': 0.07}        \n",
      "{'loss': 2.1215, 'learning_rate': 0.00019982929448974672, 'epoch': 0.07}        \n",
      "{'loss': 1.8321, 'learning_rate': 0.00019982911252369408, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7%|█▉                          | 72689/1061708 [10:52:50<146:06:56,  1.88it/s][2024-03-01 04:59:50,946] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0702, 'learning_rate': 0.0001998289486714399, 'epoch': 0.07}         \n",
      "{'loss': 2.1926, 'learning_rate': 0.0001998287665213722, 'epoch': 0.07}         \n",
      "{'loss': 2.1613, 'learning_rate': 0.00019982858427445476, 'epoch': 0.07}        \n",
      "{'loss': 2.3413, 'learning_rate': 0.00019982840193068775, 'epoch': 0.07}        \n",
      "{'loss': 2.0239, 'learning_rate': 0.0001998282194900713, 'epoch': 0.07}         \n",
      "{'loss': 2.0812, 'learning_rate': 0.00019982803695260565, 'epoch': 0.07}        \n",
      "{'loss': 2.0563, 'learning_rate': 0.00019982785431829088, 'epoch': 0.07}        \n",
      "{'loss': 2.4434, 'learning_rate': 0.00019982767158712727, 'epoch': 0.07}        \n",
      "{'loss': 2.2856, 'learning_rate': 0.00019982748875911494, 'epoch': 0.07}        \n",
      "{'loss': 2.0878, 'learning_rate': 0.00019982730583425408, 'epoch': 0.07}        \n",
      "{'loss': 2.2403, 'learning_rate': 0.0001998271228125449, 'epoch': 0.07}         \n",
      "{'loss': 1.8998, 'learning_rate': 0.00019982693969398753, 'epoch': 0.07}        \n",
      "{'loss': 2.1147, 'learning_rate': 0.00019982675647858215, 'epoch': 0.07}        \n",
      "{'loss': 2.158, 'learning_rate': 0.000199826573166329, 'epoch': 0.07}           \n",
      "{'loss': 2.3117, 'learning_rate': 0.00019982638975722815, 'epoch': 0.07}        \n",
      "{'loss': 2.1349, 'learning_rate': 0.0001998262062512799, 'epoch': 0.07}         \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019982602264848433, 'epoch': 0.07}        \n",
      "{'loss': 1.973, 'learning_rate': 0.00019982583894884165, 'epoch': 0.07}         \n",
      "{'loss': 2.4309, 'learning_rate': 0.00019982565515235207, 'epoch': 0.07}        \n",
      "{'loss': 2.2337, 'learning_rate': 0.00019982547125901574, 'epoch': 0.07}        \n",
      "{'loss': 2.2858, 'learning_rate': 0.00019982528726883282, 'epoch': 0.07}        \n",
      "{'loss': 2.1507, 'learning_rate': 0.0001998251031818035, 'epoch': 0.07}         \n",
      "{'loss': 1.9994, 'learning_rate': 0.00019982491899792803, 'epoch': 0.07}        \n",
      "{'loss': 2.2029, 'learning_rate': 0.00019982473471720646, 'epoch': 0.07}        \n",
      "{'loss': 2.2127, 'learning_rate': 0.00019982455033963907, 'epoch': 0.07}        \n",
      "{'loss': 1.8807, 'learning_rate': 0.000199824365865226, 'epoch': 0.07}          \n",
      "{'loss': 2.0487, 'learning_rate': 0.00019982418129396742, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72950/1061708 [10:55:09<145:50:49,  1.88it/s][2024-03-01 05:02:10,388] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3589, 'learning_rate': 0.00019982401509703194, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 72960/1061708 [10:55:15<145:42:12,  1.89it/s][2024-03-01 05:02:15,638] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1834, 'learning_rate': 0.0001998238488216519, 'epoch': 0.07}         \n",
      "{'loss': 2.1365, 'learning_rate': 0.0001998236639792269, 'epoch': 0.07}         \n",
      "{'loss': 2.1534, 'learning_rate': 0.00019982347903995708, 'epoch': 0.07}        \n",
      "{'loss': 2.2687, 'learning_rate': 0.0001998232940038426, 'epoch': 0.07}         \n",
      "{'loss': 2.2083, 'learning_rate': 0.0001998231088708837, 'epoch': 0.07}         \n",
      "{'loss': 2.1032, 'learning_rate': 0.0001998229236410805, 'epoch': 0.07}         \n",
      "{'loss': 2.3328, 'learning_rate': 0.0001998227383144332, 'epoch': 0.07}         \n",
      "{'loss': 2.2857, 'learning_rate': 0.00019982255289094199, 'epoch': 0.07}        \n",
      "{'loss': 2.4367, 'learning_rate': 0.00019982236737060705, 'epoch': 0.07}        \n",
      "{'loss': 2.502, 'learning_rate': 0.00019982218175342852, 'epoch': 0.07}         \n",
      "{'loss': 2.0132, 'learning_rate': 0.00019982199603940664, 'epoch': 0.07}        \n",
      "{'loss': 2.6215, 'learning_rate': 0.00019982181022854155, 'epoch': 0.07}        \n",
      "{'loss': 2.2652, 'learning_rate': 0.00019982162432083346, 'epoch': 0.07}        \n",
      "{'loss': 2.2707, 'learning_rate': 0.0001998214383162825, 'epoch': 0.07}         \n",
      "{'loss': 2.2836, 'learning_rate': 0.00019982125221488893, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73112/1061708 [10:56:36<145:59:15,  1.88it/s][2024-03-01 05:03:36,855] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7251, 'learning_rate': 0.00019982108464083436, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73121/1061708 [10:56:41<145:48:57,  1.88it/s][2024-03-01 05:03:41,569] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9756, 'learning_rate': 0.00019982091698833752, 'epoch': 0.07}        \n",
      "{'loss': 2.4147, 'learning_rate': 0.00019982073061578545, 'epoch': 0.07}        \n",
      "{'loss': 2.4476, 'learning_rate': 0.0001998205441463914, 'epoch': 0.07}         \n",
      "{'loss': 2.0933, 'learning_rate': 0.00019982035758015556, 'epoch': 0.07}        \n",
      "{'loss': 2.4268, 'learning_rate': 0.00019982017091707812, 'epoch': 0.07}        \n",
      "{'loss': 2.0417, 'learning_rate': 0.00019981998415715926, 'epoch': 0.07}        \n",
      "{'loss': 2.4639, 'learning_rate': 0.00019981979730039916, 'epoch': 0.07}        \n",
      "{'loss': 1.9206, 'learning_rate': 0.00019981961034679796, 'epoch': 0.07}        \n",
      "{'loss': 2.3209, 'learning_rate': 0.0001998194232963559, 'epoch': 0.07}         \n",
      "{'loss': 2.1426, 'learning_rate': 0.00019981923614907312, 'epoch': 0.07}        \n",
      "{'loss': 2.1542, 'learning_rate': 0.00019981904890494984, 'epoch': 0.07}        \n",
      "{'loss': 1.9874, 'learning_rate': 0.00019981886156398621, 'epoch': 0.07}        \n",
      "{'loss': 2.1669, 'learning_rate': 0.00019981867412618244, 'epoch': 0.07}        \n",
      "{'loss': 2.2809, 'learning_rate': 0.00019981848659153867, 'epoch': 0.07}        \n",
      "{'loss': 2.1131, 'learning_rate': 0.00019981829896005513, 'epoch': 0.07}        \n",
      "{'loss': 2.2547, 'learning_rate': 0.00019981811123173199, 'epoch': 0.07}        \n",
      "{'loss': 2.3899, 'learning_rate': 0.00019981792340656936, 'epoch': 0.07}        \n",
      "{'loss': 2.1963, 'learning_rate': 0.00019981773548456754, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73308/1061708 [10:58:20<145:49:47,  1.88it/s][2024-03-01 05:05:21,328] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8817, 'learning_rate': 0.00019981756627196847, 'epoch': 0.07}        \n",
      "{'loss': 2.4737, 'learning_rate': 0.00019981737816597258, 'epoch': 0.07}        \n",
      "{'loss': 1.5285, 'learning_rate': 0.00019981718996313797, 'epoch': 0.07}        \n",
      "{'loss': 2.1947, 'learning_rate': 0.0001998170016634648, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 73348/1061708 [10:58:42<146:04:19,  1.88it/s][2024-03-01 05:05:42,582] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9188, 'learning_rate': 0.00019981683211096218, 'epoch': 0.07}        \n",
      "{'loss': 2.0286, 'learning_rate': 0.0001998166436272963, 'epoch': 0.07}         \n",
      "{'loss': 1.9616, 'learning_rate': 0.00019981645504679241, 'epoch': 0.07}        \n",
      "{'loss': 2.2062, 'learning_rate': 0.00019981626636945073, 'epoch': 0.07}        \n",
      "{'loss': 2.4828, 'learning_rate': 0.0001998160775952714, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 73390/1061708 [10:59:04<145:46:09,  1.88it/s][2024-03-01 05:06:04,903] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3795, 'learning_rate': 0.00019981590761571395, 'epoch': 0.07}        \n",
      "{'loss': 2.1267, 'learning_rate': 0.0001998157186575436, 'epoch': 0.07}         \n",
      "{'loss': 2.3122, 'learning_rate': 0.00019981552960253617, 'epoch': 0.07}        \n",
      "{'loss': 1.7186, 'learning_rate': 0.00019981534045069178, 'epoch': 0.07}        \n",
      "{'loss': 1.9513, 'learning_rate': 0.00019981515120201065, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9988, 'learning_rate': 0.00019981496185649298, 'epoch': 0.07}        \n",
      "{'loss': 1.8305, 'learning_rate': 0.00019981477241413895, 'epoch': 0.07}        \n",
      "{'loss': 1.5337, 'learning_rate': 0.0001998145828749487, 'epoch': 0.07}         \n",
      "{'loss': 2.1794, 'learning_rate': 0.00019981439323892247, 'epoch': 0.07}        \n",
      "{'loss': 2.2394, 'learning_rate': 0.0001998142035060604, 'epoch': 0.07}         \n",
      "{'loss': 2.4747, 'learning_rate': 0.0001998140136763627, 'epoch': 0.07}         \n",
      "{'loss': 1.8139, 'learning_rate': 0.00019981382374982954, 'epoch': 0.07}        \n",
      "{'loss': 2.1457, 'learning_rate': 0.0001998136337264611, 'epoch': 0.07}         \n",
      "{'loss': 2.091, 'learning_rate': 0.00019981344360625757, 'epoch': 0.07}         \n",
      "{'loss': 1.9666, 'learning_rate': 0.00019981325338921919, 'epoch': 0.07}        \n",
      "{'loss': 2.002, 'learning_rate': 0.00019981306307534605, 'epoch': 0.07}         \n",
      "{'loss': 2.4149, 'learning_rate': 0.0001998128726646384, 'epoch': 0.07}         \n",
      "{'loss': 2.2833, 'learning_rate': 0.00019981268215709638, 'epoch': 0.07}        \n",
      "{'loss': 1.8248, 'learning_rate': 0.00019981249155272022, 'epoch': 0.07}        \n",
      "{'loss': 2.1006, 'learning_rate': 0.00019981230085151008, 'epoch': 0.07}        \n",
      "{'loss': 2.2015, 'learning_rate': 0.00019981211005346613, 'epoch': 0.07}        \n",
      "{'loss': 1.8508, 'learning_rate': 0.00019981191915858859, 'epoch': 0.07}        \n",
      "{'loss': 2.4054, 'learning_rate': 0.0001998117281668776, 'epoch': 0.07}         \n",
      "{'loss': 2.3432, 'learning_rate': 0.00019981153707833343, 'epoch': 0.07}        \n",
      "{'loss': 1.6787, 'learning_rate': 0.00019981134589295617, 'epoch': 0.07}        \n",
      "{'loss': 2.0109, 'learning_rate': 0.00019981115461074604, 'epoch': 0.07}        \n",
      "{'loss': 2.4392, 'learning_rate': 0.00019981096323170325, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73663/1061708 [11:01:29<148:24:34,  1.85it/s][2024-03-01 05:08:30,419] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4948, 'learning_rate': 0.00019981079090777292, 'epoch': 0.07}        \n",
      "{'loss': 1.9726, 'learning_rate': 0.00019981059934474855, 'epoch': 0.07}        \n",
      "{'loss': 2.187, 'learning_rate': 0.00019981040768489203, 'epoch': 0.07}         \n",
      "{'loss': 2.3655, 'learning_rate': 0.00019981021592820354, 'epoch': 0.07}        \n",
      "{'loss': 1.9301, 'learning_rate': 0.00019981002407468333, 'epoch': 0.07}        \n",
      "{'loss': 2.2853, 'learning_rate': 0.00019980983212433147, 'epoch': 0.07}        \n",
      "{'loss': 2.0727, 'learning_rate': 0.00019980964007714825, 'epoch': 0.07}        \n",
      "{'loss': 1.8702, 'learning_rate': 0.00019980944793313382, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73745/1061708 [11:02:13<146:49:37,  1.87it/s][2024-03-01 05:09:14,087] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1384, 'learning_rate': 0.00019980927492073032, 'epoch': 0.07}        \n",
      "{'loss': 2.0338, 'learning_rate': 0.0001998090825927371, 'epoch': 0.07}         \n",
      "{'loss': 2.1658, 'learning_rate': 0.0001998088901679132, 'epoch': 0.07}         \n",
      "{'loss': 2.1222, 'learning_rate': 0.0001998086976462588, 'epoch': 0.07}         \n",
      "{'loss': 2.0666, 'learning_rate': 0.00019980850502777413, 'epoch': 0.07}        \n",
      "{'loss': 2.1167, 'learning_rate': 0.00019980831231245936, 'epoch': 0.07}        \n",
      "{'loss': 2.193, 'learning_rate': 0.00019980811950031467, 'epoch': 0.07}         \n",
      "{'loss': 1.8574, 'learning_rate': 0.00019980792659134023, 'epoch': 0.07}        \n",
      "{'loss': 1.9017, 'learning_rate': 0.00019980773358553624, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 73830/1061708 [11:02:58<145:35:58,  1.88it/s][2024-03-01 05:09:59,334] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1764, 'learning_rate': 0.00019980755979752353, 'epoch': 0.07}        \n",
      "{'loss': 1.9367, 'learning_rate': 0.00019980736660774392, 'epoch': 0.07}        \n",
      "{'loss': 2.1162, 'learning_rate': 0.0001998071733211353, 'epoch': 0.07}         \n",
      "{'loss': 1.8385, 'learning_rate': 0.00019980697993769787, 'epoch': 0.07}        \n",
      "{'loss': 1.9726, 'learning_rate': 0.00019980678645743183, 'epoch': 0.07}        \n",
      "{'loss': 2.3132, 'learning_rate': 0.0001998065928803373, 'epoch': 0.07}         \n",
      "{'loss': 2.0363, 'learning_rate': 0.00019980639920641453, 'epoch': 0.07}        \n",
      "{'loss': 2.4455, 'learning_rate': 0.0001998062054356637, 'epoch': 0.07}         \n",
      "{'loss': 2.242, 'learning_rate': 0.000199806011568085, 'epoch': 0.07}           \n",
      "{'loss': 2.0058, 'learning_rate': 0.0001998058176036786, 'epoch': 0.07}         \n",
      "{'loss': 2.0995, 'learning_rate': 0.00019980562354244473, 'epoch': 0.07}        \n",
      "{'loss': 1.816, 'learning_rate': 0.0001998054293843835, 'epoch': 0.07}          \n",
      "{'loss': 2.1451, 'learning_rate': 0.0001998052351294952, 'epoch': 0.07}         \n",
      "{'loss': 2.1252, 'learning_rate': 0.0001998050407777799, 'epoch': 0.07}         \n",
      "{'loss': 2.2422, 'learning_rate': 0.00019980484632923785, 'epoch': 0.07}        \n",
      "{'loss': 2.3875, 'learning_rate': 0.0001998046517838693, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 73999/1061708 [11:04:28<145:30:42,  1.89it/s][2024-03-01 05:11:29,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=74000, skipped=833, lr=[0.00019980445714167433], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 05:11:29,465] [INFO] [timer.py:260:stop] epoch=0/micro_step=74000/global_step=74000, RunningAvgSamplesPerSec=1.8922874916894505, CurrSamplesPerSec=1.90618643097233, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0935, 'learning_rate': 0.00019980445714167433, 'epoch': 0.07}        \n",
      "{'loss': 1.891, 'learning_rate': 0.0001998042624026532, 'epoch': 0.07}          \n",
      "{'loss': 2.2076, 'learning_rate': 0.00019980406756680609, 'epoch': 0.07}        \n",
      "{'loss': 2.1137, 'learning_rate': 0.00019980387263413314, 'epoch': 0.07}        \n",
      "{'loss': 2.0235, 'learning_rate': 0.0001998036776046346, 'epoch': 0.07}         \n",
      "{'loss': 2.3763, 'learning_rate': 0.0001998034824783106, 'epoch': 0.07}         \n",
      "{'loss': 2.1399, 'learning_rate': 0.0001998032872551614, 'epoch': 0.07}         \n",
      "{'loss': 2.2171, 'learning_rate': 0.00019980309193518714, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74076/1061708 [11:05:09<146:23:03,  1.87it/s][2024-03-01 05:12:10,425] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1994, 'learning_rate': 0.00019980291606442506, 'epoch': 0.07}        \n",
      "{'loss': 2.1396, 'learning_rate': 0.0001998027205604837, 'epoch': 0.07}         \n",
      "{'loss': 2.0524, 'learning_rate': 0.0001998025249597179, 'epoch': 0.07}         \n",
      "{'loss': 2.1969, 'learning_rate': 0.00019980232926212777, 'epoch': 0.07}        \n",
      "{'loss': 2.1584, 'learning_rate': 0.0001998021334677135, 'epoch': 0.07}         \n",
      "{'loss': 1.8498, 'learning_rate': 0.00019980193757647533, 'epoch': 0.07}        \n",
      "{'loss': 2.2081, 'learning_rate': 0.00019980174158841342, 'epoch': 0.07}        \n",
      "{'loss': 2.4108, 'learning_rate': 0.00019980154550352795, 'epoch': 0.07}        \n",
      "{'loss': 1.7912, 'learning_rate': 0.00019980134932181913, 'epoch': 0.07}        \n",
      "{'loss': 2.0256, 'learning_rate': 0.00019980115304328716, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74177/1061708 [11:06:03<145:57:42,  1.88it/s][2024-03-01 05:13:04,277] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 74178/1061708 [11:06:04<136:50:24,  2.00it/s][2024-03-01 05:13:04,699] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0316, 'learning_rate': 0.000199800995950749, 'epoch': 0.07}          \n",
      "  7%|█▉                          | 74183/1061708 [11:06:06<145:11:52,  1.89it/s][2024-03-01 05:13:07,287] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0929, 'learning_rate': 0.0001998008191475741, 'epoch': 0.07}         \n",
      "{'loss': 2.0303, 'learning_rate': 0.00019980062260762055, 'epoch': 0.07}        \n",
      "{'loss': 2.1879, 'learning_rate': 0.00019980042597084449, 'epoch': 0.07}        \n",
      "{'loss': 1.8829, 'learning_rate': 0.00019980022923724616, 'epoch': 0.07}        \n",
      "{'loss': 2.3666, 'learning_rate': 0.00019980003240682576, 'epoch': 0.07}        \n",
      "{'loss': 2.3859, 'learning_rate': 0.00019979983547958348, 'epoch': 0.07}        \n",
      "{'loss': 1.918, 'learning_rate': 0.00019979963845551948, 'epoch': 0.07}         \n",
      "{'loss': 2.4268, 'learning_rate': 0.00019979944133463397, 'epoch': 0.07}        \n",
      "{'loss': 2.1427, 'learning_rate': 0.00019979924411692716, 'epoch': 0.07}        \n",
      "{'loss': 2.1506, 'learning_rate': 0.00019979904680239924, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74285/1061708 [11:07:01<146:41:47,  1.87it/s][2024-03-01 05:14:01,601] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2664, 'learning_rate': 0.00019979886913654217, 'epoch': 0.07}        \n",
      "{'loss': 2.524, 'learning_rate': 0.0001997986716380546, 'epoch': 0.07}          \n",
      "{'loss': 2.02, 'learning_rate': 0.0001997984740427465, 'epoch': 0.07}           \n",
      "{'loss': 2.0329, 'learning_rate': 0.000199798276350618, 'epoch': 0.07}          \n",
      "{'loss': 2.2991, 'learning_rate': 0.00019979807856166932, 'epoch': 0.07}        \n",
      "{'loss': 2.2006, 'learning_rate': 0.00019979788067590065, 'epoch': 0.07}        \n",
      "{'loss': 1.7973, 'learning_rate': 0.00019979768269331216, 'epoch': 0.07}        \n",
      "{'loss': 2.0286, 'learning_rate': 0.00019979748461390408, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74361/1061708 [11:07:41<145:13:43,  1.89it/s][2024-03-01 05:14:42,034] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.518, 'learning_rate': 0.0001997973062596562, 'epoch': 0.07}          \n",
      "{'loss': 1.9345, 'learning_rate': 0.0001997971079962914, 'epoch': 0.07}         \n",
      "{'loss': 1.8, 'learning_rate': 0.0001997969096361075, 'epoch': 0.07}            \n",
      "{'loss': 2.2127, 'learning_rate': 0.00019979671117910478, 'epoch': 0.07}        \n",
      "{'loss': 2.0659, 'learning_rate': 0.0001997965126252834, 'epoch': 0.07}         \n",
      "{'loss': 2.3095, 'learning_rate': 0.0001997963139746435, 'epoch': 0.07}         \n",
      "{'loss': 2.167, 'learning_rate': 0.00019979611522718535, 'epoch': 0.07}         \n",
      "{'loss': 2.0157, 'learning_rate': 0.0001997959163829091, 'epoch': 0.07}         \n",
      "{'loss': 1.9859, 'learning_rate': 0.00019979571744181498, 'epoch': 0.07}        \n",
      "{'loss': 2.1998, 'learning_rate': 0.00019979551840390315, 'epoch': 0.07}        \n",
      "{'loss': 2.2923, 'learning_rate': 0.00019979531926917377, 'epoch': 0.07}        \n",
      "{'loss': 2.1016, 'learning_rate': 0.0001997951200376271, 'epoch': 0.07}         \n",
      "{'loss': 2.2217, 'learning_rate': 0.00019979492070926332, 'epoch': 0.07}        \n",
      "{'loss': 1.9053, 'learning_rate': 0.00019979472128408258, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74505/1061708 [11:08:58<146:38:40,  1.87it/s][2024-03-01 05:15:58,749] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.261, 'learning_rate': 0.00019979454171864162, 'epoch': 0.07}         \n",
      "{'loss': 2.3429, 'learning_rate': 0.00019979434210950925, 'epoch': 0.07}        \n",
      "{'loss': 2.3885, 'learning_rate': 0.0001997941424035605, 'epoch': 0.07}         \n",
      "{'loss': 2.2372, 'learning_rate': 0.00019979394260079562, 'epoch': 0.07}        \n",
      "{'loss': 2.0487, 'learning_rate': 0.00019979374270121474, 'epoch': 0.07}        \n",
      "{'loss': 2.3016, 'learning_rate': 0.00019979354270481805, 'epoch': 0.07}        \n",
      "{'loss': 2.2638, 'learning_rate': 0.00019979334261160578, 'epoch': 0.07}        \n",
      "{'loss': 2.1905, 'learning_rate': 0.00019979314242157811, 'epoch': 0.07}        \n",
      "{'loss': 2.0489, 'learning_rate': 0.00019979294213473523, 'epoch': 0.07}        \n",
      "{'loss': 1.8181, 'learning_rate': 0.00019979274175107735, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74606/1061708 [11:09:52<146:29:43,  1.87it/s][2024-03-01 05:16:52,557] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 74607/1061708 [11:09:52<138:32:22,  1.98it/s][2024-03-01 05:16:52,979] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2502, 'learning_rate': 0.0001997925813744444, 'epoch': 0.07}         \n",
      "{'loss': 1.8181, 'learning_rate': 0.00019979238081651996, 'epoch': 0.07}        \n",
      "{'loss': 1.7598, 'learning_rate': 0.00019979218016178107, 'epoch': 0.07}        \n",
      "{'loss': 1.848, 'learning_rate': 0.0001997919794102279, 'epoch': 0.07}          \n",
      "{'loss': 2.1701, 'learning_rate': 0.00019979177856186068, 'epoch': 0.07}        \n",
      "{'loss': 2.0138, 'learning_rate': 0.00019979157761667958, 'epoch': 0.07}        \n",
      "{'loss': 2.1603, 'learning_rate': 0.0001997913765746848, 'epoch': 0.07}         \n",
      "{'loss': 2.0652, 'learning_rate': 0.00019979117543587652, 'epoch': 0.07}        \n",
      "{'loss': 2.0264, 'learning_rate': 0.00019979097420025493, 'epoch': 0.07}        \n",
      "{'loss': 2.3869, 'learning_rate': 0.00019979077286782024, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74708/1061708 [11:10:46<145:36:15,  1.88it/s][2024-03-01 05:17:46,738] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 74709/1061708 [11:10:46<136:34:13,  2.01it/s][2024-03-01 05:17:47,160] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3322, 'learning_rate': 0.00019979061173216718, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74711/1061708 [11:10:47<134:48:22,  2.03it/s][2024-03-01 05:17:48,118] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9147, 'learning_rate': 0.00019979043038049574, 'epoch': 0.07}        \n",
      "{'loss': 2.1025, 'learning_rate': 0.00019979022878666665, 'epoch': 0.07}        \n",
      "{'loss': 2.0428, 'learning_rate': 0.0001997900270960252, 'epoch': 0.07}         \n",
      "{'loss': 1.5948, 'learning_rate': 0.00019978982530857153, 'epoch': 0.07}        \n",
      "{'loss': 1.7862, 'learning_rate': 0.0001997896234243059, 'epoch': 0.07}         \n",
      "{'loss': 2.2883, 'learning_rate': 0.0001997894214432285, 'epoch': 0.07}         \n",
      "{'loss': 2.0024, 'learning_rate': 0.00019978921936533945, 'epoch': 0.07}        \n",
      "{'loss': 2.1311, 'learning_rate': 0.00019978901719063904, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 74799/1061708 [11:11:34<145:11:17,  1.89it/s][2024-03-01 05:18:34,944] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9051, 'learning_rate': 0.0001997888351506351, 'epoch': 0.07}         \n",
      "{'loss': 2.2373, 'learning_rate': 0.00019978863279199353, 'epoch': 0.07}        \n",
      "{'loss': 2.1369, 'learning_rate': 0.00019978843033654115, 'epoch': 0.07}        \n",
      "{'loss': 2.2284, 'learning_rate': 0.00019978822778427818, 'epoch': 0.07}        \n",
      "{'loss': 2.4564, 'learning_rate': 0.00019978802513520471, 'epoch': 0.07}        \n",
      "{'loss': 2.1398, 'learning_rate': 0.00019978782238932102, 'epoch': 0.07}        \n",
      "{'loss': 2.3275, 'learning_rate': 0.00019978761954662732, 'epoch': 0.07}        \n",
      "{'loss': 2.0736, 'learning_rate': 0.00019978741660712373, 'epoch': 0.07}        \n",
      "{'loss': 2.4719, 'learning_rate': 0.00019978721357081055, 'epoch': 0.07}        \n",
      "{'loss': 2.0521, 'learning_rate': 0.00019978701043768787, 'epoch': 0.07}        \n",
      "{'loss': 2.3425, 'learning_rate': 0.0001997868072077559, 'epoch': 0.07}         \n",
      "{'loss': 1.9458, 'learning_rate': 0.00019978660388101494, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3491, 'learning_rate': 0.0001997864004574651, 'epoch': 0.07}         \n",
      "{'loss': 2.2955, 'learning_rate': 0.00019978619693710657, 'epoch': 0.07}        \n",
      "{'loss': 2.5398, 'learning_rate': 0.00019978599331993958, 'epoch': 0.07}        \n",
      "{'loss': 1.8486, 'learning_rate': 0.0001997857896059643, 'epoch': 0.07}         \n",
      "{'loss': 2.175, 'learning_rate': 0.000199785585795181, 'epoch': 0.07}           \n",
      "{'loss': 1.8844, 'learning_rate': 0.0001997853818875898, 'epoch': 0.07}         \n",
      "{'loss': 2.3687, 'learning_rate': 0.0001997851778831909, 'epoch': 0.07}         \n",
      "{'loss': 2.0623, 'learning_rate': 0.00019978497378198457, 'epoch': 0.07}        \n",
      "{'loss': 2.3017, 'learning_rate': 0.00019978476958397093, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75000/1061708 [11:13:21<145:22:56,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 05:20:21,589 >> Saving model checkpoint to output_model/checkpoint-75000\n",
      "[INFO|trainer.py:2880] 2024-03-01 05:20:21,591 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 05:20:22,812 >> tokenizer config file saved in output_model/checkpoint-75000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 05:20:22,812 >> Special tokens file saved in output_model/checkpoint-75000/special_tokens_map.json\n",
      "[2024-03-01 05:20:22,813] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step75000 is about to be saved!\n",
      "[2024-03-01 05:20:28,096] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-75000/global_step75000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 05:20:28,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-75000/global_step75000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 05:20:41,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-75000/global_step75000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 05:20:42,704] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-75000/global_step75000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 05:20:49,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-75000/global_step75000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 05:20:49,874] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-75000/global_step75000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 05:20:49,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step75000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 05:20:49,940 >> Deleting older checkpoint [output_model/checkpoint-60000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 05:20:53,693 >> tokenizer config file saved in output_model/checkpoint-75000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 05:20:53,693 >> Special tokens file saved in output_model/checkpoint-75000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.0801, 'learning_rate': 0.0001997845652891502, 'epoch': 0.07}         \n",
      "{'loss': 2.0457, 'learning_rate': 0.0001997843608975226, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 75025/1061708 [11:14:06<145:57:57,  1.88it/s][2024-03-01 05:21:07,353] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1627, 'learning_rate': 0.00019978417686228805, 'epoch': 0.07}        \n",
      "{'loss': 2.1631, 'learning_rate': 0.00019978397228672792, 'epoch': 0.07}        \n",
      "{'loss': 2.3023, 'learning_rate': 0.00019978376761436146, 'epoch': 0.07}        \n",
      "{'loss': 1.881, 'learning_rate': 0.00019978356284518892, 'epoch': 0.07}         \n",
      "{'loss': 2.4154, 'learning_rate': 0.00019978335797921046, 'epoch': 0.07}        \n",
      "{'loss': 2.4383, 'learning_rate': 0.0001997831530164263, 'epoch': 0.07}         \n",
      "{'loss': 2.0871, 'learning_rate': 0.00019978294795683658, 'epoch': 0.07}        \n",
      "{'loss': 2.0625, 'learning_rate': 0.0001997827428004416, 'epoch': 0.07}         \n",
      "{'loss': 1.8567, 'learning_rate': 0.00019978253754724147, 'epoch': 0.07}        \n",
      "{'loss': 2.2943, 'learning_rate': 0.00019978233219723643, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75126/1061708 [11:15:00<146:25:38,  1.87it/s][2024-03-01 05:22:01,147] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 75127/1061708 [11:15:01<137:23:46,  1.99it/s][2024-03-01 05:22:01,575] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.5932, 'learning_rate': 0.00019978216784753298, 'epoch': 0.07}        \n",
      "{'loss': 2.0709, 'learning_rate': 0.00019978196232327958, 'epoch': 0.07}        \n",
      "{'loss': 2.3801, 'learning_rate': 0.00019978175670222186, 'epoch': 0.07}        \n",
      "{'loss': 1.9892, 'learning_rate': 0.00019978155098435995, 'epoch': 0.07}        \n",
      "{'loss': 2.3395, 'learning_rate': 0.0001997813451696941, 'epoch': 0.07}         \n",
      "{'loss': 2.0526, 'learning_rate': 0.00019978113925822445, 'epoch': 0.07}        \n",
      "{'loss': 2.2404, 'learning_rate': 0.00019978093324995126, 'epoch': 0.07}        \n",
      "{'loss': 2.0329, 'learning_rate': 0.00019978072714487474, 'epoch': 0.07}        \n",
      "{'loss': 2.2243, 'learning_rate': 0.00019978052094299503, 'epoch': 0.07}        \n",
      "{'loss': 2.4712, 'learning_rate': 0.00019978031464431238, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75228/1061708 [11:15:54<146:55:53,  1.86it/s][2024-03-01 05:22:55,461] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 75229/1061708 [11:15:55<137:32:57,  1.99it/s][2024-03-01 05:22:55,884] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7613, 'learning_rate': 0.00019978014953566823, 'epoch': 0.07}        \n",
      "{'loss': 2.5646, 'learning_rate': 0.00019977994306274072, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75243/1061708 [11:16:02<149:01:46,  1.84it/s][2024-03-01 05:23:03,290] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3536, 'learning_rate': 0.00019977975715433993, 'epoch': 0.07}        \n",
      "{'loss': 1.7362, 'learning_rate': 0.00019977955049748806, 'epoch': 0.07}        \n",
      "{'loss': 1.8855, 'learning_rate': 0.00019977934374383418, 'epoch': 0.07}        \n",
      "{'loss': 2.3995, 'learning_rate': 0.00019977913689337844, 'epoch': 0.07}        \n",
      "{'loss': 1.6585, 'learning_rate': 0.0001997789299461211, 'epoch': 0.07}         \n",
      "{'loss': 2.027, 'learning_rate': 0.00019977872290206236, 'epoch': 0.07}         \n",
      "{'loss': 1.8607, 'learning_rate': 0.0001997785157612024, 'epoch': 0.07}         \n",
      "{'loss': 2.097, 'learning_rate': 0.00019977830852354142, 'epoch': 0.07}         \n",
      "{'loss': 2.0479, 'learning_rate': 0.00019977810118907961, 'epoch': 0.07}        \n",
      "{'loss': 2.0254, 'learning_rate': 0.0001997778937578172, 'epoch': 0.07}         \n",
      "{'loss': 1.8403, 'learning_rate': 0.00019977768622975439, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75352/1061708 [11:17:00<145:10:45,  1.89it/s][2024-03-01 05:24:01,417] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|█▉                          | 75359/1061708 [11:17:04<145:32:10,  1.88it/s][2024-03-01 05:24:05,061] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1146, 'learning_rate': 0.00019977752013760797, 'epoch': 0.07}        \n",
      "{'loss': 2.0471, 'learning_rate': 0.00019977731243530492, 'epoch': 0.07}        \n",
      "{'loss': 2.0171, 'learning_rate': 0.00019977710463620204, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2398, 'learning_rate': 0.00019977689674029948, 'epoch': 0.07}        \n",
      "{'loss': 2.3001, 'learning_rate': 0.0001997766887475975, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 75409/1061708 [11:17:31<145:20:38,  1.88it/s][2024-03-01 05:24:31,610] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1886, 'learning_rate': 0.00019977650147140233, 'epoch': 0.07}        \n",
      "{'loss': 1.839, 'learning_rate': 0.00019977629329478197, 'epoch': 0.07}         \n",
      "{'loss': 1.8963, 'learning_rate': 0.0001997760850213627, 'epoch': 0.07}         \n",
      "{'loss': 1.9316, 'learning_rate': 0.00019977587665114484, 'epoch': 0.07}        \n",
      "{'loss': 2.1335, 'learning_rate': 0.00019977566818412846, 'epoch': 0.07}        \n",
      "{'loss': 2.3101, 'learning_rate': 0.00019977545962031389, 'epoch': 0.07}        \n",
      "{'loss': 2.3434, 'learning_rate': 0.00019977525095970123, 'epoch': 0.07}        \n",
      "{'loss': 2.0685, 'learning_rate': 0.00019977504220229072, 'epoch': 0.07}        \n",
      "{'loss': 1.7213, 'learning_rate': 0.00019977483334808257, 'epoch': 0.07}        \n",
      "{'loss': 2.0754, 'learning_rate': 0.00019977462439707696, 'epoch': 0.07}        \n",
      "{'loss': 2.2776, 'learning_rate': 0.00019977441534927418, 'epoch': 0.07}        \n",
      "{'loss': 1.7766, 'learning_rate': 0.0001997742062046743, 'epoch': 0.07}         \n",
      "{'loss': 1.6198, 'learning_rate': 0.0001997739969632776, 'epoch': 0.07}         \n",
      "{'loss': 2.3035, 'learning_rate': 0.00019977378762508425, 'epoch': 0.07}        \n",
      "{'loss': 2.1717, 'learning_rate': 0.0001997735781900945, 'epoch': 0.07}         \n",
      "{'loss': 1.731, 'learning_rate': 0.00019977336865830854, 'epoch': 0.07}         \n",
      "{'loss': 2.5774, 'learning_rate': 0.00019977315902972652, 'epoch': 0.07}        \n",
      "{'loss': 2.3433, 'learning_rate': 0.00019977294930434872, 'epoch': 0.07}        \n",
      "{'loss': 2.0386, 'learning_rate': 0.00019977273948217525, 'epoch': 0.07}        \n",
      "{'loss': 1.8551, 'learning_rate': 0.00019977252956320643, 'epoch': 0.07}        \n",
      "{'loss': 1.9258, 'learning_rate': 0.00019977231954744235, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75613/1061708 [11:19:19<148:24:12,  1.85it/s][2024-03-01 05:26:20,385] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3416, 'learning_rate': 0.00019977213045049497, 'epoch': 0.07}        \n",
      "{'loss': 2.3371, 'learning_rate': 0.0001997719202508206, 'epoch': 0.07}         \n",
      "{'loss': 2.0797, 'learning_rate': 0.00019977170995435158, 'epoch': 0.07}        \n",
      "{'loss': 1.888, 'learning_rate': 0.00019977149956108817, 'epoch': 0.07}         \n",
      "{'loss': 1.9986, 'learning_rate': 0.00019977128907103055, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75667/1061708 [11:19:48<145:39:22,  1.88it/s][2024-03-01 05:26:49,107] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.988, 'learning_rate': 0.0001997710995472198, 'epoch': 0.07}          \n",
      "{'loss': 1.9322, 'learning_rate': 0.00019977088887325375, 'epoch': 0.07}        \n",
      "{'loss': 1.8383, 'learning_rate': 0.00019977067810249407, 'epoch': 0.07}        \n",
      "{'loss': 2.2536, 'learning_rate': 0.00019977046723494097, 'epoch': 0.07}        \n",
      "{'loss': 2.1659, 'learning_rate': 0.0001997702562705947, 'epoch': 0.07}         \n",
      "{'loss': 1.8378, 'learning_rate': 0.0001997700452094554, 'epoch': 0.07}         \n",
      "{'loss': 2.2319, 'learning_rate': 0.00019976983405152333, 'epoch': 0.07}        \n",
      "{'loss': 2.3724, 'learning_rate': 0.00019976962279679862, 'epoch': 0.07}        \n",
      "{'loss': 2.4841, 'learning_rate': 0.00019976941144528155, 'epoch': 0.07}        \n",
      "{'loss': 2.1237, 'learning_rate': 0.0001997691999969723, 'epoch': 0.07}         \n",
      "{'loss': 2.2467, 'learning_rate': 0.00019976898845187107, 'epoch': 0.07}        \n",
      "{'loss': 2.2248, 'learning_rate': 0.00019976877680997807, 'epoch': 0.07}        \n",
      "{'loss': 1.999, 'learning_rate': 0.00019976856507129348, 'epoch': 0.07}         \n",
      "  7%|█▉                          | 75793/1061708 [11:20:55<149:03:21,  1.84it/s][2024-03-01 05:27:56,282] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.221, 'learning_rate': 0.00019976837442372075, 'epoch': 0.07}         \n",
      "{'loss': 2.2776, 'learning_rate': 0.00019976816250113276, 'epoch': 0.07}        \n",
      "  7%|█▉                          | 75815/1061708 [11:21:07<146:48:08,  1.87it/s][2024-03-01 05:28:07,976] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4192, 'learning_rate': 0.0001997679716880473, 'epoch': 0.07}         \n",
      "{'loss': 2.0818, 'learning_rate': 0.00019976775958155667, 'epoch': 0.07}        \n",
      "{'loss': 2.4001, 'learning_rate': 0.00019976754737827546, 'epoch': 0.07}        \n",
      "{'loss': 1.9552, 'learning_rate': 0.00019976733507820384, 'epoch': 0.07}        \n",
      "{'loss': 2.1576, 'learning_rate': 0.00019976712268134205, 'epoch': 0.07}        \n",
      "{'loss': 2.265, 'learning_rate': 0.00019976691018769034, 'epoch': 0.07}         \n",
      "{'loss': 1.8432, 'learning_rate': 0.00019976669759724883, 'epoch': 0.07}        \n",
      "{'loss': 2.0435, 'learning_rate': 0.0001997664849100178, 'epoch': 0.07}         \n",
      "{'loss': 1.9358, 'learning_rate': 0.00019976627212599742, 'epoch': 0.07}        \n",
      "{'loss': 2.3468, 'learning_rate': 0.0001997660592451879, 'epoch': 0.07}         \n",
      "{'loss': 1.9899, 'learning_rate': 0.00019976584626758945, 'epoch': 0.07}        \n",
      "{'loss': 2.2121, 'learning_rate': 0.0001997656331932023, 'epoch': 0.07}         \n",
      "{'loss': 1.6973, 'learning_rate': 0.0001997654200220266, 'epoch': 0.07}         \n",
      "{'loss': 2.1973, 'learning_rate': 0.0001997652067540626, 'epoch': 0.07}         \n",
      "{'loss': 1.9625, 'learning_rate': 0.00019976499338931052, 'epoch': 0.07}        \n",
      "{'loss': 2.3768, 'learning_rate': 0.0001997647799277705, 'epoch': 0.07}         \n",
      "{'loss': 2.3146, 'learning_rate': 0.00019976456636944283, 'epoch': 0.07}        \n",
      "{'loss': 1.877, 'learning_rate': 0.00019976435271432764, 'epoch': 0.07}         \n",
      "  7%|██                          | 75999/1061708 [11:22:45<145:29:44,  1.88it/s][2024-03-01 05:29:46,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=76000, skipped=859, lr=[0.0001997641389624252], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 05:29:46,340] [INFO] [timer.py:260:stop] epoch=0/micro_step=76000/global_step=76000, RunningAvgSamplesPerSec=1.892323630721924, CurrSamplesPerSec=1.9008691971381166, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.9376, 'learning_rate': 0.0001997641389624252, 'epoch': 0.07}         \n",
      "{'loss': 2.1821, 'learning_rate': 0.0001997639251137357, 'epoch': 0.07}         \n",
      "  7%|██                          | 76016/1061708 [11:22:54<146:18:53,  1.87it/s][2024-03-01 05:29:55,334] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 76017/1061708 [11:22:55<137:13:39,  2.00it/s][2024-03-01 05:29:55,757] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.887, 'learning_rate': 0.00019976375396509754, 'epoch': 0.07}         \n",
      "{'loss': 1.9641, 'learning_rate': 0.00019976353994219183, 'epoch': 0.07}        \n",
      "{'loss': 1.8927, 'learning_rate': 0.00019976332582249964, 'epoch': 0.07}        \n",
      "{'loss': 2.0637, 'learning_rate': 0.00019976311160602118, 'epoch': 0.07}        \n",
      "{'loss': 2.143, 'learning_rate': 0.00019976289729275665, 'epoch': 0.07}         \n",
      "{'loss': 2.2842, 'learning_rate': 0.00019976268288270622, 'epoch': 0.07}        \n",
      "  7%|██                          | 76074/1061708 [11:23:25<147:33:11,  1.86it/s][2024-03-01 05:30:26,143] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8587, 'learning_rate': 0.0001997624898309091, 'epoch': 0.07}         \n",
      "{'loss': 2.4323, 'learning_rate': 0.00019976227523696614, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1904, 'learning_rate': 0.0001997620605462379, 'epoch': 0.07}         \n",
      "{'loss': 1.9934, 'learning_rate': 0.0001997618457587246, 'epoch': 0.07}         \n",
      "{'loss': 2.0538, 'learning_rate': 0.0001997616308744265, 'epoch': 0.07}         \n",
      "{'loss': 2.3879, 'learning_rate': 0.00019976141589334373, 'epoch': 0.07}        \n",
      "{'loss': 2.2872, 'learning_rate': 0.00019976120081547657, 'epoch': 0.07}        \n",
      "{'loss': 2.2714, 'learning_rate': 0.00019976098564082514, 'epoch': 0.07}        \n",
      "{'loss': 1.9325, 'learning_rate': 0.00019976077036938972, 'epoch': 0.07}        \n",
      "{'loss': 2.0705, 'learning_rate': 0.00019976055500117054, 'epoch': 0.07}        \n",
      "  7%|██                          | 76179/1061708 [11:24:21<145:29:13,  1.88it/s][2024-03-01 05:31:22,214] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4487, 'learning_rate': 0.00019976036108702326, 'epoch': 0.07}        \n",
      "{'loss': 1.6723, 'learning_rate': 0.00019976014553491542, 'epoch': 0.07}        \n",
      "  7%|██                          | 76197/1061708 [11:24:31<146:08:23,  1.87it/s][2024-03-01 05:31:31,752] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0764, 'learning_rate': 0.00019975995145526875, 'epoch': 0.07}        \n",
      "{'loss': 2.1549, 'learning_rate': 0.00019975973571927298, 'epoch': 0.07}        \n",
      "{'loss': 2.434, 'learning_rate': 0.00019975951988649446, 'epoch': 0.07}         \n",
      "{'loss': 2.2894, 'learning_rate': 0.00019975930395693334, 'epoch': 0.07}        \n",
      "{'loss': 2.114, 'learning_rate': 0.00019975908793058982, 'epoch': 0.07}         \n",
      "{'loss': 1.8299, 'learning_rate': 0.00019975887180746416, 'epoch': 0.07}        \n",
      "{'loss': 2.4273, 'learning_rate': 0.00019975865558755654, 'epoch': 0.07}        \n",
      "{'loss': 1.6825, 'learning_rate': 0.00019975843927086718, 'epoch': 0.07}        \n",
      "{'loss': 2.1241, 'learning_rate': 0.00019975822285739627, 'epoch': 0.07}        \n",
      "{'loss': 2.1584, 'learning_rate': 0.00019975800634714402, 'epoch': 0.07}        \n",
      "{'loss': 2.3724, 'learning_rate': 0.0001997577897401107, 'epoch': 0.07}         \n",
      "{'loss': 2.0648, 'learning_rate': 0.00019975757303629642, 'epoch': 0.07}        \n",
      "{'loss': 2.3749, 'learning_rate': 0.00019975735623570145, 'epoch': 0.07}        \n",
      "  7%|██                          | 76326/1061708 [11:25:40<146:06:53,  1.87it/s][2024-03-01 05:32:40,588] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2741, 'learning_rate': 0.0001997571610324187, 'epoch': 0.07}         \n",
      "{'loss': 1.7871, 'learning_rate': 0.000199756944047941, 'epoch': 0.07}          \n",
      "{'loss': 2.0628, 'learning_rate': 0.00019975672696668318, 'epoch': 0.07}        \n",
      "{'loss': 2.558, 'learning_rate': 0.00019975650978864553, 'epoch': 0.07}         \n",
      "{'loss': 2.2887, 'learning_rate': 0.0001997562925138282, 'epoch': 0.07}         \n",
      "{'loss': 1.9319, 'learning_rate': 0.00019975607514223143, 'epoch': 0.07}        \n",
      "{'loss': 2.2418, 'learning_rate': 0.00019975585767385539, 'epoch': 0.07}        \n",
      "{'loss': 2.2572, 'learning_rate': 0.0001997556401087003, 'epoch': 0.07}         \n",
      "{'loss': 2.4783, 'learning_rate': 0.00019975542244676643, 'epoch': 0.07}        \n",
      "{'loss': 2.2562, 'learning_rate': 0.00019975520468805394, 'epoch': 0.07}        \n",
      "  7%|██                          | 76427/1061708 [11:26:34<146:05:11,  1.87it/s][2024-03-01 05:33:34,543] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 76428/1061708 [11:26:34<137:03:29,  2.00it/s][2024-03-01 05:33:34,966] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4066, 'learning_rate': 0.00019975503041140349, 'epoch': 0.07}        \n",
      "{'loss': 2.046, 'learning_rate': 0.00019975481247849002, 'epoch': 0.07}         \n",
      "{'loss': 2.0286, 'learning_rate': 0.00019975459444879855, 'epoch': 0.07}        \n",
      "  7%|██                          | 76456/1061708 [11:26:49<146:34:55,  1.87it/s][2024-03-01 05:33:49,858] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7916, 'learning_rate': 0.0001997543981393312, 'epoch': 0.07}         \n",
      "{'loss': 1.6831, 'learning_rate': 0.00019975417992576205, 'epoch': 0.07}        \n",
      "{'loss': 2.3319, 'learning_rate': 0.00019975396161541552, 'epoch': 0.07}        \n",
      "{'loss': 2.3802, 'learning_rate': 0.00019975374320829175, 'epoch': 0.07}        \n",
      "{'loss': 2.2986, 'learning_rate': 0.00019975352470439104, 'epoch': 0.07}        \n",
      "{'loss': 2.225, 'learning_rate': 0.00019975330610371355, 'epoch': 0.07}         \n",
      "{'loss': 2.0148, 'learning_rate': 0.0001997530874062595, 'epoch': 0.07}         \n",
      "{'loss': 2.3369, 'learning_rate': 0.00019975286861202913, 'epoch': 0.07}        \n",
      "{'loss': 1.97, 'learning_rate': 0.00019975264972102262, 'epoch': 0.07}          \n",
      "{'loss': 2.2863, 'learning_rate': 0.00019975243073324018, 'epoch': 0.07}        \n",
      "{'loss': 2.1739, 'learning_rate': 0.00019975221164868201, 'epoch': 0.07}        \n",
      "{'loss': 1.9946, 'learning_rate': 0.00019975199246734837, 'epoch': 0.07}        \n",
      "{'loss': 1.6044, 'learning_rate': 0.00019975177318923943, 'epoch': 0.07}        \n",
      "{'loss': 2.3278, 'learning_rate': 0.00019975155381435542, 'epoch': 0.07}        \n",
      "{'loss': 2.2417, 'learning_rate': 0.00019975133434269656, 'epoch': 0.07}        \n",
      "{'loss': 1.9603, 'learning_rate': 0.00019975111477426308, 'epoch': 0.07}        \n",
      "{'loss': 2.2427, 'learning_rate': 0.00019975089510905513, 'epoch': 0.07}        \n",
      "{'loss': 2.0251, 'learning_rate': 0.000199750675347073, 'epoch': 0.07}          \n",
      "{'loss': 1.6913, 'learning_rate': 0.0001997504554883168, 'epoch': 0.07}         \n",
      "{'loss': 1.9105, 'learning_rate': 0.00019975023553278685, 'epoch': 0.07}        \n",
      "  7%|██                          | 76657/1061708 [11:28:36<145:57:12,  1.87it/s][2024-03-01 05:35:37,227] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 76658/1061708 [11:28:37<136:51:08,  2.00it/s][2024-03-01 05:35:37,651] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1349, 'learning_rate': 0.0001997500594986859, 'epoch': 0.07}         \n",
      "{'loss': 2.6206, 'learning_rate': 0.00019974983936896367, 'epoch': 0.07}        \n",
      "{'loss': 2.2313, 'learning_rate': 0.00019974961914246824, 'epoch': 0.07}        \n",
      "{'loss': 1.8726, 'learning_rate': 0.00019974939881919982, 'epoch': 0.07}        \n",
      "{'loss': 2.2417, 'learning_rate': 0.00019974917839915866, 'epoch': 0.07}        \n",
      "{'loss': 2.234, 'learning_rate': 0.00019974895788234492, 'epoch': 0.07}         \n",
      "{'loss': 2.2344, 'learning_rate': 0.00019974873726875885, 'epoch': 0.07}        \n",
      "{'loss': 1.9364, 'learning_rate': 0.00019974851655840063, 'epoch': 0.07}        \n",
      "{'loss': 2.2961, 'learning_rate': 0.00019974829575127053, 'epoch': 0.07}        \n",
      "  7%|██                          | 76745/1061708 [11:29:23<146:45:46,  1.86it/s][2024-03-01 05:36:24,055] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1867, 'learning_rate': 0.00019974809694211362, 'epoch': 0.07}        \n",
      "{'loss': 2.1472, 'learning_rate': 0.00019974787595111745, 'epoch': 0.07}        \n",
      "{'loss': 2.2643, 'learning_rate': 0.00019974765486335003, 'epoch': 0.07}        \n",
      "{'loss': 2.147, 'learning_rate': 0.00019974743367881151, 'epoch': 0.07}         \n",
      "{'loss': 2.2309, 'learning_rate': 0.00019974721239750213, 'epoch': 0.07}        \n",
      "{'loss': 2.1264, 'learning_rate': 0.00019974699101942212, 'epoch': 0.07}        \n",
      "{'loss': 1.9734, 'learning_rate': 0.00019974676954457164, 'epoch': 0.07}        \n",
      "{'loss': 2.0364, 'learning_rate': 0.000199746547972951, 'epoch': 0.07}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8884, 'learning_rate': 0.00019974632630456033, 'epoch': 0.07}        \n",
      "{'loss': 2.1997, 'learning_rate': 0.0001997461045393999, 'epoch': 0.07}         \n",
      "{'loss': 2.4195, 'learning_rate': 0.00019974588267746986, 'epoch': 0.07}        \n",
      "{'loss': 2.1708, 'learning_rate': 0.00019974566071877045, 'epoch': 0.07}        \n",
      "{'loss': 2.0012, 'learning_rate': 0.00019974543866330192, 'epoch': 0.07}        \n",
      "{'loss': 2.1395, 'learning_rate': 0.00019974521651106448, 'epoch': 0.07}        \n",
      "  7%|██                          | 76885/1061708 [11:30:38<146:52:10,  1.86it/s][2024-03-01 05:37:38,858] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9955, 'learning_rate': 0.00019974501649131353, 'epoch': 0.07}        \n",
      "{'loss': 1.6558, 'learning_rate': 0.00019974479415521573, 'epoch': 0.07}        \n",
      "{'loss': 1.8625, 'learning_rate': 0.00019974457172234963, 'epoch': 0.07}        \n",
      "{'loss': 2.019, 'learning_rate': 0.0001997443491927154, 'epoch': 0.07}          \n",
      "{'loss': 1.9919, 'learning_rate': 0.00019974412656631336, 'epoch': 0.07}        \n",
      "{'loss': 1.8608, 'learning_rate': 0.00019974390384314364, 'epoch': 0.07}        \n",
      "{'loss': 2.2101, 'learning_rate': 0.00019974368102320647, 'epoch': 0.07}        \n",
      "{'loss': 1.8858, 'learning_rate': 0.00019974345810650212, 'epoch': 0.07}        \n",
      "{'loss': 1.7983, 'learning_rate': 0.00019974323509303074, 'epoch': 0.07}        \n",
      "{'loss': 2.3363, 'learning_rate': 0.00019974301198279258, 'epoch': 0.07}        \n",
      "  7%|██                          | 76986/1061708 [11:31:32<146:13:59,  1.87it/s][2024-03-01 05:38:32,767] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 76987/1061708 [11:31:32<136:57:41,  2.00it/s][2024-03-01 05:38:33,190] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2803, 'learning_rate': 0.0001997428334249301, 'epoch': 0.07}         \n",
      "{'loss': 2.1899, 'learning_rate': 0.00019974261014051226, 'epoch': 0.07}        \n",
      "  7%|██                          | 77004/1061708 [11:31:41<147:06:47,  1.86it/s][2024-03-01 05:38:42,178] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0672, 'learning_rate': 0.00019974240910180111, 'epoch': 0.07}        \n",
      "{'loss': 2.3442, 'learning_rate': 0.00019974218563352774, 'epoch': 0.07}        \n",
      "{'loss': 2.3294, 'learning_rate': 0.00019974196206848857, 'epoch': 0.07}        \n",
      "{'loss': 2.4398, 'learning_rate': 0.00019974173840668383, 'epoch': 0.07}        \n",
      "{'loss': 2.0145, 'learning_rate': 0.0001997415146481138, 'epoch': 0.07}         \n",
      "{'loss': 2.332, 'learning_rate': 0.00019974129079277866, 'epoch': 0.07}         \n",
      "{'loss': 2.1701, 'learning_rate': 0.00019974106684067858, 'epoch': 0.07}        \n",
      "{'loss': 1.9025, 'learning_rate': 0.00019974084279181383, 'epoch': 0.07}        \n",
      "{'loss': 2.0805, 'learning_rate': 0.0001997406186461846, 'epoch': 0.07}         \n",
      "{'loss': 2.1636, 'learning_rate': 0.00019974039440379113, 'epoch': 0.07}        \n",
      "{'loss': 2.1503, 'learning_rate': 0.00019974017006463364, 'epoch': 0.07}        \n",
      "{'loss': 1.958, 'learning_rate': 0.00019973994562871234, 'epoch': 0.07}         \n",
      "{'loss': 2.3754, 'learning_rate': 0.00019973972109602742, 'epoch': 0.07}        \n",
      "{'loss': 1.9101, 'learning_rate': 0.00019973949646657913, 'epoch': 0.07}        \n",
      "  7%|██                          | 77140/1061708 [11:32:54<145:01:50,  1.89it/s][2024-03-01 05:39:54,821] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0295, 'learning_rate': 0.0001997392942173432, 'epoch': 0.07}         \n",
      "{'loss': 2.0095, 'learning_rate': 0.00019973906940404509, 'epoch': 0.07}        \n",
      "{'loss': 1.9482, 'learning_rate': 0.00019973884449398424, 'epoch': 0.07}        \n",
      "{'loss': 2.1659, 'learning_rate': 0.00019973861948716085, 'epoch': 0.07}        \n",
      "{'loss': 2.042, 'learning_rate': 0.00019973839438357515, 'epoch': 0.07}         \n",
      "{'loss': 1.9918, 'learning_rate': 0.00019973816918322738, 'epoch': 0.07}        \n",
      "{'loss': 2.0169, 'learning_rate': 0.00019973794388611772, 'epoch': 0.07}        \n",
      "{'loss': 2.3747, 'learning_rate': 0.00019973771849224643, 'epoch': 0.07}        \n",
      "{'loss': 1.8825, 'learning_rate': 0.0001997374930016137, 'epoch': 0.07}         \n",
      "  7%|██                          | 77231/1061708 [11:33:42<145:08:50,  1.88it/s][2024-03-01 05:40:43,356] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0144, 'learning_rate': 0.0001997372899773134, 'epoch': 0.07}         \n",
      "{'loss': 2.0463, 'learning_rate': 0.00019973706430283453, 'epoch': 0.07}        \n",
      "{'loss': 2.3053, 'learning_rate': 0.00019973683853159485, 'epoch': 0.07}        \n",
      "{'loss': 1.9773, 'learning_rate': 0.00019973661266359463, 'epoch': 0.07}        \n",
      "{'loss': 2.164, 'learning_rate': 0.000199736386698834, 'epoch': 0.07}           \n",
      "{'loss': 2.1824, 'learning_rate': 0.00019973616063731324, 'epoch': 0.07}        \n",
      "{'loss': 2.228, 'learning_rate': 0.00019973593447903258, 'epoch': 0.07}         \n",
      "{'loss': 2.0944, 'learning_rate': 0.0001997357082239922, 'epoch': 0.07}         \n",
      "{'loss': 2.306, 'learning_rate': 0.00019973548187219233, 'epoch': 0.07}         \n",
      "{'loss': 2.0778, 'learning_rate': 0.00019973525542363324, 'epoch': 0.07}        \n",
      "{'loss': 2.2335, 'learning_rate': 0.00019973502887831503, 'epoch': 0.07}        \n",
      "{'loss': 2.1844, 'learning_rate': 0.00019973480223623805, 'epoch': 0.07}        \n",
      "{'loss': 2.0535, 'learning_rate': 0.00019973457549740244, 'epoch': 0.07}        \n",
      "{'loss': 2.163, 'learning_rate': 0.00019973434866180846, 'epoch': 0.07}         \n",
      "{'loss': 1.9895, 'learning_rate': 0.0001997341217294563, 'epoch': 0.07}         \n",
      "{'loss': 2.2227, 'learning_rate': 0.00019973389470034618, 'epoch': 0.07}        \n",
      "  7%|██                          | 77391/1061708 [11:35:08<145:14:49,  1.88it/s][2024-03-01 05:42:08,881] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.987, 'learning_rate': 0.00019973369029141922, 'epoch': 0.07}         \n",
      "{'loss': 2.0341, 'learning_rate': 0.00019973346307846962, 'epoch': 0.07}        \n",
      "{'loss': 2.1911, 'learning_rate': 0.00019973323576876273, 'epoch': 0.07}        \n",
      "  7%|██                          | 77425/1061708 [11:35:26<146:44:36,  1.86it/s][2024-03-01 05:42:26,978] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.96, 'learning_rate': 0.00019973303110729922, 'epoch': 0.07}          \n",
      "{'loss': 2.0392, 'learning_rate': 0.000199732803613754, 'epoch': 0.07}          \n",
      "{'loss': 2.2148, 'learning_rate': 0.00019973257602345218, 'epoch': 0.07}        \n",
      "{'loss': 2.0765, 'learning_rate': 0.0001997323483363939, 'epoch': 0.07}         \n",
      "{'loss': 2.2231, 'learning_rate': 0.00019973212055257938, 'epoch': 0.07}        \n",
      "{'loss': 1.9719, 'learning_rate': 0.00019973189267200888, 'epoch': 0.07}        \n",
      "{'loss': 2.1216, 'learning_rate': 0.00019973166469468257, 'epoch': 0.07}        \n",
      "{'loss': 1.7403, 'learning_rate': 0.0001997314366206007, 'epoch': 0.07}         \n",
      "{'loss': 2.0287, 'learning_rate': 0.0001997312084497635, 'epoch': 0.07}         \n",
      "{'loss': 2.0902, 'learning_rate': 0.00019973098018217117, 'epoch': 0.07}        \n",
      "{'loss': 2.2786, 'learning_rate': 0.00019973075181782394, 'epoch': 0.07}        \n",
      "{'loss': 2.5063, 'learning_rate': 0.00019973052335672207, 'epoch': 0.07}        \n",
      "{'loss': 2.3736, 'learning_rate': 0.0001997302947988657, 'epoch': 0.07}         \n",
      "{'loss': 2.032, 'learning_rate': 0.0001997300661442551, 'epoch': 0.07}          \n",
      "{'loss': 2.042, 'learning_rate': 0.00019972983739289051, 'epoch': 0.07}         \n",
      "{'loss': 1.6307, 'learning_rate': 0.00019972960854477213, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1626, 'learning_rate': 0.0001997293795999002, 'epoch': 0.07}         \n",
      "{'loss': 2.2058, 'learning_rate': 0.00019972915055827488, 'epoch': 0.07}        \n",
      "{'loss': 1.7104, 'learning_rate': 0.00019972892141989645, 'epoch': 0.07}        \n",
      "{'loss': 1.9811, 'learning_rate': 0.00019972869218476514, 'epoch': 0.07}        \n",
      "  7%|██                          | 77626/1061708 [11:37:13<145:49:34,  1.87it/s][2024-03-01 05:44:14,252] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 77627/1061708 [11:37:14<136:45:18,  2.00it/s][2024-03-01 05:44:14,675] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2001, 'learning_rate': 0.00019972850872699812, 'epoch': 0.07}        \n",
      "{'loss': 2.1037, 'learning_rate': 0.00019972827931771217, 'epoch': 0.07}        \n",
      "{'loss': 1.9588, 'learning_rate': 0.0001997280498116739, 'epoch': 0.07}         \n",
      "  7%|██                          | 77650/1061708 [11:37:26<145:01:28,  1.88it/s][2024-03-01 05:44:26,837] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9705, 'learning_rate': 0.00019972784317351643, 'epoch': 0.07}        \n",
      "{'loss': 1.7547, 'learning_rate': 0.00019972761348364948, 'epoch': 0.07}        \n",
      "{'loss': 1.7287, 'learning_rate': 0.00019972738369703085, 'epoch': 0.07}        \n",
      "{'loss': 1.9513, 'learning_rate': 0.00019972715381366083, 'epoch': 0.07}        \n",
      "  7%|██                          | 77697/1061708 [11:37:51<145:34:54,  1.88it/s][2024-03-01 05:44:51,860] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.298, 'learning_rate': 0.00019972694683590556, 'epoch': 0.07}         \n",
      "{'loss': 2.2997, 'learning_rate': 0.00019972671676870846, 'epoch': 0.07}        \n",
      "{'loss': 2.1305, 'learning_rate': 0.00019972648660476064, 'epoch': 0.07}        \n",
      "{'loss': 2.085, 'learning_rate': 0.00019972625634406225, 'epoch': 0.07}         \n",
      "{'loss': 2.0604, 'learning_rate': 0.00019972602598661356, 'epoch': 0.07}        \n",
      "{'loss': 2.0873, 'learning_rate': 0.00019972579553241478, 'epoch': 0.07}        \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019972556498146614, 'epoch': 0.07}        \n",
      "{'loss': 2.1913, 'learning_rate': 0.00019972533433376783, 'epoch': 0.07}        \n",
      "{'loss': 2.2687, 'learning_rate': 0.00019972510358932014, 'epoch': 0.07}        \n",
      "{'loss': 2.2364, 'learning_rate': 0.0001997248727481232, 'epoch': 0.07}         \n",
      "{'loss': 2.2987, 'learning_rate': 0.00019972464181017732, 'epoch': 0.07}        \n",
      "{'loss': 1.9396, 'learning_rate': 0.00019972441077548267, 'epoch': 0.07}        \n",
      "{'loss': 2.1609, 'learning_rate': 0.00019972417964403953, 'epoch': 0.07}        \n",
      "{'loss': 2.0116, 'learning_rate': 0.00019972394841584804, 'epoch': 0.07}        \n",
      "{'loss': 2.0703, 'learning_rate': 0.0001997237170909085, 'epoch': 0.07}         \n",
      "{'loss': 2.3121, 'learning_rate': 0.00019972348566922113, 'epoch': 0.07}        \n",
      "{'loss': 1.9845, 'learning_rate': 0.00019972325415078612, 'epoch': 0.07}        \n",
      "{'loss': 1.8589, 'learning_rate': 0.0001997230225356037, 'epoch': 0.07}         \n",
      "{'loss': 2.3172, 'learning_rate': 0.00019972279082367412, 'epoch': 0.07}        \n",
      "{'loss': 1.8239, 'learning_rate': 0.00019972255901499754, 'epoch': 0.07}        \n",
      "{'loss': 2.6038, 'learning_rate': 0.0001997223271095743, 'epoch': 0.07}         \n",
      "{'loss': 1.9065, 'learning_rate': 0.0001997220951074045, 'epoch': 0.07}         \n",
      "{'loss': 2.2242, 'learning_rate': 0.00019972186300848844, 'epoch': 0.07}        \n",
      "{'loss': 2.1019, 'learning_rate': 0.00019972163081282633, 'epoch': 0.07}        \n",
      "{'loss': 2.1951, 'learning_rate': 0.0001997213985204184, 'epoch': 0.07}         \n",
      "  7%|██                          | 77944/1061708 [11:40:03<147:28:00,  1.85it/s][2024-03-01 05:47:03,856] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3148, 'learning_rate': 0.00019972118937453375, 'epoch': 0.07}        \n",
      "{'loss': 2.4179, 'learning_rate': 0.00019972095689830933, 'epoch': 0.07}        \n",
      "{'loss': 2.0916, 'learning_rate': 0.0001997207243253398, 'epoch': 0.07}         \n",
      "{'loss': 2.0739, 'learning_rate': 0.00019972049165562528, 'epoch': 0.07}        \n",
      "{'loss': 1.9654, 'learning_rate': 0.00019972025888916604, 'epoch': 0.07}        \n",
      "  7%|██                          | 77999/1061708 [11:40:32<145:19:24,  1.88it/s][2024-03-01 05:47:33,179] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-03-01 05:47:33,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=78000, skipped=885, lr=[0.00019972004931663616], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 05:47:33,179] [INFO] [timer.py:260:stop] epoch=0/micro_step=78000/global_step=78000, RunningAvgSamplesPerSec=1.8922700552353395, CurrSamplesPerSec=2.3499462980383234, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3818, 'learning_rate': 0.00019972004931663616, 'epoch': 0.07}        \n",
      "{'loss': 2.1723, 'learning_rate': 0.0001997198163663626, 'epoch': 0.07}         \n",
      "{'loss': 1.7591, 'learning_rate': 0.00019971958331934495, 'epoch': 0.07}        \n",
      "{'loss': 1.9971, 'learning_rate': 0.00019971935017558347, 'epoch': 0.07}        \n",
      "{'loss': 2.2696, 'learning_rate': 0.00019971911693507832, 'epoch': 0.07}        \n",
      "{'loss': 2.1161, 'learning_rate': 0.00019971888359782985, 'epoch': 0.07}        \n",
      "{'loss': 2.4358, 'learning_rate': 0.00019971865016383817, 'epoch': 0.07}        \n",
      "{'loss': 2.2798, 'learning_rate': 0.00019971841663310357, 'epoch': 0.07}        \n",
      "{'loss': 2.1174, 'learning_rate': 0.0001997181830056263, 'epoch': 0.07}         \n",
      "{'loss': 2.1798, 'learning_rate': 0.0001997179492814065, 'epoch': 0.07}         \n",
      "{'loss': 2.0848, 'learning_rate': 0.00019971771546044444, 'epoch': 0.07}        \n",
      "{'loss': 1.8194, 'learning_rate': 0.00019971748154274036, 'epoch': 0.07}        \n",
      "{'loss': 1.6362, 'learning_rate': 0.0001997172475282945, 'epoch': 0.07}         \n",
      "{'loss': 2.3825, 'learning_rate': 0.00019971701341710705, 'epoch': 0.07}        \n",
      "{'loss': 1.9426, 'learning_rate': 0.00019971677920917826, 'epoch': 0.07}        \n",
      "{'loss': 1.9887, 'learning_rate': 0.00019971654490450833, 'epoch': 0.07}        \n",
      "  7%|██                          | 78155/1061708 [11:41:55<146:33:05,  1.86it/s][2024-03-01 05:48:56,467] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1272, 'learning_rate': 0.00019971633394759192, 'epoch': 0.07}        \n",
      "{'loss': 2.2329, 'learning_rate': 0.0001997160994591145, 'epoch': 0.07}         \n",
      "{'loss': 2.281, 'learning_rate': 0.00019971586487389664, 'epoch': 0.07}         \n",
      "{'loss': 2.3854, 'learning_rate': 0.0001997156301919385, 'epoch': 0.07}         \n",
      "{'loss': 2.1626, 'learning_rate': 0.0001997153954132404, 'epoch': 0.07}         \n",
      "  7%|██                          | 78205/1061708 [11:42:22<146:43:25,  1.86it/s][2024-03-01 05:49:23,126] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9541, 'learning_rate': 0.00019971518402969954, 'epoch': 0.07}        \n",
      "{'loss': 2.1041, 'learning_rate': 0.00019971494906719607, 'epoch': 0.07}        \n",
      "{'loss': 2.1444, 'learning_rate': 0.00019971471400795322, 'epoch': 0.07}        \n",
      "  7%|██                          | 78235/1061708 [11:42:38<146:32:00,  1.86it/s][2024-03-01 05:49:39,095] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.6288, 'learning_rate': 0.00019971450237192269, 'epoch': 0.07}        \n",
      "{'loss': 2.2036, 'learning_rate': 0.0001997142671288757, 'epoch': 0.07}         \n",
      "{'loss': 2.1305, 'learning_rate': 0.00019971403178909003, 'epoch': 0.07}        \n",
      "{'loss': 2.342, 'learning_rate': 0.00019971379635256594, 'epoch': 0.07}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5956, 'learning_rate': 0.00019971356081930356, 'epoch': 0.07}        \n",
      "{'loss': 2.1317, 'learning_rate': 0.0001997133251893032, 'epoch': 0.07}         \n",
      "{'loss': 1.8896, 'learning_rate': 0.00019971308946256514, 'epoch': 0.07}        \n",
      "{'loss': 2.0058, 'learning_rate': 0.00019971285363908946, 'epoch': 0.07}        \n",
      "{'loss': 2.4391, 'learning_rate': 0.00019971261771887649, 'epoch': 0.07}        \n",
      "{'loss': 2.2257, 'learning_rate': 0.00019971238170192644, 'epoch': 0.07}        \n",
      "{'loss': 2.5118, 'learning_rate': 0.00019971214558823954, 'epoch': 0.07}        \n",
      "{'loss': 1.8712, 'learning_rate': 0.000199711909377816, 'epoch': 0.07}          \n",
      "{'loss': 2.2948, 'learning_rate': 0.00019971167307065608, 'epoch': 0.07}        \n",
      "{'loss': 1.9114, 'learning_rate': 0.00019971143666675997, 'epoch': 0.07}        \n",
      "{'loss': 2.2559, 'learning_rate': 0.00019971120016612796, 'epoch': 0.07}        \n",
      "{'loss': 2.2077, 'learning_rate': 0.00019971096356876023, 'epoch': 0.07}        \n",
      "{'loss': 2.0543, 'learning_rate': 0.000199710726874657, 'epoch': 0.07}          \n",
      "{'loss': 2.0383, 'learning_rate': 0.00019971049008381852, 'epoch': 0.07}        \n",
      "{'loss': 2.1992, 'learning_rate': 0.00019971025319624503, 'epoch': 0.07}        \n",
      "{'loss': 2.0445, 'learning_rate': 0.00019971001621193674, 'epoch': 0.07}        \n",
      "  7%|██                          | 78437/1061708 [11:44:26<145:16:58,  1.88it/s][2024-03-01 05:51:26,789] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3402, 'learning_rate': 0.00019970980284335127, 'epoch': 0.07}        \n",
      "  7%|██                          | 78447/1061708 [11:44:31<144:49:23,  1.89it/s][2024-03-01 05:51:32,025] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9752, 'learning_rate': 0.00019970958939641094, 'epoch': 0.07}        \n",
      "{'loss': 2.4982, 'learning_rate': 0.00019970935214124646, 'epoch': 0.07}        \n",
      "{'loss': 2.2333, 'learning_rate': 0.0001997091147893481, 'epoch': 0.07}         \n",
      "{'loss': 2.3568, 'learning_rate': 0.00019970887734071602, 'epoch': 0.07}        \n",
      "{'loss': 2.17, 'learning_rate': 0.0001997086397953505, 'epoch': 0.07}           \n",
      "{'loss': 2.3306, 'learning_rate': 0.00019970840215325177, 'epoch': 0.07}        \n",
      "{'loss': 2.1857, 'learning_rate': 0.00019970816441442, 'epoch': 0.07}           \n",
      "{'loss': 2.0319, 'learning_rate': 0.0001997079265788555, 'epoch': 0.07}         \n",
      "{'loss': 2.1433, 'learning_rate': 0.00019970768864655848, 'epoch': 0.07}        \n",
      "{'loss': 2.4616, 'learning_rate': 0.00019970745061752916, 'epoch': 0.07}        \n",
      "  7%|██                          | 78547/1061708 [11:45:24<145:19:31,  1.88it/s][2024-03-01 05:52:25,269] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4062, 'learning_rate': 0.00019970723630869684, 'epoch': 0.07}        \n",
      "{'loss': 2.4418, 'learning_rate': 0.00019970699809587677, 'epoch': 0.07}        \n",
      "{'loss': 2.3688, 'learning_rate': 0.00019970675978632504, 'epoch': 0.07}        \n",
      "{'loss': 2.6835, 'learning_rate': 0.00019970652138004197, 'epoch': 0.07}        \n",
      "{'loss': 2.3413, 'learning_rate': 0.00019970628287702767, 'epoch': 0.07}        \n",
      "{'loss': 2.372, 'learning_rate': 0.00019970604427728249, 'epoch': 0.07}         \n",
      "{'loss': 2.2881, 'learning_rate': 0.00019970580558080657, 'epoch': 0.07}        \n",
      "{'loss': 1.897, 'learning_rate': 0.00019970556678760017, 'epoch': 0.07}         \n",
      "{'loss': 1.9285, 'learning_rate': 0.00019970532789766353, 'epoch': 0.07}        \n",
      "{'loss': 2.0988, 'learning_rate': 0.00019970508891099692, 'epoch': 0.07}        \n",
      "{'loss': 2.4601, 'learning_rate': 0.00019970484982760047, 'epoch': 0.07}        \n",
      "{'loss': 2.2891, 'learning_rate': 0.0001997046106474745, 'epoch': 0.07}         \n",
      "{'loss': 2.2674, 'learning_rate': 0.00019970437137061922, 'epoch': 0.07}        \n",
      "{'loss': 2.0501, 'learning_rate': 0.00019970413199703484, 'epoch': 0.07}        \n",
      "{'loss': 2.1228, 'learning_rate': 0.0001997038925267216, 'epoch': 0.07}         \n",
      "{'loss': 2.0665, 'learning_rate': 0.00019970365295967978, 'epoch': 0.07}        \n",
      "{'loss': 2.3361, 'learning_rate': 0.00019970341329590956, 'epoch': 0.07}        \n",
      "{'loss': 2.1167, 'learning_rate': 0.00019970317353541116, 'epoch': 0.07}        \n",
      "{'loss': 2.2766, 'learning_rate': 0.00019970293367818488, 'epoch': 0.07}        \n",
      "{'loss': 2.1349, 'learning_rate': 0.0001997026937242309, 'epoch': 0.07}         \n",
      "{'loss': 1.9117, 'learning_rate': 0.00019970245367354944, 'epoch': 0.07}        \n",
      "{'loss': 1.7452, 'learning_rate': 0.00019970221352614079, 'epoch': 0.07}        \n",
      "{'loss': 1.6383, 'learning_rate': 0.00019970197328200514, 'epoch': 0.07}        \n",
      "{'loss': 2.2774, 'learning_rate': 0.00019970173294114272, 'epoch': 0.07}        \n",
      "{'loss': 2.3752, 'learning_rate': 0.00019970149250355377, 'epoch': 0.07}        \n",
      "{'loss': 2.0196, 'learning_rate': 0.00019970125196923854, 'epoch': 0.07}        \n",
      "{'loss': 2.1552, 'learning_rate': 0.00019970101133819727, 'epoch': 0.07}        \n",
      "{'loss': 2.0289, 'learning_rate': 0.00019970077061043018, 'epoch': 0.07}        \n",
      "{'loss': 2.3387, 'learning_rate': 0.00019970052978593748, 'epoch': 0.07}        \n",
      "{'loss': 2.1399, 'learning_rate': 0.00019970028886471942, 'epoch': 0.07}        \n",
      "  7%|██                          | 78848/1061708 [11:48:05<144:46:40,  1.89it/s][2024-03-01 05:55:05,779] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 78849/1061708 [11:48:05<135:53:36,  2.01it/s][2024-03-01 05:55:06,200] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3092, 'learning_rate': 0.00019970009605810288, 'epoch': 0.07}        \n",
      "{'loss': 2.1284, 'learning_rate': 0.00019969985496277976, 'epoch': 0.07}        \n",
      "{'loss': 2.0647, 'learning_rate': 0.00019969961377073195, 'epoch': 0.07}        \n",
      "  7%|██                          | 78876/1061708 [11:48:19<145:38:57,  1.87it/s][2024-03-01 05:55:20,518] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8496, 'learning_rate': 0.00019969939661518952, 'epoch': 0.07}        \n",
      "{'loss': 2.322, 'learning_rate': 0.0001996991552393654, 'epoch': 0.07}          \n",
      "{'loss': 1.9905, 'learning_rate': 0.0001996989137668173, 'epoch': 0.07}         \n",
      "{'loss': 2.1973, 'learning_rate': 0.0001996986721975454, 'epoch': 0.07}         \n",
      "{'loss': 2.4463, 'learning_rate': 0.00019969843053154992, 'epoch': 0.07}        \n",
      "{'loss': 2.1094, 'learning_rate': 0.00019969818876883116, 'epoch': 0.07}        \n",
      "  7%|██                          | 78939/1061708 [11:48:53<144:20:41,  1.89it/s][2024-03-01 05:55:54,037] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0715, 'learning_rate': 0.000199697971099686, 'epoch': 0.07}          \n",
      "{'loss': 2.1161, 'learning_rate': 0.00019969772915319356, 'epoch': 0.07}        \n",
      "{'loss': 2.2364, 'learning_rate': 0.00019969748710997847, 'epoch': 0.07}        \n",
      "{'loss': 2.3984, 'learning_rate': 0.00019969724497004098, 'epoch': 0.07}        \n",
      "{'loss': 2.0794, 'learning_rate': 0.00019969700273338135, 'epoch': 0.07}        \n",
      "{'loss': 2.2629, 'learning_rate': 0.00019969676039999974, 'epoch': 0.07}        \n",
      "{'loss': 1.9581, 'learning_rate': 0.00019969651796989646, 'epoch': 0.07}        \n",
      "{'loss': 2.5946, 'learning_rate': 0.0001996962754430717, 'epoch': 0.07}         \n",
      "{'loss': 2.0181, 'learning_rate': 0.00019969603281952574, 'epoch': 0.07}        \n",
      "{'loss': 2.4501, 'learning_rate': 0.00019969579009925876, 'epoch': 0.07}        \n",
      "{'loss': 1.891, 'learning_rate': 0.00019969554728227102, 'epoch': 0.07}         \n",
      "{'loss': 2.4379, 'learning_rate': 0.00019969530436856277, 'epoch': 0.07}        \n",
      "{'loss': 1.6805, 'learning_rate': 0.00019969506135813424, 'epoch': 0.07}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1578, 'learning_rate': 0.00019969481825098565, 'epoch': 0.07}        \n",
      "{'loss': 2.1604, 'learning_rate': 0.00019969457504711723, 'epoch': 0.07}        \n",
      "{'loss': 1.8048, 'learning_rate': 0.00019969433174652922, 'epoch': 0.07}        \n",
      "{'loss': 2.4697, 'learning_rate': 0.00019969408834922192, 'epoch': 0.07}        \n",
      "{'loss': 2.3255, 'learning_rate': 0.0001996938448551955, 'epoch': 0.07}         \n",
      "{'loss': 1.8624, 'learning_rate': 0.00019969360126445018, 'epoch': 0.07}        \n",
      "{'loss': 2.0503, 'learning_rate': 0.00019969335757698625, 'epoch': 0.07}        \n",
      "{'loss': 2.0623, 'learning_rate': 0.00019969311379280387, 'epoch': 0.07}        \n",
      "{'loss': 2.4157, 'learning_rate': 0.0001996928699119034, 'epoch': 0.07}         \n",
      "{'loss': 1.7981, 'learning_rate': 0.00019969262593428497, 'epoch': 0.07}        \n",
      "{'loss': 2.5105, 'learning_rate': 0.00019969238185994884, 'epoch': 0.07}        \n",
      "  7%|██                          | 79174/1061708 [11:50:58<147:24:33,  1.85it/s][2024-03-01 05:57:59,461] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1294, 'learning_rate': 0.0001996921621103529, 'epoch': 0.07}         \n",
      "{'loss': 2.0394, 'learning_rate': 0.00019969191785225383, 'epoch': 0.07}        \n",
      "{'loss': 2.1259, 'learning_rate': 0.00019969167349743775, 'epoch': 0.07}        \n",
      "{'loss': 1.908, 'learning_rate': 0.00019969142904590492, 'epoch': 0.07}         \n",
      "{'loss': 2.3278, 'learning_rate': 0.0001996911844976556, 'epoch': 0.07}         \n",
      "{'loss': 1.8771, 'learning_rate': 0.00019969093985268993, 'epoch': 0.07}        \n",
      "{'loss': 2.4683, 'learning_rate': 0.0001996906951110082, 'epoch': 0.07}         \n",
      "{'loss': 2.2496, 'learning_rate': 0.00019969045027261068, 'epoch': 0.07}        \n",
      "{'loss': 1.9863, 'learning_rate': 0.0001996902053374976, 'epoch': 0.07}         \n",
      "  7%|██                          | 79267/1061708 [11:51:48<145:04:14,  1.88it/s][2024-03-01 05:58:49,003] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.7131, 'learning_rate': 0.00019968998481320418, 'epoch': 0.07}        \n",
      "{'loss': 1.8297, 'learning_rate': 0.00019968973969433214, 'epoch': 0.07}        \n",
      "{'loss': 1.899, 'learning_rate': 0.00019968949447874521, 'epoch': 0.07}         \n",
      "{'loss': 2.1341, 'learning_rate': 0.00019968924916644363, 'epoch': 0.07}        \n",
      "{'loss': 2.0006, 'learning_rate': 0.00019968900375742763, 'epoch': 0.07}        \n",
      "{'loss': 2.4672, 'learning_rate': 0.0001996887582516975, 'epoch': 0.07}         \n",
      "{'loss': 2.1672, 'learning_rate': 0.00019968851264925337, 'epoch': 0.07}        \n",
      "{'loss': 2.0196, 'learning_rate': 0.0001996882669500956, 'epoch': 0.07}         \n",
      "{'loss': 1.9944, 'learning_rate': 0.00019968802115422432, 'epoch': 0.07}        \n",
      "{'loss': 2.1254, 'learning_rate': 0.00019968777526163984, 'epoch': 0.07}        \n",
      "{'loss': 1.9084, 'learning_rate': 0.00019968752927234238, 'epoch': 0.07}        \n",
      "  7%|██                          | 79378/1061708 [11:52:47<144:53:22,  1.88it/s][2024-03-01 05:59:48,133] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1432, 'learning_rate': 0.00019968730779928526, 'epoch': 0.07}        \n",
      "{'loss': 2.3273, 'learning_rate': 0.00019968706162623378, 'epoch': 0.07}        \n",
      "{'loss': 1.9719, 'learning_rate': 0.00019968681535647003, 'epoch': 0.07}        \n",
      "  7%|██                          | 79401/1061708 [11:52:59<144:36:52,  1.89it/s][2024-03-01 06:00:00,294] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9728, 'learning_rate': 0.00019968659363099383, 'epoch': 0.07}        \n",
      "{'loss': 2.108, 'learning_rate': 0.00019968634717747738, 'epoch': 0.07}         \n",
      "{'loss': 2.1402, 'learning_rate': 0.00019968610062724932, 'epoch': 0.07}        \n",
      "{'loss': 2.0635, 'learning_rate': 0.00019968585398030993, 'epoch': 0.07}        \n",
      "{'loss': 2.2021, 'learning_rate': 0.00019968560723665938, 'epoch': 0.07}        \n",
      "{'loss': 2.2936, 'learning_rate': 0.000199685360396298, 'epoch': 0.07}          \n",
      "{'loss': 2.2734, 'learning_rate': 0.00019968511345922597, 'epoch': 0.07}        \n",
      "{'loss': 2.0856, 'learning_rate': 0.0001996848664254435, 'epoch': 0.07}         \n",
      "{'loss': 1.7616, 'learning_rate': 0.0001996846192949509, 'epoch': 0.07}         \n",
      "{'loss': 2.0981, 'learning_rate': 0.00019968437206774838, 'epoch': 0.07}        \n",
      "{'loss': 1.5574, 'learning_rate': 0.00019968412474383619, 'epoch': 0.07}        \n",
      "{'loss': 2.3371, 'learning_rate': 0.00019968387732321452, 'epoch': 0.07}        \n",
      "{'loss': 2.1182, 'learning_rate': 0.0001996836298058837, 'epoch': 0.07}         \n",
      "{'loss': 2.3302, 'learning_rate': 0.00019968338219184385, 'epoch': 0.07}        \n",
      "{'loss': 1.7269, 'learning_rate': 0.00019968313448109535, 'epoch': 0.07}        \n",
      "{'loss': 2.2311, 'learning_rate': 0.00019968288667363834, 'epoch': 0.07}        \n",
      "{'loss': 2.2799, 'learning_rate': 0.00019968263876947307, 'epoch': 0.07}        \n",
      "{'loss': 1.8209, 'learning_rate': 0.00019968239076859984, 'epoch': 0.07}        \n",
      "{'loss': 2.1776, 'learning_rate': 0.00019968214267101884, 'epoch': 0.07}        \n",
      "{'loss': 1.6777, 'learning_rate': 0.0001996818944767303, 'epoch': 0.07}         \n",
      "  7%|██                          | 79602/1061708 [11:54:46<144:38:39,  1.89it/s][2024-03-01 06:01:47,469] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  7%|██                          | 79603/1061708 [11:54:47<139:23:39,  1.96it/s][2024-03-01 06:01:47,893] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  7%|██                          | 79604/1061708 [11:54:47<132:18:46,  2.06it/s][2024-03-01 06:01:48,323] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  7%|██                          | 79606/1061708 [11:54:48<132:48:53,  2.05it/s][2024-03-01 06:01:49,281] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3229, 'learning_rate': 0.00019968174551373769, 'epoch': 0.07}        \n",
      "{'loss': 2.2561, 'learning_rate': 0.00019968149716471763, 'epoch': 0.07}        \n",
      "{'loss': 1.9463, 'learning_rate': 0.0001996812487189907, 'epoch': 0.08}         \n",
      "{'loss': 2.2417, 'learning_rate': 0.0001996810001765571, 'epoch': 0.08}         \n",
      "{'loss': 2.1951, 'learning_rate': 0.0001996807515374171, 'epoch': 0.08}         \n",
      "{'loss': 1.7174, 'learning_rate': 0.0001996805028015709, 'epoch': 0.08}         \n",
      "{'loss': 2.2261, 'learning_rate': 0.00019968025396901882, 'epoch': 0.08}        \n",
      "{'loss': 1.9228, 'learning_rate': 0.00019968000503976102, 'epoch': 0.08}        \n",
      "{'loss': 2.1818, 'learning_rate': 0.0001996797560137978, 'epoch': 0.08}         \n",
      "{'loss': 2.3252, 'learning_rate': 0.00019967950689112938, 'epoch': 0.08}        \n",
      "{'loss': 2.0346, 'learning_rate': 0.000199679257671756, 'epoch': 0.08}          \n",
      "{'loss': 2.3612, 'learning_rate': 0.00019967900835567788, 'epoch': 0.08}        \n",
      "{'loss': 1.819, 'learning_rate': 0.0001996787589428953, 'epoch': 0.08}          \n",
      "{'loss': 2.196, 'learning_rate': 0.00019967850943340847, 'epoch': 0.08}         \n",
      "{'loss': 2.2207, 'learning_rate': 0.00019967825982721764, 'epoch': 0.08}        \n",
      "{'loss': 1.6967, 'learning_rate': 0.0001996780101243231, 'epoch': 0.08}         \n",
      "{'loss': 2.5919, 'learning_rate': 0.00019967776032472502, 'epoch': 0.08}        \n",
      "{'loss': 2.1893, 'learning_rate': 0.0001996775104284237, 'epoch': 0.08}         \n",
      "{'loss': 2.5294, 'learning_rate': 0.0001996772604354193, 'epoch': 0.08}         \n",
      "{'loss': 2.4328, 'learning_rate': 0.00019967701034571218, 'epoch': 0.08}        \n",
      "{'loss': 2.4585, 'learning_rate': 0.00019967676015930248, 'epoch': 0.08}        \n",
      "{'loss': 2.0381, 'learning_rate': 0.00019967650987619048, 'epoch': 0.08}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1254, 'learning_rate': 0.00019967625949637648, 'epoch': 0.08}        \n",
      "{'loss': 2.2105, 'learning_rate': 0.0001996760090198606, 'epoch': 0.08}         \n",
      "  8%|██                          | 79845/1061708 [11:56:56<145:53:33,  1.87it/s][2024-03-01 06:03:56,746] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1703, 'learning_rate': 0.0001996757835083165, 'epoch': 0.08}         \n",
      "{'loss': 2.1264, 'learning_rate': 0.0001996755328480679, 'epoch': 0.08}         \n",
      "{'loss': 2.0436, 'learning_rate': 0.0001996752820911182, 'epoch': 0.08}         \n",
      "{'loss': 1.8256, 'learning_rate': 0.0001996750312374676, 'epoch': 0.08}         \n",
      "{'loss': 2.0252, 'learning_rate': 0.00019967478028711639, 'epoch': 0.08}        \n",
      "{'loss': 2.2119, 'learning_rate': 0.00019967452924006486, 'epoch': 0.08}        \n",
      "{'loss': 1.8769, 'learning_rate': 0.00019967427809631315, 'epoch': 0.08}        \n",
      "{'loss': 2.3897, 'learning_rate': 0.00019967402685586153, 'epoch': 0.08}        \n",
      "{'loss': 2.1938, 'learning_rate': 0.0001996737755187103, 'epoch': 0.08}         \n",
      "{'loss': 2.1146, 'learning_rate': 0.00019967352408485964, 'epoch': 0.08}        \n",
      "  8%|██                          | 79946/1061708 [11:57:50<145:36:03,  1.87it/s][2024-03-01 06:04:50,537] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██                          | 79947/1061708 [11:57:50<136:28:17,  2.00it/s][2024-03-01 06:04:50,960] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2482, 'learning_rate': 0.00019967332286815575, 'epoch': 0.08}        \n",
      "{'loss': 1.9669, 'learning_rate': 0.00019967307126024682, 'epoch': 0.08}        \n",
      "{'loss': 2.1232, 'learning_rate': 0.00019967281955563911, 'epoch': 0.08}        \n",
      "{'loss': 2.172, 'learning_rate': 0.00019967256775433302, 'epoch': 0.08}         \n",
      "{'loss': 2.1351, 'learning_rate': 0.00019967231585632864, 'epoch': 0.08}        \n",
      "  8%|██                          | 79999/1061708 [11:58:18<144:22:55,  1.89it/s][2024-03-01 06:05:18,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=80000, skipped=906, lr=[0.0001996720638616263], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 06:05:18,721] [INFO] [timer.py:260:stop] epoch=0/micro_step=80000/global_step=80000, RunningAvgSamplesPerSec=1.8922680723866667, CurrSamplesPerSec=1.8973110708716912, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.999, 'learning_rate': 0.0001996720638616263, 'epoch': 0.08}          \n",
      "  8%|██                          | 80000/1061708 [11:58:18<144:32:38,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 06:05:18,723 >> Saving model checkpoint to output_model/checkpoint-80000\n",
      "[INFO|trainer.py:2880] 2024-03-01 06:05:18,726 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 06:05:19,943 >> tokenizer config file saved in output_model/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 06:05:19,943 >> Special tokens file saved in output_model/checkpoint-80000/special_tokens_map.json\n",
      "[2024-03-01 06:05:19,944] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step80000 is about to be saved!\n",
      "[2024-03-01 06:05:25,178] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-80000/global_step80000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 06:05:25,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-80000/global_step80000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 06:05:39,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-80000/global_step80000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 06:05:39,759] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-80000/global_step80000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 06:05:46,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-80000/global_step80000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 06:05:46,906] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-80000/global_step80000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 06:05:46,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step80000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 06:05:46,977 >> Deleting older checkpoint [output_model/checkpoint-65000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 06:05:50,749 >> tokenizer config file saved in output_model/checkpoint-80000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 06:05:50,749 >> Special tokens file saved in output_model/checkpoint-80000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 1.8255, 'learning_rate': 0.00019967181177022621, 'epoch': 0.08}        \n",
      "  8%|██                          | 80018/1061708 [11:59:00<149:14:03,  1.83it/s][2024-03-01 06:06:00,661] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1598, 'learning_rate': 0.00019967158480528979, 'epoch': 0.08}        \n",
      "{'loss': 2.1031, 'learning_rate': 0.00019967133253016462, 'epoch': 0.08}        \n",
      "{'loss': 2.0832, 'learning_rate': 0.00019967108015834248, 'epoch': 0.08}        \n",
      "{'loss': 2.5071, 'learning_rate': 0.00019967082768982354, 'epoch': 0.08}        \n",
      "{'loss': 2.0012, 'learning_rate': 0.00019967057512460808, 'epoch': 0.08}        \n",
      "{'loss': 1.8767, 'learning_rate': 0.0001996703224626963, 'epoch': 0.08}         \n",
      "{'loss': 2.276, 'learning_rate': 0.0001996700697040885, 'epoch': 0.08}          \n",
      "{'loss': 1.8057, 'learning_rate': 0.00019966981684878488, 'epoch': 0.08}        \n",
      "{'loss': 2.2323, 'learning_rate': 0.00019966956389678569, 'epoch': 0.08}        \n",
      "{'loss': 1.9696, 'learning_rate': 0.0001996693108480912, 'epoch': 0.08}         \n",
      "{'loss': 2.1967, 'learning_rate': 0.00019966905770270161, 'epoch': 0.08}        \n",
      "{'loss': 2.0398, 'learning_rate': 0.00019966880446061722, 'epoch': 0.08}        \n",
      "  8%|██                          | 80130/1061708 [11:59:59<145:01:55,  1.88it/s][2024-03-01 06:07:00,263] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4919, 'learning_rate': 0.0001996685764600674, 'epoch': 0.08}         \n",
      "{'loss': 1.9077, 'learning_rate': 0.00019966832303426352, 'epoch': 0.08}        \n",
      "{'loss': 1.8978, 'learning_rate': 0.00019966806951176552, 'epoch': 0.08}        \n",
      "{'loss': 2.2326, 'learning_rate': 0.0001996678158925737, 'epoch': 0.08}         \n",
      "{'loss': 2.2041, 'learning_rate': 0.0001996675621766882, 'epoch': 0.08}         \n",
      "{'loss': 1.9018, 'learning_rate': 0.00019966730836410936, 'epoch': 0.08}        \n",
      "{'loss': 1.9673, 'learning_rate': 0.0001996670544548374, 'epoch': 0.08}         \n",
      "{'loss': 2.1048, 'learning_rate': 0.00019966680044887253, 'epoch': 0.08}        \n",
      "  8%|██                          | 80219/1061708 [12:00:47<144:54:55,  1.88it/s][2024-03-01 06:07:47,790] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8615, 'learning_rate': 0.00019966657176083197, 'epoch': 0.08}        \n",
      "{'loss': 2.2129, 'learning_rate': 0.0001996663175711513, 'epoch': 0.08}         \n",
      "{'loss': 2.075, 'learning_rate': 0.0001996660632847785, 'epoch': 0.08}          \n",
      "{'loss': 1.9276, 'learning_rate': 0.00019966580890171378, 'epoch': 0.08}        \n",
      "{'loss': 2.0911, 'learning_rate': 0.00019966555442195737, 'epoch': 0.08}        \n",
      "{'loss': 2.4239, 'learning_rate': 0.00019966529984550958, 'epoch': 0.08}        \n",
      "{'loss': 2.218, 'learning_rate': 0.00019966504517237056, 'epoch': 0.08}         \n",
      "{'loss': 2.2966, 'learning_rate': 0.0001996647904025406, 'epoch': 0.08}         \n",
      "{'loss': 1.7672, 'learning_rate': 0.00019966453553602, 'epoch': 0.08}           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0263, 'learning_rate': 0.00019966428057280894, 'epoch': 0.08}        \n",
      "{'loss': 2.0417, 'learning_rate': 0.00019966402551290774, 'epoch': 0.08}        \n",
      "  8%|██                          | 80326/1061708 [12:01:44<145:16:26,  1.88it/s][2024-03-01 06:08:44,838] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0383, 'learning_rate': 0.0001996637958763267, 'epoch': 0.08}         \n",
      "  8%|██                          | 80333/1061708 [12:01:47<148:02:12,  1.84it/s][2024-03-01 06:08:48,527] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8519, 'learning_rate': 0.000199663566161427, 'epoch': 0.08}          \n",
      "{'loss': 2.4189, 'learning_rate': 0.00019966331083079455, 'epoch': 0.08}        \n",
      "{'loss': 2.3345, 'learning_rate': 0.00019966305540347284, 'epoch': 0.08}        \n",
      "{'loss': 2.4528, 'learning_rate': 0.00019966279987946214, 'epoch': 0.08}        \n",
      "{'loss': 2.2162, 'learning_rate': 0.0001996625442587627, 'epoch': 0.08}         \n",
      "{'loss': 2.1008, 'learning_rate': 0.0001996622885413747, 'epoch': 0.08}         \n",
      "{'loss': 2.5489, 'learning_rate': 0.00019966203272729852, 'epoch': 0.08}        \n",
      "{'loss': 2.0977, 'learning_rate': 0.00019966177681653431, 'epoch': 0.08}        \n",
      "{'loss': 2.1177, 'learning_rate': 0.00019966152080908231, 'epoch': 0.08}        \n",
      "{'loss': 1.8062, 'learning_rate': 0.00019966126470494282, 'epoch': 0.08}        \n",
      "{'loss': 1.947, 'learning_rate': 0.00019966100850411607, 'epoch': 0.08}         \n",
      "{'loss': 2.5644, 'learning_rate': 0.00019966075220660229, 'epoch': 0.08}        \n",
      "{'loss': 2.2258, 'learning_rate': 0.00019966049581240176, 'epoch': 0.08}        \n",
      "{'loss': 1.7543, 'learning_rate': 0.00019966023932151472, 'epoch': 0.08}        \n",
      "  8%|██                          | 80471/1061708 [12:03:01<144:24:31,  1.89it/s][2024-03-01 06:10:02,065] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.093, 'learning_rate': 0.0001996600083970496, 'epoch': 0.08}          \n",
      "{'loss': 2.0199, 'learning_rate': 0.00019965975172245886, 'epoch': 0.08}        \n",
      "{'loss': 2.3326, 'learning_rate': 0.00019965949495118231, 'epoch': 0.08}        \n",
      "{'loss': 2.1042, 'learning_rate': 0.00019965923808322022, 'epoch': 0.08}        \n",
      "{'loss': 2.0838, 'learning_rate': 0.00019965898111857282, 'epoch': 0.08}        \n",
      "  8%|██                          | 80520/1061708 [12:03:27<144:21:14,  1.89it/s][2024-03-01 06:10:28,095] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1738, 'learning_rate': 0.00019965874976772446, 'epoch': 0.08}        \n",
      "{'loss': 2.0867, 'learning_rate': 0.0001996584926193757, 'epoch': 0.08}         \n",
      "{'loss': 2.1114, 'learning_rate': 0.00019965823537434235, 'epoch': 0.08}        \n",
      "{'loss': 2.1541, 'learning_rate': 0.0001996579780326247, 'epoch': 0.08}         \n",
      "  8%|██                          | 80568/1061708 [12:03:53<145:40:44,  1.87it/s][2024-03-01 06:10:53,610] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3973, 'learning_rate': 0.00019965774634241393, 'epoch': 0.08}        \n",
      "{'loss': 2.2723, 'learning_rate': 0.00019965748881699673, 'epoch': 0.08}        \n",
      "{'loss': 2.085, 'learning_rate': 0.00019965723119489594, 'epoch': 0.08}         \n",
      "{'loss': 1.9971, 'learning_rate': 0.0001996569734761118, 'epoch': 0.08}         \n",
      "{'loss': 2.1743, 'learning_rate': 0.00019965671566064456, 'epoch': 0.08}        \n",
      "{'loss': 2.3528, 'learning_rate': 0.00019965645774849448, 'epoch': 0.08}        \n",
      "{'loss': 2.0082, 'learning_rate': 0.0001996561997396618, 'epoch': 0.08}         \n",
      "{'loss': 1.8918, 'learning_rate': 0.00019965594163414677, 'epoch': 0.08}        \n",
      "{'loss': 1.8689, 'learning_rate': 0.00019965568343194962, 'epoch': 0.08}        \n",
      "{'loss': 2.4014, 'learning_rate': 0.00019965542513307065, 'epoch': 0.08}        \n",
      "{'loss': 2.6175, 'learning_rate': 0.00019965516673751008, 'epoch': 0.08}        \n",
      "{'loss': 2.0382, 'learning_rate': 0.00019965490824526815, 'epoch': 0.08}        \n",
      "{'loss': 2.091, 'learning_rate': 0.00019965464965634511, 'epoch': 0.08}         \n",
      "{'loss': 1.7916, 'learning_rate': 0.00019965439097074125, 'epoch': 0.08}        \n",
      "{'loss': 1.9044, 'learning_rate': 0.00019965413218845678, 'epoch': 0.08}        \n",
      "{'loss': 2.4184, 'learning_rate': 0.00019965387330949196, 'epoch': 0.08}        \n",
      "{'loss': 2.5568, 'learning_rate': 0.00019965361433384708, 'epoch': 0.08}        \n",
      "{'loss': 2.2978, 'learning_rate': 0.00019965335526152234, 'epoch': 0.08}        \n",
      "{'loss': 2.4414, 'learning_rate': 0.00019965309609251802, 'epoch': 0.08}        \n",
      "{'loss': 2.1082, 'learning_rate': 0.00019965283682683437, 'epoch': 0.08}        \n",
      "{'loss': 1.7563, 'learning_rate': 0.00019965257746447162, 'epoch': 0.08}        \n",
      "{'loss': 2.1148, 'learning_rate': 0.00019965231800543002, 'epoch': 0.08}        \n",
      "{'loss': 1.9791, 'learning_rate': 0.00019965205844970984, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 80790/1061708 [12:05:51<144:12:30,  1.89it/s][2024-03-01 06:12:51,958] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0191, 'learning_rate': 0.00019965182476690168, 'epoch': 0.08}        \n",
      "{'loss': 2.1184, 'learning_rate': 0.0001996515650274929, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 80816/1061708 [12:06:05<145:21:13,  1.87it/s][2024-03-01 06:13:05,750] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.6466, 'learning_rate': 0.00019965133117936544, 'epoch': 0.08}        \n",
      "{'loss': 2.1956, 'learning_rate': 0.00019965107125626897, 'epoch': 0.08}        \n",
      "{'loss': 2.4405, 'learning_rate': 0.00019965081123649508, 'epoch': 0.08}        \n",
      "{'loss': 2.3704, 'learning_rate': 0.0001996505511200441, 'epoch': 0.08}         \n",
      "{'loss': 2.1357, 'learning_rate': 0.00019965029090691626, 'epoch': 0.08}        \n",
      "{'loss': 2.4647, 'learning_rate': 0.00019965003059711181, 'epoch': 0.08}        \n",
      "{'loss': 2.4562, 'learning_rate': 0.00019964977019063096, 'epoch': 0.08}        \n",
      "{'loss': 2.1858, 'learning_rate': 0.00019964950968747401, 'epoch': 0.08}        \n",
      "{'loss': 2.4777, 'learning_rate': 0.0001996492490876412, 'epoch': 0.08}         \n",
      "{'loss': 2.3332, 'learning_rate': 0.00019964898839113282, 'epoch': 0.08}        \n",
      "{'loss': 1.9358, 'learning_rate': 0.00019964872759794904, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 80927/1061708 [12:07:04<144:51:41,  1.88it/s][2024-03-01 06:14:04,889] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9625, 'learning_rate': 0.00019964849280142643, 'epoch': 0.08}        \n",
      "{'loss': 2.0907, 'learning_rate': 0.0001996482318245602, 'epoch': 0.08}         \n",
      "{'loss': 2.5094, 'learning_rate': 0.00019964797075101934, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 80955/1061708 [12:07:19<146:15:38,  1.86it/s][2024-03-01 06:14:19,741] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.319, 'learning_rate': 0.00019964773570217597, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 80969/1061708 [12:07:26<144:29:12,  1.89it/s][2024-03-01 06:14:27,126] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0628, 'learning_rate': 0.0001996475005750266, 'epoch': 0.08}         \n",
      "{'loss': 2.2782, 'learning_rate': 0.00019964723923079813, 'epoch': 0.08}        \n",
      "{'loss': 2.0761, 'learning_rate': 0.00019964697778989603, 'epoch': 0.08}        \n",
      "{'loss': 1.9781, 'learning_rate': 0.00019964671625232055, 'epoch': 0.08}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0684, 'learning_rate': 0.00019964645461807189, 'epoch': 0.08}        \n",
      "{'loss': 2.0527, 'learning_rate': 0.00019964619288715034, 'epoch': 0.08}        \n",
      "{'loss': 1.9271, 'learning_rate': 0.00019964593105955617, 'epoch': 0.08}        \n",
      "{'loss': 2.1887, 'learning_rate': 0.0001996456691352896, 'epoch': 0.08}         \n",
      "{'loss': 2.1567, 'learning_rate': 0.0001996454071143509, 'epoch': 0.08}         \n",
      "{'loss': 1.9528, 'learning_rate': 0.0001996451449967403, 'epoch': 0.08}         \n",
      "{'loss': 1.9655, 'learning_rate': 0.00019964488278245812, 'epoch': 0.08}        \n",
      "{'loss': 2.5013, 'learning_rate': 0.00019964462047150457, 'epoch': 0.08}        \n",
      "{'loss': 2.1632, 'learning_rate': 0.00019964435806387986, 'epoch': 0.08}        \n",
      "{'loss': 2.1709, 'learning_rate': 0.00019964409555958432, 'epoch': 0.08}        \n",
      "{'loss': 2.0174, 'learning_rate': 0.00019964383295861815, 'epoch': 0.08}        \n",
      "{'loss': 2.0959, 'learning_rate': 0.00019964357026098164, 'epoch': 0.08}        \n",
      "{'loss': 1.7129, 'learning_rate': 0.00019964330746667503, 'epoch': 0.08}        \n",
      "{'loss': 2.4436, 'learning_rate': 0.00019964304457569857, 'epoch': 0.08}        \n",
      "{'loss': 2.1508, 'learning_rate': 0.00019964278158805256, 'epoch': 0.08}        \n",
      "{'loss': 1.9857, 'learning_rate': 0.00019964251850373722, 'epoch': 0.08}        \n",
      "{'loss': 2.0573, 'learning_rate': 0.00019964225532275275, 'epoch': 0.08}        \n",
      "{'loss': 2.0481, 'learning_rate': 0.0001996419920450995, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 81180/1061708 [12:09:19<144:14:16,  1.89it/s][2024-03-01 06:16:19,570] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.933, 'learning_rate': 0.00019964175501255993, 'epoch': 0.08}         \n",
      "{'loss': 1.9788, 'learning_rate': 0.0001996414915512366, 'epoch': 0.08}         \n",
      "{'loss': 2.32, 'learning_rate': 0.00019964122799324524, 'epoch': 0.08}          \n",
      "{'loss': 2.2528, 'learning_rate': 0.00019964096433858601, 'epoch': 0.08}        \n",
      "{'loss': 1.9869, 'learning_rate': 0.00019964070058725922, 'epoch': 0.08}        \n",
      "{'loss': 1.6249, 'learning_rate': 0.00019964043673926512, 'epoch': 0.08}        \n",
      "{'loss': 2.2176, 'learning_rate': 0.00019964017279460396, 'epoch': 0.08}        \n",
      "{'loss': 1.9248, 'learning_rate': 0.00019963990875327602, 'epoch': 0.08}        \n",
      "{'loss': 2.0174, 'learning_rate': 0.0001996396446152815, 'epoch': 0.08}         \n",
      "{'loss': 1.5191, 'learning_rate': 0.0001996393803806207, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 81281/1061708 [12:10:12<144:02:09,  1.89it/s][2024-03-01 06:17:13,339] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 81282/1061708 [12:10:13<135:08:58,  2.02it/s][2024-03-01 06:17:13,762] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.101, 'learning_rate': 0.0001996391689232925, 'epoch': 0.08}          \n",
      "{'loss': 2.0197, 'learning_rate': 0.00019963890451463306, 'epoch': 0.08}        \n",
      "{'loss': 2.4606, 'learning_rate': 0.00019963864000930803, 'epoch': 0.08}        \n",
      "{'loss': 2.0268, 'learning_rate': 0.00019963837540731767, 'epoch': 0.08}        \n",
      "{'loss': 1.8715, 'learning_rate': 0.00019963811070866226, 'epoch': 0.08}        \n",
      "{'loss': 2.2244, 'learning_rate': 0.00019963784591334207, 'epoch': 0.08}        \n",
      "{'loss': 2.2733, 'learning_rate': 0.00019963758102135731, 'epoch': 0.08}        \n",
      "{'loss': 2.3604, 'learning_rate': 0.0001996373160327083, 'epoch': 0.08}         \n",
      "{'loss': 2.1482, 'learning_rate': 0.00019963705094739524, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 81374/1061708 [12:11:02<147:02:33,  1.85it/s][2024-03-01 06:18:02,741] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8443, 'learning_rate': 0.00019963681228796595, 'epoch': 0.08}        \n",
      "{'loss': 2.1872, 'learning_rate': 0.00019963654701899195, 'epoch': 0.08}        \n",
      "{'loss': 2.2135, 'learning_rate': 0.00019963628165335466, 'epoch': 0.08}        \n",
      "{'loss': 2.1223, 'learning_rate': 0.00019963601619105435, 'epoch': 0.08}        \n",
      "{'loss': 2.431, 'learning_rate': 0.00019963575063209127, 'epoch': 0.08}         \n",
      "{'loss': 2.0609, 'learning_rate': 0.00019963548497646568, 'epoch': 0.08}        \n",
      "{'loss': 2.3146, 'learning_rate': 0.00019963521922417786, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 81448/1061708 [12:11:41<144:11:24,  1.89it/s][2024-03-01 06:18:42,074] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0435, 'learning_rate': 0.0001996349799644728, 'epoch': 0.08}         \n",
      "{'loss': 2.0285, 'learning_rate': 0.00019963471402852732, 'epoch': 0.08}        \n",
      "{'loss': 1.8474, 'learning_rate': 0.00019963444799592044, 'epoch': 0.08}        \n",
      "{'loss': 1.8373, 'learning_rate': 0.0001996341818666523, 'epoch': 0.08}         \n",
      "{'loss': 2.4406, 'learning_rate': 0.0001996339156407231, 'epoch': 0.08}         \n",
      "{'loss': 1.9435, 'learning_rate': 0.00019963364931813323, 'epoch': 0.08}        \n",
      "{'loss': 2.3883, 'learning_rate': 0.00019963338289888288, 'epoch': 0.08}        \n",
      "{'loss': 2.3905, 'learning_rate': 0.00019963311638297233, 'epoch': 0.08}        \n",
      "{'loss': 1.914, 'learning_rate': 0.00019963284977040177, 'epoch': 0.08}         \n",
      "{'loss': 1.9317, 'learning_rate': 0.00019963258306117157, 'epoch': 0.08}        \n",
      "{'loss': 1.9286, 'learning_rate': 0.0001996323162552819, 'epoch': 0.08}         \n",
      "{'loss': 2.2188, 'learning_rate': 0.0001996320493527331, 'epoch': 0.08}         \n",
      "{'loss': 2.1545, 'learning_rate': 0.00019963178235352533, 'epoch': 0.08}        \n",
      "{'loss': 2.0916, 'learning_rate': 0.00019963151525765894, 'epoch': 0.08}        \n",
      "{'loss': 1.7759, 'learning_rate': 0.0001996312480651341, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 81594/1061708 [12:12:59<147:01:52,  1.85it/s][2024-03-01 06:19:59,834] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.268, 'learning_rate': 0.00019963100750921905, 'epoch': 0.08}         \n",
      "{'loss': 2.3659, 'learning_rate': 0.000199630740133044, 'epoch': 0.08}          \n",
      "{'loss': 2.3614, 'learning_rate': 0.00019963047266021127, 'epoch': 0.08}        \n",
      "{'loss': 2.0222, 'learning_rate': 0.0001996302050907212, 'epoch': 0.08}         \n",
      "{'loss': 1.7842, 'learning_rate': 0.00019962993742457394, 'epoch': 0.08}        \n",
      "{'loss': 2.1681, 'learning_rate': 0.00019962966966176983, 'epoch': 0.08}        \n",
      "{'loss': 2.367, 'learning_rate': 0.00019962940180230914, 'epoch': 0.08}         \n",
      "{'loss': 2.3691, 'learning_rate': 0.00019962913384619207, 'epoch': 0.08}        \n",
      "{'loss': 2.2784, 'learning_rate': 0.00019962886579341893, 'epoch': 0.08}        \n",
      "{'loss': 2.4155, 'learning_rate': 0.00019962859764398994, 'epoch': 0.08}        \n",
      "{'loss': 2.3373, 'learning_rate': 0.00019962832939790536, 'epoch': 0.08}        \n",
      "{'loss': 1.7958, 'learning_rate': 0.00019962806105516551, 'epoch': 0.08}        \n",
      "{'loss': 1.9257, 'learning_rate': 0.00019962779261577062, 'epoch': 0.08}        \n",
      "{'loss': 2.0553, 'learning_rate': 0.00019962752407972088, 'epoch': 0.08}        \n",
      "{'loss': 2.3215, 'learning_rate': 0.00019962725544701667, 'epoch': 0.08}        \n",
      "{'loss': 2.2577, 'learning_rate': 0.00019962698671765816, 'epoch': 0.08}        \n",
      "{'loss': 2.129, 'learning_rate': 0.00019962671789164565, 'epoch': 0.08}         \n",
      "{'loss': 1.8236, 'learning_rate': 0.0001996264489689794, 'epoch': 0.08}         \n",
      "{'loss': 2.0445, 'learning_rate': 0.00019962617994965967, 'epoch': 0.08}        \n",
      "{'loss': 2.0631, 'learning_rate': 0.0001996259108336867, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 81795/1061708 [12:14:46<145:18:45,  1.87it/s][2024-03-01 06:21:46,920] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|██▏                         | 81796/1061708 [12:14:46<136:13:20,  2.00it/s][2024-03-01 06:21:47,343] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.215, 'learning_rate': 0.00019962569547131816, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 81807/1061708 [12:14:52<144:33:01,  1.88it/s][2024-03-01 06:21:53,148] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2371, 'learning_rate': 0.00019962545311471422, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 81815/1061708 [12:14:56<144:55:07,  1.88it/s][2024-03-01 06:21:57,320] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.7819, 'learning_rate': 0.00019962521067982175, 'epoch': 0.08}        \n",
      "{'loss': 2.1032, 'learning_rate': 0.00019962494121589935, 'epoch': 0.08}        \n",
      "{'loss': 1.9726, 'learning_rate': 0.0001996246716553249, 'epoch': 0.08}         \n",
      "{'loss': 1.7086, 'learning_rate': 0.00019962440199809868, 'epoch': 0.08}        \n",
      "{'loss': 2.0163, 'learning_rate': 0.000199624132244221, 'epoch': 0.08}          \n",
      "{'loss': 2.2043, 'learning_rate': 0.00019962386239369203, 'epoch': 0.08}        \n",
      "{'loss': 2.2158, 'learning_rate': 0.00019962359244651213, 'epoch': 0.08}        \n",
      "{'loss': 2.1324, 'learning_rate': 0.0001996233224026815, 'epoch': 0.08}         \n",
      "{'loss': 1.8636, 'learning_rate': 0.0001996230522622004, 'epoch': 0.08}         \n",
      "{'loss': 1.9339, 'learning_rate': 0.00019962278202506913, 'epoch': 0.08}        \n",
      "{'loss': 2.3539, 'learning_rate': 0.00019962251169128794, 'epoch': 0.08}        \n",
      "{'loss': 2.3413, 'learning_rate': 0.00019962224126085704, 'epoch': 0.08}        \n",
      "{'loss': 2.1128, 'learning_rate': 0.0001996219707337768, 'epoch': 0.08}         \n",
      "{'loss': 2.1251, 'learning_rate': 0.00019962170011004736, 'epoch': 0.08}        \n",
      "{'loss': 2.1513, 'learning_rate': 0.0001996214293896691, 'epoch': 0.08}         \n",
      "{'loss': 2.4163, 'learning_rate': 0.00019962115857264216, 'epoch': 0.08}        \n",
      "{'loss': 1.9716, 'learning_rate': 0.00019962088765896688, 'epoch': 0.08}        \n",
      "{'loss': 2.1767, 'learning_rate': 0.00019962061664864354, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 81999/1061708 [12:16:34<144:31:29,  1.88it/s][2024-03-01 06:23:35,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=82000, skipped=929, lr=[0.00019962034554167236], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 06:23:35,477] [INFO] [timer.py:260:stop] epoch=0/micro_step=82000/global_step=82000, RunningAvgSamplesPerSec=1.8923011422881417, CurrSamplesPerSec=1.9100614782093903, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1334, 'learning_rate': 0.00019962034554167236, 'epoch': 0.08}        \n",
      "{'loss': 1.7513, 'learning_rate': 0.0001996200743380536, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 82018/1061708 [12:16:44<144:32:48,  1.88it/s][2024-03-01 06:23:45,507] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1368, 'learning_rate': 0.00019961983017216327, 'epoch': 0.08}        \n",
      "{'loss': 2.4804, 'learning_rate': 0.0001996195587849149, 'epoch': 0.08}         \n",
      "{'loss': 2.2633, 'learning_rate': 0.0001996192873010197, 'epoch': 0.08}         \n",
      "{'loss': 2.1313, 'learning_rate': 0.00019961901572047795, 'epoch': 0.08}        \n",
      "{'loss': 1.8583, 'learning_rate': 0.00019961874404328996, 'epoch': 0.08}        \n",
      "{'loss': 1.8836, 'learning_rate': 0.00019961847226945595, 'epoch': 0.08}        \n",
      "{'loss': 2.0805, 'learning_rate': 0.00019961820039897615, 'epoch': 0.08}        \n",
      "{'loss': 1.7317, 'learning_rate': 0.0001996179284318509, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 82093/1061708 [12:17:24<147:30:05,  1.84it/s][2024-03-01 06:24:25,392] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.852, 'learning_rate': 0.0001996176835788065, 'epoch': 0.08}          \n",
      "{'loss': 1.9136, 'learning_rate': 0.00019961741142805557, 'epoch': 0.08}        \n",
      "{'loss': 2.2197, 'learning_rate': 0.00019961713918065989, 'epoch': 0.08}        \n",
      "{'loss': 2.1834, 'learning_rate': 0.0001996168668366198, 'epoch': 0.08}         \n",
      "{'loss': 1.8329, 'learning_rate': 0.00019961659439593547, 'epoch': 0.08}        \n",
      "{'loss': 2.0261, 'learning_rate': 0.00019961632185860727, 'epoch': 0.08}        \n",
      "{'loss': 2.0485, 'learning_rate': 0.00019961604922463536, 'epoch': 0.08}        \n",
      "{'loss': 1.9779, 'learning_rate': 0.00019961577649402006, 'epoch': 0.08}        \n",
      "{'loss': 1.8434, 'learning_rate': 0.00019961550366676164, 'epoch': 0.08}        \n",
      "{'loss': 1.5782, 'learning_rate': 0.00019961523074286036, 'epoch': 0.08}        \n",
      "{'loss': 1.8934, 'learning_rate': 0.00019961495772231644, 'epoch': 0.08}        \n",
      "{'loss': 1.6626, 'learning_rate': 0.00019961468460513024, 'epoch': 0.08}        \n",
      "{'loss': 2.2112, 'learning_rate': 0.00019961441139130191, 'epoch': 0.08}        \n",
      "{'loss': 2.0777, 'learning_rate': 0.0001996141380808318, 'epoch': 0.08}         \n",
      "{'loss': 2.3395, 'learning_rate': 0.00019961386467372017, 'epoch': 0.08}        \n",
      "{'loss': 2.224, 'learning_rate': 0.00019961359116996724, 'epoch': 0.08}         \n",
      "{'loss': 2.2402, 'learning_rate': 0.0001996133175695733, 'epoch': 0.08}         \n",
      "{'loss': 2.1344, 'learning_rate': 0.0001996130438725386, 'epoch': 0.08}         \n",
      "{'loss': 2.2642, 'learning_rate': 0.00019961277007886344, 'epoch': 0.08}        \n",
      "{'loss': 2.2978, 'learning_rate': 0.00019961249618854804, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 82294/1061708 [12:19:12<146:32:54,  1.86it/s][2024-03-01 06:26:12,582] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 82295/1061708 [12:19:12<136:57:12,  1.99it/s][2024-03-01 06:26:13,004] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  8%|██▏                         | 82297/1061708 [12:19:13<134:23:28,  2.02it/s][2024-03-01 06:26:13,960] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2296, 'learning_rate': 0.0001996123044078265, 'epoch': 0.08}         \n",
      "{'loss': 2.2319, 'learning_rate': 0.00019961203035322335, 'epoch': 0.08}        \n",
      "{'loss': 2.4106, 'learning_rate': 0.0001996117562019807, 'epoch': 0.08}         \n",
      "{'loss': 2.0797, 'learning_rate': 0.00019961148195409885, 'epoch': 0.08}        \n",
      "{'loss': 2.1926, 'learning_rate': 0.000199611207609578, 'epoch': 0.08}          \n",
      "{'loss': 2.0803, 'learning_rate': 0.00019961093316841845, 'epoch': 0.08}        \n",
      "{'loss': 2.121, 'learning_rate': 0.0001996106586306205, 'epoch': 0.08}          \n",
      "{'loss': 2.3259, 'learning_rate': 0.00019961038399618435, 'epoch': 0.08}        \n",
      "{'loss': 2.0775, 'learning_rate': 0.00019961010926511033, 'epoch': 0.08}        \n",
      "{'loss': 1.9366, 'learning_rate': 0.00019960983443739863, 'epoch': 0.08}        \n",
      "{'loss': 2.1674, 'learning_rate': 0.0001996095595130496, 'epoch': 0.08}         \n",
      "{'loss': 1.6831, 'learning_rate': 0.00019960928449206346, 'epoch': 0.08}        \n",
      "{'loss': 2.0107, 'learning_rate': 0.0001996090093744405, 'epoch': 0.08}         \n",
      "{'loss': 1.9717, 'learning_rate': 0.00019960873416018096, 'epoch': 0.08}        \n",
      "{'loss': 2.1143, 'learning_rate': 0.0001996084588492851, 'epoch': 0.08}         \n",
      "{'loss': 1.9333, 'learning_rate': 0.00019960818344175326, 'epoch': 0.08}        \n",
      "{'loss': 2.2405, 'learning_rate': 0.0001996079079375856, 'epoch': 0.08}         \n",
      "{'loss': 2.2881, 'learning_rate': 0.00019960763233678247, 'epoch': 0.08}        \n",
      "{'loss': 2.3407, 'learning_rate': 0.00019960735663934412, 'epoch': 0.08}        \n",
      "{'loss': 2.1679, 'learning_rate': 0.00019960708084527075, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 82498/1061708 [12:21:00<144:38:50,  1.88it/s][2024-03-01 06:28:01,245] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|██▏                         | 82499/1061708 [12:21:01<135:49:33,  2.00it/s][2024-03-01 06:28:01,670] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9165, 'learning_rate': 0.0001996068601404351, 'epoch': 0.08}         \n",
      "{'loss': 2.0536, 'learning_rate': 0.00019960658417241952, 'epoch': 0.08}        \n",
      "{'loss': 2.0339, 'learning_rate': 0.0001996063081077697, 'epoch': 0.08}         \n",
      "{'loss': 2.1012, 'learning_rate': 0.00019960603194648594, 'epoch': 0.08}        \n",
      "{'loss': 1.9745, 'learning_rate': 0.00019960575568856852, 'epoch': 0.08}        \n",
      "{'loss': 1.9928, 'learning_rate': 0.0001996054793340177, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 82556/1061708 [12:21:31<145:22:50,  1.87it/s][2024-03-01 06:28:32,029] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.056, 'learning_rate': 0.00019960523053230058, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 82560/1061708 [12:21:33<145:32:39,  1.87it/s][2024-03-01 06:28:34,100] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1351, 'learning_rate': 0.00019960498165231085, 'epoch': 0.08}        \n",
      "{'loss': 2.3022, 'learning_rate': 0.0001996047050271879, 'epoch': 0.08}         \n",
      "{'loss': 2.1155, 'learning_rate': 0.00019960442830543252, 'epoch': 0.08}        \n",
      "{'loss': 2.1093, 'learning_rate': 0.00019960415148704506, 'epoch': 0.08}        \n",
      "{'loss': 2.1494, 'learning_rate': 0.00019960387457202571, 'epoch': 0.08}        \n",
      "{'loss': 2.1638, 'learning_rate': 0.00019960359756037482, 'epoch': 0.08}        \n",
      "{'loss': 2.0072, 'learning_rate': 0.0001996033204520926, 'epoch': 0.08}         \n",
      "{'loss': 2.1686, 'learning_rate': 0.0001996030432471793, 'epoch': 0.08}         \n",
      "{'loss': 2.1551, 'learning_rate': 0.00019960276594563526, 'epoch': 0.08}        \n",
      "{'loss': 2.2036, 'learning_rate': 0.00019960248854746068, 'epoch': 0.08}        \n",
      "{'loss': 2.2394, 'learning_rate': 0.00019960221105265585, 'epoch': 0.08}        \n",
      "{'loss': 2.0877, 'learning_rate': 0.0001996019334612211, 'epoch': 0.08}         \n",
      "{'loss': 2.6469, 'learning_rate': 0.0001996016557731566, 'epoch': 0.08}         \n",
      "{'loss': 2.468, 'learning_rate': 0.00019960137798846268, 'epoch': 0.08}         \n",
      "{'loss': 2.0239, 'learning_rate': 0.0001996011001071396, 'epoch': 0.08}         \n",
      "{'loss': 2.2254, 'learning_rate': 0.00019960082212918762, 'epoch': 0.08}        \n",
      "{'loss': 2.2214, 'learning_rate': 0.00019960054405460703, 'epoch': 0.08}        \n",
      "{'loss': 1.7656, 'learning_rate': 0.0001996002658833981, 'epoch': 0.08}         \n",
      "{'loss': 2.1166, 'learning_rate': 0.00019959998761556107, 'epoch': 0.08}        \n",
      "{'loss': 2.0297, 'learning_rate': 0.0001995997092510962, 'epoch': 0.08}         \n",
      "{'loss': 1.9136, 'learning_rate': 0.0001995994307900038, 'epoch': 0.08}         \n",
      "{'loss': 2.2417, 'learning_rate': 0.00019959915223228416, 'epoch': 0.08}        \n",
      "{'loss': 2.1733, 'learning_rate': 0.00019959887357793747, 'epoch': 0.08}        \n",
      "{'loss': 2.2288, 'learning_rate': 0.00019959859482696407, 'epoch': 0.08}        \n",
      "{'loss': 2.1797, 'learning_rate': 0.0001995983159793642, 'epoch': 0.08}         \n",
      "{'loss': 1.9441, 'learning_rate': 0.00019959803703513816, 'epoch': 0.08}        \n",
      "{'loss': 1.7604, 'learning_rate': 0.00019959775799428616, 'epoch': 0.08}        \n",
      "{'loss': 2.1203, 'learning_rate': 0.00019959747885680854, 'epoch': 0.08}        \n",
      "{'loss': 1.7904, 'learning_rate': 0.0001995971996227055, 'epoch': 0.08}         \n",
      "{'loss': 1.7185, 'learning_rate': 0.00019959692029197738, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 82861/1061708 [12:24:14<144:12:29,  1.89it/s][2024-03-01 06:31:14,663] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 82862/1061708 [12:24:14<135:25:43,  2.01it/s][2024-03-01 06:31:15,086] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9335, 'learning_rate': 0.00019959669675782498, 'epoch': 0.08}        \n",
      "{'loss': 2.3066, 'learning_rate': 0.00019959641725317235, 'epoch': 0.08}        \n",
      "{'loss': 2.2058, 'learning_rate': 0.00019959613765189538, 'epoch': 0.08}        \n",
      "{'loss': 2.0075, 'learning_rate': 0.00019959585795399428, 'epoch': 0.08}        \n",
      "{'loss': 2.2322, 'learning_rate': 0.00019959557815946944, 'epoch': 0.08}        \n",
      "{'loss': 1.9049, 'learning_rate': 0.000199595298268321, 'epoch': 0.08}          \n",
      "{'loss': 2.3279, 'learning_rate': 0.0001995950182805493, 'epoch': 0.08}         \n",
      "{'loss': 2.472, 'learning_rate': 0.00019959473819615466, 'epoch': 0.08}         \n",
      "{'loss': 2.0699, 'learning_rate': 0.00019959445801513725, 'epoch': 0.08}        \n",
      "{'loss': 1.8287, 'learning_rate': 0.00019959417773749742, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 82963/1061708 [12:25:08<147:19:37,  1.85it/s][2024-03-01 06:32:08,844] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 82964/1061708 [12:25:08<137:33:09,  1.98it/s][2024-03-01 06:32:09,267] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1469, 'learning_rate': 0.00019959395344581758, 'epoch': 0.08}        \n",
      "{'loss': 2.2175, 'learning_rate': 0.00019959367299425803, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 82980/1061708 [12:25:17<144:09:43,  1.89it/s][2024-03-01 06:32:17,711] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9874, 'learning_rate': 0.00019959342050524285, 'epoch': 0.08}        \n",
      "{'loss': 1.9292, 'learning_rate': 0.00019959313987010231, 'epoch': 0.08}        \n",
      "{'loss': 2.1144, 'learning_rate': 0.0001995928591383406, 'epoch': 0.08}         \n",
      "{'loss': 2.3415, 'learning_rate': 0.00019959257830995796, 'epoch': 0.08}        \n",
      "{'loss': 2.2348, 'learning_rate': 0.00019959229738495475, 'epoch': 0.08}        \n",
      "{'loss': 2.1106, 'learning_rate': 0.00019959201636333114, 'epoch': 0.08}        \n",
      "{'loss': 2.1836, 'learning_rate': 0.00019959173524508746, 'epoch': 0.08}        \n",
      "{'loss': 2.0353, 'learning_rate': 0.000199591454030224, 'epoch': 0.08}          \n",
      "{'loss': 2.1815, 'learning_rate': 0.000199591172718741, 'epoch': 0.08}          \n",
      "{'loss': 1.8929, 'learning_rate': 0.00019959089131063872, 'epoch': 0.08}        \n",
      "{'loss': 2.1907, 'learning_rate': 0.00019959060980591744, 'epoch': 0.08}        \n",
      "{'loss': 1.9157, 'learning_rate': 0.00019959032820457745, 'epoch': 0.08}        \n",
      "{'loss': 2.1756, 'learning_rate': 0.00019959004650661904, 'epoch': 0.08}        \n",
      "{'loss': 1.848, 'learning_rate': 0.00019958976471204243, 'epoch': 0.08}         \n",
      "{'loss': 2.0045, 'learning_rate': 0.00019958948282084796, 'epoch': 0.08}        \n",
      "{'loss': 1.8242, 'learning_rate': 0.00019958920083303582, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83144/1061708 [12:26:44<146:25:45,  1.86it/s][2024-03-01 06:33:45,055] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.4204, 'learning_rate': 0.00019958894696139707, 'epoch': 0.08}        \n",
      "{'loss': 1.8264, 'learning_rate': 0.0001995886647900122, 'epoch': 0.08}         \n",
      "{'loss': 1.8417, 'learning_rate': 0.00019958838252201054, 'epoch': 0.08}        \n",
      "{'loss': 1.9386, 'learning_rate': 0.0001995881001573923, 'epoch': 0.08}         \n",
      "{'loss': 2.0824, 'learning_rate': 0.0001995878176961578, 'epoch': 0.08}         \n",
      "{'loss': 2.1048, 'learning_rate': 0.00019958753513830727, 'epoch': 0.08}        \n",
      "{'loss': 2.0799, 'learning_rate': 0.00019958725248384106, 'epoch': 0.08}        \n",
      "{'loss': 2.5082, 'learning_rate': 0.0001995869697327593, 'epoch': 0.08}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0242, 'learning_rate': 0.00019958668688506244, 'epoch': 0.08}        \n",
      "{'loss': 1.7753, 'learning_rate': 0.0001995864039407506, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 83245/1061708 [12:27:38<145:10:07,  1.87it/s][2024-03-01 06:34:38,786] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 83246/1061708 [12:27:38<135:57:53,  2.00it/s][2024-03-01 06:34:39,208] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2319, 'learning_rate': 0.0001995861775157386, 'epoch': 0.08}         \n",
      "{'loss': 2.5166, 'learning_rate': 0.00019958589439752068, 'epoch': 0.08}        \n",
      "{'loss': 2.4784, 'learning_rate': 0.00019958561118268856, 'epoch': 0.08}        \n",
      "{'loss': 2.2202, 'learning_rate': 0.0001995853278712426, 'epoch': 0.08}         \n",
      "{'loss': 2.3182, 'learning_rate': 0.00019958504446318303, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83298/1061708 [12:28:06<144:14:27,  1.88it/s][2024-03-01 06:35:06,878] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1056, 'learning_rate': 0.00019958478931332506, 'epoch': 0.08}        \n",
      "{'loss': 2.4216, 'learning_rate': 0.00019958450572170043, 'epoch': 0.08}        \n",
      "{'loss': 2.2316, 'learning_rate': 0.00019958422203346298, 'epoch': 0.08}        \n",
      "{'loss': 1.9501, 'learning_rate': 0.00019958393824861306, 'epoch': 0.08}        \n",
      "{'loss': 2.2427, 'learning_rate': 0.0001995836543671509, 'epoch': 0.08}         \n",
      "{'loss': 2.2329, 'learning_rate': 0.00019958337038907673, 'epoch': 0.08}        \n",
      "{'loss': 2.5701, 'learning_rate': 0.00019958308631439089, 'epoch': 0.08}        \n",
      "{'loss': 2.0811, 'learning_rate': 0.0001995828021430936, 'epoch': 0.08}         \n",
      "{'loss': 2.4428, 'learning_rate': 0.00019958251787518521, 'epoch': 0.08}        \n",
      "{'loss': 2.5743, 'learning_rate': 0.00019958223351066595, 'epoch': 0.08}        \n",
      "{'loss': 2.1582, 'learning_rate': 0.0001995819490495361, 'epoch': 0.08}         \n",
      "{'loss': 2.2865, 'learning_rate': 0.00019958166449179592, 'epoch': 0.08}        \n",
      "{'loss': 1.8507, 'learning_rate': 0.0001995813798374457, 'epoch': 0.08}         \n",
      "{'loss': 2.1195, 'learning_rate': 0.00019958109508648572, 'epoch': 0.08}        \n",
      "{'loss': 2.3257, 'learning_rate': 0.00019958081023891628, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83446/1061708 [12:29:25<145:00:02,  1.87it/s][2024-03-01 06:36:25,677] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9779, 'learning_rate': 0.0001995805537935029, 'epoch': 0.08}         \n",
      "{'loss': 2.2204, 'learning_rate': 0.00019958026876237616, 'epoch': 0.08}        \n",
      "{'loss': 2.0698, 'learning_rate': 0.00019957998363464077, 'epoch': 0.08}        \n",
      "{'loss': 2.0693, 'learning_rate': 0.00019957969841029696, 'epoch': 0.08}        \n",
      "{'loss': 2.333, 'learning_rate': 0.000199579413089345, 'epoch': 0.08}           \n",
      "  8%|██▏                         | 83495/1061708 [12:29:51<145:31:46,  1.87it/s][2024-03-01 06:36:51,742] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2812, 'learning_rate': 0.00019957915621788855, 'epoch': 0.08}        \n",
      "{'loss': 2.0607, 'learning_rate': 0.00019957887071338193, 'epoch': 0.08}        \n",
      "{'loss': 1.8882, 'learning_rate': 0.00019957858511226797, 'epoch': 0.08}        \n",
      "{'loss': 1.9948, 'learning_rate': 0.00019957829941454695, 'epoch': 0.08}        \n",
      "{'loss': 2.1885, 'learning_rate': 0.00019957801362021916, 'epoch': 0.08}        \n",
      "{'loss': 2.1145, 'learning_rate': 0.00019957772772928492, 'epoch': 0.08}        \n",
      "{'loss': 2.2597, 'learning_rate': 0.0001995774417417444, 'epoch': 0.08}         \n",
      "{'loss': 1.9587, 'learning_rate': 0.00019957715565759797, 'epoch': 0.08}        \n",
      "{'loss': 1.9684, 'learning_rate': 0.00019957686947684588, 'epoch': 0.08}        \n",
      "{'loss': 2.2814, 'learning_rate': 0.00019957658319948838, 'epoch': 0.08}        \n",
      "{'loss': 2.0824, 'learning_rate': 0.00019957629682552578, 'epoch': 0.08}        \n",
      "{'loss': 2.2722, 'learning_rate': 0.00019957601035495837, 'epoch': 0.08}        \n",
      "{'loss': 1.9792, 'learning_rate': 0.0001995757237877864, 'epoch': 0.08}         \n",
      "{'loss': 2.1528, 'learning_rate': 0.00019957543712401015, 'epoch': 0.08}        \n",
      "{'loss': 2.1749, 'learning_rate': 0.0001995751503636299, 'epoch': 0.08}         \n",
      "{'loss': 2.0775, 'learning_rate': 0.00019957486350664593, 'epoch': 0.08}        \n",
      "{'loss': 2.3332, 'learning_rate': 0.00019957457655305855, 'epoch': 0.08}        \n",
      "{'loss': 2.3952, 'learning_rate': 0.00019957428950286796, 'epoch': 0.08}        \n",
      "{'loss': 2.0685, 'learning_rate': 0.0001995740023560745, 'epoch': 0.08}         \n",
      "{'loss': 2.0039, 'learning_rate': 0.00019957371511267847, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83696/1061708 [12:31:38<145:04:08,  1.87it/s][2024-03-01 06:38:38,891] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 83697/1061708 [12:31:38<135:53:12,  2.00it/s][2024-03-01 06:38:39,313] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1436, 'learning_rate': 0.00019957348524840792, 'epoch': 0.08}        \n",
      "{'loss': 2.1798, 'learning_rate': 0.0001995731978311279, 'epoch': 0.08}         \n",
      "{'loss': 2.2314, 'learning_rate': 0.00019957291031724603, 'epoch': 0.08}        \n",
      "{'loss': 1.839, 'learning_rate': 0.0001995726227067626, 'epoch': 0.08}          \n",
      "{'loss': 1.9796, 'learning_rate': 0.00019957233499967795, 'epoch': 0.08}        \n",
      "{'loss': 2.033, 'learning_rate': 0.00019957204719599228, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 83750/1061708 [12:32:06<144:02:54,  1.89it/s][2024-03-01 06:39:07,498] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9093, 'learning_rate': 0.00019957178809008156, 'epoch': 0.08}        \n",
      "{'loss': 2.1899, 'learning_rate': 0.0001995715001028548, 'epoch': 0.08}         \n",
      "{'loss': 2.182, 'learning_rate': 0.00019957121201902785, 'epoch': 0.08}         \n",
      "{'loss': 2.1104, 'learning_rate': 0.000199570923838601, 'epoch': 0.08}          \n",
      "{'loss': 1.8137, 'learning_rate': 0.00019957063556157455, 'epoch': 0.08}        \n",
      "{'loss': 2.6295, 'learning_rate': 0.00019957034718794873, 'epoch': 0.08}        \n",
      "{'loss': 1.9968, 'learning_rate': 0.0001995700587177239, 'epoch': 0.08}         \n",
      "{'loss': 1.8804, 'learning_rate': 0.00019956977015090025, 'epoch': 0.08}        \n",
      "{'loss': 2.0912, 'learning_rate': 0.00019956948148747814, 'epoch': 0.08}        \n",
      "{'loss': 1.9726, 'learning_rate': 0.00019956919272745774, 'epoch': 0.08}        \n",
      "{'loss': 1.9278, 'learning_rate': 0.0001995689038708395, 'epoch': 0.08}         \n",
      "{'loss': 2.1078, 'learning_rate': 0.00019956861491762353, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83870/1061708 [12:33:10<143:49:20,  1.89it/s][2024-03-01 06:40:11,383] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 83871/1061708 [12:33:11<135:32:17,  2.00it/s][2024-03-01 06:40:11,811] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9071, 'learning_rate': 0.00019956838368550065, 'epoch': 0.08}        \n",
      "{'loss': 1.8664, 'learning_rate': 0.00019956809455840962, 'epoch': 0.08}        \n",
      "{'loss': 2.2521, 'learning_rate': 0.00019956780533472173, 'epoch': 0.08}        \n",
      "{'loss': 1.947, 'learning_rate': 0.00019956751601443723, 'epoch': 0.08}         \n",
      "{'loss': 2.2243, 'learning_rate': 0.00019956722659755641, 'epoch': 0.08}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2572, 'learning_rate': 0.00019956693708407958, 'epoch': 0.08}        \n",
      "{'loss': 1.9193, 'learning_rate': 0.000199566647474007, 'epoch': 0.08}          \n",
      "{'loss': 2.3692, 'learning_rate': 0.00019956635776733893, 'epoch': 0.08}        \n",
      "{'loss': 1.722, 'learning_rate': 0.0001995660679640757, 'epoch': 0.08}          \n",
      "{'loss': 2.0553, 'learning_rate': 0.00019956577806421754, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 83973/1061708 [12:34:05<146:52:13,  1.85it/s][2024-03-01 06:41:06,117] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2564, 'learning_rate': 0.0001995655170717568, 'epoch': 0.08}         \n",
      "{'loss': 1.8632, 'learning_rate': 0.0001995652269883691, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 83999/1061708 [12:34:19<143:47:55,  1.89it/s][2024-03-01 06:41:19,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=84000, skipped=955, lr=[0.0001995649368083873], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 06:41:20,001] [INFO] [timer.py:260:stop] epoch=0/micro_step=84000/global_step=84000, RunningAvgSamplesPerSec=1.8923450517027725, CurrSamplesPerSec=1.8966813195857268, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.626, 'learning_rate': 0.0001995649368083873, 'epoch': 0.08}          \n",
      "{'loss': 2.2397, 'learning_rate': 0.0001995646465318117, 'epoch': 0.08}         \n",
      "{'loss': 2.1416, 'learning_rate': 0.00019956435615864257, 'epoch': 0.08}        \n",
      "{'loss': 1.7557, 'learning_rate': 0.0001995640656888802, 'epoch': 0.08}         \n",
      "{'loss': 2.0117, 'learning_rate': 0.00019956377512252485, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84042/1061708 [12:34:42<145:01:35,  1.87it/s][2024-03-01 06:41:42,924] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0652, 'learning_rate': 0.0001995635135302183, 'epoch': 0.08}         \n",
      "{'loss': 2.2685, 'learning_rate': 0.00019956322278033713, 'epoch': 0.08}        \n",
      "{'loss': 2.032, 'learning_rate': 0.0001995629319338638, 'epoch': 0.08}          \n",
      "{'loss': 2.0602, 'learning_rate': 0.00019956264099079858, 'epoch': 0.08}        \n",
      "{'loss': 2.3635, 'learning_rate': 0.0001995623499511418, 'epoch': 0.08}         \n",
      "{'loss': 2.0724, 'learning_rate': 0.00019956205881489368, 'epoch': 0.08}        \n",
      "{'loss': 2.394, 'learning_rate': 0.0001995617675820546, 'epoch': 0.08}          \n",
      "{'loss': 2.2549, 'learning_rate': 0.00019956147625262475, 'epoch': 0.08}        \n",
      "{'loss': 1.8854, 'learning_rate': 0.0001995611848266045, 'epoch': 0.08}         \n",
      "{'loss': 2.0227, 'learning_rate': 0.000199560893303994, 'epoch': 0.08}          \n",
      "{'loss': 2.1567, 'learning_rate': 0.00019956060168479364, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84159/1061708 [12:35:44<143:53:01,  1.89it/s][2024-03-01 06:42:45,218] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5487, 'learning_rate': 0.00019956033914492923, 'epoch': 0.08}        \n",
      "{'loss': 1.8374, 'learning_rate': 0.00019956004734220887, 'epoch': 0.08}        \n",
      "{'loss': 2.1916, 'learning_rate': 0.00019955975544289943, 'epoch': 0.08}        \n",
      "{'loss': 2.0642, 'learning_rate': 0.00019955946344700127, 'epoch': 0.08}        \n",
      "{'loss': 2.2419, 'learning_rate': 0.00019955917135451457, 'epoch': 0.08}        \n",
      "{'loss': 2.0792, 'learning_rate': 0.00019955887916543966, 'epoch': 0.08}        \n",
      "{'loss': 1.705, 'learning_rate': 0.00019955858687977678, 'epoch': 0.08}         \n",
      "{'loss': 1.81, 'learning_rate': 0.0001995582944975263, 'epoch': 0.08}           \n",
      "  8%|██▏                         | 84234/1061708 [12:36:24<146:20:27,  1.86it/s][2024-03-01 06:43:25,171] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0851, 'learning_rate': 0.00019955803127091865, 'epoch': 0.08}        \n",
      "{'loss': 2.1656, 'learning_rate': 0.0001995577387051524, 'epoch': 0.08}         \n",
      "{'loss': 2.2226, 'learning_rate': 0.00019955744604279932, 'epoch': 0.08}        \n",
      "{'loss': 1.9495, 'learning_rate': 0.0001995571532838597, 'epoch': 0.08}         \n",
      "{'loss': 2.1818, 'learning_rate': 0.00019955686042833382, 'epoch': 0.08}        \n",
      "{'loss': 2.017, 'learning_rate': 0.00019955656747622195, 'epoch': 0.08}         \n",
      "{'loss': 2.5548, 'learning_rate': 0.00019955627442752442, 'epoch': 0.08}        \n",
      "{'loss': 1.8905, 'learning_rate': 0.0001995559812822415, 'epoch': 0.08}         \n",
      "{'loss': 1.9898, 'learning_rate': 0.00019955568804037342, 'epoch': 0.08}        \n",
      "{'loss': 1.8324, 'learning_rate': 0.0001995553947019205, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 84337/1061708 [12:37:19<144:36:08,  1.88it/s][2024-03-01 06:44:20,121] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0014, 'learning_rate': 0.0001995551306147331, 'epoch': 0.08}         \n",
      "{'loss': 1.8353, 'learning_rate': 0.00019955483709276978, 'epoch': 0.08}        \n",
      "{'loss': 1.8612, 'learning_rate': 0.00019955454347422247, 'epoch': 0.08}        \n",
      "{'loss': 1.9409, 'learning_rate': 0.00019955424975909143, 'epoch': 0.08}        \n",
      "{'loss': 1.9602, 'learning_rate': 0.0001995539559473769, 'epoch': 0.08}         \n",
      "{'loss': 2.0791, 'learning_rate': 0.00019955366203907928, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84393/1061708 [12:37:49<147:47:55,  1.84it/s][2024-03-01 06:44:49,973] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2633, 'learning_rate': 0.00019955339743903304, 'epoch': 0.08}        \n",
      "{'loss': 1.754, 'learning_rate': 0.00019955310334722817, 'epoch': 0.08}         \n",
      "{'loss': 2.0103, 'learning_rate': 0.000199552809158841, 'epoch': 0.08}          \n",
      "{'loss': 2.0141, 'learning_rate': 0.00019955251487387176, 'epoch': 0.08}        \n",
      "{'loss': 2.014, 'learning_rate': 0.00019955222049232078, 'epoch': 0.08}         \n",
      "{'loss': 2.2958, 'learning_rate': 0.0001995519260141883, 'epoch': 0.08}         \n",
      "{'loss': 1.9932, 'learning_rate': 0.00019955163143947467, 'epoch': 0.08}        \n",
      "{'loss': 2.1264, 'learning_rate': 0.00019955133676818012, 'epoch': 0.08}        \n",
      "{'loss': 2.0732, 'learning_rate': 0.00019955104200030497, 'epoch': 0.08}        \n",
      "{'loss': 1.9089, 'learning_rate': 0.00019955074713584944, 'epoch': 0.08}        \n",
      "{'loss': 1.8073, 'learning_rate': 0.00019955045217481392, 'epoch': 0.08}        \n",
      "{'loss': 1.7583, 'learning_rate': 0.00019955015711719862, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84519/1061708 [12:38:56<144:07:21,  1.88it/s][2024-03-01 06:45:57,230] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9032, 'learning_rate': 0.00019954989148276942, 'epoch': 0.08}        \n",
      "{'loss': 2.2065, 'learning_rate': 0.00019954959624165336, 'epoch': 0.08}        \n",
      "{'loss': 2.1466, 'learning_rate': 0.0001995493009039584, 'epoch': 0.08}         \n",
      "{'loss': 2.0075, 'learning_rate': 0.00019954900546968477, 'epoch': 0.08}        \n",
      "{'loss': 2.3424, 'learning_rate': 0.0001995487099388328, 'epoch': 0.08}         \n",
      "{'loss': 1.5121, 'learning_rate': 0.00019954841431140278, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84579/1061708 [12:39:28<144:26:10,  1.88it/s][2024-03-01 06:46:29,168] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8617, 'learning_rate': 0.00019954814816414173, 'epoch': 0.08}        \n",
      "{'loss': 2.0645, 'learning_rate': 0.0001995478523532142, 'epoch': 0.08}         \n",
      "{'loss': 2.113, 'learning_rate': 0.0001995475564457094, 'epoch': 0.08}          \n",
      "{'loss': 2.2124, 'learning_rate': 0.0001995472604416276, 'epoch': 0.08}         \n",
      "{'loss': 2.2008, 'learning_rate': 0.0001995469643409692, 'epoch': 0.08}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5867, 'learning_rate': 0.00019954666814373445, 'epoch': 0.08}        \n",
      "{'loss': 1.7566, 'learning_rate': 0.00019954637184992356, 'epoch': 0.08}        \n",
      "{'loss': 2.4146, 'learning_rate': 0.00019954607545953688, 'epoch': 0.08}        \n",
      "{'loss': 1.9074, 'learning_rate': 0.0001995457789725747, 'epoch': 0.08}         \n",
      "{'loss': 2.0507, 'learning_rate': 0.0001995454823890373, 'epoch': 0.08}         \n",
      "{'loss': 2.3288, 'learning_rate': 0.00019954518570892496, 'epoch': 0.08}        \n",
      "{'loss': 1.758, 'learning_rate': 0.000199544888932238, 'epoch': 0.08}           \n",
      "{'loss': 1.857, 'learning_rate': 0.00019954459205897665, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 84707/1061708 [12:40:36<144:04:58,  1.88it/s][2024-03-01 06:47:37,313] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.451, 'learning_rate': 0.00019954432479047057, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 84713/1061708 [12:40:39<146:04:37,  1.86it/s][2024-03-01 06:47:40,436] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3549, 'learning_rate': 0.00019954405744373975, 'epoch': 0.08}        \n",
      "{'loss': 2.4604, 'learning_rate': 0.0001995437603000717, 'epoch': 0.08}         \n",
      "{'loss': 1.8688, 'learning_rate': 0.00019954346305983044, 'epoch': 0.08}        \n",
      "{'loss': 1.6928, 'learning_rate': 0.00019954316572301617, 'epoch': 0.08}        \n",
      "{'loss': 1.946, 'learning_rate': 0.0001995428682896292, 'epoch': 0.08}          \n",
      "{'loss': 2.1516, 'learning_rate': 0.00019954257075966987, 'epoch': 0.08}        \n",
      "{'loss': 2.0433, 'learning_rate': 0.0001995422731331384, 'epoch': 0.08}         \n",
      "{'loss': 2.3439, 'learning_rate': 0.00019954197541003512, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 84791/1061708 [12:41:21<143:34:26,  1.89it/s][2024-03-01 06:48:21,893] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0201, 'learning_rate': 0.00019954170737667348, 'epoch': 0.08}        \n",
      "{'loss': 2.217, 'learning_rate': 0.00019954140947008454, 'epoch': 0.08}         \n",
      "{'loss': 2.2211, 'learning_rate': 0.0001995411114669246, 'epoch': 0.08}         \n",
      "{'loss': 1.8517, 'learning_rate': 0.00019954081336719397, 'epoch': 0.08}        \n",
      "{'loss': 2.159, 'learning_rate': 0.00019954051517089293, 'epoch': 0.08}         \n",
      "{'loss': 1.907, 'learning_rate': 0.00019954021687802178, 'epoch': 0.08}         \n",
      "{'loss': 1.9151, 'learning_rate': 0.0001995399184885808, 'epoch': 0.08}         \n",
      "{'loss': 2.2289, 'learning_rate': 0.0001995396200025703, 'epoch': 0.08}         \n",
      "{'loss': 2.4132, 'learning_rate': 0.00019953932141999052, 'epoch': 0.08}        \n",
      "{'loss': 1.7612, 'learning_rate': 0.0001995390227408418, 'epoch': 0.08}         \n",
      "{'loss': 2.3933, 'learning_rate': 0.00019953872396512442, 'epoch': 0.08}        \n",
      "{'loss': 2.0451, 'learning_rate': 0.00019953842509283861, 'epoch': 0.08}        \n",
      "{'loss': 1.8602, 'learning_rate': 0.00019953812612398476, 'epoch': 0.08}        \n",
      "{'loss': 1.8255, 'learning_rate': 0.0001995378270585631, 'epoch': 0.08}         \n",
      "{'loss': 2.0592, 'learning_rate': 0.00019953752789657396, 'epoch': 0.08}        \n",
      "{'loss': 2.0329, 'learning_rate': 0.00019953722863801755, 'epoch': 0.08}        \n",
      "{'loss': 2.0923, 'learning_rate': 0.00019953692928289426, 'epoch': 0.08}        \n",
      "{'loss': 2.5719, 'learning_rate': 0.0001995366298312043, 'epoch': 0.08}         \n",
      "{'loss': 1.7781, 'learning_rate': 0.000199536330282948, 'epoch': 0.08}          \n",
      "{'loss': 2.3839, 'learning_rate': 0.00019953603063812563, 'epoch': 0.08}        \n",
      "{'loss': 2.3391, 'learning_rate': 0.0001995357308967375, 'epoch': 0.08}         \n",
      "  8%|██▏                         | 85000/1061708 [12:43:12<143:59:58,  1.88it/s][INFO|trainer.py:2868] 2024-03-01 06:50:12,892 >> Saving model checkpoint to output_model/checkpoint-85000\n",
      "[INFO|trainer.py:2880] 2024-03-01 06:50:12,895 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 06:50:14,114 >> tokenizer config file saved in output_model/checkpoint-85000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 06:50:14,114 >> Special tokens file saved in output_model/checkpoint-85000/special_tokens_map.json\n",
      "[2024-03-01 06:50:14,115] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step85000 is about to be saved!\n",
      "[2024-03-01 06:50:19,359] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-85000/global_step85000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 06:50:19,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-85000/global_step85000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 06:50:33,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-85000/global_step85000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 06:50:33,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-85000/global_step85000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 06:50:41,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-85000/global_step85000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 06:50:41,049] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-85000/global_step85000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 06:50:41,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step85000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 06:50:41,123 >> Deleting older checkpoint [output_model/checkpoint-70000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 06:50:44,871 >> tokenizer config file saved in output_model/checkpoint-85000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 06:50:44,871 >> Special tokens file saved in output_model/checkpoint-85000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.211, 'learning_rate': 0.00019953543105878392, 'epoch': 0.08}         \n",
      "{'loss': 1.8995, 'learning_rate': 0.00019953513112426513, 'epoch': 0.08}        \n",
      "{'loss': 2.2305, 'learning_rate': 0.00019953483109318146, 'epoch': 0.08}        \n",
      "{'loss': 1.9258, 'learning_rate': 0.00019953453096553317, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 85046/1061708 [12:44:09<143:37:13,  1.89it/s][2024-03-01 06:51:09,648] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▏                         | 85049/1061708 [12:44:10<139:03:27,  1.95it/s][2024-03-01 06:51:11,169] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.5793, 'learning_rate': 0.00019953429079388823, 'epoch': 0.08}        \n",
      "{'loss': 2.3582, 'learning_rate': 0.0001995339904924244, 'epoch': 0.08}         \n",
      "{'loss': 1.9847, 'learning_rate': 0.0001995336900943968, 'epoch': 0.08}         \n",
      "{'loss': 1.5657, 'learning_rate': 0.00019953338959980568, 'epoch': 0.08}        \n",
      "{'loss': 2.3315, 'learning_rate': 0.00019953308900865136, 'epoch': 0.08}        \n",
      "{'loss': 2.49, 'learning_rate': 0.00019953278832093415, 'epoch': 0.08}          \n",
      "{'loss': 1.9992, 'learning_rate': 0.0001995324875366543, 'epoch': 0.08}         \n",
      "{'loss': 1.9364, 'learning_rate': 0.00019953218665581213, 'epoch': 0.08}        \n",
      "{'loss': 2.1044, 'learning_rate': 0.00019953188567840792, 'epoch': 0.08}        \n",
      "{'loss': 2.3145, 'learning_rate': 0.00019953158460444197, 'epoch': 0.08}        \n",
      "{'loss': 2.0303, 'learning_rate': 0.00019953128343391457, 'epoch': 0.08}        \n",
      "{'loss': 1.9436, 'learning_rate': 0.000199530982166826, 'epoch': 0.08}          \n",
      "{'loss': 2.4094, 'learning_rate': 0.00019953068080317654, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 85178/1061708 [12:45:19<144:59:57,  1.87it/s][2024-03-01 06:52:20,084] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1297, 'learning_rate': 0.00019953040949333275, 'epoch': 0.08}        \n",
      "{'loss': 2.2339, 'learning_rate': 0.00019953010794621846, 'epoch': 0.08}        \n",
      "{'loss': 2.1297, 'learning_rate': 0.00019952980630254417, 'epoch': 0.08}        \n",
      "{'loss': 2.0779, 'learning_rate': 0.00019952950456231014, 'epoch': 0.08}        \n",
      "{'loss': 2.1884, 'learning_rate': 0.00019952920272551668, 'epoch': 0.08}        \n",
      "{'loss': 2.1094, 'learning_rate': 0.0001995289007921641, 'epoch': 0.08}         \n",
      "{'loss': 2.2381, 'learning_rate': 0.0001995285987622526, 'epoch': 0.08}         \n",
      "{'loss': 2.0203, 'learning_rate': 0.00019952829663578262, 'epoch': 0.08}        \n",
      "{'loss': 2.1927, 'learning_rate': 0.00019952799441275433, 'epoch': 0.08}        \n",
      "  8%|██▏                         | 85261/1061708 [12:46:03<144:40:23,  1.87it/s][2024-03-01 06:53:04,484] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.908, 'learning_rate': 0.0001995277223294718, 'epoch': 0.08}          \n",
      "{'loss': 2.0638, 'learning_rate': 0.00019952741992298366, 'epoch': 0.08}        \n",
      "{'loss': 2.2104, 'learning_rate': 0.00019952711741993807, 'epoch': 0.08}        \n",
      "{'loss': 2.1705, 'learning_rate': 0.0001995268148203354, 'epoch': 0.08}         \n",
      "{'loss': 2.2036, 'learning_rate': 0.00019952651212417588, 'epoch': 0.08}        \n",
      "{'loss': 2.0729, 'learning_rate': 0.00019952620933145983, 'epoch': 0.08}        \n",
      "{'loss': 1.8823, 'learning_rate': 0.00019952590644218754, 'epoch': 0.08}        \n",
      "{'loss': 1.9711, 'learning_rate': 0.00019952560345635928, 'epoch': 0.08}        \n",
      "{'loss': 2.143, 'learning_rate': 0.0001995253003739754, 'epoch': 0.08}          \n",
      "{'loss': 2.3726, 'learning_rate': 0.00019952499719503618, 'epoch': 0.08}        \n",
      "{'loss': 2.2587, 'learning_rate': 0.00019952469391954186, 'epoch': 0.08}        \n",
      "{'loss': 2.0996, 'learning_rate': 0.0001995243905474928, 'epoch': 0.08}         \n",
      "{'loss': 1.6696, 'learning_rate': 0.00019952408707888926, 'epoch': 0.08}        \n",
      "{'loss': 2.1436, 'learning_rate': 0.00019952378351373152, 'epoch': 0.08}        \n",
      "{'loss': 2.1012, 'learning_rate': 0.00019952347985201994, 'epoch': 0.08}        \n",
      "{'loss': 2.0108, 'learning_rate': 0.00019952317609375474, 'epoch': 0.08}        \n",
      "{'loss': 1.6674, 'learning_rate': 0.00019952287223893626, 'epoch': 0.08}        \n",
      "{'loss': 2.1937, 'learning_rate': 0.00019952256828756478, 'epoch': 0.08}        \n",
      "{'loss': 2.0427, 'learning_rate': 0.00019952226423964058, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 85450/1061708 [12:47:44<143:46:07,  1.89it/s][2024-03-01 06:54:45,239] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0802, 'learning_rate': 0.00019952199051395648, 'epoch': 0.08}        \n",
      "{'loss': 2.3062, 'learning_rate': 0.00019952168628258296, 'epoch': 0.08}        \n",
      "{'loss': 2.1579, 'learning_rate': 0.0001995213819546576, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 85481/1061708 [12:48:01<143:51:58,  1.88it/s][2024-03-01 06:55:01,719] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.753, 'learning_rate': 0.00019952110797697318, 'epoch': 0.08}         \n",
      "{'loss': 2.0624, 'learning_rate': 0.0001995208034656001, 'epoch': 0.08}         \n",
      "{'loss': 2.0984, 'learning_rate': 0.00019952049885767602, 'epoch': 0.08}        \n",
      "{'loss': 2.563, 'learning_rate': 0.00019952019415320126, 'epoch': 0.08}         \n",
      "{'loss': 1.7904, 'learning_rate': 0.0001995198893521761, 'epoch': 0.08}         \n",
      "{'loss': 2.0553, 'learning_rate': 0.00019951958445460084, 'epoch': 0.08}        \n",
      "{'loss': 1.7382, 'learning_rate': 0.00019951927946047576, 'epoch': 0.08}        \n",
      "{'loss': 2.073, 'learning_rate': 0.00019951897436980116, 'epoch': 0.08}         \n",
      "{'loss': 2.2521, 'learning_rate': 0.00019951866918257736, 'epoch': 0.08}        \n",
      "{'loss': 2.1586, 'learning_rate': 0.0001995183638988046, 'epoch': 0.08}         \n",
      "{'loss': 2.0219, 'learning_rate': 0.00019951805851848325, 'epoch': 0.08}        \n",
      "{'loss': 2.0761, 'learning_rate': 0.00019951775304161354, 'epoch': 0.08}        \n",
      "{'loss': 2.0643, 'learning_rate': 0.00019951744746819582, 'epoch': 0.08}        \n",
      "{'loss': 1.9484, 'learning_rate': 0.00019951714179823035, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 85627/1061708 [12:49:19<144:25:10,  1.88it/s][2024-03-01 06:56:19,653] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9976, 'learning_rate': 0.00019951686661271337, 'epoch': 0.08}        \n",
      "{'loss': 2.3447, 'learning_rate': 0.00019951656075930803, 'epoch': 0.08}        \n",
      "{'loss': 2.2377, 'learning_rate': 0.00019951625480935578, 'epoch': 0.08}        \n",
      "{'loss': 1.8748, 'learning_rate': 0.000199515948762857, 'epoch': 0.08}          \n",
      "  8%|██▎                         | 85664/1061708 [12:49:38<146:25:56,  1.85it/s][2024-03-01 06:56:39,346] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0561, 'learning_rate': 0.00019951567323846098, 'epoch': 0.08}        \n",
      "{'loss': 2.1614, 'learning_rate': 0.0001995153670085245, 'epoch': 0.08}         \n",
      "{'loss': 2.2184, 'learning_rate': 0.00019951506068204228, 'epoch': 0.08}        \n",
      "{'loss': 2.0942, 'learning_rate': 0.00019951475425901465, 'epoch': 0.08}        \n",
      "{'loss': 1.7868, 'learning_rate': 0.0001995144477394419, 'epoch': 0.08}         \n",
      "{'loss': 2.4585, 'learning_rate': 0.00019951414112332432, 'epoch': 0.08}        \n",
      "{'loss': 2.4268, 'learning_rate': 0.00019951383441066221, 'epoch': 0.08}        \n",
      "{'loss': 2.1896, 'learning_rate': 0.00019951352760145586, 'epoch': 0.08}        \n",
      "{'loss': 1.743, 'learning_rate': 0.0001995132206957056, 'epoch': 0.08}          \n",
      "{'loss': 2.3937, 'learning_rate': 0.00019951291369341164, 'epoch': 0.08}        \n",
      "{'loss': 1.8108, 'learning_rate': 0.0001995126065945744, 'epoch': 0.08}         \n",
      "{'loss': 2.1145, 'learning_rate': 0.00019951229939919413, 'epoch': 0.08}        \n",
      "{'loss': 2.117, 'learning_rate': 0.0001995119921072711, 'epoch': 0.08}          \n",
      "{'loss': 2.2704, 'learning_rate': 0.0001995116847188056, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 85806/1061708 [12:50:54<144:44:23,  1.87it/s][2024-03-01 06:57:55,259] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0516, 'learning_rate': 0.00019951140798664312, 'epoch': 0.08}        \n",
      "{'loss': 2.167, 'learning_rate': 0.00019951110041474781, 'epoch': 0.08}         \n",
      "{'loss': 1.771, 'learning_rate': 0.00019951079274631092, 'epoch': 0.08}         \n",
      "{'loss': 2.1036, 'learning_rate': 0.00019951048498133275, 'epoch': 0.08}        \n",
      "{'loss': 1.997, 'learning_rate': 0.0001995101771198136, 'epoch': 0.08}          \n",
      "{'loss': 2.1564, 'learning_rate': 0.00019950986916175378, 'epoch': 0.08}        \n",
      "{'loss': 1.9745, 'learning_rate': 0.00019950956110715356, 'epoch': 0.08}        \n",
      "{'loss': 2.2574, 'learning_rate': 0.00019950925295601324, 'epoch': 0.08}        \n",
      "{'loss': 1.9895, 'learning_rate': 0.00019950894470833314, 'epoch': 0.08}        \n",
      "{'loss': 2.3811, 'learning_rate': 0.00019950863636411357, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 85907/1061708 [12:51:48<144:10:24,  1.88it/s][2024-03-01 06:58:49,149] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 85908/1061708 [12:51:49<135:40:15,  2.00it/s][2024-03-01 06:58:49,573] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1233, 'learning_rate': 0.0001995083896192297, 'epoch': 0.08}         \n",
      "{'loss': 2.2936, 'learning_rate': 0.0001995080811012398, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 85922/1061708 [12:51:56<144:07:53,  1.88it/s][2024-03-01 06:58:57,009] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9393, 'learning_rate': 0.00019950780335250832, 'epoch': 0.08}        \n",
      "{'loss': 2.0768, 'learning_rate': 0.00019950749465109523, 'epoch': 0.08}        \n",
      "{'loss': 2.0779, 'learning_rate': 0.00019950718585314404, 'epoch': 0.08}        \n",
      "{'loss': 2.0959, 'learning_rate': 0.00019950687695865512, 'epoch': 0.08}        \n",
      "{'loss': 2.4541, 'learning_rate': 0.00019950656796762867, 'epoch': 0.08}        \n",
      "{'loss': 1.8415, 'learning_rate': 0.00019950625888006507, 'epoch': 0.08}        \n",
      "{'loss': 2.1003, 'learning_rate': 0.00019950594969596458, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 85999/1061708 [12:52:37<143:59:17,  1.88it/s][2024-03-01 06:59:38,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=86000, skipped=977, lr=[0.00019950564041532748], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 06:59:38,164] [INFO] [timer.py:260:stop] epoch=0/micro_step=86000/global_step=86000, RunningAvgSamplesPerSec=1.892331178801748, CurrSamplesPerSec=1.8948253873742578, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1204, 'learning_rate': 0.00019950564041532748, 'epoch': 0.08}        \n",
      "{'loss': 2.2566, 'learning_rate': 0.00019950533103815413, 'epoch': 0.08}        \n",
      "{'loss': 2.1788, 'learning_rate': 0.00019950502156444478, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86023/1061708 [12:52:50<147:12:52,  1.84it/s][2024-03-01 06:59:50,917] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2159, 'learning_rate': 0.00019950474295556836, 'epoch': 0.08}        \n",
      "{'loss': 2.1673, 'learning_rate': 0.00019950443329844149, 'epoch': 0.08}        \n",
      "{'loss': 2.0262, 'learning_rate': 0.00019950412354477947, 'epoch': 0.08}        \n",
      "{'loss': 2.1014, 'learning_rate': 0.00019950381369458268, 'epoch': 0.08}        \n",
      "{'loss': 2.5093, 'learning_rate': 0.00019950350374785137, 'epoch': 0.08}        \n",
      "{'loss': 2.2698, 'learning_rate': 0.00019950319370458584, 'epoch': 0.08}        \n",
      "{'loss': 2.5207, 'learning_rate': 0.0001995028835647864, 'epoch': 0.08}         \n",
      "{'loss': 2.242, 'learning_rate': 0.00019950257332845337, 'epoch': 0.08}         \n",
      "{'loss': 2.0896, 'learning_rate': 0.000199502262995587, 'epoch': 0.08}          \n",
      "{'loss': 1.7866, 'learning_rate': 0.00019950195256618764, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86124/1061708 [12:53:44<146:12:21,  1.85it/s][2024-03-01 07:00:44,817] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 86125/1061708 [12:53:44<136:43:04,  1.98it/s][2024-03-01 07:00:45,240] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9896, 'learning_rate': 0.0001995017041531646, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 86136/1061708 [12:53:50<144:33:25,  1.87it/s][2024-03-01 07:00:51,050] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1367, 'learning_rate': 0.00019950142461466633, 'epoch': 0.08}        \n",
      "{'loss': 2.1401, 'learning_rate': 0.00019950111392462937, 'epoch': 0.08}        \n",
      "{'loss': 2.467, 'learning_rate': 0.00019950080313806053, 'epoch': 0.08}         \n",
      "{'loss': 2.421, 'learning_rate': 0.00019950049225496007, 'epoch': 0.08}         \n",
      "{'loss': 2.1262, 'learning_rate': 0.00019950018127532835, 'epoch': 0.08}        \n",
      "{'loss': 2.0234, 'learning_rate': 0.00019949987019916564, 'epoch': 0.08}        \n",
      "{'loss': 2.1454, 'learning_rate': 0.0001994995590264722, 'epoch': 0.08}         \n",
      "{'loss': 2.2484, 'learning_rate': 0.00019949924775724843, 'epoch': 0.08}        \n",
      "{'loss': 1.9468, 'learning_rate': 0.00019949893639149456, 'epoch': 0.08}        \n",
      "{'loss': 1.8645, 'learning_rate': 0.0001994986249292109, 'epoch': 0.08}         \n",
      "{'loss': 1.7312, 'learning_rate': 0.00019949831337039777, 'epoch': 0.08}        \n",
      "{'loss': 2.3384, 'learning_rate': 0.00019949800171505547, 'epoch': 0.08}        \n",
      "{'loss': 2.2074, 'learning_rate': 0.00019949768996318433, 'epoch': 0.08}        \n",
      "{'loss': 2.3402, 'learning_rate': 0.00019949737811478455, 'epoch': 0.08}        \n",
      "{'loss': 2.0319, 'learning_rate': 0.00019949706616985657, 'epoch': 0.08}        \n",
      "{'loss': 2.4097, 'learning_rate': 0.00019949675412840058, 'epoch': 0.08}        \n",
      "{'loss': 1.9383, 'learning_rate': 0.00019949644199041694, 'epoch': 0.08}        \n",
      "{'loss': 2.2072, 'learning_rate': 0.00019949612975590596, 'epoch': 0.08}        \n",
      "{'loss': 2.2179, 'learning_rate': 0.00019949581742486792, 'epoch': 0.08}        \n",
      "{'loss': 2.2679, 'learning_rate': 0.00019949550499730312, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86337/1061708 [12:55:37<144:09:03,  1.88it/s][2024-03-01 07:02:38,362] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 86338/1061708 [12:55:38<135:09:21,  2.00it/s][2024-03-01 07:02:38,785] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0507, 'learning_rate': 0.0001994952549857522, 'epoch': 0.08}         \n",
      "{'loss': 2.2935, 'learning_rate': 0.00019949494238444003, 'epoch': 0.08}        \n",
      "{'loss': 2.134, 'learning_rate': 0.00019949462968660192, 'epoch': 0.08}         \n",
      "{'loss': 1.6904, 'learning_rate': 0.00019949431689223824, 'epoch': 0.08}        \n",
      "{'loss': 1.9117, 'learning_rate': 0.00019949400400134923, 'epoch': 0.08}        \n",
      "{'loss': 2.0279, 'learning_rate': 0.00019949369101393525, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86396/1061708 [12:56:09<144:29:46,  1.87it/s][2024-03-01 07:03:09,655] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0097, 'learning_rate': 0.00019949340924273404, 'epoch': 0.08}        \n",
      "{'loss': 1.9279, 'learning_rate': 0.00019949309607192342, 'epoch': 0.08}        \n",
      "{'loss': 2.2426, 'learning_rate': 0.00019949278280458868, 'epoch': 0.08}        \n",
      "{'loss': 1.8822, 'learning_rate': 0.0001994924694407301, 'epoch': 0.08}         \n",
      "{'loss': 2.2217, 'learning_rate': 0.00019949215598034803, 'epoch': 0.08}        \n",
      "{'loss': 2.0127, 'learning_rate': 0.0001994918424234428, 'epoch': 0.08}         \n",
      "{'loss': 2.1508, 'learning_rate': 0.00019949152877001464, 'epoch': 0.08}        \n",
      "{'loss': 2.365, 'learning_rate': 0.00019949121502006384, 'epoch': 0.08}         \n",
      "{'loss': 2.3893, 'learning_rate': 0.0001994909011735908, 'epoch': 0.08}         \n",
      "{'loss': 2.4166, 'learning_rate': 0.0001994905872305958, 'epoch': 0.08}         \n",
      "{'loss': 1.8186, 'learning_rate': 0.00019949027319107908, 'epoch': 0.08}        \n",
      "{'loss': 2.3117, 'learning_rate': 0.00019948995905504102, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86519/1061708 [12:57:14<143:56:24,  1.88it/s][2024-03-01 07:04:15,298] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8554, 'learning_rate': 0.00019948967625008123, 'epoch': 0.08}        \n",
      "{'loss': 1.8668, 'learning_rate': 0.0001994893619306534, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 86533/1061708 [12:57:22<147:14:56,  1.84it/s][2024-03-01 07:04:22,713] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0559, 'learning_rate': 0.0001994890789606433, 'epoch': 0.08}         \n",
      "{'loss': 1.8921, 'learning_rate': 0.0001994887644578268, 'epoch': 0.08}         \n",
      "{'loss': 1.839, 'learning_rate': 0.00019948844985849036, 'epoch': 0.08}         \n",
      "{'loss': 1.8792, 'learning_rate': 0.0001994881351626343, 'epoch': 0.08}         \n",
      "{'loss': 2.547, 'learning_rate': 0.00019948782037025898, 'epoch': 0.08}         \n",
      "{'loss': 2.21, 'learning_rate': 0.00019948750548136465, 'epoch': 0.08}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3053, 'learning_rate': 0.00019948719049595165, 'epoch': 0.08}        \n",
      "{'loss': 2.0357, 'learning_rate': 0.00019948687541402027, 'epoch': 0.08}        \n",
      "{'loss': 2.0342, 'learning_rate': 0.0001994865602355708, 'epoch': 0.08}         \n",
      "{'loss': 1.98, 'learning_rate': 0.00019948624496060355, 'epoch': 0.08}          \n",
      "  8%|██▎                         | 86637/1061708 [12:58:17<144:14:44,  1.88it/s][2024-03-01 07:05:18,219] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0517, 'learning_rate': 0.00019948596113061057, 'epoch': 0.08}        \n",
      "{'loss': 2.4388, 'learning_rate': 0.0001994856456722604, 'epoch': 0.08}         \n",
      "{'loss': 1.933, 'learning_rate': 0.00019948533011739333, 'epoch': 0.08}         \n",
      "{'loss': 2.5181, 'learning_rate': 0.00019948501446600972, 'epoch': 0.08}        \n",
      "{'loss': 1.8927, 'learning_rate': 0.0001994846987181098, 'epoch': 0.08}         \n",
      "{'loss': 2.245, 'learning_rate': 0.00019948438287369392, 'epoch': 0.08}         \n",
      "{'loss': 2.0704, 'learning_rate': 0.0001994840669327624, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 86702/1061708 [12:58:52<143:48:54,  1.88it/s][2024-03-01 07:05:52,854] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0532, 'learning_rate': 0.0001994837825034034, 'epoch': 0.08}         \n",
      "{'loss': 2.1168, 'learning_rate': 0.00019948346637909295, 'epoch': 0.08}        \n",
      "{'loss': 1.8353, 'learning_rate': 0.00019948315015826774, 'epoch': 0.08}        \n",
      "{'loss': 1.8869, 'learning_rate': 0.00019948283384092807, 'epoch': 0.08}        \n",
      "{'loss': 2.1993, 'learning_rate': 0.00019948251742707424, 'epoch': 0.08}        \n",
      "{'loss': 2.1242, 'learning_rate': 0.00019948220091670654, 'epoch': 0.08}        \n",
      "{'loss': 1.9427, 'learning_rate': 0.00019948188430982534, 'epoch': 0.08}        \n",
      "{'loss': 1.8886, 'learning_rate': 0.00019948156760643087, 'epoch': 0.08}        \n",
      "{'loss': 1.8183, 'learning_rate': 0.0001994812508065235, 'epoch': 0.08}         \n",
      "{'loss': 2.4539, 'learning_rate': 0.0001994809339101035, 'epoch': 0.08}         \n",
      "{'loss': 1.8635, 'learning_rate': 0.00019948061691717116, 'epoch': 0.08}        \n",
      "{'loss': 2.3126, 'learning_rate': 0.00019948029982772684, 'epoch': 0.08}        \n",
      "{'loss': 1.8787, 'learning_rate': 0.00019947998264177081, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86831/1061708 [13:00:01<143:35:08,  1.89it/s][2024-03-01 07:07:01,665] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1426, 'learning_rate': 0.00019947969709189315, 'epoch': 0.08}        \n",
      "{'loss': 2.1996, 'learning_rate': 0.00019947937972256577, 'epoch': 0.08}        \n",
      "{'loss': 2.233, 'learning_rate': 0.00019947906225672754, 'epoch': 0.08}         \n",
      "{'loss': 1.8028, 'learning_rate': 0.00019947874469437884, 'epoch': 0.08}        \n",
      "{'loss': 1.9869, 'learning_rate': 0.00019947842703551996, 'epoch': 0.08}        \n",
      "{'loss': 1.9813, 'learning_rate': 0.00019947810928015115, 'epoch': 0.08}        \n",
      "{'loss': 2.1784, 'learning_rate': 0.00019947779142827284, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 86909/1061708 [13:00:42<143:41:47,  1.88it/s][2024-03-01 07:07:43,251] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0732, 'learning_rate': 0.00019947750527906687, 'epoch': 0.08}        \n",
      "{'loss': 2.033, 'learning_rate': 0.0001994771872438212, 'epoch': 0.08}          \n",
      "{'loss': 2.1793, 'learning_rate': 0.00019947686911206682, 'epoch': 0.08}        \n",
      "{'loss': 1.9952, 'learning_rate': 0.00019947655088380407, 'epoch': 0.08}        \n",
      "{'loss': 1.7816, 'learning_rate': 0.00019947623255903328, 'epoch': 0.08}        \n",
      "{'loss': 2.079, 'learning_rate': 0.00019947591413775473, 'epoch': 0.08}         \n",
      "{'loss': 2.4327, 'learning_rate': 0.00019947559561996875, 'epoch': 0.08}        \n",
      "{'loss': 2.4008, 'learning_rate': 0.00019947527700567562, 'epoch': 0.08}        \n",
      "{'loss': 1.9497, 'learning_rate': 0.00019947495829487566, 'epoch': 0.08}        \n",
      "{'loss': 1.9037, 'learning_rate': 0.0001994746394875692, 'epoch': 0.08}         \n",
      "{'loss': 2.0484, 'learning_rate': 0.00019947432058375653, 'epoch': 0.08}        \n",
      "{'loss': 2.0356, 'learning_rate': 0.00019947400158343794, 'epoch': 0.08}        \n",
      "{'loss': 2.2289, 'learning_rate': 0.00019947368248661378, 'epoch': 0.08}        \n",
      "{'loss': 1.8888, 'learning_rate': 0.00019947336329328438, 'epoch': 0.08}        \n",
      "{'loss': 1.5941, 'learning_rate': 0.00019947304400344995, 'epoch': 0.08}        \n",
      "{'loss': 1.9778, 'learning_rate': 0.00019947272461711085, 'epoch': 0.08}        \n",
      "{'loss': 2.1374, 'learning_rate': 0.00019947240513426746, 'epoch': 0.08}        \n",
      "{'loss': 2.058, 'learning_rate': 0.00019947208555492002, 'epoch': 0.08}         \n",
      "{'loss': 2.1668, 'learning_rate': 0.00019947176587906885, 'epoch': 0.08}        \n",
      "{'loss': 2.2215, 'learning_rate': 0.00019947144610671423, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 87109/1061708 [13:02:29<143:35:47,  1.89it/s][2024-03-01 07:09:29,928] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1019, 'learning_rate': 0.00019947115822908492, 'epoch': 0.08}        \n",
      "{'loss': 2.2018, 'learning_rate': 0.00019947083827337466, 'epoch': 0.08}        \n",
      "{'loss': 2.3645, 'learning_rate': 0.0001994705182211619, 'epoch': 0.08}         \n",
      "{'loss': 1.9972, 'learning_rate': 0.00019947019807244694, 'epoch': 0.08}        \n",
      "{'loss': 2.2084, 'learning_rate': 0.00019946987782723007, 'epoch': 0.08}        \n",
      "{'loss': 1.9794, 'learning_rate': 0.0001994695574855116, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 87163/1061708 [13:02:58<146:29:39,  1.85it/s][2024-03-01 07:09:58,628] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2308, 'learning_rate': 0.0001994692690954564, 'epoch': 0.08}         \n",
      "{'loss': 2.1945, 'learning_rate': 0.0001994689485703858, 'epoch': 0.08}         \n",
      "{'loss': 2.0476, 'learning_rate': 0.00019946862794881448, 'epoch': 0.08}        \n",
      "{'loss': 2.1807, 'learning_rate': 0.00019946830723074278, 'epoch': 0.08}        \n",
      "{'loss': 2.452, 'learning_rate': 0.00019946798641617107, 'epoch': 0.08}         \n",
      "{'loss': 1.8926, 'learning_rate': 0.00019946766550509956, 'epoch': 0.08}        \n",
      "{'loss': 2.4888, 'learning_rate': 0.00019946734449752864, 'epoch': 0.08}        \n",
      "{'loss': 1.9519, 'learning_rate': 0.00019946702339345858, 'epoch': 0.08}        \n",
      "{'loss': 2.16, 'learning_rate': 0.0001994667021928897, 'epoch': 0.08}           \n",
      "{'loss': 1.9174, 'learning_rate': 0.00019946638089582232, 'epoch': 0.08}        \n",
      "{'loss': 1.9587, 'learning_rate': 0.00019946605950225675, 'epoch': 0.08}        \n",
      "{'loss': 2.0057, 'learning_rate': 0.00019946573801219326, 'epoch': 0.08}        \n",
      "{'loss': 2.2323, 'learning_rate': 0.00019946541642563225, 'epoch': 0.08}        \n",
      "{'loss': 2.2347, 'learning_rate': 0.00019946509474257394, 'epoch': 0.08}        \n",
      "{'loss': 2.1277, 'learning_rate': 0.0001994647729630187, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 87313/1061708 [13:04:18<147:04:53,  1.84it/s][2024-03-01 07:11:18,587] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 87317/1061708 [13:04:20<144:49:21,  1.87it/s][2024-03-01 07:11:20,649] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9999, 'learning_rate': 0.0001994645154698969, 'epoch': 0.08}         \n",
      "{'loss': 2.1339, 'learning_rate': 0.00019946419351664796, 'epoch': 0.08}        \n",
      "{'loss': 2.2119, 'learning_rate': 0.0001994638714669029, 'epoch': 0.08}         \n",
      "{'loss': 1.8891, 'learning_rate': 0.00019946354932066213, 'epoch': 0.08}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.208, 'learning_rate': 0.0001994632270779259, 'epoch': 0.08}          \n",
      "{'loss': 2.6081, 'learning_rate': 0.00019946290473869452, 'epoch': 0.08}        \n",
      "{'loss': 2.1753, 'learning_rate': 0.00019946258230296832, 'epoch': 0.08}        \n",
      "{'loss': 2.1048, 'learning_rate': 0.0001994622597707476, 'epoch': 0.08}         \n",
      "{'loss': 2.2354, 'learning_rate': 0.0001994619371420327, 'epoch': 0.08}         \n",
      "{'loss': 2.2083, 'learning_rate': 0.00019946161441682393, 'epoch': 0.08}        \n",
      "{'loss': 1.9497, 'learning_rate': 0.0001994612915951216, 'epoch': 0.08}         \n",
      "{'loss': 2.0279, 'learning_rate': 0.00019946096867692596, 'epoch': 0.08}        \n",
      "{'loss': 2.4475, 'learning_rate': 0.0001994606456622374, 'epoch': 0.08}         \n",
      "{'loss': 2.029, 'learning_rate': 0.00019946032255105622, 'epoch': 0.08}         \n",
      "{'loss': 1.9968, 'learning_rate': 0.00019945999934338273, 'epoch': 0.08}        \n",
      "{'loss': 2.0652, 'learning_rate': 0.0001994596760392172, 'epoch': 0.08}         \n",
      "{'loss': 2.1121, 'learning_rate': 0.00019945935263856, 'epoch': 0.08}           \n",
      "  8%|██▎                         | 87482/1061708 [13:05:48<143:22:28,  1.89it/s][2024-03-01 07:12:48,594] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1509, 'learning_rate': 0.00019945906149546841, 'epoch': 0.08}        \n",
      "{'loss': 2.0123, 'learning_rate': 0.00019945873791147784, 'epoch': 0.08}        \n",
      "{'loss': 2.2369, 'learning_rate': 0.00019945841423099655, 'epoch': 0.08}        \n",
      "{'loss': 2.1049, 'learning_rate': 0.00019945809045402474, 'epoch': 0.08}        \n",
      "{'loss': 1.6771, 'learning_rate': 0.0001994577665805628, 'epoch': 0.08}         \n",
      "{'loss': 2.1423, 'learning_rate': 0.000199457442610611, 'epoch': 0.08}          \n",
      "  8%|██▎                         | 87546/1061708 [13:06:22<144:10:43,  1.88it/s][2024-03-01 07:13:22,658] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3013, 'learning_rate': 0.00019945715095515587, 'epoch': 0.08}        \n",
      "{'loss': 2.063, 'learning_rate': 0.00019945682680187426, 'epoch': 0.08}         \n",
      "{'loss': 2.0346, 'learning_rate': 0.00019945650255210376, 'epoch': 0.08}        \n",
      "{'loss': 1.9246, 'learning_rate': 0.00019945617820584462, 'epoch': 0.08}        \n",
      "{'loss': 2.2567, 'learning_rate': 0.00019945585376309716, 'epoch': 0.08}        \n",
      "{'loss': 2.5449, 'learning_rate': 0.00019945552922386175, 'epoch': 0.08}        \n",
      "{'loss': 2.2704, 'learning_rate': 0.00019945520458813866, 'epoch': 0.08}        \n",
      "{'loss': 1.9916, 'learning_rate': 0.00019945487985592825, 'epoch': 0.08}        \n",
      "{'loss': 2.1413, 'learning_rate': 0.00019945455502723075, 'epoch': 0.08}        \n",
      "{'loss': 2.1143, 'learning_rate': 0.00019945423010204655, 'epoch': 0.08}        \n",
      "{'loss': 2.111, 'learning_rate': 0.00019945390508037595, 'epoch': 0.08}         \n",
      "{'loss': 1.8307, 'learning_rate': 0.00019945357996221927, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 87669/1061708 [13:07:27<143:30:10,  1.89it/s][2024-03-01 07:14:28,127] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3438, 'learning_rate': 0.0001994532872733829, 'epoch': 0.08}         \n",
      "{'loss': 2.1666, 'learning_rate': 0.0001994529619719035, 'epoch': 0.08}         \n",
      "{'loss': 1.9883, 'learning_rate': 0.00019945263657393893, 'epoch': 0.08}        \n",
      "{'loss': 1.7945, 'learning_rate': 0.0001994523110794895, 'epoch': 0.08}         \n",
      "{'loss': 1.9204, 'learning_rate': 0.00019945198548855552, 'epoch': 0.08}        \n",
      "{'loss': 1.9111, 'learning_rate': 0.00019945165980113732, 'epoch': 0.08}        \n",
      "{'loss': 2.2831, 'learning_rate': 0.0001994513340172352, 'epoch': 0.08}         \n",
      "{'loss': 2.3065, 'learning_rate': 0.00019945100813684947, 'epoch': 0.08}        \n",
      "{'loss': 2.0517, 'learning_rate': 0.0001994506821599805, 'epoch': 0.08}         \n",
      "{'loss': 2.1061, 'learning_rate': 0.00019945035608662852, 'epoch': 0.08}        \n",
      "{'loss': 1.9496, 'learning_rate': 0.00019945002991679392, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 87770/1061708 [13:08:21<143:17:12,  1.89it/s][2024-03-01 07:15:21,887] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 87771/1061708 [13:08:21<134:31:24,  2.01it/s][2024-03-01 07:15:22,308] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1229, 'learning_rate': 0.00019944976891145895, 'epoch': 0.08}        \n",
      "{'loss': 2.3571, 'learning_rate': 0.00019944944256795636, 'epoch': 0.08}        \n",
      "{'loss': 2.209, 'learning_rate': 0.00019944911612797202, 'epoch': 0.08}         \n",
      "{'loss': 2.1543, 'learning_rate': 0.00019944878959150624, 'epoch': 0.08}        \n",
      "{'loss': 1.7234, 'learning_rate': 0.0001994484629585593, 'epoch': 0.08}         \n",
      "{'loss': 2.2761, 'learning_rate': 0.00019944813622913162, 'epoch': 0.08}        \n",
      "{'loss': 2.105, 'learning_rate': 0.00019944780940322335, 'epoch': 0.08}         \n",
      "{'loss': 1.8551, 'learning_rate': 0.000199447482480835, 'epoch': 0.08}          \n",
      "  8%|██▎                         | 87855/1061708 [13:09:06<144:54:18,  1.87it/s][2024-03-01 07:16:07,005] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7707, 'learning_rate': 0.00019944718816819514, 'epoch': 0.08}        \n",
      "{'loss': 1.6795, 'learning_rate': 0.00019944686106249527, 'epoch': 0.08}        \n",
      "{'loss': 2.0746, 'learning_rate': 0.00019944653386031616, 'epoch': 0.08}        \n",
      "{'loss': 2.3313, 'learning_rate': 0.00019944620656165812, 'epoch': 0.08}        \n",
      "{'loss': 1.9418, 'learning_rate': 0.00019944587916652143, 'epoch': 0.08}        \n",
      "{'loss': 2.4162, 'learning_rate': 0.00019944555167490644, 'epoch': 0.08}        \n",
      "{'loss': 2.1528, 'learning_rate': 0.00019944522408681348, 'epoch': 0.08}        \n",
      "{'loss': 1.9339, 'learning_rate': 0.00019944489640224284, 'epoch': 0.08}        \n",
      "{'loss': 1.8635, 'learning_rate': 0.00019944456862119484, 'epoch': 0.08}        \n",
      "{'loss': 2.1316, 'learning_rate': 0.0001994442407436698, 'epoch': 0.08}         \n",
      "{'loss': 2.3928, 'learning_rate': 0.00019944391276966808, 'epoch': 0.08}        \n",
      "{'loss': 1.9919, 'learning_rate': 0.00019944358469918997, 'epoch': 0.08}        \n",
      "{'loss': 2.0182, 'learning_rate': 0.00019944325653223572, 'epoch': 0.08}        \n",
      "{'loss': 2.5336, 'learning_rate': 0.00019944292826880574, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 87999/1061708 [13:10:23<143:23:28,  1.89it/s][2024-03-01 07:17:23,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=88000, skipped=1000, lr=[0.00019944259990890034], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 07:17:23,787] [INFO] [timer.py:260:stop] epoch=0/micro_step=88000/global_step=88000, RunningAvgSamplesPerSec=1.8923312721333019, CurrSamplesPerSec=1.9046681422158123, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1783, 'learning_rate': 0.00019944259990890034, 'epoch': 0.08}        \n",
      "{'loss': 2.5006, 'learning_rate': 0.00019944227145251978, 'epoch': 0.08}        \n",
      "{'loss': 1.7187, 'learning_rate': 0.00019944194289966448, 'epoch': 0.08}        \n",
      "{'loss': 1.8612, 'learning_rate': 0.00019944161425033464, 'epoch': 0.08}        \n",
      "{'loss': 2.0039, 'learning_rate': 0.00019944128550453062, 'epoch': 0.08}        \n",
      "{'loss': 2.1232, 'learning_rate': 0.00019944095666225278, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88056/1061708 [13:10:53<144:17:31,  1.87it/s][2024-03-01 07:17:54,069] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 88057/1061708 [13:10:53<135:11:52,  2.00it/s][2024-03-01 07:17:54,490] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0874, 'learning_rate': 0.00019944069351896955, 'epoch': 0.08}        \n",
      "{'loss': 1.9811, 'learning_rate': 0.00019944036450303957, 'epoch': 0.08}        \n",
      "{'loss': 2.0731, 'learning_rate': 0.00019944003539063666, 'epoch': 0.08}        \n",
      "{'loss': 1.8402, 'learning_rate': 0.00019943970618176108, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88095/1061708 [13:11:14<145:04:06,  1.86it/s][2024-03-01 07:18:14,651] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0927, 'learning_rate': 0.00019943940981128925, 'epoch': 0.08}        \n",
      "{'loss': 2.5416, 'learning_rate': 0.00019943908041911658, 'epoch': 0.08}        \n",
      "{'loss': 2.2515, 'learning_rate': 0.00019943875093047218, 'epoch': 0.08}        \n",
      "{'loss': 2.2673, 'learning_rate': 0.0001994384213453564, 'epoch': 0.08}         \n",
      "{'loss': 2.1154, 'learning_rate': 0.00019943809166376954, 'epoch': 0.08}        \n",
      "{'loss': 1.9528, 'learning_rate': 0.00019943776188571194, 'epoch': 0.08}        \n",
      "{'loss': 2.71, 'learning_rate': 0.0001994374320111839, 'epoch': 0.08}           \n",
      "{'loss': 1.8771, 'learning_rate': 0.00019943710204018577, 'epoch': 0.08}        \n",
      "{'loss': 1.9014, 'learning_rate': 0.00019943677197271784, 'epoch': 0.08}        \n",
      "{'loss': 2.065, 'learning_rate': 0.00019943644180878044, 'epoch': 0.08}         \n",
      "{'loss': 1.8633, 'learning_rate': 0.00019943611154837388, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88204/1061708 [13:12:12<145:23:30,  1.86it/s][2024-03-01 07:19:12,747] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1123, 'learning_rate': 0.00019943581423152712, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88211/1061708 [13:12:15<143:34:21,  1.88it/s][2024-03-01 07:19:16,396] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2567, 'learning_rate': 0.00019943551683654088, 'epoch': 0.08}        \n",
      "{'loss': 2.0113, 'learning_rate': 0.0001994351863060224, 'epoch': 0.08}         \n",
      "{'loss': 2.0125, 'learning_rate': 0.00019943485567903602, 'epoch': 0.08}        \n",
      "{'loss': 2.2961, 'learning_rate': 0.000199434524955582, 'epoch': 0.08}          \n",
      "{'loss': 1.9855, 'learning_rate': 0.00019943419413566074, 'epoch': 0.08}        \n",
      "{'loss': 2.334, 'learning_rate': 0.00019943386321927246, 'epoch': 0.08}         \n",
      "{'loss': 2.2582, 'learning_rate': 0.00019943353220641753, 'epoch': 0.08}        \n",
      "{'loss': 2.254, 'learning_rate': 0.00019943320109709633, 'epoch': 0.08}         \n",
      "{'loss': 2.2588, 'learning_rate': 0.0001994328698913091, 'epoch': 0.08}         \n",
      "{'loss': 2.0044, 'learning_rate': 0.00019943253858905618, 'epoch': 0.08}        \n",
      "{'loss': 2.2535, 'learning_rate': 0.0001994322071903379, 'epoch': 0.08}         \n",
      "{'loss': 1.7146, 'learning_rate': 0.00019943187569515457, 'epoch': 0.08}        \n",
      "{'loss': 2.121, 'learning_rate': 0.0001994315441035065, 'epoch': 0.08}          \n",
      "{'loss': 2.1191, 'learning_rate': 0.00019943121241539405, 'epoch': 0.08}        \n",
      "{'loss': 2.3766, 'learning_rate': 0.00019943088063081757, 'epoch': 0.08}        \n",
      "{'loss': 2.2175, 'learning_rate': 0.00019943054874977726, 'epoch': 0.08}        \n",
      "{'loss': 2.249, 'learning_rate': 0.00019943021677227359, 'epoch': 0.08}         \n",
      "{'loss': 2.1815, 'learning_rate': 0.00019942988469830676, 'epoch': 0.08}        \n",
      "{'loss': 2.1626, 'learning_rate': 0.00019942955252787717, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88408/1061708 [13:14:00<143:33:28,  1.88it/s][2024-03-01 07:21:01,334] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.7323, 'learning_rate': 0.0001994292534920151, 'epoch': 0.08}         \n",
      "{'loss': 2.2703, 'learning_rate': 0.0001994289211383071, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 88422/1061708 [13:14:08<143:37:27,  1.88it/s][2024-03-01 07:21:08,742] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7624, 'learning_rate': 0.000199428621937495, 'epoch': 0.08}          \n",
      "{'loss': 1.8404, 'learning_rate': 0.00019942828940050974, 'epoch': 0.08}        \n",
      "{'loss': 1.8803, 'learning_rate': 0.00019942795676706324, 'epoch': 0.08}        \n",
      "{'loss': 2.3371, 'learning_rate': 0.00019942762403715584, 'epoch': 0.08}        \n",
      "{'loss': 2.0573, 'learning_rate': 0.00019942729121078784, 'epoch': 0.08}        \n",
      "{'loss': 2.1147, 'learning_rate': 0.00019942695828795956, 'epoch': 0.08}        \n",
      "{'loss': 1.8658, 'learning_rate': 0.00019942662526867136, 'epoch': 0.08}        \n",
      "{'loss': 2.3064, 'learning_rate': 0.0001994262921529235, 'epoch': 0.08}         \n",
      "{'loss': 1.8416, 'learning_rate': 0.00019942595894071637, 'epoch': 0.08}        \n",
      "{'loss': 2.1672, 'learning_rate': 0.00019942562563205022, 'epoch': 0.08}        \n",
      "{'loss': 1.6885, 'learning_rate': 0.00019942529222692542, 'epoch': 0.08}        \n",
      "{'loss': 2.4854, 'learning_rate': 0.00019942495872534233, 'epoch': 0.08}        \n",
      "{'loss': 1.7398, 'learning_rate': 0.00019942462512730118, 'epoch': 0.08}        \n",
      "{'loss': 1.7721, 'learning_rate': 0.0001994242914328024, 'epoch': 0.08}         \n",
      "{'loss': 2.2765, 'learning_rate': 0.0001994239576418462, 'epoch': 0.08}         \n",
      "{'loss': 2.281, 'learning_rate': 0.000199423623754433, 'epoch': 0.08}           \n",
      "{'loss': 2.3067, 'learning_rate': 0.00019942328977056307, 'epoch': 0.08}        \n",
      "{'loss': 1.8241, 'learning_rate': 0.00019942295569023676, 'epoch': 0.08}        \n",
      "{'loss': 2.0571, 'learning_rate': 0.00019942262151345438, 'epoch': 0.08}        \n",
      "{'loss': 2.316, 'learning_rate': 0.00019942228724021627, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 88623/1061708 [13:15:55<147:33:41,  1.83it/s][2024-03-01 07:22:56,087] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 88624/1061708 [13:15:55<137:31:57,  1.97it/s][2024-03-01 07:22:56,509] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  8%|██▎                         | 88626/1061708 [13:15:56<134:24:01,  2.01it/s][2024-03-01 07:22:57,468] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8204, 'learning_rate': 0.00019942205319155857, 'epoch': 0.08}        \n",
      "{'loss': 2.2057, 'learning_rate': 0.00019942171875434646, 'epoch': 0.08}        \n",
      "{'loss': 2.0345, 'learning_rate': 0.00019942138422067944, 'epoch': 0.08}        \n",
      "{'loss': 1.7694, 'learning_rate': 0.00019942104959055792, 'epoch': 0.08}        \n",
      "{'loss': 1.8579, 'learning_rate': 0.00019942071486398218, 'epoch': 0.08}        \n",
      "{'loss': 1.9111, 'learning_rate': 0.00019942038004095252, 'epoch': 0.08}        \n",
      "{'loss': 2.2184, 'learning_rate': 0.0001994200451214693, 'epoch': 0.08}         \n",
      "{'loss': 2.1216, 'learning_rate': 0.00019941971010553283, 'epoch': 0.08}        \n",
      "{'loss': 2.3523, 'learning_rate': 0.00019941937499314347, 'epoch': 0.08}        \n",
      "{'loss': 1.7511, 'learning_rate': 0.00019941903978430153, 'epoch': 0.08}        \n",
      "{'loss': 1.986, 'learning_rate': 0.00019941870447900728, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 88730/1061708 [13:16:52<143:29:53,  1.88it/s][2024-03-01 07:23:52,906] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.5845, 'learning_rate': 0.00019941840262177607, 'epoch': 0.08}        \n",
      "{'loss': 2.2336, 'learning_rate': 0.0001994180671332234, 'epoch': 0.08}         \n",
      "{'loss': 1.8654, 'learning_rate': 0.00019941773154821945, 'epoch': 0.08}        \n",
      "{'loss': 2.119, 'learning_rate': 0.0001994173958667645, 'epoch': 0.08}          \n",
      "  8%|██▎                         | 88775/1061708 [13:17:16<145:13:10,  1.86it/s][2024-03-01 07:24:16,865] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9172, 'learning_rate': 0.00019941709367098968, 'epoch': 0.08}        \n",
      "{'loss': 1.6549, 'learning_rate': 0.00019941675780627875, 'epoch': 0.08}        \n",
      "{'loss': 2.1437, 'learning_rate': 0.00019941642184511773, 'epoch': 0.08}        \n",
      "{'loss': 2.2328, 'learning_rate': 0.000199416085787507, 'epoch': 0.08}          \n",
      "{'loss': 2.0929, 'learning_rate': 0.00019941574963344686, 'epoch': 0.08}        \n",
      "{'loss': 2.2544, 'learning_rate': 0.00019941541338293765, 'epoch': 0.08}        \n",
      "{'loss': 2.1331, 'learning_rate': 0.00019941507703597971, 'epoch': 0.08}        \n",
      "{'loss': 1.8892, 'learning_rate': 0.00019941474059257332, 'epoch': 0.08}        \n",
      "{'loss': 2.0862, 'learning_rate': 0.00019941440405271885, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 88866/1061708 [13:18:04<144:06:38,  1.88it/s][2024-03-01 07:25:05,382] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.1514, 'learning_rate': 0.00019941410108438698, 'epoch': 0.08}        \n",
      "{'loss': 1.9996, 'learning_rate': 0.00019941376436128204, 'epoch': 0.08}        \n",
      "{'loss': 2.5367, 'learning_rate': 0.00019941342754172992, 'epoch': 0.08}        \n",
      "{'loss': 1.9265, 'learning_rate': 0.000199413090625731, 'epoch': 0.08}          \n",
      "{'loss': 1.9464, 'learning_rate': 0.00019941275361328557, 'epoch': 0.08}        \n",
      "{'loss': 2.2872, 'learning_rate': 0.00019941241650439398, 'epoch': 0.08}        \n",
      "{'loss': 1.749, 'learning_rate': 0.00019941207929905655, 'epoch': 0.08}         \n",
      "{'loss': 2.5408, 'learning_rate': 0.0001994117419972736, 'epoch': 0.08}         \n",
      "{'loss': 2.4018, 'learning_rate': 0.0001994114045990455, 'epoch': 0.08}         \n",
      "{'loss': 2.1822, 'learning_rate': 0.0001994110671043725, 'epoch': 0.08}         \n",
      "{'loss': 2.0893, 'learning_rate': 0.00019941072951325497, 'epoch': 0.08}        \n",
      "{'loss': 2.0843, 'learning_rate': 0.00019941039182569325, 'epoch': 0.08}        \n",
      "{'loss': 1.9924, 'learning_rate': 0.00019941005404168767, 'epoch': 0.08}        \n",
      "{'loss': 2.0304, 'learning_rate': 0.0001994097161612385, 'epoch': 0.08}         \n",
      "{'loss': 2.4521, 'learning_rate': 0.00019940937818434615, 'epoch': 0.08}        \n",
      "{'loss': 2.1571, 'learning_rate': 0.00019940904011101087, 'epoch': 0.08}        \n",
      "{'loss': 2.5253, 'learning_rate': 0.00019940870194123304, 'epoch': 0.08}        \n",
      "{'loss': 2.1767, 'learning_rate': 0.000199408363675013, 'epoch': 0.08}          \n",
      "{'loss': 2.0853, 'learning_rate': 0.00019940802531235102, 'epoch': 0.08}        \n",
      "{'loss': 1.8778, 'learning_rate': 0.00019940768685324748, 'epoch': 0.08}        \n",
      "{'loss': 2.4202, 'learning_rate': 0.00019940734829770266, 'epoch': 0.08}        \n",
      "{'loss': 1.8327, 'learning_rate': 0.00019940700964571692, 'epoch': 0.08}        \n",
      "{'loss': 2.2994, 'learning_rate': 0.0001994066708972906, 'epoch': 0.08}         \n",
      "{'loss': 1.7532, 'learning_rate': 0.00019940633205242403, 'epoch': 0.08}        \n",
      "{'loss': 2.2715, 'learning_rate': 0.0001994059931111175, 'epoch': 0.08}         \n",
      "{'loss': 2.2873, 'learning_rate': 0.00019940565407337136, 'epoch': 0.08}        \n",
      "{'loss': 2.0205, 'learning_rate': 0.00019940531493918594, 'epoch': 0.08}        \n",
      "{'loss': 2.3674, 'learning_rate': 0.00019940497570856158, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 89145/1061708 [13:20:33<144:29:57,  1.87it/s][2024-03-01 07:27:34,223] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.576, 'learning_rate': 0.00019940467031854463, 'epoch': 0.08}         \n",
      "{'loss': 2.2777, 'learning_rate': 0.00019940433090468715, 'epoch': 0.08}        \n",
      "{'loss': 1.9657, 'learning_rate': 0.0001994039913943917, 'epoch': 0.08}         \n",
      "{'loss': 2.2124, 'learning_rate': 0.00019940365178765856, 'epoch': 0.08}        \n",
      "{'loss': 2.1777, 'learning_rate': 0.00019940331208448807, 'epoch': 0.08}        \n",
      "{'loss': 2.0151, 'learning_rate': 0.00019940297228488062, 'epoch': 0.08}        \n",
      "{'loss': 2.3502, 'learning_rate': 0.00019940263238883645, 'epoch': 0.08}        \n",
      "{'loss': 2.0677, 'learning_rate': 0.00019940229239635596, 'epoch': 0.08}        \n",
      "{'loss': 1.7865, 'learning_rate': 0.00019940195230743943, 'epoch': 0.08}        \n",
      "{'loss': 1.8514, 'learning_rate': 0.00019940161212208724, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 89246/1061708 [13:21:27<144:01:37,  1.88it/s][2024-03-01 07:28:27,952] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 89247/1061708 [13:21:27<135:02:06,  2.00it/s][2024-03-01 07:28:28,378] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0928, 'learning_rate': 0.000199401339904372, 'epoch': 0.08}          \n",
      "{'loss': 1.6882, 'learning_rate': 0.00019940099954543635, 'epoch': 0.08}        \n",
      "{'loss': 2.4244, 'learning_rate': 0.000199400659090066, 'epoch': 0.08}          \n",
      "{'loss': 1.883, 'learning_rate': 0.00019940031853826117, 'epoch': 0.08}         \n",
      "{'loss': 2.1427, 'learning_rate': 0.00019939997789002227, 'epoch': 0.08}        \n",
      "{'loss': 2.2371, 'learning_rate': 0.00019939963714534956, 'epoch': 0.08}        \n",
      "{'loss': 2.0736, 'learning_rate': 0.00019939929630424344, 'epoch': 0.08}        \n",
      "{'loss': 1.7481, 'learning_rate': 0.0001993989553667042, 'epoch': 0.08}         \n",
      "{'loss': 2.165, 'learning_rate': 0.00019939861433273216, 'epoch': 0.08}         \n",
      "{'loss': 2.1756, 'learning_rate': 0.0001993982732023277, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 89348/1061708 [13:22:21<143:20:16,  1.88it/s][2024-03-01 07:29:22,133] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 89349/1061708 [13:22:22<134:31:27,  2.01it/s][2024-03-01 07:29:22,556] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3245, 'learning_rate': 0.00019939800022857296, 'epoch': 0.08}        \n",
      "{'loss': 2.2229, 'learning_rate': 0.0001993976589245909, 'epoch': 0.08}         \n",
      "{'loss': 1.9894, 'learning_rate': 0.0001993973175241773, 'epoch': 0.08}         \n",
      "{'loss': 2.291, 'learning_rate': 0.00019939697602733256, 'epoch': 0.08}         \n",
      "{'loss': 1.9176, 'learning_rate': 0.00019939663443405695, 'epoch': 0.08}        \n",
      "{'loss': 1.9745, 'learning_rate': 0.00019939629274435077, 'epoch': 0.08}        \n",
      "{'loss': 1.7857, 'learning_rate': 0.0001993959509582144, 'epoch': 0.08}         \n",
      "{'loss': 2.1709, 'learning_rate': 0.00019939560907564818, 'epoch': 0.08}        \n",
      "{'loss': 2.054, 'learning_rate': 0.00019939526709665244, 'epoch': 0.08}         \n",
      "{'loss': 1.9399, 'learning_rate': 0.00019939492502122749, 'epoch': 0.08}        \n",
      "{'loss': 2.2057, 'learning_rate': 0.00019939458284937365, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 89450/1061708 [13:23:15<143:07:13,  1.89it/s][2024-03-01 07:30:16,300] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 89451/1061708 [13:23:16<134:29:48,  2.01it/s][2024-03-01 07:30:16,723] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7273, 'learning_rate': 0.000199394309042462, 'epoch': 0.08}          \n",
      "{'loss': 2.1335, 'learning_rate': 0.00019939396669703703, 'epoch': 0.08}        \n",
      "{'loss': 2.2426, 'learning_rate': 0.00019939362425518413, 'epoch': 0.08}        \n",
      "{'loss': 1.8944, 'learning_rate': 0.00019939328171690362, 'epoch': 0.08}        \n",
      "{'loss': 2.5185, 'learning_rate': 0.00019939293908219583, 'epoch': 0.08}        \n",
      "{'loss': 1.8559, 'learning_rate': 0.0001993925963510611, 'epoch': 0.08}         \n",
      "{'loss': 2.3554, 'learning_rate': 0.00019939225352349972, 'epoch': 0.08}        \n",
      "{'loss': 2.2989, 'learning_rate': 0.00019939191059951212, 'epoch': 0.08}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.143, 'learning_rate': 0.00019939156757909854, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 89543/1061708 [13:24:05<146:27:14,  1.84it/s][2024-03-01 07:31:05,753] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8776, 'learning_rate': 0.00019939125877828243, 'epoch': 0.08}        \n",
      "{'loss': 2.2173, 'learning_rate': 0.00019939091557466044, 'epoch': 0.08}        \n",
      "{'loss': 2.161, 'learning_rate': 0.00019939057227461352, 'epoch': 0.08}         \n",
      "{'loss': 1.9281, 'learning_rate': 0.00019939022887814194, 'epoch': 0.08}        \n",
      "{'loss': 2.2568, 'learning_rate': 0.00019938988538524606, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 89599/1061708 [13:24:35<143:12:10,  1.89it/s][2024-03-01 07:31:35,557] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3517, 'learning_rate': 0.0001993895761591972, 'epoch': 0.08}         \n",
      "{'loss': 2.0159, 'learning_rate': 0.00019938923248309602, 'epoch': 0.08}        \n",
      "{'loss': 1.9236, 'learning_rate': 0.00019938888871057152, 'epoch': 0.08}        \n",
      "{'loss': 2.0841, 'learning_rate': 0.00019938854484162393, 'epoch': 0.08}        \n",
      "{'loss': 2.0618, 'learning_rate': 0.00019938820087625375, 'epoch': 0.08}        \n",
      "{'loss': 2.0608, 'learning_rate': 0.00019938785681446116, 'epoch': 0.08}        \n",
      "{'loss': 2.1534, 'learning_rate': 0.0001993875126562466, 'epoch': 0.08}         \n",
      "{'loss': 1.7537, 'learning_rate': 0.00019938716840161032, 'epoch': 0.08}        \n",
      "{'loss': 1.8322, 'learning_rate': 0.00019938682405055272, 'epoch': 0.08}        \n",
      "{'loss': 1.5141, 'learning_rate': 0.00019938647960307407, 'epoch': 0.08}        \n",
      "{'loss': 2.3386, 'learning_rate': 0.00019938613505917477, 'epoch': 0.08}        \n",
      "{'loss': 2.3838, 'learning_rate': 0.00019938579041885515, 'epoch': 0.08}        \n",
      "{'loss': 2.0127, 'learning_rate': 0.00019938544568211548, 'epoch': 0.08}        \n",
      "{'loss': 1.8951, 'learning_rate': 0.00019938510084895614, 'epoch': 0.08}        \n",
      "{'loss': 2.1335, 'learning_rate': 0.00019938475591937745, 'epoch': 0.08}        \n",
      "{'loss': 1.9114, 'learning_rate': 0.0001993844108933798, 'epoch': 0.08}         \n",
      "{'loss': 2.0977, 'learning_rate': 0.00019938406577096345, 'epoch': 0.08}        \n",
      "{'loss': 1.8004, 'learning_rate': 0.00019938372055212879, 'epoch': 0.08}        \n",
      "{'loss': 2.1606, 'learning_rate': 0.0001993833752368761, 'epoch': 0.08}         \n",
      "{'loss': 1.9714, 'learning_rate': 0.00019938302982520573, 'epoch': 0.08}        \n",
      "{'loss': 2.3473, 'learning_rate': 0.0001993826843171181, 'epoch': 0.08}         \n",
      "{'loss': 1.8918, 'learning_rate': 0.0001993823387126134, 'epoch': 0.08}         \n",
      "{'loss': 2.1464, 'learning_rate': 0.00019938199301169207, 'epoch': 0.08}        \n",
      "{'loss': 2.0416, 'learning_rate': 0.00019938164721435442, 'epoch': 0.08}        \n",
      "{'loss': 2.1467, 'learning_rate': 0.00019938130132060076, 'epoch': 0.08}        \n",
      "{'loss': 2.4464, 'learning_rate': 0.00019938095533043147, 'epoch': 0.08}        \n",
      "{'loss': 2.2756, 'learning_rate': 0.00019938060924384684, 'epoch': 0.08}        \n",
      "{'loss': 1.8212, 'learning_rate': 0.00019938026306084723, 'epoch': 0.08}        \n",
      "{'loss': 2.0232, 'learning_rate': 0.00019937991678143299, 'epoch': 0.08}        \n",
      "{'loss': 2.2033, 'learning_rate': 0.0001993795704056044, 'epoch': 0.08}         \n",
      "{'loss': 2.2951, 'learning_rate': 0.0001993792239333619, 'epoch': 0.08}         \n",
      "  8%|██▎                         | 89900/1061708 [13:27:15<143:09:00,  1.89it/s][2024-03-01 07:34:16,050] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  8%|██▎                         | 89901/1061708 [13:27:15<134:27:07,  2.01it/s][2024-03-01 07:34:16,473] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  8%|██▎                         | 89908/1061708 [13:27:19<142:15:35,  1.90it/s][2024-03-01 07:34:20,117] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2836, 'learning_rate': 0.00019937898134542596, 'epoch': 0.08}        \n",
      "{'loss': 1.9636, 'learning_rate': 0.00019937863470928044, 'epoch': 0.08}        \n",
      "{'loss': 2.1297, 'learning_rate': 0.00019937828797672187, 'epoch': 0.08}        \n",
      "{'loss': 1.8032, 'learning_rate': 0.00019937794114775054, 'epoch': 0.08}        \n",
      "{'loss': 1.8235, 'learning_rate': 0.00019937759422236683, 'epoch': 0.08}        \n",
      "{'loss': 2.2902, 'learning_rate': 0.00019937724720057106, 'epoch': 0.08}        \n",
      "{'loss': 2.012, 'learning_rate': 0.0001993769000823636, 'epoch': 0.08}          \n",
      "{'loss': 2.291, 'learning_rate': 0.0001993765528677447, 'epoch': 0.08}          \n",
      "{'loss': 2.0941, 'learning_rate': 0.00019937620555671477, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 89999/1061708 [13:28:08<143:05:55,  1.89it/s][2024-03-01 07:35:08,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=90000, skipped=1025, lr=[0.00019937585814927414], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 07:35:08,688] [INFO] [timer.py:260:stop] epoch=0/micro_step=90000/global_step=90000, RunningAvgSamplesPerSec=1.8923609693587466, CurrSamplesPerSec=1.902800007258606, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.2549, 'learning_rate': 0.00019937585814927414, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 90000/1061708 [13:28:08<143:06:13,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 07:35:08,691 >> Saving model checkpoint to output_model/checkpoint-90000\n",
      "[INFO|trainer.py:2880] 2024-03-01 07:35:08,694 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 07:35:09,894 >> tokenizer config file saved in output_model/checkpoint-90000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 07:35:09,894 >> Special tokens file saved in output_model/checkpoint-90000/special_tokens_map.json\n",
      "[2024-03-01 07:35:09,896] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step90000 is about to be saved!\n",
      "[2024-03-01 07:35:15,119] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-90000/global_step90000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 07:35:15,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-90000/global_step90000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 07:35:28,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-90000/global_step90000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 07:35:29,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-90000/global_step90000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 07:35:36,804] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-90000/global_step90000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 07:35:36,804] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-90000/global_step90000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 07:35:36,804] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step90000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 07:35:36,883 >> Deleting older checkpoint [output_model/checkpoint-75000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 07:35:40,639 >> tokenizer config file saved in output_model/checkpoint-90000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 07:35:40,639 >> Special tokens file saved in output_model/checkpoint-90000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.2769, 'learning_rate': 0.00019937551064542314, 'epoch': 0.08}        \n",
      "{'loss': 1.8154, 'learning_rate': 0.0001993751630451621, 'epoch': 0.08}         \n",
      "{'loss': 2.3427, 'learning_rate': 0.00019937481534849135, 'epoch': 0.08}        \n",
      "  8%|██▎                         | 90038/1061708 [13:29:00<142:00:33,  1.90it/s][2024-03-01 07:36:01,111] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0035, 'learning_rate': 0.00019937450233905769, 'epoch': 0.08}        \n",
      "{'loss': 1.9499, 'learning_rate': 0.00019937415445920944, 'epoch': 0.08}        \n",
      "{'loss': 2.3083, 'learning_rate': 0.00019937380648295247, 'epoch': 0.08}        \n",
      "{'loss': 2.3707, 'learning_rate': 0.00019937345841028714, 'epoch': 0.08}        \n",
      "{'loss': 2.3373, 'learning_rate': 0.00019937311024121376, 'epoch': 0.08}        \n",
      "{'loss': 1.859, 'learning_rate': 0.00019937276197573266, 'epoch': 0.08}         \n",
      "{'loss': 2.0337, 'learning_rate': 0.00019937241361384417, 'epoch': 0.08}        \n",
      "{'loss': 1.9305, 'learning_rate': 0.0001993720651555487, 'epoch': 0.08}         \n",
      "{'loss': 2.2591, 'learning_rate': 0.0001993717166008465, 'epoch': 0.08}         \n",
      "  8%|██▍                         | 90122/1061708 [13:29:45<142:45:05,  1.89it/s][2024-03-01 07:36:45,639] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9563, 'learning_rate': 0.0001993714028191871, 'epoch': 0.08}         \n",
      "{'loss': 2.1883, 'learning_rate': 0.00019937105408131313, 'epoch': 0.08}        \n",
      "{'loss': 1.8279, 'learning_rate': 0.00019937070524703344, 'epoch': 0.08}        \n",
      "{'loss': 1.9558, 'learning_rate': 0.00019937035631634837, 'epoch': 0.08}        \n",
      "{'loss': 1.9344, 'learning_rate': 0.00019937000728925826, 'epoch': 0.08}        \n",
      "{'loss': 2.313, 'learning_rate': 0.0001993696581657635, 'epoch': 0.08}          \n",
      "{'loss': 2.2923, 'learning_rate': 0.00019936930894586433, 'epoch': 0.08}        \n",
      "{'loss': 2.2535, 'learning_rate': 0.00019936895962956118, 'epoch': 0.08}        \n",
      "{'loss': 1.9558, 'learning_rate': 0.00019936861021685432, 'epoch': 0.08}        \n",
      "{'loss': 1.7601, 'learning_rate': 0.00019936826070774414, 'epoch': 0.08}        \n",
      "{'loss': 2.4553, 'learning_rate': 0.00019936791110223095, 'epoch': 0.08}        \n",
      "{'loss': 2.0985, 'learning_rate': 0.00019936756140031506, 'epoch': 0.08}        \n",
      "{'loss': 1.9074, 'learning_rate': 0.0001993672116019969, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 90250/1061708 [13:30:53<143:03:06,  1.89it/s][2024-03-01 07:37:53,826] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.117, 'learning_rate': 0.0001993668967010868, 'epoch': 0.09}          \n",
      "{'loss': 1.9523, 'learning_rate': 0.00019936654671960516, 'epoch': 0.09}        \n",
      "{'loss': 2.2131, 'learning_rate': 0.00019936619664172217, 'epoch': 0.09}        \n",
      "{'loss': 2.1657, 'learning_rate': 0.00019936584646743817, 'epoch': 0.09}        \n",
      "{'loss': 2.3462, 'learning_rate': 0.00019936549619675348, 'epoch': 0.09}        \n",
      "{'loss': 2.1384, 'learning_rate': 0.0001993651458296685, 'epoch': 0.09}         \n",
      "{'loss': 2.2216, 'learning_rate': 0.00019936479536618356, 'epoch': 0.09}        \n",
      "{'loss': 2.073, 'learning_rate': 0.00019936444480629894, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 90338/1061708 [13:31:40<143:12:27,  1.88it/s][2024-03-01 07:38:40,613] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1085, 'learning_rate': 0.0001993641292199814, 'epoch': 0.09}         \n",
      "{'loss': 1.9766, 'learning_rate': 0.00019936377847693838, 'epoch': 0.09}        \n",
      "{'loss': 2.1701, 'learning_rate': 0.00019936342763749677, 'epoch': 0.09}        \n",
      "{'loss': 1.6932, 'learning_rate': 0.0001993630767016568, 'epoch': 0.09}         \n",
      "{'loss': 2.2129, 'learning_rate': 0.0001993627256694189, 'epoch': 0.09}         \n",
      "{'loss': 1.8646, 'learning_rate': 0.00019936237454078334, 'epoch': 0.09}        \n",
      "{'loss': 2.3227, 'learning_rate': 0.0001993620233157505, 'epoch': 0.09}         \n",
      "{'loss': 2.0633, 'learning_rate': 0.00019936167199432068, 'epoch': 0.09}        \n",
      "{'loss': 2.0452, 'learning_rate': 0.00019936132057649428, 'epoch': 0.09}        \n",
      "{'loss': 2.1349, 'learning_rate': 0.0001993609690622716, 'epoch': 0.09}         \n",
      "{'loss': 2.0366, 'learning_rate': 0.000199360617451653, 'epoch': 0.09}          \n",
      "  9%|██▍                         | 90446/1061708 [13:32:37<143:32:47,  1.88it/s][2024-03-01 07:39:38,004] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3038, 'learning_rate': 0.000199360300919678, 'epoch': 0.09}          \n",
      "{'loss': 2.361, 'learning_rate': 0.00019935994912590808, 'epoch': 0.09}         \n",
      "{'loss': 2.0812, 'learning_rate': 0.0001993595972357432, 'epoch': 0.09}         \n",
      "{'loss': 2.1227, 'learning_rate': 0.00019935924524918373, 'epoch': 0.09}        \n",
      "{'loss': 2.0537, 'learning_rate': 0.00019935889316623, 'epoch': 0.09}           \n",
      "{'loss': 1.7886, 'learning_rate': 0.00019935854098688236, 'epoch': 0.09}        \n",
      "{'loss': 2.0117, 'learning_rate': 0.00019935818871114113, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 90514/1061708 [13:33:13<144:56:53,  1.86it/s][2024-03-01 07:40:14,141] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0726, 'learning_rate': 0.00019935787158055783, 'epoch': 0.09}        \n",
      "{'loss': 2.1827, 'learning_rate': 0.00019935751912166977, 'epoch': 0.09}        \n",
      "{'loss': 1.897, 'learning_rate': 0.00019935716656638915, 'epoch': 0.09}         \n",
      "{'loss': 2.0844, 'learning_rate': 0.00019935681391471626, 'epoch': 0.09}        \n",
      "{'loss': 1.9121, 'learning_rate': 0.00019935646116665148, 'epoch': 0.09}        \n",
      "{'loss': 2.5662, 'learning_rate': 0.00019935610832219513, 'epoch': 0.09}        \n",
      "{'loss': 2.0393, 'learning_rate': 0.0001993557553813476, 'epoch': 0.09}         \n",
      "{'loss': 1.7392, 'learning_rate': 0.0001993554023441092, 'epoch': 0.09}         \n",
      "{'loss': 2.1342, 'learning_rate': 0.00019935504921048025, 'epoch': 0.09}        \n",
      "{'loss': 1.978, 'learning_rate': 0.00019935469598046112, 'epoch': 0.09}         \n",
      "{'loss': 2.1029, 'learning_rate': 0.00019935434265405216, 'epoch': 0.09}        \n",
      "{'loss': 2.4389, 'learning_rate': 0.00019935398923125367, 'epoch': 0.09}        \n",
      "{'loss': 1.8062, 'learning_rate': 0.00019935363571206604, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 90641/1061708 [13:34:21<142:38:26,  1.89it/s][2024-03-01 07:41:21,659] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2593, 'learning_rate': 0.00019935331746238474, 'epoch': 0.09}        \n",
      "{'loss': 2.6008, 'learning_rate': 0.00019935296376005868, 'epoch': 0.09}        \n",
      "{'loss': 1.9737, 'learning_rate': 0.00019935260996134443, 'epoch': 0.09}        \n",
      "{'loss': 2.0566, 'learning_rate': 0.00019935225606624237, 'epoch': 0.09}        \n",
      "{'loss': 2.0415, 'learning_rate': 0.00019935190207475282, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 90691/1061708 [13:34:47<142:23:35,  1.89it/s][2024-03-01 07:41:48,166] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0198, 'learning_rate': 0.00019935158340000125, 'epoch': 0.09}        \n",
      "{'loss': 2.0845, 'learning_rate': 0.00019935122922537643, 'epoch': 0.09}        \n",
      "{'loss': 2.3119, 'learning_rate': 0.00019935087495436517, 'epoch': 0.09}        \n",
      "{'loss': 1.8573, 'learning_rate': 0.00019935052058696776, 'epoch': 0.09}        \n",
      "{'loss': 2.23, 'learning_rate': 0.00019935016612318457, 'epoch': 0.09}          \n",
      "{'loss': 2.0581, 'learning_rate': 0.0001993498115630159, 'epoch': 0.09}         \n",
      "{'loss': 2.1422, 'learning_rate': 0.00019934945690646216, 'epoch': 0.09}        \n",
      "{'loss': 2.2375, 'learning_rate': 0.00019934910215352366, 'epoch': 0.09}        \n",
      "{'loss': 1.7495, 'learning_rate': 0.00019934874730420072, 'epoch': 0.09}        \n",
      "{'loss': 2.172, 'learning_rate': 0.00019934839235849375, 'epoch': 0.09}         \n",
      "{'loss': 2.1797, 'learning_rate': 0.000199348037316403, 'epoch': 0.09}          \n",
      "{'loss': 2.2123, 'learning_rate': 0.0001993476821779289, 'epoch': 0.09}         \n",
      "{'loss': 2.2594, 'learning_rate': 0.00019934732694307176, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0652, 'learning_rate': 0.0001993469716118319, 'epoch': 0.09}         \n",
      "{'loss': 1.6152, 'learning_rate': 0.00019934661618420973, 'epoch': 0.09}        \n",
      "{'loss': 1.949, 'learning_rate': 0.00019934626066020554, 'epoch': 0.09}         \n",
      "{'loss': 1.9246, 'learning_rate': 0.0001993459050398197, 'epoch': 0.09}         \n",
      "{'loss': 1.7271, 'learning_rate': 0.0001993455493230525, 'epoch': 0.09}         \n",
      "{'loss': 2.1336, 'learning_rate': 0.00019934519350990439, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 90885/1061708 [13:36:30<144:37:39,  1.86it/s][2024-03-01 07:43:31,510] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9132, 'learning_rate': 0.0001993448731956656, 'epoch': 0.09}         \n",
      "{'loss': 1.8199, 'learning_rate': 0.0001993445171993946, 'epoch': 0.09}         \n",
      "{'loss': 2.1485, 'learning_rate': 0.00019934416110674362, 'epoch': 0.09}        \n",
      "{'loss': 2.488, 'learning_rate': 0.000199343804917713, 'epoch': 0.09}           \n",
      "{'loss': 2.3294, 'learning_rate': 0.0001993434486323031, 'epoch': 0.09}         \n",
      "{'loss': 2.0549, 'learning_rate': 0.0001993430922505143, 'epoch': 0.09}         \n",
      "{'loss': 2.3087, 'learning_rate': 0.0001993427357723469, 'epoch': 0.09}         \n",
      "{'loss': 2.0907, 'learning_rate': 0.00019934237919780125, 'epoch': 0.09}        \n",
      "{'loss': 2.0446, 'learning_rate': 0.0001993420225268777, 'epoch': 0.09}         \n",
      "{'loss': 2.1584, 'learning_rate': 0.00019934166575957663, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 90986/1061708 [13:37:24<143:43:45,  1.88it/s][2024-03-01 07:44:25,255] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 90987/1061708 [13:37:25<134:43:52,  2.00it/s][2024-03-01 07:44:25,677] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9651, 'learning_rate': 0.00019934138027634415, 'epoch': 0.09}        \n",
      "{'loss': 2.1324, 'learning_rate': 0.00019934102333556437, 'epoch': 0.09}        \n",
      "{'loss': 2.0102, 'learning_rate': 0.000199340666298408, 'epoch': 0.09}          \n",
      "{'loss': 2.1408, 'learning_rate': 0.0001993403091648754, 'epoch': 0.09}         \n",
      "{'loss': 2.036, 'learning_rate': 0.00019933995193496687, 'epoch': 0.09}         \n",
      "{'loss': 2.1898, 'learning_rate': 0.00019933959460868286, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 91045/1061708 [13:37:55<144:43:54,  1.86it/s][2024-03-01 07:44:56,505] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8503, 'learning_rate': 0.00019933927293262643, 'epoch': 0.09}        \n",
      "{'loss': 2.2261, 'learning_rate': 0.0001993389154232298, 'epoch': 0.09}         \n",
      "{'loss': 2.0613, 'learning_rate': 0.0001993385578174587, 'epoch': 0.09}         \n",
      "{'loss': 1.9312, 'learning_rate': 0.0001993382001153134, 'epoch': 0.09}         \n",
      "{'loss': 2.0156, 'learning_rate': 0.00019933784231679426, 'epoch': 0.09}        \n",
      "{'loss': 2.109, 'learning_rate': 0.00019933748442190158, 'epoch': 0.09}         \n",
      "{'loss': 2.3345, 'learning_rate': 0.0001993371264306358, 'epoch': 0.09}         \n",
      "{'loss': 1.6527, 'learning_rate': 0.00019933676834299718, 'epoch': 0.09}        \n",
      "{'loss': 1.7776, 'learning_rate': 0.00019933641015898617, 'epoch': 0.09}        \n",
      "{'loss': 1.9579, 'learning_rate': 0.00019933605187860302, 'epoch': 0.09}        \n",
      "{'loss': 2.2052, 'learning_rate': 0.00019933569350184813, 'epoch': 0.09}        \n",
      "{'loss': 1.9208, 'learning_rate': 0.00019933533502872183, 'epoch': 0.09}        \n",
      "{'loss': 1.9138, 'learning_rate': 0.00019933497645922444, 'epoch': 0.09}        \n",
      "{'loss': 2.0787, 'learning_rate': 0.0001993346177933564, 'epoch': 0.09}         \n",
      "{'loss': 2.064, 'learning_rate': 0.00019933425903111797, 'epoch': 0.09}         \n",
      "{'loss': 2.1578, 'learning_rate': 0.0001993339001725095, 'epoch': 0.09}         \n",
      "{'loss': 1.9801, 'learning_rate': 0.00019933354121753137, 'epoch': 0.09}        \n",
      "{'loss': 2.3586, 'learning_rate': 0.00019933318216618396, 'epoch': 0.09}        \n",
      "{'loss': 1.9051, 'learning_rate': 0.00019933282301846754, 'epoch': 0.09}        \n",
      "{'loss': 2.0023, 'learning_rate': 0.0001993324637743825, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 91246/1061708 [13:39:42<143:33:09,  1.88it/s][2024-03-01 07:46:43,455] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 91247/1061708 [13:39:43<134:32:05,  2.00it/s][2024-03-01 07:46:43,877] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.218, 'learning_rate': 0.0001993321763097293, 'epoch': 0.09}          \n",
      "{'loss': 2.2984, 'learning_rate': 0.00019933181689218163, 'epoch': 0.09}        \n",
      "{'loss': 2.1535, 'learning_rate': 0.0001993314573782663, 'epoch': 0.09}         \n",
      "{'loss': 2.3073, 'learning_rate': 0.0001993310977679837, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 91287/1061708 [13:40:04<142:58:31,  1.89it/s][2024-03-01 07:47:05,110] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.35, 'learning_rate': 0.0001993307740363356, 'epoch': 0.09}           \n",
      "{'loss': 2.3744, 'learning_rate': 0.00019933041424295608, 'epoch': 0.09}        \n",
      "{'loss': 2.2309, 'learning_rate': 0.00019933005435321025, 'epoch': 0.09}        \n",
      "{'loss': 1.7569, 'learning_rate': 0.00019932969436709853, 'epoch': 0.09}        \n",
      "{'loss': 2.226, 'learning_rate': 0.00019932933428462119, 'epoch': 0.09}         \n",
      "{'loss': 2.0154, 'learning_rate': 0.00019932897410577862, 'epoch': 0.09}        \n",
      "{'loss': 1.7994, 'learning_rate': 0.00019932861383057117, 'epoch': 0.09}        \n",
      "{'loss': 1.6412, 'learning_rate': 0.00019932825345899918, 'epoch': 0.09}        \n",
      "{'loss': 2.4848, 'learning_rate': 0.000199327892991063, 'epoch': 0.09}          \n",
      "{'loss': 1.8474, 'learning_rate': 0.00019932753242676298, 'epoch': 0.09}        \n",
      "{'loss': 2.3483, 'learning_rate': 0.00019932717176609948, 'epoch': 0.09}        \n",
      "{'loss': 2.1939, 'learning_rate': 0.00019932681100907283, 'epoch': 0.09}        \n",
      "{'loss': 2.4667, 'learning_rate': 0.0001993264501556834, 'epoch': 0.09}         \n",
      "{'loss': 1.9716, 'learning_rate': 0.00019932608920593148, 'epoch': 0.09}        \n",
      "{'loss': 1.9115, 'learning_rate': 0.00019932572815981752, 'epoch': 0.09}        \n",
      "{'loss': 2.0463, 'learning_rate': 0.00019932536701734182, 'epoch': 0.09}        \n",
      "{'loss': 2.2203, 'learning_rate': 0.00019932500577850472, 'epoch': 0.09}        \n",
      "{'loss': 2.0841, 'learning_rate': 0.0001993246444433066, 'epoch': 0.09}         \n",
      "{'loss': 2.0702, 'learning_rate': 0.00019932428301174774, 'epoch': 0.09}        \n",
      "{'loss': 2.1586, 'learning_rate': 0.0001993239214838286, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 91488/1061708 [13:41:51<143:10:53,  1.88it/s][2024-03-01 07:48:52,049] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 91489/1061708 [13:41:51<134:25:33,  2.00it/s][2024-03-01 07:48:52,476] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0793, 'learning_rate': 0.00019932363219211406, 'epoch': 0.09}        \n",
      "{'loss': 2.3837, 'learning_rate': 0.00019932327049074718, 'epoch': 0.09}        \n",
      "{'loss': 1.8394, 'learning_rate': 0.00019932290869302093, 'epoch': 0.09}        \n",
      "{'loss': 2.2165, 'learning_rate': 0.0001993225467989357, 'epoch': 0.09}         \n",
      "{'loss': 2.1086, 'learning_rate': 0.00019932218480849182, 'epoch': 0.09}        \n",
      "{'loss': 2.0434, 'learning_rate': 0.00019932182272168963, 'epoch': 0.09}        \n",
      "{'loss': 1.9588, 'learning_rate': 0.00019932146053852946, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9635, 'learning_rate': 0.00019932109825901174, 'epoch': 0.09}        \n",
      "{'loss': 1.7716, 'learning_rate': 0.00019932073588313674, 'epoch': 0.09}        \n",
      "{'loss': 2.0415, 'learning_rate': 0.00019932037341090483, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 91585/1061708 [13:42:43<143:58:43,  1.87it/s][2024-03-01 07:49:43,543] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1827, 'learning_rate': 0.00019932004710351126, 'epoch': 0.09}        \n",
      "{'loss': 2.0513, 'learning_rate': 0.00019931968444820226, 'epoch': 0.09}        \n",
      "{'loss': 2.0582, 'learning_rate': 0.00019931932169653735, 'epoch': 0.09}        \n",
      "{'loss': 2.0764, 'learning_rate': 0.00019931895884851692, 'epoch': 0.09}        \n",
      "{'loss': 1.9797, 'learning_rate': 0.00019931859590414136, 'epoch': 0.09}        \n",
      "{'loss': 2.0875, 'learning_rate': 0.00019931823286341093, 'epoch': 0.09}        \n",
      "{'loss': 1.9336, 'learning_rate': 0.00019931786972632605, 'epoch': 0.09}        \n",
      "{'loss': 2.2879, 'learning_rate': 0.00019931750649288706, 'epoch': 0.09}        \n",
      "{'loss': 1.6369, 'learning_rate': 0.0001993171431630943, 'epoch': 0.09}         \n",
      "{'loss': 2.0353, 'learning_rate': 0.00019931677973694813, 'epoch': 0.09}        \n",
      "{'loss': 2.1557, 'learning_rate': 0.0001993164162144489, 'epoch': 0.09}         \n",
      "{'loss': 2.2079, 'learning_rate': 0.00019931605259559696, 'epoch': 0.09}        \n",
      "{'loss': 2.0308, 'learning_rate': 0.00019931568888039268, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 91711/1061708 [13:43:49<142:30:38,  1.89it/s][2024-03-01 07:50:50,526] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9906, 'learning_rate': 0.00019931536145432785, 'epoch': 0.09}        \n",
      "{'loss': 1.9508, 'learning_rate': 0.0001993149975560551, 'epoch': 0.09}         \n",
      "{'loss': 2.208, 'learning_rate': 0.00019931463356143096, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 91745/1061708 [13:44:08<144:53:42,  1.86it/s][2024-03-01 07:51:08,627] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1854, 'learning_rate': 0.00019931430588388918, 'epoch': 0.09}        \n",
      "{'loss': 2.1188, 'learning_rate': 0.00019931394170619852, 'epoch': 0.09}        \n",
      "{'loss': 1.7481, 'learning_rate': 0.00019931357743215754, 'epoch': 0.09}        \n",
      "{'loss': 2.0955, 'learning_rate': 0.00019931321306176661, 'epoch': 0.09}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00019931284859502614, 'epoch': 0.09}        \n",
      "{'loss': 2.167, 'learning_rate': 0.00019931248403193638, 'epoch': 0.09}         \n",
      "{'loss': 2.2154, 'learning_rate': 0.00019931211937249772, 'epoch': 0.09}        \n",
      "{'loss': 2.2222, 'learning_rate': 0.0001993117546167105, 'epoch': 0.09}         \n",
      "{'loss': 1.9553, 'learning_rate': 0.00019931138976457516, 'epoch': 0.09}        \n",
      "{'loss': 2.3344, 'learning_rate': 0.00019931102481609197, 'epoch': 0.09}        \n",
      "{'loss': 1.8076, 'learning_rate': 0.0001993106597712613, 'epoch': 0.09}         \n",
      "{'loss': 2.0259, 'learning_rate': 0.0001993102946300835, 'epoch': 0.09}         \n",
      "{'loss': 2.0209, 'learning_rate': 0.00019930992939255895, 'epoch': 0.09}        \n",
      "{'loss': 2.2394, 'learning_rate': 0.000199309564058688, 'epoch': 0.09}          \n",
      "{'loss': 2.0432, 'learning_rate': 0.00019930919862847096, 'epoch': 0.09}        \n",
      "{'loss': 2.2188, 'learning_rate': 0.00019930883310190822, 'epoch': 0.09}        \n",
      "{'loss': 2.2164, 'learning_rate': 0.00019930846747900013, 'epoch': 0.09}        \n",
      "{'loss': 2.3377, 'learning_rate': 0.00019930810175974707, 'epoch': 0.09}        \n",
      "{'loss': 2.0255, 'learning_rate': 0.00019930773594414937, 'epoch': 0.09}        \n",
      "{'loss': 2.3654, 'learning_rate': 0.00019930737003220737, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 91946/1061708 [13:45:57<143:56:24,  1.87it/s][2024-03-01 07:52:57,778] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 91947/1061708 [13:45:57<134:44:05,  2.00it/s][2024-03-01 07:52:58,201] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.103, 'learning_rate': 0.0001993070772332861, 'epoch': 0.09}          \n",
      "{'loss': 2.0776, 'learning_rate': 0.00019930671114792533, 'epoch': 0.09}        \n",
      "{'loss': 1.8142, 'learning_rate': 0.00019930634496622123, 'epoch': 0.09}        \n",
      "{'loss': 2.2917, 'learning_rate': 0.00019930597868817417, 'epoch': 0.09}        \n",
      "{'loss': 2.2438, 'learning_rate': 0.0001993056123137846, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 91999/1061708 [13:46:25<145:02:30,  1.86it/s][2024-03-01 07:53:25,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=92000, skipped=1047, lr=[0.00019930524584305275], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 07:53:26,015] [INFO] [timer.py:260:stop] epoch=0/micro_step=92000/global_step=92000, RunningAvgSamplesPerSec=1.8923611129736069, CurrSamplesPerSec=1.8464290871183706, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0228, 'learning_rate': 0.00019930524584305275, 'epoch': 0.09}        \n",
      "{'loss': 2.0634, 'learning_rate': 0.00019930487927597904, 'epoch': 0.09}        \n",
      "{'loss': 1.9939, 'learning_rate': 0.0001993045126125638, 'epoch': 0.09}         \n",
      "{'loss': 2.0716, 'learning_rate': 0.00019930414585280745, 'epoch': 0.09}        \n",
      "{'loss': 1.8969, 'learning_rate': 0.00019930377899671028, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92048/1061708 [13:46:51<142:36:35,  1.89it/s][2024-03-01 07:53:52,403] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92049/1061708 [13:46:52<133:50:37,  2.01it/s][2024-03-01 07:53:52,824] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0146, 'learning_rate': 0.0001993034854424674, 'epoch': 0.09}         \n",
      "{'loss': 2.1162, 'learning_rate': 0.00019930311841295767, 'epoch': 0.09}        \n",
      "{'loss': 1.6867, 'learning_rate': 0.00019930275128710815, 'epoch': 0.09}        \n",
      "{'loss': 2.1374, 'learning_rate': 0.0001993023840649192, 'epoch': 0.09}         \n",
      "{'loss': 2.0008, 'learning_rate': 0.00019930201674639114, 'epoch': 0.09}        \n",
      "{'loss': 2.0516, 'learning_rate': 0.00019930164933152432, 'epoch': 0.09}        \n",
      "{'loss': 1.9823, 'learning_rate': 0.00019930128182031915, 'epoch': 0.09}        \n",
      "{'loss': 2.0974, 'learning_rate': 0.00019930091421277598, 'epoch': 0.09}        \n",
      "{'loss': 2.0312, 'learning_rate': 0.0001993005465088951, 'epoch': 0.09}         \n",
      "{'loss': 2.1135, 'learning_rate': 0.00019930017870867692, 'epoch': 0.09}        \n",
      "{'loss': 2.2183, 'learning_rate': 0.0001992998108121218, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 92150/1061708 [13:47:45<142:35:55,  1.89it/s][2024-03-01 07:54:46,518] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92151/1061708 [13:47:46<133:55:16,  2.01it/s][2024-03-01 07:54:46,941] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9111, 'learning_rate': 0.0001992995164255153, 'epoch': 0.09}         \n",
      "{'loss': 2.0562, 'learning_rate': 0.0001992991483555546, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 92175/1061708 [13:47:59<144:10:40,  1.87it/s][2024-03-01 07:54:59,675] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1503, 'learning_rate': 0.00019929881701022265, 'epoch': 0.09}        \n",
      "{'loss': 2.0692, 'learning_rate': 0.00019929844875722394, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1122, 'learning_rate': 0.00019929808040788994, 'epoch': 0.09}        \n",
      "{'loss': 1.7734, 'learning_rate': 0.00019929771196222098, 'epoch': 0.09}        \n",
      "{'loss': 2.1254, 'learning_rate': 0.00019929734342021747, 'epoch': 0.09}        \n",
      "{'loss': 2.3034, 'learning_rate': 0.00019929697478187982, 'epoch': 0.09}        \n",
      "{'loss': 2.163, 'learning_rate': 0.00019929660604720825, 'epoch': 0.09}         \n",
      "{'loss': 1.7979, 'learning_rate': 0.00019929623721620323, 'epoch': 0.09}        \n",
      "{'loss': 1.5765, 'learning_rate': 0.0001992958682888651, 'epoch': 0.09}         \n",
      "{'loss': 2.4049, 'learning_rate': 0.00019929549926519418, 'epoch': 0.09}        \n",
      "{'loss': 2.0393, 'learning_rate': 0.00019929513014519082, 'epoch': 0.09}        \n",
      "{'loss': 1.9679, 'learning_rate': 0.00019929476092885544, 'epoch': 0.09}        \n",
      "{'loss': 1.8967, 'learning_rate': 0.00019929439161618835, 'epoch': 0.09}        \n",
      "{'loss': 2.0817, 'learning_rate': 0.00019929402220718992, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92311/1061708 [13:49:11<142:22:12,  1.89it/s][2024-03-01 07:56:12,059] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92319/1061708 [13:49:15<142:23:49,  1.89it/s][2024-03-01 07:56:16,229] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0421, 'learning_rate': 0.00019929372661063285, 'epoch': 0.09}        \n",
      "{'loss': 2.0759, 'learning_rate': 0.00019929335702823891, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92330/1061708 [13:49:21<142:09:32,  1.89it/s][2024-03-01 07:56:21,983] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3461, 'learning_rate': 0.00019929302432172195, 'epoch': 0.09}        \n",
      "{'loss': 2.3407, 'learning_rate': 0.0001992926545563007, 'epoch': 0.09}         \n",
      "{'loss': 2.4628, 'learning_rate': 0.00019929228469454978, 'epoch': 0.09}        \n",
      "{'loss': 2.2563, 'learning_rate': 0.0001992919147364696, 'epoch': 0.09}         \n",
      "{'loss': 1.9753, 'learning_rate': 0.00019929154468206045, 'epoch': 0.09}        \n",
      "{'loss': 2.0636, 'learning_rate': 0.00019929117453132276, 'epoch': 0.09}        \n",
      "{'loss': 1.6952, 'learning_rate': 0.00019929080428425681, 'epoch': 0.09}        \n",
      "{'loss': 2.0729, 'learning_rate': 0.00019929043394086303, 'epoch': 0.09}        \n",
      "{'loss': 2.0828, 'learning_rate': 0.00019929006350114173, 'epoch': 0.09}        \n",
      "{'loss': 2.2262, 'learning_rate': 0.00019928969296509333, 'epoch': 0.09}        \n",
      "{'loss': 2.0339, 'learning_rate': 0.00019928932233271814, 'epoch': 0.09}        \n",
      "{'loss': 2.3511, 'learning_rate': 0.0001992889516040165, 'epoch': 0.09}         \n",
      "{'loss': 1.853, 'learning_rate': 0.00019928858077898883, 'epoch': 0.09}         \n",
      "{'loss': 1.6627, 'learning_rate': 0.00019928820985763547, 'epoch': 0.09}        \n",
      "{'loss': 1.7877, 'learning_rate': 0.00019928783883995673, 'epoch': 0.09}        \n",
      "{'loss': 1.956, 'learning_rate': 0.00019928746772595303, 'epoch': 0.09}         \n",
      "{'loss': 2.2488, 'learning_rate': 0.0001992870965156247, 'epoch': 0.09}         \n",
      "{'loss': 2.2768, 'learning_rate': 0.0001992867252089721, 'epoch': 0.09}         \n",
      "{'loss': 1.9026, 'learning_rate': 0.00019928635380599564, 'epoch': 0.09}        \n",
      "{'loss': 1.8854, 'learning_rate': 0.00019928598230669562, 'epoch': 0.09}        \n",
      "{'loss': 2.1233, 'learning_rate': 0.00019928561071107243, 'epoch': 0.09}        \n",
      "{'loss': 2.4039, 'learning_rate': 0.00019928523901912638, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92557/1061708 [13:51:22<143:31:35,  1.88it/s][2024-03-01 07:58:22,987] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0909, 'learning_rate': 0.00019928490441401925, 'epoch': 0.09}        \n",
      "{'loss': 1.6729, 'learning_rate': 0.00019928453253906088, 'epoch': 0.09}        \n",
      "{'loss': 2.2715, 'learning_rate': 0.0001992841605677807, 'epoch': 0.09}         \n",
      "{'loss': 1.7608, 'learning_rate': 0.00019928378850017915, 'epoch': 0.09}        \n",
      "{'loss': 2.0172, 'learning_rate': 0.0001992834163362565, 'epoch': 0.09}         \n",
      "{'loss': 1.9259, 'learning_rate': 0.0001992830440760132, 'epoch': 0.09}         \n",
      "{'loss': 2.1027, 'learning_rate': 0.00019928267171944957, 'epoch': 0.09}        \n",
      "{'loss': 2.3231, 'learning_rate': 0.000199282299266566, 'epoch': 0.09}          \n",
      "{'loss': 2.1187, 'learning_rate': 0.00019928192671736278, 'epoch': 0.09}        \n",
      "{'loss': 2.1662, 'learning_rate': 0.0001992815540718403, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 92658/1061708 [13:52:16<143:04:44,  1.88it/s][2024-03-01 07:59:16,831] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92659/1061708 [13:52:16<134:12:44,  2.01it/s][2024-03-01 07:59:17,256] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1512, 'learning_rate': 0.00019928125588607274, 'epoch': 0.09}        \n",
      "{'loss': 2.325, 'learning_rate': 0.00019928088306717654, 'epoch': 0.09}         \n",
      "{'loss': 1.87, 'learning_rate': 0.00019928051015196213, 'epoch': 0.09}          \n",
      "{'loss': 2.1082, 'learning_rate': 0.00019928013714042985, 'epoch': 0.09}        \n",
      "{'loss': 2.0583, 'learning_rate': 0.00019927976403258003, 'epoch': 0.09}        \n",
      "{'loss': 2.0203, 'learning_rate': 0.0001992793908284131, 'epoch': 0.09}         \n",
      "{'loss': 2.0744, 'learning_rate': 0.00019927901752792937, 'epoch': 0.09}        \n",
      "{'loss': 2.0745, 'learning_rate': 0.0001992786441311292, 'epoch': 0.09}         \n",
      "{'loss': 2.2498, 'learning_rate': 0.000199278270638013, 'epoch': 0.09}          \n",
      "{'loss': 2.2644, 'learning_rate': 0.0001992778970485811, 'epoch': 0.09}         \n",
      "{'loss': 2.0313, 'learning_rate': 0.00019927752336283385, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92760/1061708 [13:53:10<142:35:59,  1.89it/s][2024-03-01 08:00:11,061] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92761/1061708 [13:53:10<134:02:57,  2.01it/s][2024-03-01 08:00:11,487] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7891, 'learning_rate': 0.00019927722434488927, 'epoch': 0.09}        \n",
      "{'loss': 2.0505, 'learning_rate': 0.00019927685048577533, 'epoch': 0.09}        \n",
      "{'loss': 2.4651, 'learning_rate': 0.00019927647653034708, 'epoch': 0.09}        \n",
      "{'loss': 2.1099, 'learning_rate': 0.00019927610247860486, 'epoch': 0.09}        \n",
      "{'loss': 2.2106, 'learning_rate': 0.00019927572833054903, 'epoch': 0.09}        \n",
      "{'loss': 2.1101, 'learning_rate': 0.00019927535408618003, 'epoch': 0.09}        \n",
      "{'loss': 2.039, 'learning_rate': 0.00019927497974549813, 'epoch': 0.09}         \n",
      "{'loss': 1.9886, 'learning_rate': 0.0001992746053085037, 'epoch': 0.09}         \n",
      "{'loss': 1.8244, 'learning_rate': 0.00019927423077519715, 'epoch': 0.09}        \n",
      "{'loss': 2.2521, 'learning_rate': 0.00019927385614557883, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 92862/1061708 [13:54:04<142:27:05,  1.89it/s][2024-03-01 08:01:05,367] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 92863/1061708 [13:54:05<137:45:31,  1.95it/s][2024-03-01 08:01:05,789] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1188, 'learning_rate': 0.0001992735563725399, 'epoch': 0.09}         \n",
      "{'loss': 2.2396, 'learning_rate': 0.0001992731815695613, 'epoch': 0.09}         \n",
      "{'loss': 2.5565, 'learning_rate': 0.00019927280667027193, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9%|██▍                         | 92899/1061708 [13:54:24<142:59:02,  1.88it/s][2024-03-01 08:01:24,924] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9953, 'learning_rate': 0.00019927246917856607, 'epoch': 0.09}        \n",
      "{'loss': 2.0523, 'learning_rate': 0.00019927209409628725, 'epoch': 0.09}        \n",
      "{'loss': 1.9675, 'learning_rate': 0.00019927171891769872, 'epoch': 0.09}        \n",
      "{'loss': 2.1543, 'learning_rate': 0.00019927134364280087, 'epoch': 0.09}        \n",
      "{'loss': 2.2014, 'learning_rate': 0.00019927096827159404, 'epoch': 0.09}        \n",
      "{'loss': 2.0771, 'learning_rate': 0.0001992705928040786, 'epoch': 0.09}         \n",
      "{'loss': 2.5072, 'learning_rate': 0.00019927021724025487, 'epoch': 0.09}        \n",
      "{'loss': 2.3483, 'learning_rate': 0.0001992698415801233, 'epoch': 0.09}         \n",
      "{'loss': 1.9687, 'learning_rate': 0.00019926946582368417, 'epoch': 0.09}        \n",
      "{'loss': 2.2638, 'learning_rate': 0.00019926908997093792, 'epoch': 0.09}        \n",
      "{'loss': 2.1147, 'learning_rate': 0.00019926871402188483, 'epoch': 0.09}        \n",
      "{'loss': 2.2038, 'learning_rate': 0.00019926833797652535, 'epoch': 0.09}        \n",
      "{'loss': 2.1924, 'learning_rate': 0.0001992679618348598, 'epoch': 0.09}         \n",
      "{'loss': 2.3093, 'learning_rate': 0.0001992675855968886, 'epoch': 0.09}         \n",
      "{'loss': 2.4935, 'learning_rate': 0.000199267209262612, 'epoch': 0.09}          \n",
      "{'loss': 2.0542, 'learning_rate': 0.00019926683283203044, 'epoch': 0.09}        \n",
      "{'loss': 1.8741, 'learning_rate': 0.0001992664563051443, 'epoch': 0.09}         \n",
      "{'loss': 2.1409, 'learning_rate': 0.00019926607968195393, 'epoch': 0.09}        \n",
      "{'loss': 1.8603, 'learning_rate': 0.0001992657029624597, 'epoch': 0.09}         \n",
      "{'loss': 2.1348, 'learning_rate': 0.00019926532614666193, 'epoch': 0.09}        \n",
      "{'loss': 2.2141, 'learning_rate': 0.00019926494923456104, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93100/1061708 [13:56:11<142:32:58,  1.89it/s][2024-03-01 08:03:12,251] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 93101/1061708 [13:56:12<133:53:32,  2.01it/s][2024-03-01 08:03:12,675] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0122, 'learning_rate': 0.0001992646476355423, 'epoch': 0.09}         \n",
      "{'loss': 1.982, 'learning_rate': 0.0001992642705500967, 'epoch': 0.09}          \n",
      "{'loss': 1.9784, 'learning_rate': 0.00019926389336834898, 'epoch': 0.09}        \n",
      "{'loss': 1.7976, 'learning_rate': 0.00019926351609029946, 'epoch': 0.09}        \n",
      "{'loss': 1.9804, 'learning_rate': 0.0001992631387159486, 'epoch': 0.09}         \n",
      "{'loss': 2.2918, 'learning_rate': 0.00019926276124529672, 'epoch': 0.09}        \n",
      "{'loss': 1.9702, 'learning_rate': 0.00019926238367834419, 'epoch': 0.09}        \n",
      "{'loss': 2.2174, 'learning_rate': 0.00019926200601509133, 'epoch': 0.09}        \n",
      "{'loss': 2.0089, 'learning_rate': 0.00019926162825553858, 'epoch': 0.09}        \n",
      "{'loss': 2.1405, 'learning_rate': 0.0001992612503996863, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 93202/1061708 [13:57:05<142:35:53,  1.89it/s][2024-03-01 08:04:06,533] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 93203/1061708 [13:57:06<137:22:23,  1.96it/s][2024-03-01 08:04:06,956] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0446, 'learning_rate': 0.00019926094804566903, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93219/1061708 [13:57:14<142:45:13,  1.88it/s][2024-03-01 08:04:15,420] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8054, 'learning_rate': 0.00019926060782373095, 'epoch': 0.09}        \n",
      "{'loss': 2.1768, 'learning_rate': 0.00019926022970787172, 'epoch': 0.09}        \n",
      "{'loss': 2.3423, 'learning_rate': 0.00019925985149571424, 'epoch': 0.09}        \n",
      "{'loss': 2.2253, 'learning_rate': 0.00019925947318725896, 'epoch': 0.09}        \n",
      "{'loss': 2.1816, 'learning_rate': 0.00019925909478250625, 'epoch': 0.09}        \n",
      "{'loss': 1.872, 'learning_rate': 0.0001992587162814564, 'epoch': 0.09}          \n",
      "{'loss': 2.0201, 'learning_rate': 0.00019925833768410985, 'epoch': 0.09}        \n",
      "{'loss': 2.2171, 'learning_rate': 0.00019925795899046692, 'epoch': 0.09}        \n",
      "{'loss': 2.1161, 'learning_rate': 0.000199257580200528, 'epoch': 0.09}          \n",
      "{'loss': 2.1691, 'learning_rate': 0.0001992572013142935, 'epoch': 0.09}         \n",
      "{'loss': 2.0885, 'learning_rate': 0.00019925682233176367, 'epoch': 0.09}        \n",
      "{'loss': 2.346, 'learning_rate': 0.000199256443252939, 'epoch': 0.09}           \n",
      "  9%|██▍                         | 93336/1061708 [13:58:17<143:31:23,  1.87it/s][2024-03-01 08:05:17,813] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9698, 'learning_rate': 0.00019925610199966496, 'epoch': 0.09}        \n",
      "{'loss': 2.0638, 'learning_rate': 0.000199255722737881, 'epoch': 0.09}          \n",
      "{'loss': 1.5916, 'learning_rate': 0.00019925534337980323, 'epoch': 0.09}        \n",
      "{'loss': 2.3162, 'learning_rate': 0.00019925496392543196, 'epoch': 0.09}        \n",
      "{'loss': 2.1475, 'learning_rate': 0.00019925458437476765, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93383/1061708 [13:58:42<146:14:21,  1.84it/s][2024-03-01 08:05:42,888] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9963, 'learning_rate': 0.00019925424269683949, 'epoch': 0.09}        \n",
      "{'loss': 2.1109, 'learning_rate': 0.00019925386296321931, 'epoch': 0.09}        \n",
      "{'loss': 2.6158, 'learning_rate': 0.0001992534831333071, 'epoch': 0.09}         \n",
      "{'loss': 2.5238, 'learning_rate': 0.00019925310320710328, 'epoch': 0.09}        \n",
      "{'loss': 2.0963, 'learning_rate': 0.00019925272318460814, 'epoch': 0.09}        \n",
      "{'loss': 1.996, 'learning_rate': 0.0001992523430658221, 'epoch': 0.09}          \n",
      "{'loss': 2.1049, 'learning_rate': 0.0001992519628507455, 'epoch': 0.09}         \n",
      "{'loss': 1.9543, 'learning_rate': 0.00019925158253937878, 'epoch': 0.09}        \n",
      "{'loss': 2.1244, 'learning_rate': 0.0001992512021317222, 'epoch': 0.09}         \n",
      "{'loss': 2.3604, 'learning_rate': 0.0001992508216277762, 'epoch': 0.09}         \n",
      "{'loss': 2.1624, 'learning_rate': 0.00019925044102754114, 'epoch': 0.09}        \n",
      "{'loss': 1.9532, 'learning_rate': 0.00019925006033101735, 'epoch': 0.09}        \n",
      "{'loss': 2.0205, 'learning_rate': 0.00019924967953820528, 'epoch': 0.09}        \n",
      "{'loss': 2.2918, 'learning_rate': 0.00019924929864910523, 'epoch': 0.09}        \n",
      "{'loss': 2.157, 'learning_rate': 0.00019924891766371757, 'epoch': 0.09}         \n",
      "{'loss': 1.905, 'learning_rate': 0.00019924853658204273, 'epoch': 0.09}         \n",
      "{'loss': 2.067, 'learning_rate': 0.000199248155404081, 'epoch': 0.09}           \n",
      "  9%|██▍                         | 93555/1061708 [14:00:13<143:44:04,  1.87it/s][2024-03-01 08:07:14,497] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9857, 'learning_rate': 0.00019924781226159048, 'epoch': 0.09}        \n",
      "{'loss': 1.9334, 'learning_rate': 0.0001992474309006848, 'epoch': 0.09}         \n",
      "{'loss': 1.9376, 'learning_rate': 0.0001992470494434933, 'epoch': 0.09}         \n",
      "{'loss': 2.0624, 'learning_rate': 0.00019924666789001641, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93591/1061708 [14:00:33<142:32:01,  1.89it/s][2024-03-01 08:07:33,596] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8953, 'learning_rate': 0.00019924632440956346, 'epoch': 0.09}        \n",
      "{'loss': 2.1227, 'learning_rate': 0.00019924594267314532, 'epoch': 0.09}        \n",
      "{'loss': 2.0429, 'learning_rate': 0.00019924556084044282, 'epoch': 0.09}        \n",
      "{'loss': 2.1623, 'learning_rate': 0.00019924517891145636, 'epoch': 0.09}        \n",
      "{'loss': 2.2634, 'learning_rate': 0.0001992447968861863, 'epoch': 0.09}         \n",
      "{'loss': 2.1002, 'learning_rate': 0.00019924441476463302, 'epoch': 0.09}        \n",
      "{'loss': 1.9396, 'learning_rate': 0.00019924403254679691, 'epoch': 0.09}        \n",
      "{'loss': 2.1316, 'learning_rate': 0.0001992436502326783, 'epoch': 0.09}         \n",
      "{'loss': 2.0003, 'learning_rate': 0.00019924326782227758, 'epoch': 0.09}        \n",
      "{'loss': 1.9583, 'learning_rate': 0.0001992428853155951, 'epoch': 0.09}         \n",
      "{'loss': 2.2574, 'learning_rate': 0.00019924250271263132, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93703/1061708 [14:01:32<146:07:32,  1.84it/s][2024-03-01 08:08:33,253] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 93708/1061708 [14:01:35<143:45:13,  1.87it/s][2024-03-01 08:08:35,850] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2388, 'learning_rate': 0.0001992421965609379, 'epoch': 0.09}         \n",
      "{'loss': 2.3328, 'learning_rate': 0.00019924181378466854, 'epoch': 0.09}        \n",
      "{'loss': 2.2227, 'learning_rate': 0.00019924143091211888, 'epoch': 0.09}        \n",
      "{'loss': 2.1938, 'learning_rate': 0.00019924104794328924, 'epoch': 0.09}        \n",
      "{'loss': 2.3603, 'learning_rate': 0.00019924066487818002, 'epoch': 0.09}        \n",
      "{'loss': 2.0631, 'learning_rate': 0.0001992402817167916, 'epoch': 0.09}         \n",
      "{'loss': 1.911, 'learning_rate': 0.0001992398984591243, 'epoch': 0.09}          \n",
      "{'loss': 2.2938, 'learning_rate': 0.00019923951510517854, 'epoch': 0.09}        \n",
      "{'loss': 2.1208, 'learning_rate': 0.0001992391316549547, 'epoch': 0.09}         \n",
      "{'loss': 2.2675, 'learning_rate': 0.00019923874810845313, 'epoch': 0.09}        \n",
      "{'loss': 1.8869, 'learning_rate': 0.00019923836446567419, 'epoch': 0.09}        \n",
      "{'loss': 2.1976, 'learning_rate': 0.00019923798072661825, 'epoch': 0.09}        \n",
      "{'loss': 2.1889, 'learning_rate': 0.00019923759689128575, 'epoch': 0.09}        \n",
      "{'loss': 1.8066, 'learning_rate': 0.000199237212959677, 'epoch': 0.09}          \n",
      "  9%|██▍                         | 93842/1061708 [14:02:46<142:52:37,  1.88it/s][2024-03-01 08:09:47,341] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▍                         | 93847/1061708 [14:02:49<144:05:46,  1.87it/s][2024-03-01 08:09:49,940] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8171, 'learning_rate': 0.00019923690574507133, 'epoch': 0.09}        \n",
      "{'loss': 2.0882, 'learning_rate': 0.00019923652164016627, 'epoch': 0.09}        \n",
      "{'loss': 2.3183, 'learning_rate': 0.00019923613743898604, 'epoch': 0.09}        \n",
      "{'loss': 1.9118, 'learning_rate': 0.000199235753141531, 'epoch': 0.09}          \n",
      "{'loss': 1.948, 'learning_rate': 0.00019923536874780147, 'epoch': 0.09}         \n",
      "{'loss': 1.9685, 'learning_rate': 0.00019923498425779786, 'epoch': 0.09}        \n",
      "{'loss': 1.8389, 'learning_rate': 0.00019923459967152057, 'epoch': 0.09}        \n",
      "{'loss': 1.9951, 'learning_rate': 0.00019923421498896992, 'epoch': 0.09}        \n",
      "{'loss': 2.2747, 'learning_rate': 0.00019923383021014637, 'epoch': 0.09}        \n",
      "{'loss': 2.2525, 'learning_rate': 0.00019923344533505016, 'epoch': 0.09}        \n",
      "{'loss': 2.0726, 'learning_rate': 0.00019923306036368182, 'epoch': 0.09}        \n",
      "{'loss': 2.1425, 'learning_rate': 0.00019923267529604159, 'epoch': 0.09}        \n",
      "{'loss': 1.9866, 'learning_rate': 0.00019923229013212992, 'epoch': 0.09}        \n",
      "{'loss': 2.2288, 'learning_rate': 0.00019923190487194716, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 93985/1061708 [14:04:02<143:41:38,  1.87it/s][2024-03-01 08:11:03,463] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7352, 'learning_rate': 0.0001992315580554712, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 93999/1061708 [14:04:10<142:31:03,  1.89it/s][2024-03-01 08:11:10,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=94000, skipped=1077, lr=[0.0001992311726123744], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 08:11:10,944] [INFO] [timer.py:260:stop] epoch=0/micro_step=94000/global_step=94000, RunningAvgSamplesPerSec=1.8923878001657057, CurrSamplesPerSec=1.907295079544447, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0932, 'learning_rate': 0.0001992311726123744, 'epoch': 0.09}         \n",
      "{'loss': 2.1722, 'learning_rate': 0.00019923078707300757, 'epoch': 0.09}        \n",
      "{'loss': 1.977, 'learning_rate': 0.00019923040143737114, 'epoch': 0.09}         \n",
      "{'loss': 2.3089, 'learning_rate': 0.00019923001570546544, 'epoch': 0.09}        \n",
      "{'loss': 2.0944, 'learning_rate': 0.00019922962987729085, 'epoch': 0.09}        \n",
      "{'loss': 2.1106, 'learning_rate': 0.00019922924395284776, 'epoch': 0.09}        \n",
      "{'loss': 2.0728, 'learning_rate': 0.00019922885793213654, 'epoch': 0.09}        \n",
      "{'loss': 2.0592, 'learning_rate': 0.00019922847181515755, 'epoch': 0.09}        \n",
      "{'loss': 1.8309, 'learning_rate': 0.00019922808560191116, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94084/1061708 [14:04:55<144:28:25,  1.86it/s][2024-03-01 08:11:56,126] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1615, 'learning_rate': 0.00019922773792768115, 'epoch': 0.09}        \n",
      "{'loss': 1.9412, 'learning_rate': 0.00019922735153152777, 'epoch': 0.09}        \n",
      "{'loss': 2.2676, 'learning_rate': 0.0001992269650391081, 'epoch': 0.09}         \n",
      "{'loss': 1.6741, 'learning_rate': 0.00019922657845042254, 'epoch': 0.09}        \n",
      "{'loss': 2.3186, 'learning_rate': 0.0001992261917654714, 'epoch': 0.09}         \n",
      "{'loss': 2.1038, 'learning_rate': 0.00019922580498425507, 'epoch': 0.09}        \n",
      "{'loss': 1.7737, 'learning_rate': 0.00019922541810677395, 'epoch': 0.09}        \n",
      "{'loss': 2.0563, 'learning_rate': 0.00019922503113302845, 'epoch': 0.09}        \n",
      "{'loss': 2.0188, 'learning_rate': 0.00019922464406301886, 'epoch': 0.09}        \n",
      "{'loss': 2.1917, 'learning_rate': 0.0001992242568967456, 'epoch': 0.09}         \n",
      "{'loss': 2.0103, 'learning_rate': 0.00019922386963420907, 'epoch': 0.09}        \n",
      "{'loss': 2.1257, 'learning_rate': 0.00019922348227540958, 'epoch': 0.09}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.0001992230948203476, 'epoch': 0.09}         \n",
      "{'loss': 2.1931, 'learning_rate': 0.00019922270726902342, 'epoch': 0.09}        \n",
      "{'loss': 2.0762, 'learning_rate': 0.00019922231962143743, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94230/1061708 [14:06:13<142:14:26,  1.89it/s][2024-03-01 08:13:13,902] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7132, 'learning_rate': 0.00019922197065630655, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94245/1061708 [14:06:21<143:34:25,  1.87it/s][2024-03-01 08:13:21,804] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9731, 'learning_rate': 0.00019922162161320416, 'epoch': 0.09}        \n",
      "{'loss': 1.9926, 'learning_rate': 0.0001992212336960872, 'epoch': 0.09}         \n",
      "{'loss': 2.2271, 'learning_rate': 0.00019922084568270982, 'epoch': 0.09}        \n",
      "{'loss': 2.4353, 'learning_rate': 0.0001992204575730725, 'epoch': 0.09}         \n",
      "{'loss': 2.1035, 'learning_rate': 0.00019922006936717556, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9244, 'learning_rate': 0.00019921968106501937, 'epoch': 0.09}        \n",
      "{'loss': 2.4922, 'learning_rate': 0.00019921929266660432, 'epoch': 0.09}        \n",
      "{'loss': 1.984, 'learning_rate': 0.0001992189041719308, 'epoch': 0.09}          \n",
      "{'loss': 1.9788, 'learning_rate': 0.0001992185155809992, 'epoch': 0.09}         \n",
      "{'loss': 1.9289, 'learning_rate': 0.00019921812689380986, 'epoch': 0.09}        \n",
      "{'loss': 1.9216, 'learning_rate': 0.00019921773811036315, 'epoch': 0.09}        \n",
      "{'loss': 2.2476, 'learning_rate': 0.0001992173492306595, 'epoch': 0.09}         \n",
      "{'loss': 2.1493, 'learning_rate': 0.0001992169602546992, 'epoch': 0.09}         \n",
      "{'loss': 2.0584, 'learning_rate': 0.00019921657118248277, 'epoch': 0.09}        \n",
      "{'loss': 2.0654, 'learning_rate': 0.00019921618201401042, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94399/1061708 [14:07:43<142:13:16,  1.89it/s][2024-03-01 08:14:43,722] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3262, 'learning_rate': 0.0001992158316800869, 'epoch': 0.09}         \n",
      "{'loss': 2.1683, 'learning_rate': 0.00019921544232872953, 'epoch': 0.09}        \n",
      "{'loss': 2.209, 'learning_rate': 0.0001992150528811174, 'epoch': 0.09}          \n",
      "{'loss': 2.2154, 'learning_rate': 0.00019921466333725095, 'epoch': 0.09}        \n",
      "{'loss': 2.2875, 'learning_rate': 0.0001992142736971305, 'epoch': 0.09}         \n",
      "{'loss': 1.7309, 'learning_rate': 0.00019921388396075638, 'epoch': 0.09}        \n",
      "{'loss': 2.2162, 'learning_rate': 0.0001992134941281291, 'epoch': 0.09}         \n",
      "  9%|██▍                         | 94464/1061708 [14:08:17<144:42:40,  1.86it/s][2024-03-01 08:15:18,303] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1098, 'learning_rate': 0.00019921314319646828, 'epoch': 0.09}        \n",
      "{'loss': 2.0722, 'learning_rate': 0.0001992127531809609, 'epoch': 0.09}         \n",
      "{'loss': 1.8514, 'learning_rate': 0.00019921236306920132, 'epoch': 0.09}        \n",
      "{'loss': 2.4342, 'learning_rate': 0.00019921197286119, 'epoch': 0.09}           \n",
      "{'loss': 2.1018, 'learning_rate': 0.0001992115825569273, 'epoch': 0.09}         \n",
      "{'loss': 1.6864, 'learning_rate': 0.00019921119215641364, 'epoch': 0.09}        \n",
      "{'loss': 2.0643, 'learning_rate': 0.00019921080165964935, 'epoch': 0.09}        \n",
      "{'loss': 2.1458, 'learning_rate': 0.0001992104110666348, 'epoch': 0.09}         \n",
      "{'loss': 1.7877, 'learning_rate': 0.00019921002037737038, 'epoch': 0.09}        \n",
      "{'loss': 2.3638, 'learning_rate': 0.00019920962959185648, 'epoch': 0.09}        \n",
      "{'loss': 2.0624, 'learning_rate': 0.0001992092387100935, 'epoch': 0.09}         \n",
      "{'loss': 2.2046, 'learning_rate': 0.00019920884773208175, 'epoch': 0.09}        \n",
      "{'loss': 2.3048, 'learning_rate': 0.00019920845665782167, 'epoch': 0.09}        \n",
      "{'loss': 2.2162, 'learning_rate': 0.00019920806548731363, 'epoch': 0.09}        \n",
      "{'loss': 1.7798, 'learning_rate': 0.000199207674220558, 'epoch': 0.09}          \n",
      "{'loss': 2.5255, 'learning_rate': 0.00019920728285755516, 'epoch': 0.09}        \n",
      "{'loss': 2.118, 'learning_rate': 0.00019920689139830554, 'epoch': 0.09}         \n",
      "{'loss': 2.1406, 'learning_rate': 0.00019920649984280943, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94647/1061708 [14:09:55<143:55:27,  1.87it/s][2024-03-01 08:16:55,680] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9123, 'learning_rate': 0.0001992061473605725, 'epoch': 0.09}         \n",
      "{'loss': 1.9631, 'learning_rate': 0.00019920575562220919, 'epoch': 0.09}        \n",
      "  9%|██▍                         | 94669/1061708 [14:10:06<142:08:06,  1.89it/s][2024-03-01 08:17:07,295] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2489, 'learning_rate': 0.0001992054029753924, 'epoch': 0.09}         \n",
      "{'loss': 2.3704, 'learning_rate': 0.00019920501105416327, 'epoch': 0.09}        \n",
      "{'loss': 1.7141, 'learning_rate': 0.00019920461903668947, 'epoch': 0.09}        \n",
      "{'loss': 1.7916, 'learning_rate': 0.00019920422692297146, 'epoch': 0.09}        \n",
      "{'loss': 2.1951, 'learning_rate': 0.00019920383471300958, 'epoch': 0.09}        \n",
      "{'loss': 1.9334, 'learning_rate': 0.0001992034424068042, 'epoch': 0.09}         \n",
      "{'loss': 1.8571, 'learning_rate': 0.00019920305000435576, 'epoch': 0.09}        \n",
      "{'loss': 1.9539, 'learning_rate': 0.00019920265750566455, 'epoch': 0.09}        \n",
      "{'loss': 2.1551, 'learning_rate': 0.00019920226491073101, 'epoch': 0.09}        \n",
      "{'loss': 2.0344, 'learning_rate': 0.00019920187221955554, 'epoch': 0.09}        \n",
      "{'loss': 2.2098, 'learning_rate': 0.00019920147943213845, 'epoch': 0.09}        \n",
      "{'loss': 1.9562, 'learning_rate': 0.00019920108654848017, 'epoch': 0.09}        \n",
      "{'loss': 2.4896, 'learning_rate': 0.0001992006935685811, 'epoch': 0.09}         \n",
      "{'loss': 1.9837, 'learning_rate': 0.00019920030049244155, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 94806/1061708 [14:11:19<142:56:49,  1.88it/s][2024-03-01 08:18:20,109] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2002, 'learning_rate': 0.00019919994664163073, 'epoch': 0.09}        \n",
      "{'loss': 2.0931, 'learning_rate': 0.00019919955338263545, 'epoch': 0.09}        \n",
      "{'loss': 2.2925, 'learning_rate': 0.0001991991600274008, 'epoch': 0.09}         \n",
      "{'loss': 1.9373, 'learning_rate': 0.00019919876657592723, 'epoch': 0.09}        \n",
      "{'loss': 2.0774, 'learning_rate': 0.0001991983730282151, 'epoch': 0.09}         \n",
      "{'loss': 2.2979, 'learning_rate': 0.00019919797938426477, 'epoch': 0.09}        \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019919758564407663, 'epoch': 0.09}        \n",
      "{'loss': 1.975, 'learning_rate': 0.00019919719180765109, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 94883/1061708 [14:12:00<144:51:32,  1.85it/s][2024-03-01 08:19:01,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3995, 'learning_rate': 0.00019919683727258542, 'epoch': 0.09}        \n",
      "{'loss': 1.8692, 'learning_rate': 0.00019919644325330982, 'epoch': 0.09}        \n",
      "{'loss': 1.7708, 'learning_rate': 0.0001991960491377979, 'epoch': 0.09}         \n",
      "{'loss': 2.0001, 'learning_rate': 0.00019919565492605007, 'epoch': 0.09}        \n",
      "{'loss': 2.2684, 'learning_rate': 0.00019919526061806667, 'epoch': 0.09}        \n",
      "{'loss': 2.248, 'learning_rate': 0.00019919486621384813, 'epoch': 0.09}         \n",
      "{'loss': 2.1384, 'learning_rate': 0.0001991944717133948, 'epoch': 0.09}         \n",
      "{'loss': 2.1838, 'learning_rate': 0.00019919407711670707, 'epoch': 0.09}        \n",
      "{'loss': 2.1254, 'learning_rate': 0.0001991936824237853, 'epoch': 0.09}         \n",
      "{'loss': 1.587, 'learning_rate': 0.00019919328763462992, 'epoch': 0.09}         \n",
      "{'loss': 2.3941, 'learning_rate': 0.00019919289274924129, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 94990/1061708 [14:12:57<142:28:21,  1.88it/s][2024-03-01 08:19:58,008] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.114, 'learning_rate': 0.0001991925372701124, 'epoch': 0.09}          \n",
      "  9%|██▌                         | 95000/1061708 [14:13:02<142:19:47,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 08:20:02,822 >> Saving model checkpoint to output_model/checkpoint-95000\n",
      "[INFO|trainer.py:2880] 2024-03-01 08:20:02,825 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 08:20:04,046 >> tokenizer config file saved in output_model/checkpoint-95000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 08:20:04,047 >> Special tokens file saved in output_model/checkpoint-95000/special_tokens_map.json\n",
      "[2024-03-01 08:20:04,048] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step95000 is about to be saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-01 08:20:09,280] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-95000/global_step95000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 08:20:09,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-95000/global_step95000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 08:20:23,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-95000/global_step95000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 08:20:24,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-95000/global_step95000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 08:20:31,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-95000/global_step95000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 08:20:31,333] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-95000/global_step95000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 08:20:31,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step95000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 08:20:31,417 >> Deleting older checkpoint [output_model/checkpoint-80000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 08:20:35,202 >> tokenizer config file saved in output_model/checkpoint-95000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 08:20:35,202 >> Special tokens file saved in output_model/checkpoint-95000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.3473, 'learning_rate': 0.00019919214220188164, 'epoch': 0.09}        \n",
      "{'loss': 1.8924, 'learning_rate': 0.00019919174703741875, 'epoch': 0.09}        \n",
      "{'loss': 2.0642, 'learning_rate': 0.0001991913517767241, 'epoch': 0.09}         \n",
      "{'loss': 1.8576, 'learning_rate': 0.0001991909564197981, 'epoch': 0.09}         \n",
      "{'loss': 2.2282, 'learning_rate': 0.00019919056096664108, 'epoch': 0.09}        \n",
      "{'loss': 2.2, 'learning_rate': 0.00019919016541725343, 'epoch': 0.09}           \n",
      "{'loss': 1.8765, 'learning_rate': 0.00019918976977163563, 'epoch': 0.09}        \n",
      "{'loss': 2.3187, 'learning_rate': 0.00019918937402978792, 'epoch': 0.09}        \n",
      "{'loss': 1.7272, 'learning_rate': 0.0001991889781917108, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 95091/1061708 [14:14:23<142:14:49,  1.89it/s][2024-03-01 08:21:23,931] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 95092/1061708 [14:14:23<133:37:13,  2.01it/s][2024-03-01 08:21:24,355] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9029, 'learning_rate': 0.00019918866145196416, 'epoch': 0.09}        \n",
      "{'loss': 1.8722, 'learning_rate': 0.00019918826544067497, 'epoch': 0.09}        \n",
      "{'loss': 2.1894, 'learning_rate': 0.0001991878693331574, 'epoch': 0.09}         \n",
      "{'loss': 2.3161, 'learning_rate': 0.00019918747312941184, 'epoch': 0.09}        \n",
      "{'loss': 2.375, 'learning_rate': 0.0001991870768294387, 'epoch': 0.09}          \n",
      "{'loss': 2.2119, 'learning_rate': 0.0001991866804332383, 'epoch': 0.09}         \n",
      "{'loss': 1.847, 'learning_rate': 0.00019918628394081106, 'epoch': 0.09}         \n",
      "{'loss': 2.156, 'learning_rate': 0.0001991858873521574, 'epoch': 0.09}          \n",
      "{'loss': 2.0577, 'learning_rate': 0.00019918549066727764, 'epoch': 0.09}        \n",
      "{'loss': 2.1265, 'learning_rate': 0.00019918509388617216, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95193/1061708 [14:15:17<145:52:58,  1.84it/s][2024-03-01 08:22:18,338] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 95194/1061708 [14:15:18<136:28:13,  1.97it/s][2024-03-01 08:22:18,763] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3006, 'learning_rate': 0.0001991847763920056, 'epoch': 0.09}         \n",
      "{'loss': 1.8401, 'learning_rate': 0.0001991843794376949, 'epoch': 0.09}         \n",
      "{'loss': 1.6713, 'learning_rate': 0.00019918398238715957, 'epoch': 0.09}        \n",
      "{'loss': 2.0914, 'learning_rate': 0.00019918358524040003, 'epoch': 0.09}        \n",
      "{'loss': 2.2952, 'learning_rate': 0.00019918318799741665, 'epoch': 0.09}        \n",
      "{'loss': 2.19, 'learning_rate': 0.0001991827906582098, 'epoch': 0.09}           \n",
      "{'loss': 1.9016, 'learning_rate': 0.00019918239322277992, 'epoch': 0.09}        \n",
      "{'loss': 2.2771, 'learning_rate': 0.00019918199569112736, 'epoch': 0.09}        \n",
      "{'loss': 2.2131, 'learning_rate': 0.00019918159806325245, 'epoch': 0.09}        \n",
      "{'loss': 1.5729, 'learning_rate': 0.00019918120033915568, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95295/1061708 [14:16:12<144:07:24,  1.86it/s][2024-03-01 08:23:12,715] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 95296/1061708 [14:16:12<134:52:32,  1.99it/s][2024-03-01 08:23:13,138] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9004, 'learning_rate': 0.00019918088209059875, 'epoch': 0.09}        \n",
      "{'loss': 2.1668, 'learning_rate': 0.00019918048419330348, 'epoch': 0.09}        \n",
      "{'loss': 1.7511, 'learning_rate': 0.0001991800861997874, 'epoch': 0.09}         \n",
      "{'loss': 2.1435, 'learning_rate': 0.00019917968811005088, 'epoch': 0.09}        \n",
      "{'loss': 2.2095, 'learning_rate': 0.0001991792899240943, 'epoch': 0.09}         \n",
      "{'loss': 1.9089, 'learning_rate': 0.00019917889164191806, 'epoch': 0.09}        \n",
      "{'loss': 1.8215, 'learning_rate': 0.00019917849326352253, 'epoch': 0.09}        \n",
      "{'loss': 2.024, 'learning_rate': 0.00019917809478890808, 'epoch': 0.09}         \n",
      "{'loss': 2.1426, 'learning_rate': 0.00019917769621807514, 'epoch': 0.09}        \n",
      "{'loss': 2.2639, 'learning_rate': 0.00019917729755102408, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95397/1061708 [14:17:06<142:31:49,  1.88it/s][2024-03-01 08:24:06,911] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 95398/1061708 [14:17:06<133:39:22,  2.01it/s][2024-03-01 08:24:07,333] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2753, 'learning_rate': 0.00019917697854810641, 'epoch': 0.09}        \n",
      "{'loss': 1.7541, 'learning_rate': 0.0001991765797078637, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 95416/1061708 [14:17:16<142:56:42,  1.88it/s][2024-03-01 08:24:16,833] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9914, 'learning_rate': 0.00019917622066937968, 'epoch': 0.09}        \n",
      "{'loss': 2.4733, 'learning_rate': 0.0001991758216463249, 'epoch': 0.09}         \n",
      "{'loss': 2.142, 'learning_rate': 0.00019917542252705382, 'epoch': 0.09}         \n",
      "{'loss': 1.9813, 'learning_rate': 0.00019917502331156685, 'epoch': 0.09}        \n",
      "{'loss': 1.9198, 'learning_rate': 0.00019917462399986433, 'epoch': 0.09}        \n",
      "{'loss': 2.3148, 'learning_rate': 0.00019917422459194664, 'epoch': 0.09}        \n",
      "{'loss': 1.9134, 'learning_rate': 0.0001991738250878142, 'epoch': 0.09}         \n",
      "{'loss': 2.1118, 'learning_rate': 0.00019917342548746744, 'epoch': 0.09}        \n",
      "{'loss': 1.8385, 'learning_rate': 0.00019917302579090665, 'epoch': 0.09}        \n",
      "{'loss': 2.3667, 'learning_rate': 0.0001991726259981323, 'epoch': 0.09}         \n",
      "{'loss': 1.9744, 'learning_rate': 0.0001991722261091447, 'epoch': 0.09}         \n",
      "{'loss': 1.9051, 'learning_rate': 0.0001991718261239443, 'epoch': 0.09}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0866, 'learning_rate': 0.00019917142604253152, 'epoch': 0.09}        \n",
      "{'loss': 1.6043, 'learning_rate': 0.00019917102586490665, 'epoch': 0.09}        \n",
      "{'loss': 1.9631, 'learning_rate': 0.00019917062559107014, 'epoch': 0.09}        \n",
      "{'loss': 2.1448, 'learning_rate': 0.00019917022522102237, 'epoch': 0.09}        \n",
      "{'loss': 1.8459, 'learning_rate': 0.0001991698247547637, 'epoch': 0.09}         \n",
      "{'loss': 2.2901, 'learning_rate': 0.00019916942419229456, 'epoch': 0.09}        \n",
      "{'loss': 2.6036, 'learning_rate': 0.0001991690235336153, 'epoch': 0.09}         \n",
      "{'loss': 2.4275, 'learning_rate': 0.00019916862277872635, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95617/1061708 [14:19:03<142:57:45,  1.88it/s][2024-03-01 08:26:03,830] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 95618/1061708 [14:19:03<134:04:20,  2.00it/s][2024-03-01 08:26:04,255] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9829, 'learning_rate': 0.00019916830210554447, 'epoch': 0.09}        \n",
      "{'loss': 1.8756, 'learning_rate': 0.000199167901177479, 'epoch': 0.09}          \n",
      "{'loss': 2.1074, 'learning_rate': 0.00019916750015320492, 'epoch': 0.09}        \n",
      "{'loss': 1.878, 'learning_rate': 0.00019916709903272263, 'epoch': 0.09}         \n",
      "{'loss': 2.1999, 'learning_rate': 0.00019916669781603247, 'epoch': 0.09}        \n",
      "{'loss': 2.1375, 'learning_rate': 0.00019916629650313484, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95672/1061708 [14:19:32<142:22:43,  1.88it/s][2024-03-01 08:26:33,006] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3531, 'learning_rate': 0.00019916593523926993, 'epoch': 0.09}        \n",
      "{'loss': 2.2565, 'learning_rate': 0.00019916553374357924, 'epoch': 0.09}        \n",
      "{'loss': 1.6956, 'learning_rate': 0.0001991651321516822, 'epoch': 0.09}         \n",
      "{'loss': 1.8101, 'learning_rate': 0.00019916473046357926, 'epoch': 0.09}        \n",
      "{'loss': 2.3726, 'learning_rate': 0.00019916432867927078, 'epoch': 0.09}        \n",
      "{'loss': 1.938, 'learning_rate': 0.0001991639267987571, 'epoch': 0.09}          \n",
      "{'loss': 1.676, 'learning_rate': 0.0001991635248220387, 'epoch': 0.09}          \n",
      "{'loss': 2.0656, 'learning_rate': 0.0001991631227491159, 'epoch': 0.09}         \n",
      "{'loss': 1.7003, 'learning_rate': 0.00019916272057998908, 'epoch': 0.09}        \n",
      "{'loss': 2.0075, 'learning_rate': 0.0001991623183146587, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 95777/1061708 [14:20:28<142:53:37,  1.88it/s][2024-03-01 08:27:28,973] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0077, 'learning_rate': 0.0001991619561936076, 'epoch': 0.09}         \n",
      "{'loss': 2.2792, 'learning_rate': 0.0001991615537454915, 'epoch': 0.09}         \n",
      "{'loss': 2.1672, 'learning_rate': 0.00019916115120117284, 'epoch': 0.09}        \n",
      "{'loss': 2.1331, 'learning_rate': 0.0001991607485606522, 'epoch': 0.09}         \n",
      "{'loss': 1.9943, 'learning_rate': 0.0001991603458239298, 'epoch': 0.09}         \n",
      "{'loss': 2.0989, 'learning_rate': 0.00019915994299100619, 'epoch': 0.09}        \n",
      "{'loss': 1.8393, 'learning_rate': 0.00019915954006188163, 'epoch': 0.09}        \n",
      "{'loss': 2.6174, 'learning_rate': 0.00019915913703655655, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 95855/1061708 [14:21:10<143:52:26,  1.86it/s][2024-03-01 08:28:10,556] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9234, 'learning_rate': 0.0001991587742315129, 'epoch': 0.09}         \n",
      "{'loss': 2.0611, 'learning_rate': 0.0001991583710234079, 'epoch': 0.09}         \n",
      "{'loss': 2.0559, 'learning_rate': 0.0001991579677191036, 'epoch': 0.09}         \n",
      "{'loss': 1.9887, 'learning_rate': 0.00019915756431860028, 'epoch': 0.09}        \n",
      "{'loss': 2.3305, 'learning_rate': 0.00019915716082189838, 'epoch': 0.09}        \n",
      "{'loss': 1.9103, 'learning_rate': 0.00019915675722899826, 'epoch': 0.09}        \n",
      "{'loss': 2.01, 'learning_rate': 0.00019915635353990032, 'epoch': 0.09}          \n",
      "{'loss': 2.0868, 'learning_rate': 0.000199155949754605, 'epoch': 0.09}          \n",
      "{'loss': 2.311, 'learning_rate': 0.0001991555458731126, 'epoch': 0.09}          \n",
      "{'loss': 2.0942, 'learning_rate': 0.00019915514189542362, 'epoch': 0.09}        \n",
      "{'loss': 2.0219, 'learning_rate': 0.00019915473782153837, 'epoch': 0.09}        \n",
      "{'loss': 2.4522, 'learning_rate': 0.00019915433365145726, 'epoch': 0.09}        \n",
      "{'loss': 2.1544, 'learning_rate': 0.0001991539293851807, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 95986/1061708 [14:22:19<143:13:44,  1.87it/s][2024-03-01 08:29:20,430] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0562, 'learning_rate': 0.000199153565463285, 'epoch': 0.09}          \n",
      "  9%|██▌                         | 95992/1061708 [14:22:23<142:51:38,  1.88it/s][2024-03-01 08:29:23,594] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  9%|██▌                         | 95999/1061708 [14:22:26<142:42:48,  1.88it/s][2024-03-01 08:29:27,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=96000, skipped=1103, lr=[0.0001991532014634716], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 08:29:27,349] [INFO] [timer.py:260:stop] epoch=0/micro_step=96000/global_step=96000, RunningAvgSamplesPerSec=1.8924430864008874, CurrSamplesPerSec=1.8920670665252004, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.6727, 'learning_rate': 0.0001991532014634716, 'epoch': 0.09}         \n",
      "{'loss': 2.2816, 'learning_rate': 0.00019915279692784982, 'epoch': 0.09}        \n",
      "{'loss': 2.1919, 'learning_rate': 0.0001991523922960341, 'epoch': 0.09}         \n",
      "{'loss': 1.9382, 'learning_rate': 0.0001991519875680248, 'epoch': 0.09}         \n",
      "{'loss': 2.3395, 'learning_rate': 0.0001991515827438223, 'epoch': 0.09}         \n",
      "{'loss': 1.7689, 'learning_rate': 0.00019915117782342705, 'epoch': 0.09}        \n",
      "{'loss': 2.0069, 'learning_rate': 0.00019915077280683937, 'epoch': 0.09}        \n",
      "{'loss': 2.2454, 'learning_rate': 0.00019915036769405968, 'epoch': 0.09}        \n",
      "{'loss': 2.1124, 'learning_rate': 0.0001991499624850884, 'epoch': 0.09}         \n",
      "{'loss': 2.1419, 'learning_rate': 0.0001991495571799259, 'epoch': 0.09}         \n",
      "{'loss': 2.1858, 'learning_rate': 0.00019914915177857257, 'epoch': 0.09}        \n",
      "{'loss': 2.3187, 'learning_rate': 0.0001991487462810288, 'epoch': 0.09}         \n",
      "{'loss': 1.9097, 'learning_rate': 0.00019914834068729498, 'epoch': 0.09}        \n",
      "{'loss': 2.1343, 'learning_rate': 0.00019914793499737154, 'epoch': 0.09}        \n",
      "{'loss': 2.1175, 'learning_rate': 0.00019914752921125882, 'epoch': 0.09}        \n",
      "{'loss': 2.0607, 'learning_rate': 0.00019914712332895729, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96153/1061708 [14:23:48<145:28:15,  1.84it/s][2024-03-01 08:30:49,500] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0765, 'learning_rate': 0.00019914675795264475, 'epoch': 0.09}        \n",
      "{'loss': 1.8626, 'learning_rate': 0.00019914635188758543, 'epoch': 0.09}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.0001991459457263384, 'epoch': 0.09}         \n",
      "{'loss': 2.0256, 'learning_rate': 0.00019914553946890408, 'epoch': 0.09}        \n",
      "{'loss': 1.8542, 'learning_rate': 0.0001991451331152828, 'epoch': 0.09}         \n",
      "{'loss': 2.1974, 'learning_rate': 0.000199144726665475, 'epoch': 0.09}          \n",
      "{'loss': 2.2079, 'learning_rate': 0.00019914432011948106, 'epoch': 0.09}        \n",
      "{'loss': 2.3648, 'learning_rate': 0.00019914391347730138, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8566, 'learning_rate': 0.00019914350673893636, 'epoch': 0.09}        \n",
      "{'loss': 2.3856, 'learning_rate': 0.00019914309990438635, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96254/1061708 [14:24:42<144:28:05,  1.86it/s][2024-03-01 08:31:43,374] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 96255/1061708 [14:24:43<135:11:20,  1.98it/s][2024-03-01 08:31:43,799] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8125, 'learning_rate': 0.00019914277436749347, 'epoch': 0.09}        \n",
      "{'loss': 2.287, 'learning_rate': 0.00019914236735981154, 'epoch': 0.09}         \n",
      "{'loss': 2.2511, 'learning_rate': 0.0001991419602559458, 'epoch': 0.09}         \n",
      "{'loss': 2.143, 'learning_rate': 0.00019914155305589658, 'epoch': 0.09}         \n",
      "{'loss': 1.7726, 'learning_rate': 0.00019914114575966428, 'epoch': 0.09}        \n",
      "{'loss': 2.3872, 'learning_rate': 0.00019914073836724933, 'epoch': 0.09}        \n",
      "{'loss': 2.181, 'learning_rate': 0.00019914033087865214, 'epoch': 0.09}         \n",
      "{'loss': 2.0684, 'learning_rate': 0.00019913992329387306, 'epoch': 0.09}        \n",
      "{'loss': 2.0857, 'learning_rate': 0.00019913951561291245, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96346/1061708 [14:25:31<143:08:24,  1.87it/s][2024-03-01 08:32:32,316] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0014, 'learning_rate': 0.00019913914861781313, 'epoch': 0.09}        \n",
      "{'loss': 2.4906, 'learning_rate': 0.00019913874075410884, 'epoch': 0.09}        \n",
      "{'loss': 2.3324, 'learning_rate': 0.00019913833279422422, 'epoch': 0.09}        \n",
      "{'loss': 2.2902, 'learning_rate': 0.00019913792473815966, 'epoch': 0.09}        \n",
      "{'loss': 1.9071, 'learning_rate': 0.00019913751658591554, 'epoch': 0.09}        \n",
      "{'loss': 2.0116, 'learning_rate': 0.00019913710833749233, 'epoch': 0.09}        \n",
      "{'loss': 2.314, 'learning_rate': 0.00019913669999289035, 'epoch': 0.09}         \n",
      "{'loss': 1.8043, 'learning_rate': 0.00019913629155211, 'epoch': 0.09}           \n",
      "{'loss': 1.9943, 'learning_rate': 0.0001991358830151517, 'epoch': 0.09}         \n",
      "{'loss': 1.7871, 'learning_rate': 0.00019913547438201584, 'epoch': 0.09}        \n",
      "{'loss': 2.2081, 'learning_rate': 0.00019913506565270283, 'epoch': 0.09}        \n",
      "{'loss': 2.2175, 'learning_rate': 0.00019913465682721307, 'epoch': 0.09}        \n",
      "{'loss': 2.4602, 'learning_rate': 0.0001991342479055469, 'epoch': 0.09}         \n",
      "{'loss': 1.8204, 'learning_rate': 0.0001991338388877048, 'epoch': 0.09}         \n",
      "{'loss': 2.0796, 'learning_rate': 0.0001991334297736871, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 96499/1061708 [14:26:53<142:27:46,  1.88it/s][2024-03-01 08:33:53,985] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8565, 'learning_rate': 0.00019913306148884135, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96505/1061708 [14:26:56<142:39:36,  1.88it/s][2024-03-01 08:33:57,111] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2024, 'learning_rate': 0.00019913269312609404, 'epoch': 0.09}        \n",
      "{'loss': 2.0941, 'learning_rate': 0.00019913228374278684, 'epoch': 0.09}        \n",
      "{'loss': 2.4141, 'learning_rate': 0.00019913187426330556, 'epoch': 0.09}        \n",
      "{'loss': 1.5625, 'learning_rate': 0.00019913146468765062, 'epoch': 0.09}        \n",
      "{'loss': 2.2503, 'learning_rate': 0.0001991310550158224, 'epoch': 0.09}         \n",
      "{'loss': 1.6702, 'learning_rate': 0.00019913064524782132, 'epoch': 0.09}        \n",
      "{'loss': 2.0335, 'learning_rate': 0.00019913023538364777, 'epoch': 0.09}        \n",
      "{'loss': 2.0503, 'learning_rate': 0.00019912982542330212, 'epoch': 0.09}        \n",
      "{'loss': 2.1501, 'learning_rate': 0.0001991294153667848, 'epoch': 0.09}         \n",
      "{'loss': 2.3486, 'learning_rate': 0.00019912900521409617, 'epoch': 0.09}        \n",
      "{'loss': 1.8894, 'learning_rate': 0.00019912859496523669, 'epoch': 0.09}        \n",
      "{'loss': 2.1927, 'learning_rate': 0.0001991281846202067, 'epoch': 0.09}         \n",
      "{'loss': 1.8665, 'learning_rate': 0.0001991277741790066, 'epoch': 0.09}         \n",
      "{'loss': 1.7556, 'learning_rate': 0.00019912736364163686, 'epoch': 0.09}        \n",
      "{'loss': 2.2761, 'learning_rate': 0.00019912695300809777, 'epoch': 0.09}        \n",
      "{'loss': 2.011, 'learning_rate': 0.00019912654227838983, 'epoch': 0.09}         \n",
      "{'loss': 2.0438, 'learning_rate': 0.0001991261314525134, 'epoch': 0.09}         \n",
      "{'loss': 2.0275, 'learning_rate': 0.00019912572053046881, 'epoch': 0.09}        \n",
      "{'loss': 2.0673, 'learning_rate': 0.00019912530951225657, 'epoch': 0.09}        \n",
      "{'loss': 1.9244, 'learning_rate': 0.000199124898397877, 'epoch': 0.09}          \n",
      "  9%|██▌                         | 96706/1061708 [14:28:43<143:03:31,  1.87it/s][2024-03-01 08:35:44,333] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 96707/1061708 [14:28:44<134:08:18,  2.00it/s][2024-03-01 08:35:44,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.786, 'learning_rate': 0.00019912456943713317, 'epoch': 0.09}         \n",
      "{'loss': 2.0164, 'learning_rate': 0.0001991241581496535, 'epoch': 0.09}         \n",
      "{'loss': 2.1396, 'learning_rate': 0.0001991237467660076, 'epoch': 0.09}         \n",
      "{'loss': 2.28, 'learning_rate': 0.00019912333528619597, 'epoch': 0.09}          \n",
      "{'loss': 1.779, 'learning_rate': 0.0001991229237102189, 'epoch': 0.09}          \n",
      "{'loss': 2.3245, 'learning_rate': 0.00019912251203807685, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96766/1061708 [14:29:15<143:02:01,  1.87it/s][2024-03-01 08:36:16,175] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3256, 'learning_rate': 0.0001991221414509283, 'epoch': 0.09}         \n",
      "{'loss': 2.1133, 'learning_rate': 0.0001991217295960739, 'epoch': 0.09}         \n",
      "{'loss': 1.9819, 'learning_rate': 0.00019912131764505565, 'epoch': 0.09}        \n",
      "{'loss': 2.1595, 'learning_rate': 0.00019912090559787398, 'epoch': 0.09}        \n",
      "{'loss': 2.1632, 'learning_rate': 0.0001991204934545293, 'epoch': 0.09}         \n",
      "{'loss': 2.2823, 'learning_rate': 0.00019912008121502196, 'epoch': 0.09}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.0001991196688793524, 'epoch': 0.09}         \n",
      "{'loss': 2.2703, 'learning_rate': 0.00019911925644752103, 'epoch': 0.09}        \n",
      "{'loss': 2.1358, 'learning_rate': 0.00019911884391952823, 'epoch': 0.09}        \n",
      "{'loss': 1.8937, 'learning_rate': 0.0001991184312953744, 'epoch': 0.09}         \n",
      "{'loss': 1.8726, 'learning_rate': 0.00019911801857505992, 'epoch': 0.09}        \n",
      "{'loss': 2.1928, 'learning_rate': 0.00019911760575858523, 'epoch': 0.09}        \n",
      "{'loss': 2.5291, 'learning_rate': 0.00019911719284595073, 'epoch': 0.09}        \n",
      "{'loss': 2.1354, 'learning_rate': 0.00019911677983715676, 'epoch': 0.09}        \n",
      "{'loss': 2.3663, 'learning_rate': 0.0001991163667322038, 'epoch': 0.09}         \n",
      "{'loss': 1.9406, 'learning_rate': 0.00019911595353109217, 'epoch': 0.09}        \n",
      "{'loss': 1.8725, 'learning_rate': 0.00019911554023382237, 'epoch': 0.09}        \n",
      "{'loss': 2.2904, 'learning_rate': 0.0001991151268403947, 'epoch': 0.09}         \n",
      "{'loss': 2.3797, 'learning_rate': 0.00019911471335080965, 'epoch': 0.09}        \n",
      "{'loss': 1.8764, 'learning_rate': 0.00019911429976506755, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96967/1061708 [14:31:02<142:24:04,  1.88it/s][2024-03-01 08:38:03,142] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9%|██▌                         | 96968/1061708 [14:31:03<133:31:50,  2.01it/s][2024-03-01 08:38:03,564] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7756, 'learning_rate': 0.0001991139688272411, 'epoch': 0.09}         \n",
      "{'loss': 1.8643, 'learning_rate': 0.00019911355506841737, 'epoch': 0.09}        \n",
      "{'loss': 2.3792, 'learning_rate': 0.00019911314121343776, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 96999/1061708 [14:31:19<141:49:05,  1.89it/s][2024-03-01 08:38:19,962] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4312, 'learning_rate': 0.00019911276866174316, 'epoch': 0.09}        \n",
      "{'loss': 1.9467, 'learning_rate': 0.00019911235462406843, 'epoch': 0.09}        \n",
      "{'loss': 2.0102, 'learning_rate': 0.000199111940490239, 'epoch': 0.09}          \n",
      "{'loss': 2.2477, 'learning_rate': 0.0001991115262602552, 'epoch': 0.09}         \n",
      "{'loss': 2.2336, 'learning_rate': 0.0001991111119341175, 'epoch': 0.09}         \n",
      "{'loss': 2.0068, 'learning_rate': 0.00019911069751182627, 'epoch': 0.09}        \n",
      "{'loss': 1.9289, 'learning_rate': 0.0001991102829933819, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 97065/1061708 [14:31:54<144:36:14,  1.85it/s][2024-03-01 08:38:55,034] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2285, 'learning_rate': 0.0001991099098445714, 'epoch': 0.09}         \n",
      "{'loss': 2.2651, 'learning_rate': 0.00019910949514343723, 'epoch': 0.09}        \n",
      "{'loss': 1.8126, 'learning_rate': 0.00019910908034615106, 'epoch': 0.09}        \n",
      "{'loss': 1.9736, 'learning_rate': 0.00019910866545271336, 'epoch': 0.09}        \n",
      "{'loss': 2.495, 'learning_rate': 0.0001991082504631245, 'epoch': 0.09}          \n",
      "{'loss': 1.9976, 'learning_rate': 0.00019910783537738488, 'epoch': 0.09}        \n",
      "{'loss': 2.1734, 'learning_rate': 0.0001991074201954949, 'epoch': 0.09}         \n",
      "{'loss': 1.9775, 'learning_rate': 0.00019910700491745502, 'epoch': 0.09}        \n",
      "{'loss': 2.2219, 'learning_rate': 0.00019910658954326558, 'epoch': 0.09}        \n",
      "{'loss': 1.8102, 'learning_rate': 0.00019910617407292697, 'epoch': 0.09}        \n",
      "{'loss': 2.3271, 'learning_rate': 0.00019910575850643966, 'epoch': 0.09}        \n",
      "{'loss': 2.1882, 'learning_rate': 0.00019910534284380403, 'epoch': 0.09}        \n",
      "{'loss': 1.9114, 'learning_rate': 0.00019910492708502043, 'epoch': 0.09}        \n",
      "{'loss': 2.0379, 'learning_rate': 0.00019910451123008932, 'epoch': 0.09}        \n",
      "{'loss': 1.7211, 'learning_rate': 0.00019910409527901105, 'epoch': 0.09}        \n",
      "{'loss': 2.0658, 'learning_rate': 0.0001991036792317861, 'epoch': 0.09}         \n",
      "{'loss': 1.9705, 'learning_rate': 0.00019910326308841483, 'epoch': 0.09}        \n",
      "{'loss': 1.7977, 'learning_rate': 0.00019910284684889767, 'epoch': 0.09}        \n",
      "{'loss': 2.2426, 'learning_rate': 0.00019910243051323497, 'epoch': 0.09}        \n",
      "{'loss': 2.1856, 'learning_rate': 0.00019910201408142715, 'epoch': 0.09}        \n",
      "{'loss': 1.6969, 'learning_rate': 0.00019910159755347464, 'epoch': 0.09}        \n",
      "{'loss': 2.132, 'learning_rate': 0.00019910118092937787, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 97284/1061708 [14:33:50<143:48:37,  1.86it/s][2024-03-01 08:40:51,527] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2047, 'learning_rate': 0.0001991008058854877, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 97292/1061708 [14:33:55<143:12:24,  1.87it/s][2024-03-01 08:40:55,766] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1622, 'learning_rate': 0.00019910043076372128, 'epoch': 0.09}        \n",
      "{'loss': 1.9563, 'learning_rate': 0.00019910001387042262, 'epoch': 0.09}        \n",
      "{'loss': 1.975, 'learning_rate': 0.00019909959688098114, 'epoch': 0.09}         \n",
      "{'loss': 1.6348, 'learning_rate': 0.00019909917979539737, 'epoch': 0.09}        \n",
      "{'loss': 2.0924, 'learning_rate': 0.00019909876261367163, 'epoch': 0.09}        \n",
      "{'loss': 2.0408, 'learning_rate': 0.00019909834533580432, 'epoch': 0.09}        \n",
      "{'loss': 1.7762, 'learning_rate': 0.0001990979279617959, 'epoch': 0.09}         \n",
      "{'loss': 2.2837, 'learning_rate': 0.00019909751049164672, 'epoch': 0.09}        \n",
      "{'loss': 1.7226, 'learning_rate': 0.0001990970929253572, 'epoch': 0.09}         \n",
      "{'loss': 2.2396, 'learning_rate': 0.0001990966752629278, 'epoch': 0.09}         \n",
      "{'loss': 2.2218, 'learning_rate': 0.00019909625750435883, 'epoch': 0.09}        \n",
      "{'loss': 2.4795, 'learning_rate': 0.00019909583964965077, 'epoch': 0.09}        \n",
      "{'loss': 2.1253, 'learning_rate': 0.00019909542169880399, 'epoch': 0.09}        \n",
      "{'loss': 2.202, 'learning_rate': 0.0001990950036518189, 'epoch': 0.09}          \n",
      "{'loss': 2.5743, 'learning_rate': 0.00019909458550869592, 'epoch': 0.09}        \n",
      "{'loss': 2.0422, 'learning_rate': 0.00019909416726943545, 'epoch': 0.09}        \n",
      "{'loss': 2.296, 'learning_rate': 0.00019909374893403787, 'epoch': 0.09}         \n",
      "{'loss': 1.9944, 'learning_rate': 0.00019909333050250366, 'epoch': 0.09}        \n",
      "{'loss': 1.8223, 'learning_rate': 0.0001990929119748331, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 97481/1061708 [14:35:35<142:14:55,  1.88it/s][2024-03-01 08:42:36,352] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0955, 'learning_rate': 0.00019909253521773347, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 97492/1061708 [14:35:41<144:56:04,  1.85it/s][2024-03-01 08:42:42,231] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9588, 'learning_rate': 0.00019909215838276404, 'epoch': 0.09}        \n",
      "{'loss': 2.0075, 'learning_rate': 0.0001990917395859141, 'epoch': 0.09}         \n",
      "{'loss': 2.0127, 'learning_rate': 0.00019909132069292944, 'epoch': 0.09}        \n",
      "{'loss': 2.03, 'learning_rate': 0.00019909090170381043, 'epoch': 0.09}          \n",
      "{'loss': 1.9429, 'learning_rate': 0.00019909048261855752, 'epoch': 0.09}        \n",
      "{'loss': 2.1413, 'learning_rate': 0.0001990900634371711, 'epoch': 0.09}         \n",
      "{'loss': 2.0839, 'learning_rate': 0.00019908964415965157, 'epoch': 0.09}        \n",
      "{'loss': 2.383, 'learning_rate': 0.0001990892247859993, 'epoch': 0.09}          \n",
      "{'loss': 2.245, 'learning_rate': 0.0001990888053162148, 'epoch': 0.09}          \n",
      "{'loss': 2.0909, 'learning_rate': 0.00019908838575029836, 'epoch': 0.09}        \n",
      "{'loss': 2.2526, 'learning_rate': 0.00019908796608825047, 'epoch': 0.09}        \n",
      "{'loss': 2.3223, 'learning_rate': 0.00019908754633007151, 'epoch': 0.09}        \n",
      "{'loss': 2.0468, 'learning_rate': 0.00019908712647576186, 'epoch': 0.09}        \n",
      "{'loss': 2.0567, 'learning_rate': 0.00019908670652532198, 'epoch': 0.09}        \n",
      "{'loss': 2.094, 'learning_rate': 0.00019908628647875224, 'epoch': 0.09}         \n",
      "{'loss': 2.5228, 'learning_rate': 0.00019908586633605305, 'epoch': 0.09}        \n",
      "{'loss': 2.2949, 'learning_rate': 0.00019908544609722483, 'epoch': 0.09}        \n",
      "{'loss': 2.1624, 'learning_rate': 0.00019908502576226797, 'epoch': 0.09}        \n",
      "{'loss': 2.0965, 'learning_rate': 0.00019908460533118292, 'epoch': 0.09}        \n",
      "{'loss': 1.8395, 'learning_rate': 0.00019908418480397004, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 97691/1061708 [14:37:27<141:44:17,  1.89it/s][2024-03-01 08:44:28,239] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9757, 'learning_rate': 0.00019908380624728952, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 97703/1061708 [14:37:34<145:03:16,  1.85it/s][2024-03-01 08:44:34,569] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7041, 'learning_rate': 0.00019908342761274607, 'epoch': 0.09}        \n",
      "{'loss': 1.9264, 'learning_rate': 0.0001990830068163775, 'epoch': 0.09}         \n",
      "{'loss': 2.0261, 'learning_rate': 0.00019908258592388266, 'epoch': 0.09}        \n",
      "{'loss': 1.784, 'learning_rate': 0.00019908216493526198, 'epoch': 0.09}         \n",
      "{'loss': 1.9889, 'learning_rate': 0.00019908174385051584, 'epoch': 0.09}        \n",
      "{'loss': 1.9594, 'learning_rate': 0.00019908132266964465, 'epoch': 0.09}        \n",
      "{'loss': 2.4167, 'learning_rate': 0.00019908090139264886, 'epoch': 0.09}        \n",
      "{'loss': 1.7849, 'learning_rate': 0.00019908048001952884, 'epoch': 0.09}        \n",
      "{'loss': 2.3714, 'learning_rate': 0.000199080058550285, 'epoch': 0.09}          \n",
      "{'loss': 2.204, 'learning_rate': 0.00019907963698491778, 'epoch': 0.09}         \n",
      "{'loss': 2.1899, 'learning_rate': 0.00019907921532342755, 'epoch': 0.09}        \n",
      "{'loss': 2.2482, 'learning_rate': 0.00019907879356581475, 'epoch': 0.09}        \n",
      "{'loss': 1.9244, 'learning_rate': 0.00019907837171207978, 'epoch': 0.09}        \n",
      "{'loss': 2.1219, 'learning_rate': 0.00019907794976222306, 'epoch': 0.09}        \n",
      "{'loss': 2.2941, 'learning_rate': 0.00019907752771624495, 'epoch': 0.09}        \n",
      "{'loss': 1.9695, 'learning_rate': 0.00019907710557414592, 'epoch': 0.09}        \n",
      "{'loss': 1.7872, 'learning_rate': 0.00019907668333592633, 'epoch': 0.09}        \n",
      "{'loss': 2.3175, 'learning_rate': 0.00019907626100158665, 'epoch': 0.09}        \n",
      "{'loss': 1.9925, 'learning_rate': 0.00019907583857112722, 'epoch': 0.09}        \n",
      "{'loss': 1.925, 'learning_rate': 0.0001990754160445485, 'epoch': 0.09}          \n",
      "  9%|██▌                         | 97904/1061708 [14:39:21<144:17:18,  1.86it/s][2024-03-01 08:46:21,795] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 97905/1061708 [14:39:21<135:05:15,  1.98it/s][2024-03-01 08:46:22,219] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4762, 'learning_rate': 0.00019907507795407988, 'epoch': 0.09}        \n",
      "{'loss': 2.1354, 'learning_rate': 0.00019907465525448745, 'epoch': 0.09}        \n",
      "{'loss': 1.9811, 'learning_rate': 0.00019907423245877685, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 97938/1061708 [14:39:39<142:36:29,  1.88it/s][2024-03-01 08:46:39,777] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1538, 'learning_rate': 0.00019907385186045663, 'epoch': 0.09}        \n",
      "{'loss': 2.4098, 'learning_rate': 0.00019907342888212263, 'epoch': 0.09}        \n",
      "{'loss': 2.1849, 'learning_rate': 0.0001990730058076717, 'epoch': 0.09}         \n",
      "{'loss': 1.8615, 'learning_rate': 0.00019907258263710423, 'epoch': 0.09}        \n",
      "{'loss': 2.1447, 'learning_rate': 0.00019907215937042057, 'epoch': 0.09}        \n",
      "{'loss': 2.3768, 'learning_rate': 0.00019907173600762116, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 97999/1061708 [14:40:11<141:45:32,  1.89it/s][2024-03-01 08:47:12,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=98000, skipped=1125, lr=[0.00019907131254870647], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 08:47:12,384] [INFO] [timer.py:260:stop] epoch=0/micro_step=98000/global_step=98000, RunningAvgSamplesPerSec=1.8924628834798862, CurrSamplesPerSec=1.8950573942996973, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1259, 'learning_rate': 0.00019907131254870647, 'epoch': 0.09}        \n",
      "{'loss': 2.2548, 'learning_rate': 0.00019907088899367685, 'epoch': 0.09}        \n",
      "{'loss': 2.1323, 'learning_rate': 0.0001990704653425327, 'epoch': 0.09}         \n",
      "{'loss': 2.1935, 'learning_rate': 0.00019907004159527448, 'epoch': 0.09}        \n",
      "{'loss': 2.1568, 'learning_rate': 0.00019906961775190257, 'epoch': 0.09}        \n",
      "{'loss': 1.7306, 'learning_rate': 0.0001990691938124174, 'epoch': 0.09}         \n",
      "{'loss': 2.3323, 'learning_rate': 0.00019906876977681935, 'epoch': 0.09}        \n",
      "{'loss': 2.2609, 'learning_rate': 0.0001990683456451089, 'epoch': 0.09}         \n",
      "{'loss': 1.8918, 'learning_rate': 0.00019906792141728637, 'epoch': 0.09}        \n",
      "{'loss': 2.0459, 'learning_rate': 0.0001990674970933522, 'epoch': 0.09}         \n",
      "{'loss': 1.8954, 'learning_rate': 0.00019906707267330684, 'epoch': 0.09}        \n",
      "{'loss': 2.1917, 'learning_rate': 0.0001990666481571507, 'epoch': 0.09}         \n",
      "{'loss': 2.2726, 'learning_rate': 0.00019906622354488412, 'epoch': 0.09}        \n",
      "{'loss': 2.0565, 'learning_rate': 0.00019906579883650758, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98139/1061708 [14:41:26<142:13:26,  1.88it/s][2024-03-01 08:48:27,014] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0138, 'learning_rate': 0.00019906541651679502, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98140/1061708 [14:41:26<133:31:11,  2.00it/s][2024-03-01 08:48:27,440] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  9%|██▌                         | 98149/1061708 [14:41:31<141:23:21,  1.89it/s][2024-03-01 08:48:32,151] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2309, 'learning_rate': 0.00019906507661169624, 'epoch': 0.09}        \n",
      "{'loss': 1.9472, 'learning_rate': 0.00019906465164382483, 'epoch': 0.09}        \n",
      "{'loss': 1.5063, 'learning_rate': 0.00019906422657984496, 'epoch': 0.09}        \n",
      "{'loss': 2.0356, 'learning_rate': 0.00019906380141975706, 'epoch': 0.09}        \n",
      "{'loss': 2.0764, 'learning_rate': 0.0001990633761635615, 'epoch': 0.09}         \n",
      "{'loss': 2.1564, 'learning_rate': 0.00019906295081125873, 'epoch': 0.09}        \n",
      "{'loss': 1.8119, 'learning_rate': 0.00019906252536284918, 'epoch': 0.09}        \n",
      "{'loss': 2.0502, 'learning_rate': 0.00019906209981833325, 'epoch': 0.09}        \n",
      "{'loss': 2.3944, 'learning_rate': 0.00019906167417771132, 'epoch': 0.09}        \n",
      "{'loss': 2.2073, 'learning_rate': 0.00019906124844098382, 'epoch': 0.09}        \n",
      "{'loss': 2.2785, 'learning_rate': 0.00019906082260815117, 'epoch': 0.09}        \n",
      "{'loss': 1.9005, 'learning_rate': 0.0001990603966792138, 'epoch': 0.09}         \n",
      "{'loss': 2.1669, 'learning_rate': 0.00019905997065417208, 'epoch': 0.09}        \n",
      "{'loss': 1.6217, 'learning_rate': 0.00019905954453302646, 'epoch': 0.09}        \n",
      "{'loss': 1.9057, 'learning_rate': 0.00019905911831577733, 'epoch': 0.09}        \n",
      "{'loss': 2.4813, 'learning_rate': 0.00019905869200242512, 'epoch': 0.09}        \n",
      "{'loss': 2.0541, 'learning_rate': 0.00019905826559297026, 'epoch': 0.09}        \n",
      "{'loss': 2.3039, 'learning_rate': 0.00019905783908741312, 'epoch': 0.09}        \n",
      "{'loss': 2.0477, 'learning_rate': 0.00019905741248575415, 'epoch': 0.09}        \n",
      "{'loss': 1.9553, 'learning_rate': 0.00019905698578799371, 'epoch': 0.09}        \n",
      "{'loss': 1.7309, 'learning_rate': 0.00019905655899413231, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98350/1061708 [14:43:18<141:55:36,  1.89it/s][2024-03-01 08:50:19,394] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 98351/1061708 [14:43:19<133:19:47,  2.01it/s][2024-03-01 08:50:19,816] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  9%|██▌                         | 98353/1061708 [14:43:20<131:34:52,  2.03it/s][2024-03-01 08:50:20,775] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.092, 'learning_rate': 0.00019905626018124943, 'epoch': 0.09}         \n",
      "{'loss': 2.0673, 'learning_rate': 0.00019905583322401725, 'epoch': 0.09}        \n",
      "{'loss': 1.9049, 'learning_rate': 0.00019905540617068512, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0693, 'learning_rate': 0.00019905497902125355, 'epoch': 0.09}        \n",
      "{'loss': 1.9417, 'learning_rate': 0.00019905455177572287, 'epoch': 0.09}        \n",
      "{'loss': 2.0709, 'learning_rate': 0.00019905412443409359, 'epoch': 0.09}        \n",
      "{'loss': 2.0577, 'learning_rate': 0.000199053696996366, 'epoch': 0.09}          \n",
      "{'loss': 2.3242, 'learning_rate': 0.00019905326946254067, 'epoch': 0.09}        \n",
      "{'loss': 2.0568, 'learning_rate': 0.00019905284183261785, 'epoch': 0.09}        \n",
      "{'loss': 2.1059, 'learning_rate': 0.0001990524141065981, 'epoch': 0.09}         \n",
      "{'loss': 1.9569, 'learning_rate': 0.00019905198628448171, 'epoch': 0.09}        \n",
      "{'loss': 1.7657, 'learning_rate': 0.00019905155836626918, 'epoch': 0.09}        \n",
      "{'loss': 2.0072, 'learning_rate': 0.00019905113035196092, 'epoch': 0.09}        \n",
      "{'loss': 2.2127, 'learning_rate': 0.00019905070224155732, 'epoch': 0.09}        \n",
      "{'loss': 1.952, 'learning_rate': 0.00019905027403505876, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 98505/1061708 [14:44:41<143:21:25,  1.87it/s][2024-03-01 08:51:41,873] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.982, 'learning_rate': 0.00019904988856704926, 'epoch': 0.09}         \n",
      "{'loss': 2.4213, 'learning_rate': 0.00019904946017797152, 'epoch': 0.09}        \n",
      "{'loss': 1.8029, 'learning_rate': 0.00019904903169280007, 'epoch': 0.09}        \n",
      "{'loss': 1.8342, 'learning_rate': 0.00019904860311153533, 'epoch': 0.09}        \n",
      "{'loss': 2.4675, 'learning_rate': 0.00019904817443417767, 'epoch': 0.09}        \n",
      "{'loss': 1.7591, 'learning_rate': 0.00019904774566072757, 'epoch': 0.09}        \n",
      "{'loss': 2.1848, 'learning_rate': 0.0001990473167911854, 'epoch': 0.09}         \n",
      "{'loss': 2.0838, 'learning_rate': 0.00019904688782555163, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98583/1061708 [14:45:22<145:09:08,  1.84it/s][2024-03-01 08:52:23,475] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3685, 'learning_rate': 0.0001990465016743232, 'epoch': 0.09}         \n",
      "{'loss': 2.0607, 'learning_rate': 0.00019904607252611644, 'epoch': 0.09}        \n",
      "{'loss': 2.0002, 'learning_rate': 0.00019904564328181924, 'epoch': 0.09}        \n",
      "{'loss': 2.0512, 'learning_rate': 0.00019904521394143205, 'epoch': 0.09}        \n",
      "{'loss': 2.0217, 'learning_rate': 0.00019904478450495527, 'epoch': 0.09}        \n",
      "{'loss': 1.854, 'learning_rate': 0.00019904435497238932, 'epoch': 0.09}         \n",
      "{'loss': 2.5996, 'learning_rate': 0.00019904392534373456, 'epoch': 0.09}        \n",
      "{'loss': 2.2818, 'learning_rate': 0.0001990434956189915, 'epoch': 0.09}         \n",
      "{'loss': 2.0312, 'learning_rate': 0.00019904306579816047, 'epoch': 0.09}        \n",
      "{'loss': 2.2714, 'learning_rate': 0.00019904263588124197, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98686/1061708 [14:46:17<142:47:23,  1.87it/s][2024-03-01 08:53:18,413] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1351, 'learning_rate': 0.0001990422488738608, 'epoch': 0.09}         \n",
      "{'loss': 2.092, 'learning_rate': 0.00019904181877437716, 'epoch': 0.09}         \n",
      "{'loss': 2.1164, 'learning_rate': 0.00019904138857880724, 'epoch': 0.09}        \n",
      "{'loss': 2.0213, 'learning_rate': 0.0001990409582871514, 'epoch': 0.09}         \n",
      "{'loss': 1.9835, 'learning_rate': 0.0001990405278994101, 'epoch': 0.09}         \n",
      "{'loss': 1.7466, 'learning_rate': 0.00019904009741558376, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98744/1061708 [14:46:48<143:57:21,  1.86it/s][2024-03-01 08:53:49,326] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4205, 'learning_rate': 0.00019903970989798765, 'epoch': 0.09}        \n",
      "{'loss': 2.2491, 'learning_rate': 0.00019903927923160087, 'epoch': 0.09}        \n",
      "{'loss': 1.9021, 'learning_rate': 0.00019903884846913026, 'epoch': 0.09}        \n",
      "{'loss': 2.5818, 'learning_rate': 0.00019903841761057623, 'epoch': 0.09}        \n",
      "{'loss': 1.9424, 'learning_rate': 0.00019903798665593916, 'epoch': 0.09}        \n",
      "{'loss': 2.3756, 'learning_rate': 0.00019903755560521952, 'epoch': 0.09}        \n",
      "{'loss': 1.8207, 'learning_rate': 0.00019903712445841772, 'epoch': 0.09}        \n",
      "{'loss': 1.9896, 'learning_rate': 0.00019903669321553417, 'epoch': 0.09}        \n",
      "{'loss': 2.0093, 'learning_rate': 0.0001990362618765693, 'epoch': 0.09}         \n",
      "{'loss': 2.0403, 'learning_rate': 0.00019903583044152348, 'epoch': 0.09}        \n",
      "{'loss': 2.1033, 'learning_rate': 0.00019903539891039725, 'epoch': 0.09}        \n",
      "{'loss': 2.107, 'learning_rate': 0.00019903496728319086, 'epoch': 0.09}         \n",
      "{'loss': 1.9474, 'learning_rate': 0.00019903453555990486, 'epoch': 0.09}        \n",
      "{'loss': 2.0449, 'learning_rate': 0.0001990341037405396, 'epoch': 0.09}         \n",
      "{'loss': 2.1893, 'learning_rate': 0.00019903367182509554, 'epoch': 0.09}        \n",
      "{'loss': 2.0541, 'learning_rate': 0.00019903323981357312, 'epoch': 0.09}        \n",
      "{'loss': 2.3837, 'learning_rate': 0.00019903280770597266, 'epoch': 0.09}        \n",
      "{'loss': 2.0442, 'learning_rate': 0.00019903237550229465, 'epoch': 0.09}        \n",
      "{'loss': 1.8996, 'learning_rate': 0.00019903194320253952, 'epoch': 0.09}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.00019903151080670765, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 98945/1061708 [14:48:36<143:15:45,  1.87it/s][2024-03-01 08:55:36,598] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 98946/1061708 [14:48:36<134:11:30,  1.99it/s][2024-03-01 08:55:37,020] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1749, 'learning_rate': 0.00019903116482086723, 'epoch': 0.09}        \n",
      "{'loss': 1.7583, 'learning_rate': 0.0001990307322520983, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 98968/1061708 [14:48:48<142:09:55,  1.88it/s][2024-03-01 08:55:48,693] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9594, 'learning_rate': 0.0001990303428580617, 'epoch': 0.09}         \n",
      "{'loss': 2.1062, 'learning_rate': 0.00019902991010674963, 'epoch': 0.09}        \n",
      "{'loss': 1.7831, 'learning_rate': 0.0001990294772593628, 'epoch': 0.09}         \n",
      "{'loss': 2.4459, 'learning_rate': 0.00019902904431590165, 'epoch': 0.09}        \n",
      "{'loss': 1.9472, 'learning_rate': 0.0001990286112763666, 'epoch': 0.09}         \n",
      "{'loss': 2.0033, 'learning_rate': 0.00019902817814075802, 'epoch': 0.09}        \n",
      "{'loss': 2.2498, 'learning_rate': 0.0001990277449090764, 'epoch': 0.09}         \n",
      "{'loss': 2.0653, 'learning_rate': 0.00019902731158132213, 'epoch': 0.09}        \n",
      "{'loss': 1.685, 'learning_rate': 0.00019902687815749563, 'epoch': 0.09}         \n",
      "{'loss': 1.9453, 'learning_rate': 0.00019902644463759735, 'epoch': 0.09}        \n",
      "{'loss': 2.2681, 'learning_rate': 0.00019902601102162764, 'epoch': 0.09}        \n",
      "{'loss': 1.937, 'learning_rate': 0.00019902557730958696, 'epoch': 0.09}         \n",
      "{'loss': 2.3568, 'learning_rate': 0.00019902514350147578, 'epoch': 0.09}        \n",
      "{'loss': 2.1983, 'learning_rate': 0.00019902470959729445, 'epoch': 0.09}        \n",
      "{'loss': 1.7333, 'learning_rate': 0.0001990242755970434, 'epoch': 0.09}         \n",
      "{'loss': 1.7768, 'learning_rate': 0.0001990238415007231, 'epoch': 0.09}         \n",
      "{'loss': 2.5398, 'learning_rate': 0.00019902340730833392, 'epoch': 0.09}        \n",
      "{'loss': 2.0809, 'learning_rate': 0.00019902297301987632, 'epoch': 0.09}        \n",
      "{'loss': 2.1273, 'learning_rate': 0.00019902253863535068, 'epoch': 0.09}        \n",
      "{'loss': 2.2502, 'learning_rate': 0.00019902210415475747, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 99163/1061708 [14:50:32<144:58:48,  1.84it/s][2024-03-01 08:57:32,625] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8581, 'learning_rate': 0.0001990217130400861, 'epoch': 0.09}         \n",
      "{'loss': 2.0181, 'learning_rate': 0.0001990212783769656, 'epoch': 0.09}         \n",
      "{'loss': 1.7667, 'learning_rate': 0.00019902084361777872, 'epoch': 0.09}        \n",
      "{'loss': 1.7917, 'learning_rate': 0.0001990204087625259, 'epoch': 0.09}         \n",
      "{'loss': 2.5823, 'learning_rate': 0.00019901997381120755, 'epoch': 0.09}        \n",
      "{'loss': 1.6947, 'learning_rate': 0.00019901953876382407, 'epoch': 0.09}        \n",
      "{'loss': 1.6325, 'learning_rate': 0.0001990191036203759, 'epoch': 0.09}         \n",
      "{'loss': 2.1112, 'learning_rate': 0.00019901866838086348, 'epoch': 0.09}        \n",
      "{'loss': 2.3152, 'learning_rate': 0.00019901823304528722, 'epoch': 0.09}        \n",
      "{'loss': 2.0375, 'learning_rate': 0.00019901779761364753, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 99264/1061708 [14:51:25<143:56:46,  1.86it/s][2024-03-01 08:58:26,424] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 99265/1061708 [14:51:26<134:34:47,  1.99it/s][2024-03-01 08:58:26,848] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7785, 'learning_rate': 0.00019901744919917044, 'epoch': 0.09}        \n",
      "{'loss': 2.135, 'learning_rate': 0.00019901701359461766, 'epoch': 0.09}         \n",
      "{'loss': 2.4689, 'learning_rate': 0.00019901657789400266, 'epoch': 0.09}        \n",
      "{'loss': 2.4291, 'learning_rate': 0.00019901614209732582, 'epoch': 0.09}        \n",
      "{'loss': 1.8738, 'learning_rate': 0.0001990157062045876, 'epoch': 0.09}         \n",
      "{'loss': 2.3443, 'learning_rate': 0.00019901527021578842, 'epoch': 0.09}        \n",
      "{'loss': 2.1031, 'learning_rate': 0.00019901483413092868, 'epoch': 0.09}        \n",
      "{'loss': 1.9756, 'learning_rate': 0.00019901439795000885, 'epoch': 0.09}        \n",
      "{'loss': 2.043, 'learning_rate': 0.00019901396167302927, 'epoch': 0.09}         \n",
      "{'loss': 2.0262, 'learning_rate': 0.00019901352529999046, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 99366/1061708 [14:52:20<142:42:22,  1.87it/s][2024-03-01 08:59:20,645] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 99367/1061708 [14:52:20<133:41:42,  2.00it/s][2024-03-01 08:59:21,068] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.118, 'learning_rate': 0.000199013176132397, 'epoch': 0.09}           \n",
      "{'loss': 1.6711, 'learning_rate': 0.00019901273958645256, 'epoch': 0.09}        \n",
      "{'loss': 1.8394, 'learning_rate': 0.00019901230294445, 'epoch': 0.09}           \n",
      "{'loss': 1.8926, 'learning_rate': 0.0001990118662063898, 'epoch': 0.09}         \n",
      "{'loss': 1.9781, 'learning_rate': 0.0001990114293722723, 'epoch': 0.09}         \n",
      "{'loss': 1.9208, 'learning_rate': 0.00019901099244209802, 'epoch': 0.09}        \n",
      "{'loss': 2.0679, 'learning_rate': 0.00019901055541586736, 'epoch': 0.09}        \n",
      "{'loss': 2.1353, 'learning_rate': 0.00019901011829358072, 'epoch': 0.09}        \n",
      "{'loss': 1.9768, 'learning_rate': 0.00019900968107523852, 'epoch': 0.09}        \n",
      "{'loss': 2.2602, 'learning_rate': 0.0001990092437608412, 'epoch': 0.09}         \n",
      "  9%|██▌                         | 99468/1061708 [14:53:14<141:45:42,  1.89it/s][2024-03-01 09:00:14,808] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                         | 99469/1061708 [14:53:14<133:03:10,  2.01it/s][2024-03-01 09:00:15,230] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0078, 'learning_rate': 0.00019900889384016394, 'epoch': 0.09}        \n",
      "  9%|██▌                         | 99479/1061708 [14:53:19<140:58:29,  1.90it/s][2024-03-01 09:00:20,460] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0755, 'learning_rate': 0.00019900850010592044, 'epoch': 0.09}        \n",
      "{'loss': 2.153, 'learning_rate': 0.00019900806253217643, 'epoch': 0.09}         \n",
      "{'loss': 1.6953, 'learning_rate': 0.00019900762486237884, 'epoch': 0.09}        \n",
      "{'loss': 1.9127, 'learning_rate': 0.00019900718709652812, 'epoch': 0.09}        \n",
      "{'loss': 2.2299, 'learning_rate': 0.00019900674923462473, 'epoch': 0.09}        \n",
      "{'loss': 1.9284, 'learning_rate': 0.00019900631127666904, 'epoch': 0.09}        \n",
      "{'loss': 2.0987, 'learning_rate': 0.00019900587322266148, 'epoch': 0.09}        \n",
      "{'loss': 2.4527, 'learning_rate': 0.0001990054350726025, 'epoch': 0.09}         \n",
      "{'loss': 1.9055, 'learning_rate': 0.00019900499682649252, 'epoch': 0.09}        \n",
      "{'loss': 1.8741, 'learning_rate': 0.00019900455848433196, 'epoch': 0.09}        \n",
      "{'loss': 1.8032, 'learning_rate': 0.00019900412004612126, 'epoch': 0.09}        \n",
      "{'loss': 1.9973, 'learning_rate': 0.0001990036815118608, 'epoch': 0.09}         \n",
      "{'loss': 2.1355, 'learning_rate': 0.00019900324288155109, 'epoch': 0.09}        \n",
      "{'loss': 2.5667, 'learning_rate': 0.00019900280415519246, 'epoch': 0.09}        \n",
      "  9%|██▋                         | 99614/1061708 [14:54:31<143:37:19,  1.86it/s][2024-03-01 09:01:32,353] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9647, 'learning_rate': 0.00019900240921934828, 'epoch': 0.09}        \n",
      "{'loss': 2.1673, 'learning_rate': 0.00019900197031049794, 'epoch': 0.09}        \n",
      "  9%|██▋                         | 99635/1061708 [14:54:42<142:53:39,  1.87it/s][2024-03-01 09:01:43,476] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0418, 'learning_rate': 0.0001990015752104119, 'epoch': 0.09}         \n",
      "{'loss': 2.2103, 'learning_rate': 0.00019900113611907142, 'epoch': 0.09}        \n",
      "{'loss': 2.3492, 'learning_rate': 0.00019900069693168411, 'epoch': 0.09}        \n",
      "{'loss': 2.0041, 'learning_rate': 0.00019900025764825043, 'epoch': 0.09}        \n",
      "{'loss': 2.1368, 'learning_rate': 0.00019899981826877072, 'epoch': 0.09}        \n",
      "{'loss': 2.3438, 'learning_rate': 0.00019899937879324552, 'epoch': 0.09}        \n",
      "{'loss': 2.1074, 'learning_rate': 0.0001989989392216751, 'epoch': 0.09}         \n",
      "{'loss': 1.8443, 'learning_rate': 0.00019899849955406005, 'epoch': 0.09}        \n",
      "{'loss': 2.3952, 'learning_rate': 0.0001989980597904007, 'epoch': 0.09}         \n",
      "{'loss': 1.8597, 'learning_rate': 0.0001989976199306975, 'epoch': 0.09}         \n",
      "{'loss': 1.9487, 'learning_rate': 0.00019899717997495087, 'epoch': 0.09}        \n",
      "{'loss': 1.8889, 'learning_rate': 0.00019899673992316124, 'epoch': 0.09}        \n",
      "{'loss': 2.2278, 'learning_rate': 0.00019899629977532905, 'epoch': 0.09}        \n",
      "  9%|██▋                         | 99762/1061708 [14:55:50<141:31:59,  1.89it/s][2024-03-01 09:02:51,106] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7455, 'learning_rate': 0.00019899590356016404, 'epoch': 0.09}        \n",
      "{'loss': 2.353, 'learning_rate': 0.00019899546322985217, 'epoch': 0.09}         \n",
      "{'loss': 2.1822, 'learning_rate': 0.00019899502280349893, 'epoch': 0.09}        \n",
      "{'loss': 2.0271, 'learning_rate': 0.00019899458228110482, 'epoch': 0.09}        \n",
      "{'loss': 2.4003, 'learning_rate': 0.0001989941416626702, 'epoch': 0.09}         \n",
      "{'loss': 1.9169, 'learning_rate': 0.00019899370094819555, 'epoch': 0.09}        \n",
      "  9%|██▋                         | 99822/1061708 [14:56:22<141:40:10,  1.89it/s][2024-03-01 09:03:23,067] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2362, 'learning_rate': 0.0001989933042230545, 'epoch': 0.09}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3517, 'learning_rate': 0.00019899286332610494, 'epoch': 0.09}        \n",
      "{'loss': 2.1687, 'learning_rate': 0.00019899242233311658, 'epoch': 0.09}        \n",
      "{'loss': 2.014, 'learning_rate': 0.00019899198124408984, 'epoch': 0.09}         \n",
      "{'loss': 2.2734, 'learning_rate': 0.00019899154005902515, 'epoch': 0.09}        \n",
      "{'loss': 2.1192, 'learning_rate': 0.0001989910987779229, 'epoch': 0.09}         \n",
      "{'loss': 1.9786, 'learning_rate': 0.0001989906574007836, 'epoch': 0.09}         \n",
      "{'loss': 2.193, 'learning_rate': 0.00019899021592760763, 'epoch': 0.09}         \n",
      "{'loss': 2.1255, 'learning_rate': 0.00019898977435839541, 'epoch': 0.09}        \n",
      "{'loss': 1.8096, 'learning_rate': 0.0001989893326931474, 'epoch': 0.09}         \n",
      "{'loss': 2.0985, 'learning_rate': 0.00019898889093186397, 'epoch': 0.09}        \n",
      "{'loss': 2.1171, 'learning_rate': 0.0001989884490745456, 'epoch': 0.09}         \n",
      "  9%|██▋                         | 99943/1061708 [14:57:26<144:27:05,  1.85it/s][2024-03-01 09:04:27,461] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2713, 'learning_rate': 0.00019898805132084952, 'epoch': 0.09}        \n",
      "{'loss': 2.087, 'learning_rate': 0.00019898760928106595, 'epoch': 0.09}         \n",
      "{'loss': 2.3912, 'learning_rate': 0.00019898716714524862, 'epoch': 0.09}        \n",
      "{'loss': 2.0745, 'learning_rate': 0.00019898672491339804, 'epoch': 0.09}        \n",
      "{'loss': 2.4619, 'learning_rate': 0.00019898628258551462, 'epoch': 0.09}        \n",
      "  9%|██▋                         | 99990/1061708 [14:57:51<141:12:59,  1.89it/s][2024-03-01 09:04:52,420] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  9%|██▋                         | 99999/1061708 [14:57:56<141:44:28,  1.88it/s][2024-03-01 09:04:57,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=100000, skipped=1152, lr=[0.00019898588440831177], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 09:04:57,252] [INFO] [timer.py:260:stop] epoch=0/micro_step=100000/global_step=100000, RunningAvgSamplesPerSec=1.8924884561889663, CurrSamplesPerSec=1.9048982402086971, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.7414, 'learning_rate': 0.00019898588440831177, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100000/1061708 [14:57:57<141:38:45,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 09:04:57,254 >> Saving model checkpoint to output_model/checkpoint-100000\n",
      "[INFO|trainer.py:2880] 2024-03-01 09:04:57,257 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 09:04:58,485 >> tokenizer config file saved in output_model/checkpoint-100000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 09:04:58,485 >> Special tokens file saved in output_model/checkpoint-100000/special_tokens_map.json\n",
      "[2024-03-01 09:04:58,487] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100000 is about to be saved!\n",
      "[2024-03-01 09:05:03,780] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-100000/global_step100000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 09:05:03,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100000/global_step100000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 09:05:17,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100000/global_step100000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 09:05:18,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-100000/global_step100000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 09:05:25,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-100000/global_step100000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 09:05:25,758] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-100000/global_step100000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 09:05:25,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 09:05:25,846 >> Deleting older checkpoint [output_model/checkpoint-85000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 09:05:29,614 >> tokenizer config file saved in output_model/checkpoint-100000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 09:05:29,614 >> Special tokens file saved in output_model/checkpoint-100000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 1.8515, 'learning_rate': 0.00019898544189796708, 'epoch': 0.09}        \n",
      "{'loss': 2.0982, 'learning_rate': 0.00019898499929159083, 'epoch': 0.09}        \n",
      "{'loss': 1.6389, 'learning_rate': 0.00019898455658918333, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100038/1061708 [14:58:49<140:56:09,  1.90it/s][2024-03-01 09:05:50,085] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9481, 'learning_rate': 0.00019898415807491034, 'epoch': 0.09}        \n",
      "{'loss': 1.6904, 'learning_rate': 0.00019898371519004482, 'epoch': 0.09}        \n",
      "{'loss': 2.3105, 'learning_rate': 0.00019898327220914936, 'epoch': 0.09}        \n",
      "{'loss': 2.3037, 'learning_rate': 0.0001989828291322244, 'epoch': 0.09}         \n",
      "{'loss': 2.231, 'learning_rate': 0.0001989823859592704, 'epoch': 0.09}          \n",
      "{'loss': 2.5087, 'learning_rate': 0.00019898194269028774, 'epoch': 0.09}        \n",
      "{'loss': 2.3122, 'learning_rate': 0.00019898149932527686, 'epoch': 0.09}        \n",
      "{'loss': 1.6968, 'learning_rate': 0.00019898105586423816, 'epoch': 0.09}        \n",
      "{'loss': 1.5941, 'learning_rate': 0.00019898061230717216, 'epoch': 0.09}        \n",
      "{'loss': 2.1821, 'learning_rate': 0.00019898016865407922, 'epoch': 0.09}        \n",
      "{'loss': 2.4337, 'learning_rate': 0.0001989797249049598, 'epoch': 0.09}         \n",
      "{'loss': 1.903, 'learning_rate': 0.00019897928105981432, 'epoch': 0.09}         \n",
      "{'loss': 2.3057, 'learning_rate': 0.00019897883711864317, 'epoch': 0.09}        \n",
      "{'loss': 2.0811, 'learning_rate': 0.0001989783930814469, 'epoch': 0.09}         \n",
      "{'loss': 2.2168, 'learning_rate': 0.0001989779489482258, 'epoch': 0.09}         \n",
      "{'loss': 1.9767, 'learning_rate': 0.00019897750471898034, 'epoch': 0.09}        \n",
      "{'loss': 2.2222, 'learning_rate': 0.00019897706039371103, 'epoch': 0.09}        \n",
      "{'loss': 2.3009, 'learning_rate': 0.00019897661597241824, 'epoch': 0.09}        \n",
      "{'loss': 1.8776, 'learning_rate': 0.00019897617145510235, 'epoch': 0.09}        \n",
      "{'loss': 2.0017, 'learning_rate': 0.00019897572684176388, 'epoch': 0.09}        \n",
      "{'loss': 2.2267, 'learning_rate': 0.00019897528213240327, 'epoch': 0.09}        \n",
      "{'loss': 1.7486, 'learning_rate': 0.00019897483732702085, 'epoch': 0.09}        \n",
      "{'loss': 1.7806, 'learning_rate': 0.00019897439242561713, 'epoch': 0.09}        \n",
      "{'loss': 2.0948, 'learning_rate': 0.0001989739474281925, 'epoch': 0.09}         \n",
      "{'loss': 1.9738, 'learning_rate': 0.00019897350233474747, 'epoch': 0.09}        \n",
      "{'loss': 1.947, 'learning_rate': 0.00019897305714528237, 'epoch': 0.09}         \n",
      "{'loss': 1.7705, 'learning_rate': 0.0001989726118597977, 'epoch': 0.09}         \n",
      "{'loss': 2.2978, 'learning_rate': 0.00019897216647829383, 'epoch': 0.09}        \n",
      "{'loss': 2.4816, 'learning_rate': 0.00019897172100077128, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100321/1061708 [15:01:20<141:34:42,  1.89it/s][2024-03-01 09:08:21,185] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.815, 'learning_rate': 0.00019897131998890532, 'epoch': 0.09}         \n",
      "{'loss': 2.2616, 'learning_rate': 0.00019897087432894835, 'epoch': 0.09}        \n",
      "{'loss': 2.3136, 'learning_rate': 0.00019897042857297393, 'epoch': 0.09}        \n",
      "{'loss': 1.6408, 'learning_rate': 0.00019896998272098246, 'epoch': 0.09}        \n",
      "{'loss': 2.4022, 'learning_rate': 0.00019896953677297436, 'epoch': 0.09}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2847, 'learning_rate': 0.00019896909072895008, 'epoch': 0.09}        \n",
      "{'loss': 2.4738, 'learning_rate': 0.0001989686445889101, 'epoch': 0.09}         \n",
      "{'loss': 2.2616, 'learning_rate': 0.00019896819835285475, 'epoch': 0.09}        \n",
      "{'loss': 2.0284, 'learning_rate': 0.00019896775202078457, 'epoch': 0.09}        \n",
      "{'loss': 2.0986, 'learning_rate': 0.00019896730559269994, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100422/1061708 [15:02:14<141:34:45,  1.89it/s][2024-03-01 09:09:15,099] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                        | 100423/1061708 [15:02:14<136:49:18,  1.95it/s][2024-03-01 09:09:15,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8627, 'learning_rate': 0.00019896694838110213, 'epoch': 0.09}        \n",
      "{'loss': 2.1333, 'learning_rate': 0.00019896650178019256, 'epoch': 0.09}        \n",
      "{'loss': 2.447, 'learning_rate': 0.00019896605508326978, 'epoch': 0.09}         \n",
      "{'loss': 2.0907, 'learning_rate': 0.0001989656082903342, 'epoch': 0.09}         \n",
      "{'loss': 2.42, 'learning_rate': 0.00019896516140138624, 'epoch': 0.09}          \n",
      "{'loss': 2.2623, 'learning_rate': 0.00019896471441642634, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100486/1061708 [15:02:48<142:40:28,  1.87it/s][2024-03-01 09:09:49,114] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.911, 'learning_rate': 0.00019896431204787264, 'epoch': 0.09}         \n",
      "{'loss': 2.0247, 'learning_rate': 0.00019896386488049128, 'epoch': 0.09}        \n",
      "{'loss': 2.0357, 'learning_rate': 0.0001989634176170992, 'epoch': 0.09}         \n",
      "{'loss': 1.6505, 'learning_rate': 0.00019896297025769695, 'epoch': 0.09}        \n",
      "{'loss': 2.2272, 'learning_rate': 0.0001989625228022849, 'epoch': 0.09}         \n",
      "{'loss': 2.434, 'learning_rate': 0.00019896207525086344, 'epoch': 0.09}         \n",
      "{'loss': 2.1446, 'learning_rate': 0.00019896162760343303, 'epoch': 0.09}        \n",
      "{'loss': 1.9191, 'learning_rate': 0.00019896117985999413, 'epoch': 0.09}        \n",
      "{'loss': 2.205, 'learning_rate': 0.00019896073202054718, 'epoch': 0.09}         \n",
      "{'loss': 1.9858, 'learning_rate': 0.0001989602840850926, 'epoch': 0.09}         \n",
      "{'loss': 1.9394, 'learning_rate': 0.0001989598360536308, 'epoch': 0.09}         \n",
      "{'loss': 1.7865, 'learning_rate': 0.00019895938792616224, 'epoch': 0.09}        \n",
      "{'loss': 1.8704, 'learning_rate': 0.00019895893970268733, 'epoch': 0.09}        \n",
      "{'loss': 2.1706, 'learning_rate': 0.00019895849138320656, 'epoch': 0.09}        \n",
      "{'loss': 1.9611, 'learning_rate': 0.00019895804296772029, 'epoch': 0.09}        \n",
      "{'loss': 2.0445, 'learning_rate': 0.00019895759445622902, 'epoch': 0.09}        \n",
      "{'loss': 2.147, 'learning_rate': 0.00019895714584873314, 'epoch': 0.09}         \n",
      "{'loss': 2.0188, 'learning_rate': 0.00019895669714523314, 'epoch': 0.09}        \n",
      "{'loss': 2.2056, 'learning_rate': 0.00019895624834572937, 'epoch': 0.09}        \n",
      "{'loss': 2.1576, 'learning_rate': 0.00019895579945022237, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100687/1061708 [15:04:35<141:45:20,  1.88it/s][2024-03-01 09:11:36,163] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  9%|██▌                        | 100688/1061708 [15:04:36<132:58:31,  2.01it/s][2024-03-01 09:11:36,585] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8558, 'learning_rate': 0.00019895544026469466, 'epoch': 0.09}        \n",
      "{'loss': 2.1085, 'learning_rate': 0.0001989549911963828, 'epoch': 0.09}         \n",
      "{'loss': 2.0915, 'learning_rate': 0.0001989545420320689, 'epoch': 0.09}         \n",
      "{'loss': 1.9825, 'learning_rate': 0.00019895409277175335, 'epoch': 0.09}        \n",
      "{'loss': 2.0619, 'learning_rate': 0.00019895364341543663, 'epoch': 0.09}        \n",
      "{'loss': 1.7294, 'learning_rate': 0.00019895319396311914, 'epoch': 0.09}        \n",
      "  9%|██▌                        | 100742/1061708 [15:05:04<141:35:49,  1.89it/s][2024-03-01 09:12:05,332] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0604, 'learning_rate': 0.00019895278937395313, 'epoch': 0.09}        \n",
      "{'loss': 1.7615, 'learning_rate': 0.0001989523397392354, 'epoch': 0.09}         \n",
      "{'loss': 2.107, 'learning_rate': 0.00019895189000851814, 'epoch': 0.09}         \n",
      "  9%|██▌                        | 100776/1061708 [15:05:22<142:27:23,  1.87it/s][2024-03-01 09:12:23,402] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.3223, 'learning_rate': 0.00019895148516879346, 'epoch': 0.09}        \n",
      "{'loss': 2.0444, 'learning_rate': 0.0001989510352556784, 'epoch': 0.09}         \n",
      "{'loss': 2.2142, 'learning_rate': 0.00019895058524656512, 'epoch': 0.09}        \n",
      "{'loss': 2.3642, 'learning_rate': 0.00019895013514145403, 'epoch': 0.09}        \n",
      "{'loss': 2.0729, 'learning_rate': 0.0001989496849403456, 'epoch': 0.09}         \n",
      "{'loss': 2.2951, 'learning_rate': 0.00019894923464324027, 'epoch': 0.09}        \n",
      "{'loss': 2.1233, 'learning_rate': 0.00019894878425013844, 'epoch': 0.09}        \n",
      "{'loss': 2.1992, 'learning_rate': 0.00019894833376104057, 'epoch': 0.09}        \n",
      "{'loss': 1.8696, 'learning_rate': 0.00019894788317594708, 'epoch': 0.09}        \n",
      "{'loss': 1.6924, 'learning_rate': 0.00019894743249485844, 'epoch': 0.1}         \n",
      "{'loss': 2.2171, 'learning_rate': 0.00019894698171777505, 'epoch': 0.1}         \n",
      "{'loss': 2.0201, 'learning_rate': 0.00019894653084469738, 'epoch': 0.1}         \n",
      "{'loss': 2.0097, 'learning_rate': 0.00019894607987562586, 'epoch': 0.1}         \n",
      "{'loss': 1.6262, 'learning_rate': 0.0001989456288105609, 'epoch': 0.1}          \n",
      "{'loss': 2.4728, 'learning_rate': 0.00019894517764950295, 'epoch': 0.1}         \n",
      "{'loss': 1.933, 'learning_rate': 0.00019894472639245248, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 100939/1061708 [15:06:49<141:41:37,  1.88it/s][2024-03-01 09:13:50,434] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8982, 'learning_rate': 0.00019894432017903378, 'epoch': 0.1}         \n",
      "{'loss': 2.2337, 'learning_rate': 0.00019894386873959866, 'epoch': 0.1}         \n",
      "{'loss': 1.986, 'learning_rate': 0.0001989434172041723, 'epoch': 0.1}           \n",
      "{'loss': 2.4739, 'learning_rate': 0.0001989429655727551, 'epoch': 0.1}          \n",
      "{'loss': 2.3293, 'learning_rate': 0.00019894251384534748, 'epoch': 0.1}         \n",
      "{'loss': 2.4052, 'learning_rate': 0.00019894206202194988, 'epoch': 0.1}         \n",
      "{'loss': 1.8333, 'learning_rate': 0.00019894161010256277, 'epoch': 0.1}         \n",
      "{'loss': 2.0495, 'learning_rate': 0.0001989411580871866, 'epoch': 0.1}          \n",
      "{'loss': 1.9651, 'learning_rate': 0.00019894070597582176, 'epoch': 0.1}         \n",
      "{'loss': 1.67, 'learning_rate': 0.00019894025376846874, 'epoch': 0.1}           \n",
      "{'loss': 2.2545, 'learning_rate': 0.00019893980146512793, 'epoch': 0.1}         \n",
      "{'loss': 2.1742, 'learning_rate': 0.0001989393490657998, 'epoch': 0.1}          \n",
      "{'loss': 1.8749, 'learning_rate': 0.00019893889657048478, 'epoch': 0.1}         \n",
      "{'loss': 1.7151, 'learning_rate': 0.00019893844397918332, 'epoch': 0.1}         \n",
      "{'loss': 2.2611, 'learning_rate': 0.00019893799129189584, 'epoch': 0.1}         \n",
      "{'loss': 2.1651, 'learning_rate': 0.00019893753850862278, 'epoch': 0.1}         \n",
      "{'loss': 1.8335, 'learning_rate': 0.0001989370856293646, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 101100/1061708 [15:08:15<141:34:45,  1.88it/s][2024-03-01 09:15:16,273] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2989, 'learning_rate': 0.0001989366779559653, 'epoch': 0.1}          \n",
      "{'loss': 2.1097, 'learning_rate': 0.00019893622489433653, 'epoch': 0.1}         \n",
      "{'loss': 1.6661, 'learning_rate': 0.00019893577173672394, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101132/1061708 [15:08:32<141:28:34,  1.89it/s][2024-03-01 09:15:33,305] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1457, 'learning_rate': 0.00019893536381280678, 'epoch': 0.1}         \n",
      "{'loss': 2.0375, 'learning_rate': 0.00019893491047282607, 'epoch': 0.1}         \n",
      "{'loss': 2.3259, 'learning_rate': 0.00019893445703686279, 'epoch': 0.1}         \n",
      "{'loss': 1.9515, 'learning_rate': 0.00019893400350491733, 'epoch': 0.1}         \n",
      "{'loss': 1.9393, 'learning_rate': 0.0001989335498769902, 'epoch': 0.1}          \n",
      "{'loss': 2.4788, 'learning_rate': 0.00019893309615308177, 'epoch': 0.1}         \n",
      "{'loss': 1.6616, 'learning_rate': 0.00019893264233319255, 'epoch': 0.1}         \n",
      "{'loss': 2.1326, 'learning_rate': 0.00019893218841732292, 'epoch': 0.1}         \n",
      "{'loss': 2.2331, 'learning_rate': 0.00019893173440547338, 'epoch': 0.1}         \n",
      "{'loss': 1.9739, 'learning_rate': 0.00019893128029764432, 'epoch': 0.1}         \n",
      "{'loss': 2.288, 'learning_rate': 0.0001989308260938362, 'epoch': 0.1}           \n",
      "{'loss': 2.3425, 'learning_rate': 0.00019893037179404943, 'epoch': 0.1}         \n",
      "{'loss': 2.1469, 'learning_rate': 0.00019892991739828451, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101267/1061708 [15:09:44<142:11:02,  1.88it/s][2024-03-01 09:16:45,338] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1475, 'learning_rate': 0.0001989295083600351, 'epoch': 0.1}          \n",
      "{'loss': 2.4027, 'learning_rate': 0.00019892905378191285, 'epoch': 0.1}         \n",
      "{'loss': 2.0683, 'learning_rate': 0.0001989285991078137, 'epoch': 0.1}          \n",
      "{'loss': 2.2255, 'learning_rate': 0.00019892814433773807, 'epoch': 0.1}         \n",
      "{'loss': 2.0969, 'learning_rate': 0.00019892768947168645, 'epoch': 0.1}         \n",
      "{'loss': 1.9661, 'learning_rate': 0.00019892723450965922, 'epoch': 0.1}         \n",
      "{'loss': 2.227, 'learning_rate': 0.00019892677945165688, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 101339/1061708 [15:10:23<141:24:08,  1.89it/s][2024-03-01 09:17:23,701] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3187, 'learning_rate': 0.0001989263698173964, 'epoch': 0.1}          \n",
      "{'loss': 1.9579, 'learning_rate': 0.0001989259145770425, 'epoch': 0.1}          \n",
      "{'loss': 2.1192, 'learning_rate': 0.00019892545924071474, 'epoch': 0.1}         \n",
      "{'loss': 2.1066, 'learning_rate': 0.00019892500380841357, 'epoch': 0.1}         \n",
      "{'loss': 1.932, 'learning_rate': 0.00019892454828013942, 'epoch': 0.1}          \n",
      "{'loss': 1.9248, 'learning_rate': 0.00019892409265589276, 'epoch': 0.1}         \n",
      "{'loss': 2.2685, 'learning_rate': 0.000198923636935674, 'epoch': 0.1}           \n",
      "{'loss': 1.9351, 'learning_rate': 0.00019892318111948358, 'epoch': 0.1}         \n",
      "{'loss': 1.8458, 'learning_rate': 0.000198922725207322, 'epoch': 0.1}           \n",
      "{'loss': 2.3213, 'learning_rate': 0.0001989222691991896, 'epoch': 0.1}          \n",
      "{'loss': 2.278, 'learning_rate': 0.0001989218130950869, 'epoch': 0.1}           \n",
      "{'loss': 2.292, 'learning_rate': 0.00019892135689501434, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 101451/1061708 [15:11:22<141:18:53,  1.89it/s][2024-03-01 09:18:23,306] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5503, 'learning_rate': 0.00019892094623289515, 'epoch': 0.1}         \n",
      "{'loss': 2.1805, 'learning_rate': 0.000198920489850481, 'epoch': 0.1}           \n",
      "{'loss': 2.1337, 'learning_rate': 0.00019892003337209832, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101480/1061708 [15:11:38<141:07:24,  1.89it/s][2024-03-01 09:18:38,666] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4952, 'learning_rate': 0.00019891962245950108, 'epoch': 0.1}         \n",
      "{'loss': 2.087, 'learning_rate': 0.00019891916579877928, 'epoch': 0.1}          \n",
      "{'loss': 2.3491, 'learning_rate': 0.00019891870904209015, 'epoch': 0.1}         \n",
      "{'loss': 2.0992, 'learning_rate': 0.00019891825218943415, 'epoch': 0.1}         \n",
      "{'loss': 2.4229, 'learning_rate': 0.00019891779524081174, 'epoch': 0.1}         \n",
      "{'loss': 1.9443, 'learning_rate': 0.00019891733819622334, 'epoch': 0.1}         \n",
      "{'loss': 2.6984, 'learning_rate': 0.0001989168810556694, 'epoch': 0.1}          \n",
      "{'loss': 2.4274, 'learning_rate': 0.00019891642381915037, 'epoch': 0.1}         \n",
      "{'loss': 1.8921, 'learning_rate': 0.00019891596648666671, 'epoch': 0.1}         \n",
      "{'loss': 2.0722, 'learning_rate': 0.00019891550905821884, 'epoch': 0.1}         \n",
      "{'loss': 1.9571, 'learning_rate': 0.00019891505153380717, 'epoch': 0.1}         \n",
      "{'loss': 2.0348, 'learning_rate': 0.00019891459391343223, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101607/1061708 [15:12:45<142:03:46,  1.88it/s][2024-03-01 09:19:46,359] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.5609, 'learning_rate': 0.0001989141819730465, 'epoch': 0.1}          \n",
      "{'loss': 1.8467, 'learning_rate': 0.00019891372417034245, 'epoch': 0.1}         \n",
      "{'loss': 1.809, 'learning_rate': 0.0001989132662716764, 'epoch': 0.1}           \n",
      "{'loss': 2.2514, 'learning_rate': 0.00019891280827704874, 'epoch': 0.1}         \n",
      "{'loss': 1.6738, 'learning_rate': 0.00019891235018645994, 'epoch': 0.1}         \n",
      "{'loss': 2.0869, 'learning_rate': 0.00019891189199991045, 'epoch': 0.1}         \n",
      "{'loss': 2.1709, 'learning_rate': 0.00019891143371740072, 'epoch': 0.1}         \n",
      "{'loss': 2.1792, 'learning_rate': 0.00019891097533893115, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101684/1061708 [15:13:26<143:41:43,  1.86it/s][2024-03-01 09:20:27,399] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7386, 'learning_rate': 0.0001989105627162633, 'epoch': 0.1}          \n",
      "{'loss': 1.9733, 'learning_rate': 0.00019891010415547133, 'epoch': 0.1}         \n",
      "{'loss': 2.2325, 'learning_rate': 0.00019890964549872087, 'epoch': 0.1}         \n",
      "{'loss': 2.2206, 'learning_rate': 0.0001989091867460123, 'epoch': 0.1}          \n",
      "{'loss': 1.826, 'learning_rate': 0.00019890872789734613, 'epoch': 0.1}          \n",
      "{'loss': 1.907, 'learning_rate': 0.00019890826895272278, 'epoch': 0.1}          \n",
      "{'loss': 2.2779, 'learning_rate': 0.00019890780991214268, 'epoch': 0.1}         \n",
      "{'loss': 2.2076, 'learning_rate': 0.0001989073507756063, 'epoch': 0.1}          \n",
      "{'loss': 1.8521, 'learning_rate': 0.00019890689154311404, 'epoch': 0.1}         \n",
      "{'loss': 2.4361, 'learning_rate': 0.0001989064322146664, 'epoch': 0.1}          \n",
      "{'loss': 2.2737, 'learning_rate': 0.00019890597279026382, 'epoch': 0.1}         \n",
      "{'loss': 2.2689, 'learning_rate': 0.0001989055132699067, 'epoch': 0.1}          \n",
      "{'loss': 1.7103, 'learning_rate': 0.00019890505365359554, 'epoch': 0.1}         \n",
      "{'loss': 1.8845, 'learning_rate': 0.00019890459394133076, 'epoch': 0.1}         \n",
      "{'loss': 2.2055, 'learning_rate': 0.00019890413413311278, 'epoch': 0.1}         \n",
      "{'loss': 2.1193, 'learning_rate': 0.00019890367422894213, 'epoch': 0.1}         \n",
      "{'loss': 2.2163, 'learning_rate': 0.00019890321422881915, 'epoch': 0.1}         \n",
      "{'loss': 1.8001, 'learning_rate': 0.00019890275413274436, 'epoch': 0.1}         \n",
      "{'loss': 1.8745, 'learning_rate': 0.0001989022939407182, 'epoch': 0.1}          \n",
      "{'loss': 2.0074, 'learning_rate': 0.00019890183365274108, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101885/1061708 [15:15:14<142:44:21,  1.87it/s][2024-03-01 09:22:14,626] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|██▌                        | 101886/1061708 [15:15:14<133:38:10,  2.00it/s][2024-03-01 09:22:15,048] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2761, 'learning_rate': 0.00019890146535327498, 'epoch': 0.1}         \n",
      "{'loss': 2.2461, 'learning_rate': 0.00019890100489258728, 'epoch': 0.1}         \n",
      "{'loss': 2.0164, 'learning_rate': 0.00019890054433594991, 'epoch': 0.1}         \n",
      "{'loss': 2.3757, 'learning_rate': 0.00019890008368336325, 'epoch': 0.1}         \n",
      "{'loss': 2.0381, 'learning_rate': 0.00019889962293482785, 'epoch': 0.1}         \n",
      "{'loss': 1.6851, 'learning_rate': 0.00019889916209034407, 'epoch': 0.1}         \n",
      "{'loss': 2.1187, 'learning_rate': 0.00019889870114991241, 'epoch': 0.1}         \n",
      "{'loss': 1.8797, 'learning_rate': 0.00019889824011353328, 'epoch': 0.1}         \n",
      "{'loss': 1.9144, 'learning_rate': 0.00019889777898120714, 'epoch': 0.1}         \n",
      "{'loss': 1.7934, 'learning_rate': 0.00019889731775293444, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101987/1061708 [15:16:08<141:31:15,  1.88it/s][2024-03-01 09:23:08,814] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▌                        | 101988/1061708 [15:16:08<132:44:31,  2.01it/s][2024-03-01 09:23:09,236] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2622, 'learning_rate': 0.00019889694870123506, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 101999/1061708 [15:16:14<140:52:59,  1.89it/s][2024-03-01 09:23:15,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=102000, skipped=1174, lr=[0.0001988964873002597], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 09:23:15,094] [INFO] [timer.py:260:stop] epoch=0/micro_step=102000/global_step=102000, RunningAvgSamplesPerSec=1.8924919955477757, CurrSamplesPerSec=1.9092137695590328, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.2501, 'learning_rate': 0.0001988964873002597, 'epoch': 0.1}          \n",
      "{'loss': 2.4171, 'learning_rate': 0.00019889602580333899, 'epoch': 0.1}         \n",
      "{'loss': 2.0938, 'learning_rate': 0.00019889556421047347, 'epoch': 0.1}         \n",
      "{'loss': 1.8824, 'learning_rate': 0.0001988951025216635, 'epoch': 0.1}          \n",
      "{'loss': 2.0406, 'learning_rate': 0.00019889464073690957, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102047/1061708 [15:16:40<141:42:13,  1.88it/s][2024-03-01 09:23:40,586] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9069, 'learning_rate': 0.00019889422504859933, 'epoch': 0.1}         \n",
      "{'loss': 2.1425, 'learning_rate': 0.0001988937630815531, 'epoch': 0.1}          \n",
      "{'loss': 1.9453, 'learning_rate': 0.00019889330101856425, 'epoch': 0.1}         \n",
      "{'loss': 1.7995, 'learning_rate': 0.00019889283885963314, 'epoch': 0.1}         \n",
      "{'loss': 1.8539, 'learning_rate': 0.00019889237660476028, 'epoch': 0.1}         \n",
      "{'loss': 2.3454, 'learning_rate': 0.0001988919142539461, 'epoch': 0.1}          \n",
      "{'loss': 2.1512, 'learning_rate': 0.00019889145180719106, 'epoch': 0.1}         \n",
      "{'loss': 2.389, 'learning_rate': 0.00019889098926449556, 'epoch': 0.1}          \n",
      "{'loss': 2.161, 'learning_rate': 0.0001988905266258601, 'epoch': 0.1}           \n",
      "{'loss': 1.8271, 'learning_rate': 0.00019889006389128513, 'epoch': 0.1}         \n",
      "{'loss': 2.1719, 'learning_rate': 0.0001988896010607711, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 102150/1061708 [15:17:34<141:14:48,  1.89it/s][2024-03-01 09:24:35,324] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9368, 'learning_rate': 0.0001988891844312809, 'epoch': 0.1}          \n",
      "{'loss': 2.0251, 'learning_rate': 0.00019888872141848385, 'epoch': 0.1}         \n",
      "{'loss': 1.8418, 'learning_rate': 0.000198888258309749, 'epoch': 0.1}           \n",
      "{'loss': 2.0128, 'learning_rate': 0.00019888779510507688, 'epoch': 0.1}         \n",
      "{'loss': 1.9484, 'learning_rate': 0.00019888733180446785, 'epoch': 0.1}         \n",
      "{'loss': 1.8592, 'learning_rate': 0.00019888686840792237, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102217/1061708 [15:18:10<141:31:12,  1.88it/s][2024-03-01 09:25:10,955] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8711, 'learning_rate': 0.0001988864512690062, 'epoch': 0.1}          \n",
      "{'loss': 2.1118, 'learning_rate': 0.00019888598769018278, 'epoch': 0.1}         \n",
      "{'loss': 2.3453, 'learning_rate': 0.00019888552401542425, 'epoch': 0.1}         \n",
      "{'loss': 2.4555, 'learning_rate': 0.00019888506024473102, 'epoch': 0.1}         \n",
      "{'loss': 1.9287, 'learning_rate': 0.0001988845963781036, 'epoch': 0.1}          \n",
      "{'loss': 1.5946, 'learning_rate': 0.00019888413241554237, 'epoch': 0.1}         \n",
      "{'loss': 2.2936, 'learning_rate': 0.00019888366835704786, 'epoch': 0.1}         \n",
      "{'loss': 1.8159, 'learning_rate': 0.00019888320420262046, 'epoch': 0.1}         \n",
      "{'loss': 2.1203, 'learning_rate': 0.00019888273995226062, 'epoch': 0.1}         \n",
      "{'loss': 1.8954, 'learning_rate': 0.00019888227560596887, 'epoch': 0.1}         \n",
      "{'loss': 2.1607, 'learning_rate': 0.00019888181116374553, 'epoch': 0.1}         \n",
      "{'loss': 2.0089, 'learning_rate': 0.00019888134662559117, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102339/1061708 [15:19:15<140:59:33,  1.89it/s][2024-03-01 09:26:15,827] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2654, 'learning_rate': 0.00019888092845923153, 'epoch': 0.1}         \n",
      "{'loss': 1.874, 'learning_rate': 0.00019888046373880936, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 102350/1061708 [15:19:21<140:32:46,  1.90it/s][2024-03-01 09:26:21,581] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3522, 'learning_rate': 0.00019888004540840945, 'epoch': 0.1}         \n",
      "{'loss': 1.9211, 'learning_rate': 0.00019887958050572113, 'epoch': 0.1}         \n",
      "{'loss': 2.2463, 'learning_rate': 0.00019887911550710389, 'epoch': 0.1}         \n",
      "{'loss': 1.6991, 'learning_rate': 0.0001988786504125582, 'epoch': 0.1}          \n",
      "{'loss': 1.9804, 'learning_rate': 0.0001988781852220845, 'epoch': 0.1}          \n",
      "{'loss': 2.4569, 'learning_rate': 0.00019887771993568328, 'epoch': 0.1}         \n",
      "{'loss': 1.8349, 'learning_rate': 0.0001988772545533549, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 102426/1061708 [15:20:01<141:58:28,  1.88it/s][2024-03-01 09:27:02,007] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.4, 'learning_rate': 0.00019887683562724214, 'epoch': 0.1}            \n",
      "{'loss': 2.0365, 'learning_rate': 0.00019887637006265352, 'epoch': 0.1}         \n",
      "{'loss': 2.1573, 'learning_rate': 0.00019887590440213914, 'epoch': 0.1}         \n",
      "{'loss': 2.0584, 'learning_rate': 0.00019887543864569943, 'epoch': 0.1}         \n",
      "{'loss': 2.2499, 'learning_rate': 0.00019887497279333483, 'epoch': 0.1}         \n",
      "{'loss': 2.2944, 'learning_rate': 0.0001988745068450458, 'epoch': 0.1}          \n",
      "{'loss': 2.1822, 'learning_rate': 0.00019887404080083277, 'epoch': 0.1}         \n",
      "{'loss': 1.667, 'learning_rate': 0.00019887357466069622, 'epoch': 0.1}          \n",
      "{'loss': 2.03, 'learning_rate': 0.0001988731084246366, 'epoch': 0.1}            \n",
      "{'loss': 1.7782, 'learning_rate': 0.00019887264209265433, 'epoch': 0.1}         \n",
      "{'loss': 2.101, 'learning_rate': 0.0001988721756647499, 'epoch': 0.1}           \n",
      "{'loss': 1.9609, 'learning_rate': 0.00019887170914092376, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102548/1061708 [15:21:06<141:27:41,  1.88it/s][2024-03-01 09:28:06,978] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8452, 'learning_rate': 0.00019887128918746753, 'epoch': 0.1}         \n",
      "{'loss': 1.7377, 'learning_rate': 0.00019887082248139131, 'epoch': 0.1}         \n",
      "{'loss': 1.9336, 'learning_rate': 0.00019887035567939476, 'epoch': 0.1}         \n",
      "{'loss': 2.3308, 'learning_rate': 0.00019886988878147822, 'epoch': 0.1}         \n",
      "{'loss': 1.8053, 'learning_rate': 0.00019886942178764218, 'epoch': 0.1}         \n",
      "{'loss': 1.9271, 'learning_rate': 0.0001988689546978871, 'epoch': 0.1}          \n",
      "{'loss': 1.7958, 'learning_rate': 0.0001988684875122134, 'epoch': 0.1}          \n",
      "{'loss': 2.2406, 'learning_rate': 0.00019886802023062156, 'epoch': 0.1}         \n",
      "{'loss': 2.0863, 'learning_rate': 0.00019886755285311204, 'epoch': 0.1}         \n",
      "{'loss': 1.9653, 'learning_rate': 0.0001988670853796853, 'epoch': 0.1}          \n",
      "{'loss': 1.956, 'learning_rate': 0.00019886661781034177, 'epoch': 0.1}          \n",
      "{'loss': 1.8912, 'learning_rate': 0.00019886615014508188, 'epoch': 0.1}         \n",
      "{'loss': 2.2412, 'learning_rate': 0.00019886568238390614, 'epoch': 0.1}         \n",
      "{'loss': 2.1212, 'learning_rate': 0.00019886521452681496, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102687/1061708 [15:22:20<141:28:51,  1.88it/s][2024-03-01 09:29:20,916] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2589, 'learning_rate': 0.00019886479337342562, 'epoch': 0.1}         \n",
      "{'loss': 2.2363, 'learning_rate': 0.0001988643253340964, 'epoch': 0.1}          \n",
      "{'loss': 2.584, 'learning_rate': 0.00019886385719885307, 'epoch': 0.1}          \n",
      "{'loss': 2.193, 'learning_rate': 0.00019886338896769613, 'epoch': 0.1}          \n",
      " 10%|██▌                        | 102729/1061708 [15:22:42<140:54:28,  1.89it/s][2024-03-01 09:29:43,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7568, 'learning_rate': 0.00019886296747764905, 'epoch': 0.1}         \n",
      "{'loss': 1.8325, 'learning_rate': 0.0001988624990642574, 'epoch': 0.1}          \n",
      "{'loss': 2.1386, 'learning_rate': 0.0001988620305549534, 'epoch': 0.1}          \n",
      "{'loss': 2.2952, 'learning_rate': 0.00019886156194973755, 'epoch': 0.1}         \n",
      "{'loss': 2.1282, 'learning_rate': 0.00019886109324861024, 'epoch': 0.1}         \n",
      "{'loss': 2.1386, 'learning_rate': 0.000198860624451572, 'epoch': 0.1}           \n",
      "{'loss': 2.0818, 'learning_rate': 0.0001988601555586232, 'epoch': 0.1}          \n",
      "{'loss': 1.7002, 'learning_rate': 0.00019885968656976438, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 102803/1061708 [15:23:21<144:12:43,  1.85it/s][2024-03-01 09:30:22,505] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2375, 'learning_rate': 0.00019885926439778867, 'epoch': 0.1}         \n",
      "{'loss': 2.1268, 'learning_rate': 0.00019885879522670198, 'epoch': 0.1}         \n",
      "{'loss': 2.0052, 'learning_rate': 0.00019885832595970655, 'epoch': 0.1}         \n",
      "{'loss': 2.3566, 'learning_rate': 0.00019885785659680286, 'epoch': 0.1}         \n",
      "{'loss': 2.155, 'learning_rate': 0.0001988573871379913, 'epoch': 0.1}           \n",
      "{'loss': 1.9921, 'learning_rate': 0.00019885691758327235, 'epoch': 0.1}         \n",
      "{'loss': 2.1608, 'learning_rate': 0.00019885644793264654, 'epoch': 0.1}         \n",
      "{'loss': 1.9591, 'learning_rate': 0.0001988559781861142, 'epoch': 0.1}          \n",
      "{'loss': 2.2299, 'learning_rate': 0.00019885550834367592, 'epoch': 0.1}         \n",
      "{'loss': 2.1575, 'learning_rate': 0.00019885503840533203, 'epoch': 0.1}         \n",
      "{'loss': 2.1066, 'learning_rate': 0.0001988545683710831, 'epoch': 0.1}          \n",
      "{'loss': 1.6175, 'learning_rate': 0.0001988540982409295, 'epoch': 0.1}          \n",
      "{'loss': 2.1631, 'learning_rate': 0.00019885362801487172, 'epoch': 0.1}         \n",
      "{'loss': 2.0565, 'learning_rate': 0.0001988531576929102, 'epoch': 0.1}          \n",
      "{'loss': 1.8003, 'learning_rate': 0.00019885268727504543, 'epoch': 0.1}         \n",
      "{'loss': 2.1383, 'learning_rate': 0.00019885221676127783, 'epoch': 0.1}         \n",
      "{'loss': 2.3538, 'learning_rate': 0.00019885174615160786, 'epoch': 0.1}         \n",
      "{'loss': 2.0863, 'learning_rate': 0.000198851275446036, 'epoch': 0.1}           \n",
      "{'loss': 2.0457, 'learning_rate': 0.00019885080464456274, 'epoch': 0.1}         \n",
      "{'loss': 2.147, 'learning_rate': 0.00019885033374718845, 'epoch': 0.1}          \n",
      "{'loss': 2.0673, 'learning_rate': 0.0001988498627539136, 'epoch': 0.1}          \n",
      "{'loss': 1.8375, 'learning_rate': 0.00019884939166473875, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 103021/1061708 [15:25:18<140:50:28,  1.89it/s][2024-03-01 09:32:18,540] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1195, 'learning_rate': 0.00019884896760248718, 'epoch': 0.1}         \n",
      "{'loss': 2.2359, 'learning_rate': 0.0001988484963311034, 'epoch': 0.1}          \n",
      "{'loss': 2.0123, 'learning_rate': 0.00019884802496382086, 'epoch': 0.1}         \n",
      "{'loss': 2.3247, 'learning_rate': 0.00019884755350064006, 'epoch': 0.1}         \n",
      "{'loss': 2.1675, 'learning_rate': 0.0001988470819415614, 'epoch': 0.1}          \n",
      "{'loss': 2.3525, 'learning_rate': 0.0001988466102865854, 'epoch': 0.1}          \n",
      "{'loss': 1.9838, 'learning_rate': 0.00019884613853571244, 'epoch': 0.1}         \n",
      " 10%|██▌                        | 103093/1061708 [15:25:56<144:56:33,  1.84it/s][2024-03-01 09:32:56,869] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      " 10%|██▌                        | 103098/1061708 [15:25:58<142:13:54,  1.87it/s][2024-03-01 09:32:59,460] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.961, 'learning_rate': 0.00019884576106596864, 'epoch': 0.1}          \n",
      "{'loss': 1.8127, 'learning_rate': 0.0001988452891424824, 'epoch': 0.1}          \n",
      "{'loss': 2.2405, 'learning_rate': 0.00019884481712310052, 'epoch': 0.1}         \n",
      "{'loss': 2.0768, 'learning_rate': 0.00019884434500782344, 'epoch': 0.1}         \n",
      "{'loss': 1.8017, 'learning_rate': 0.0001988438727966517, 'epoch': 0.1}          \n",
      "{'loss': 2.2398, 'learning_rate': 0.00019884340048958565, 'epoch': 0.1}         \n",
      "{'loss': 2.1108, 'learning_rate': 0.0001988429280866258, 'epoch': 0.1}          \n",
      "{'loss': 2.0327, 'learning_rate': 0.0001988424555877726, 'epoch': 0.1}          \n",
      "{'loss': 2.3303, 'learning_rate': 0.00019884198299302655, 'epoch': 0.1}         \n",
      "{'loss': 2.1605, 'learning_rate': 0.00019884151030238802, 'epoch': 0.1}         \n",
      "{'loss': 2.1157, 'learning_rate': 0.00019884103751585753, 'epoch': 0.1}         \n",
      "{'loss': 2.0007, 'learning_rate': 0.00019884056463343556, 'epoch': 0.1}         \n",
      "{'loss': 2.0744, 'learning_rate': 0.00019884009165512252, 'epoch': 0.1}         \n",
      "{'loss': 1.9919, 'learning_rate': 0.00019883961858091888, 'epoch': 0.1}         \n",
      "{'loss': 1.8687, 'learning_rate': 0.00019883914541082512, 'epoch': 0.1}         \n",
      "{'loss': 2.1308, 'learning_rate': 0.00019883867214484164, 'epoch': 0.1}         \n",
      "{'loss': 2.0264, 'learning_rate': 0.000198838198782969, 'epoch': 0.1}           \n",
      "{'loss': 1.9286, 'learning_rate': 0.00019883772532520758, 'epoch': 0.1}         \n",
      "{'loss': 2.4101, 'learning_rate': 0.00019883725177155783, 'epoch': 0.1}         \n",
      "{'loss': 1.9787, 'learning_rate': 0.00019883677812202027, 'epoch': 0.1}         \n",
      "{'loss': 1.8922, 'learning_rate': 0.00019883630437659533, 'epoch': 0.1}         \n",
      "{'loss': 1.7943, 'learning_rate': 0.00019883583053528344, 'epoch': 0.1}         \n",
      "{'loss': 2.5214, 'learning_rate': 0.00019883535659808514, 'epoch': 0.1}         \n",
      "{'loss': 2.0553, 'learning_rate': 0.00019883488256500082, 'epoch': 0.1}         \n",
      "{'loss': 2.5705, 'learning_rate': 0.00019883440843603096, 'epoch': 0.1}         \n",
      "{'loss': 2.23, 'learning_rate': 0.000198833934211176, 'epoch': 0.1}             \n",
      "{'loss': 1.8013, 'learning_rate': 0.00019883345989043645, 'epoch': 0.1}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1165, 'learning_rate': 0.0001988329854738127, 'epoch': 0.1}          \n",
      "{'loss': 2.5078, 'learning_rate': 0.00019883251096130527, 'epoch': 0.1}         \n",
      "{'loss': 1.9156, 'learning_rate': 0.0001988320363529146, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 103399/1061708 [15:28:39<141:19:15,  1.88it/s][2024-03-01 09:35:39,696] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0404, 'learning_rate': 0.00019883160912338322, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103400/1061708 [15:28:39<132:37:44,  2.01it/s][2024-03-01 09:35:40,119] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9723, 'learning_rate': 0.00019883118181618711, 'epoch': 0.1}         \n",
      "{'loss': 2.6683, 'learning_rate': 0.0001988307069393258, 'epoch': 0.1}          \n",
      "{'loss': 2.1774, 'learning_rate': 0.00019883023196658303, 'epoch': 0.1}         \n",
      "{'loss': 2.1759, 'learning_rate': 0.00019882975689795922, 'epoch': 0.1}         \n",
      "{'loss': 2.1725, 'learning_rate': 0.00019882928173345485, 'epoch': 0.1}         \n",
      "{'loss': 2.0052, 'learning_rate': 0.00019882880647307033, 'epoch': 0.1}         \n",
      "{'loss': 2.0991, 'learning_rate': 0.00019882833111680622, 'epoch': 0.1}         \n",
      "{'loss': 1.8391, 'learning_rate': 0.0001988278556646629, 'epoch': 0.1}          \n",
      "{'loss': 2.2367, 'learning_rate': 0.00019882738011664088, 'epoch': 0.1}         \n",
      "{'loss': 2.0444, 'learning_rate': 0.00019882690447274058, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103501/1061708 [15:29:33<140:44:40,  1.89it/s][2024-03-01 09:36:33,795] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 103502/1061708 [15:29:33<132:10:49,  2.01it/s][2024-03-01 09:36:34,217] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1808, 'learning_rate': 0.00019882652388858828, 'epoch': 0.1}         \n",
      "{'loss': 2.3448, 'learning_rate': 0.00019882604807210826, 'epoch': 0.1}         \n",
      "{'loss': 1.9872, 'learning_rate': 0.0001988255721597513, 'epoch': 0.1}          \n",
      "{'loss': 1.8847, 'learning_rate': 0.0001988250961515178, 'epoch': 0.1}          \n",
      "{'loss': 1.9127, 'learning_rate': 0.00019882462004740825, 'epoch': 0.1}         \n",
      "{'loss': 1.9921, 'learning_rate': 0.00019882414384742313, 'epoch': 0.1}         \n",
      "{'loss': 2.2582, 'learning_rate': 0.0001988236675515629, 'epoch': 0.1}          \n",
      "{'loss': 1.888, 'learning_rate': 0.000198823191159828, 'epoch': 0.1}            \n",
      "{'loss': 1.8924, 'learning_rate': 0.0001988227146722189, 'epoch': 0.1}          \n",
      "{'loss': 2.1934, 'learning_rate': 0.00019882223808873604, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103603/1061708 [15:30:27<144:04:28,  1.85it/s][2024-03-01 09:37:27,966] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 103604/1061708 [15:30:27<134:40:00,  1.98it/s][2024-03-01 09:37:28,389] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2144, 'learning_rate': 0.000198821856752921, 'epoch': 0.1}           \n",
      " 10%|██▋                        | 103615/1061708 [15:30:33<142:09:41,  1.87it/s][2024-03-01 09:37:34,196] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0406, 'learning_rate': 0.0001988214276767863, 'epoch': 0.1}          \n",
      "{'loss': 1.7641, 'learning_rate': 0.00019882095083444668, 'epoch': 0.1}         \n",
      "{'loss': 1.9756, 'learning_rate': 0.00019882047389623505, 'epoch': 0.1}         \n",
      "{'loss': 1.851, 'learning_rate': 0.00019881999686215183, 'epoch': 0.1}          \n",
      "{'loss': 2.0998, 'learning_rate': 0.00019881951973219755, 'epoch': 0.1}         \n",
      "{'loss': 2.2006, 'learning_rate': 0.0001988190425063726, 'epoch': 0.1}          \n",
      "{'loss': 1.9137, 'learning_rate': 0.0001988185651846775, 'epoch': 0.1}          \n",
      "{'loss': 2.2706, 'learning_rate': 0.00019881808776711266, 'epoch': 0.1}         \n",
      "{'loss': 1.7527, 'learning_rate': 0.0001988176102536786, 'epoch': 0.1}          \n",
      "{'loss': 2.0809, 'learning_rate': 0.00019881713264437574, 'epoch': 0.1}         \n",
      "{'loss': 1.9572, 'learning_rate': 0.00019881665493920453, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103725/1061708 [15:31:32<142:21:52,  1.87it/s][2024-03-01 09:38:32,729] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.6774, 'learning_rate': 0.00019881622492258348, 'epoch': 0.1}         \n",
      "{'loss': 2.1595, 'learning_rate': 0.00019881574703526375, 'epoch': 0.1}         \n",
      "{'loss': 2.1136, 'learning_rate': 0.00019881526905207709, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103750/1061708 [15:31:45<140:47:06,  1.89it/s][2024-03-01 09:38:45,968] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0804, 'learning_rate': 0.00019881483878524318, 'epoch': 0.1}         \n",
      "{'loss': 2.392, 'learning_rate': 0.00019881436061991052, 'epoch': 0.1}          \n",
      "{'loss': 1.8293, 'learning_rate': 0.00019881388235871224, 'epoch': 0.1}         \n",
      "{'loss': 2.1509, 'learning_rate': 0.00019881340400164875, 'epoch': 0.1}         \n",
      "{'loss': 1.9817, 'learning_rate': 0.0001988129255487206, 'epoch': 0.1}          \n",
      "{'loss': 2.1007, 'learning_rate': 0.00019881244699992818, 'epoch': 0.1}         \n",
      "{'loss': 1.9706, 'learning_rate': 0.00019881196835527202, 'epoch': 0.1}         \n",
      "{'loss': 1.8638, 'learning_rate': 0.00019881148961475251, 'epoch': 0.1}         \n",
      "{'loss': 1.6254, 'learning_rate': 0.00019881101077837018, 'epoch': 0.1}         \n",
      "{'loss': 2.0648, 'learning_rate': 0.0001988105318461255, 'epoch': 0.1}          \n",
      "{'loss': 1.6957, 'learning_rate': 0.00019881005281801884, 'epoch': 0.1}         \n",
      "{'loss': 1.6461, 'learning_rate': 0.00019880957369405075, 'epoch': 0.1}         \n",
      "{'loss': 1.7215, 'learning_rate': 0.00019880909447422167, 'epoch': 0.1}         \n",
      "{'loss': 1.9002, 'learning_rate': 0.00019880861515853208, 'epoch': 0.1}         \n",
      "{'loss': 1.9652, 'learning_rate': 0.00019880813574698242, 'epoch': 0.1}         \n",
      "{'loss': 2.0421, 'learning_rate': 0.0001988076562395732, 'epoch': 0.1}          \n",
      "{'loss': 1.9854, 'learning_rate': 0.00019880717663630482, 'epoch': 0.1}         \n",
      "{'loss': 2.2238, 'learning_rate': 0.0001988066969371778, 'epoch': 0.1}          \n",
      "{'loss': 1.7709, 'learning_rate': 0.00019880621714219254, 'epoch': 0.1}         \n",
      "{'loss': 2.1391, 'learning_rate': 0.0001988057372513496, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 103951/1061708 [15:33:32<140:41:55,  1.89it/s][2024-03-01 09:40:32,874] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 103952/1061708 [15:33:32<132:06:51,  2.01it/s][2024-03-01 09:40:33,296] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1894, 'learning_rate': 0.00019880535326965797, 'epoch': 0.1}         \n",
      "{'loss': 1.9433, 'learning_rate': 0.00019880487320627225, 'epoch': 0.1}         \n",
      "{'loss': 1.7325, 'learning_rate': 0.00019880439304703016, 'epoch': 0.1}         \n",
      "{'loss': 2.1079, 'learning_rate': 0.00019880391279193207, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 103999/1061708 [15:33:57<140:56:04,  1.89it/s][2024-03-01 09:40:58,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=104000, skipped=1198, lr=[0.00019880343244097848], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 09:40:58,385] [INFO] [timer.py:260:stop] epoch=0/micro_step=104000/global_step=104000, RunningAvgSamplesPerSec=1.8925627372787617, CurrSamplesPerSec=1.9048238415432224, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3536, 'learning_rate': 0.00019880343244097848, 'epoch': 0.1}         \n",
      "{'loss': 2.4596, 'learning_rate': 0.00019880295199416984, 'epoch': 0.1}         \n",
      "{'loss': 2.1055, 'learning_rate': 0.00019880247145150664, 'epoch': 0.1}         \n",
      "{'loss': 2.3834, 'learning_rate': 0.0001988019908129894, 'epoch': 0.1}          \n",
      "{'loss': 2.3766, 'learning_rate': 0.0001988015100786185, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 104042/1061708 [15:34:20<140:54:40,  1.89it/s][2024-03-01 09:41:21,228] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1616, 'learning_rate': 0.00019880107733573018, 'epoch': 0.1}         \n",
      "{'loss': 1.8626, 'learning_rate': 0.00019880059641923868, 'epoch': 0.1}         \n",
      "{'loss': 2.2053, 'learning_rate': 0.0001988001154068949, 'epoch': 0.1}          \n",
      "{'loss': 1.995, 'learning_rate': 0.00019879963429869928, 'epoch': 0.1}          \n",
      "{'loss': 2.0463, 'learning_rate': 0.00019879915309465234, 'epoch': 0.1}         \n",
      "{'loss': 1.9998, 'learning_rate': 0.00019879867179475449, 'epoch': 0.1}         \n",
      "{'loss': 2.1385, 'learning_rate': 0.00019879819039900624, 'epoch': 0.1}         \n",
      "{'loss': 2.3513, 'learning_rate': 0.00019879770890740805, 'epoch': 0.1}         \n",
      "{'loss': 2.4619, 'learning_rate': 0.0001987972273199604, 'epoch': 0.1}          \n",
      "{'loss': 2.2618, 'learning_rate': 0.00019879674563666374, 'epoch': 0.1}         \n",
      "{'loss': 2.2197, 'learning_rate': 0.00019879626385751852, 'epoch': 0.1}         \n",
      "{'loss': 2.2136, 'learning_rate': 0.00019879578198252524, 'epoch': 0.1}         \n",
      "{'loss': 2.1493, 'learning_rate': 0.00019879530001168434, 'epoch': 0.1}         \n",
      "{'loss': 2.297, 'learning_rate': 0.00019879481794499632, 'epoch': 0.1}          \n",
      "{'loss': 1.9883, 'learning_rate': 0.00019879433578246162, 'epoch': 0.1}         \n",
      "{'loss': 1.8207, 'learning_rate': 0.00019879385352408068, 'epoch': 0.1}         \n",
      "{'loss': 2.0003, 'learning_rate': 0.00019879337116985405, 'epoch': 0.1}         \n",
      "{'loss': 1.9497, 'learning_rate': 0.0001987928887197821, 'epoch': 0.1}          \n",
      "{'loss': 2.0562, 'learning_rate': 0.00019879240617386542, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 104237/1061708 [15:36:04<141:19:11,  1.88it/s][2024-03-01 09:43:05,109] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3534, 'learning_rate': 0.00019879197180059345, 'epoch': 0.1}         \n",
      "{'loss': 2.4156, 'learning_rate': 0.00019879148907257291, 'epoch': 0.1}         \n",
      "{'loss': 2.0786, 'learning_rate': 0.00019879100624870894, 'epoch': 0.1}         \n",
      "{'loss': 1.9389, 'learning_rate': 0.00019879052332900195, 'epoch': 0.1}         \n",
      "{'loss': 2.3456, 'learning_rate': 0.00019879004031345249, 'epoch': 0.1}         \n",
      "{'loss': 2.1258, 'learning_rate': 0.000198789557202061, 'epoch': 0.1}           \n",
      "{'loss': 2.2148, 'learning_rate': 0.00019878907399482792, 'epoch': 0.1}         \n",
      "{'loss': 2.2369, 'learning_rate': 0.00019878859069175374, 'epoch': 0.1}         \n",
      "{'loss': 2.1996, 'learning_rate': 0.00019878810729283896, 'epoch': 0.1}         \n",
      "{'loss': 2.1249, 'learning_rate': 0.00019878762379808398, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 104338/1061708 [15:36:58<141:02:10,  1.89it/s][2024-03-01 09:43:58,808] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 104339/1061708 [15:36:58<132:22:03,  2.01it/s][2024-03-01 09:43:59,230] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1368, 'learning_rate': 0.0001987872369332754, 'epoch': 0.1}          \n",
      "{'loss': 2.1002, 'learning_rate': 0.00019878675326600936, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 104354/1061708 [15:37:06<143:03:24,  1.86it/s][2024-03-01 09:44:07,136] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0942, 'learning_rate': 0.00019878631788352764, 'epoch': 0.1}         \n",
      "{'loss': 2.2311, 'learning_rate': 0.00019878583403416816, 'epoch': 0.1}         \n",
      "{'loss': 2.0911, 'learning_rate': 0.00019878535008897071, 'epoch': 0.1}         \n",
      "{'loss': 1.9609, 'learning_rate': 0.0001987848660479358, 'epoch': 0.1}          \n",
      "{'loss': 1.8773, 'learning_rate': 0.0001987843819110639, 'epoch': 0.1}          \n",
      "{'loss': 2.0554, 'learning_rate': 0.00019878389767835538, 'epoch': 0.1}         \n",
      "{'loss': 2.2247, 'learning_rate': 0.0001987834133498108, 'epoch': 0.1}          \n",
      "{'loss': 1.8243, 'learning_rate': 0.00019878292892543066, 'epoch': 0.1}         \n",
      "{'loss': 1.794, 'learning_rate': 0.0001987824444052153, 'epoch': 0.1}           \n",
      "{'loss': 2.1714, 'learning_rate': 0.00019878195978916536, 'epoch': 0.1}         \n",
      "{'loss': 2.0858, 'learning_rate': 0.00019878147507728118, 'epoch': 0.1}         \n",
      "{'loss': 2.3579, 'learning_rate': 0.00019878099026956328, 'epoch': 0.1}         \n",
      "{'loss': 1.7852, 'learning_rate': 0.0001987805053660121, 'epoch': 0.1}          \n",
      "{'loss': 1.8656, 'learning_rate': 0.00019878002036662816, 'epoch': 0.1}         \n",
      "{'loss': 2.1548, 'learning_rate': 0.0001987795352714119, 'epoch': 0.1}          \n",
      "{'loss': 2.1032, 'learning_rate': 0.0001987790500803638, 'epoch': 0.1}          \n",
      "{'loss': 2.179, 'learning_rate': 0.00019877856479348432, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 104527/1061708 [15:38:38<141:32:46,  1.88it/s][2024-03-01 09:45:39,206] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0495, 'learning_rate': 0.00019877812795335734, 'epoch': 0.1}         \n",
      "{'loss': 1.8502, 'learning_rate': 0.00019877764248439953, 'epoch': 0.1}         \n",
      "{'loss': 2.1909, 'learning_rate': 0.00019877715691961174, 'epoch': 0.1}         \n",
      "{'loss': 2.4558, 'learning_rate': 0.00019877667125899442, 'epoch': 0.1}         \n",
      "{'loss': 2.0765, 'learning_rate': 0.00019877618550254798, 'epoch': 0.1}         \n",
      "{'loss': 2.0404, 'learning_rate': 0.00019877569965027299, 'epoch': 0.1}         \n",
      "{'loss': 1.9641, 'learning_rate': 0.00019877521370216986, 'epoch': 0.1}         \n",
      "{'loss': 1.9355, 'learning_rate': 0.0001987747276582391, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 104602/1061708 [15:39:18<140:21:22,  1.89it/s][2024-03-01 09:46:19,066] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      " 10%|██▋                        | 104608/1061708 [15:39:21<141:02:32,  1.88it/s][2024-03-01 09:46:22,172] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0956, 'learning_rate': 0.0001987743387540989, 'epoch': 0.1}          \n",
      "{'loss': 2.3348, 'learning_rate': 0.0001987738525376795, 'epoch': 0.1}          \n",
      "{'loss': 2.2868, 'learning_rate': 0.0001987733662254338, 'epoch': 0.1}          \n",
      "{'loss': 1.9118, 'learning_rate': 0.00019877287981736222, 'epoch': 0.1}         \n",
      "{'loss': 1.7734, 'learning_rate': 0.0001987723933134653, 'epoch': 0.1}          \n",
      "{'loss': 2.0308, 'learning_rate': 0.0001987719067137434, 'epoch': 0.1}          \n",
      "{'loss': 2.1731, 'learning_rate': 0.0001987714200181971, 'epoch': 0.1}          \n",
      "{'loss': 2.1037, 'learning_rate': 0.00019877093322682684, 'epoch': 0.1}         \n",
      "{'loss': 2.2326, 'learning_rate': 0.00019877044633963306, 'epoch': 0.1}         \n",
      "{'loss': 2.3117, 'learning_rate': 0.00019876995935661624, 'epoch': 0.1}         \n",
      "{'loss': 1.8845, 'learning_rate': 0.00019876947227777687, 'epoch': 0.1}         \n",
      "{'loss': 2.2952, 'learning_rate': 0.00019876898510311545, 'epoch': 0.1}         \n",
      "{'loss': 2.1594, 'learning_rate': 0.0001987684978326324, 'epoch': 0.1}          \n",
      "{'loss': 1.8312, 'learning_rate': 0.00019876801046632818, 'epoch': 0.1}         \n",
      "{'loss': 1.9627, 'learning_rate': 0.00019876752300420334, 'epoch': 0.1}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9401, 'learning_rate': 0.00019876703544625826, 'epoch': 0.1}         \n",
      "{'loss': 2.0141, 'learning_rate': 0.0001987665477924935, 'epoch': 0.1}          \n",
      "{'loss': 2.1818, 'learning_rate': 0.0001987660600429095, 'epoch': 0.1}          \n",
      "{'loss': 2.5144, 'learning_rate': 0.00019876557219750671, 'epoch': 0.1}         \n",
      "{'loss': 2.0427, 'learning_rate': 0.00019876508425628565, 'epoch': 0.1}         \n",
      "{'loss': 2.0998, 'learning_rate': 0.00019876459621924673, 'epoch': 0.1}         \n",
      "{'loss': 1.9182, 'learning_rate': 0.00019876410808639046, 'epoch': 0.1}         \n",
      "{'loss': 1.9523, 'learning_rate': 0.0001987636198577173, 'epoch': 0.1}          \n",
      "{'loss': 2.093, 'learning_rate': 0.00019876313153322777, 'epoch': 0.1}          \n",
      "{'loss': 2.4651, 'learning_rate': 0.0001987626431129223, 'epoch': 0.1}          \n",
      "{'loss': 1.7896, 'learning_rate': 0.00019876215459680138, 'epoch': 0.1}         \n",
      "{'loss': 2.3551, 'learning_rate': 0.00019876166598486544, 'epoch': 0.1}         \n",
      "{'loss': 1.8102, 'learning_rate': 0.00019876117727711504, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 104887/1061708 [15:41:50<141:20:23,  1.88it/s][2024-03-01 09:48:50,772] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3275, 'learning_rate': 0.00019876073735821862, 'epoch': 0.1}         \n",
      "{'loss': 1.955, 'learning_rate': 0.00019876024846842195, 'epoch': 0.1}          \n",
      "{'loss': 1.9078, 'learning_rate': 0.0001987597594828121, 'epoch': 0.1}          \n",
      "{'loss': 2.099, 'learning_rate': 0.00019875927040138963, 'epoch': 0.1}          \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019875878122415497, 'epoch': 0.1}         \n",
      "{'loss': 1.624, 'learning_rate': 0.0001987582919511086, 'epoch': 0.1}           \n",
      "{'loss': 1.9406, 'learning_rate': 0.00019875780258225102, 'epoch': 0.1}         \n",
      "{'loss': 1.8963, 'learning_rate': 0.00019875731311758262, 'epoch': 0.1}         \n",
      "{'loss': 1.9039, 'learning_rate': 0.00019875682355710396, 'epoch': 0.1}         \n",
      "{'loss': 2.1374, 'learning_rate': 0.0001987563339008155, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 104988/1061708 [15:42:43<140:56:08,  1.89it/s][2024-03-01 09:49:44,514] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 104989/1061708 [15:42:44<132:14:51,  2.01it/s][2024-03-01 09:49:44,936] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3642, 'learning_rate': 0.00019875594210680197, 'epoch': 0.1}         \n",
      "{'loss': 1.5398, 'learning_rate': 0.00019875545227805702, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105000/1061708 [15:42:50<140:13:13,  1.90it/s][INFO|trainer.py:2868] 2024-03-01 09:49:50,263 >> Saving model checkpoint to output_model/checkpoint-105000\n",
      "[INFO|trainer.py:2880] 2024-03-01 09:49:50,266 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 09:49:51,493 >> tokenizer config file saved in output_model/checkpoint-105000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 09:49:51,493 >> Special tokens file saved in output_model/checkpoint-105000/special_tokens_map.json\n",
      "[2024-03-01 09:49:51,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step105000 is about to be saved!\n",
      "[2024-03-01 09:49:56,779] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-105000/global_step105000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 09:49:56,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-105000/global_step105000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 09:50:10,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-105000/global_step105000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 09:50:11,527] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-105000/global_step105000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 09:50:18,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-105000/global_step105000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 09:50:18,732] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-105000/global_step105000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 09:50:18,732] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step105000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 09:50:18,826 >> Deleting older checkpoint [output_model/checkpoint-90000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 09:50:22,596 >> tokenizer config file saved in output_model/checkpoint-105000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 09:50:22,596 >> Special tokens file saved in output_model/checkpoint-105000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.337, 'learning_rate': 0.00019875496235350362, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105017/1061708 [15:43:31<148:13:52,  1.79it/s][2024-03-01 09:50:31,974] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8128, 'learning_rate': 0.0001987545213394897, 'epoch': 0.1}          \n",
      "{'loss': 1.9242, 'learning_rate': 0.00019875403123290148, 'epoch': 0.1}         \n",
      "{'loss': 2.3632, 'learning_rate': 0.00019875354103050618, 'epoch': 0.1}         \n",
      "{'loss': 1.7674, 'learning_rate': 0.0001987530507323042, 'epoch': 0.1}          \n",
      "{'loss': 2.199, 'learning_rate': 0.00019875256033829612, 'epoch': 0.1}          \n",
      "{'loss': 2.0812, 'learning_rate': 0.00019875206984848234, 'epoch': 0.1}         \n",
      "{'loss': 2.003, 'learning_rate': 0.00019875157926286337, 'epoch': 0.1}          \n",
      "{'loss': 1.849, 'learning_rate': 0.00019875108858143968, 'epoch': 0.1}          \n",
      "{'loss': 1.8249, 'learning_rate': 0.00019875059780421174, 'epoch': 0.1}         \n",
      "{'loss': 1.674, 'learning_rate': 0.00019875010693118, 'epoch': 0.1}             \n",
      "{'loss': 2.1528, 'learning_rate': 0.000198749615962345, 'epoch': 0.1}           \n",
      "{'loss': 1.6044, 'learning_rate': 0.00019874912489770718, 'epoch': 0.1}         \n",
      "{'loss': 2.4778, 'learning_rate': 0.000198748633737267, 'epoch': 0.1}           \n",
      " 10%|██▋                        | 105148/1061708 [15:44:41<141:18:03,  1.88it/s][2024-03-01 09:51:41,580] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2017, 'learning_rate': 0.00019874819161096023, 'epoch': 0.1}         \n",
      "{'loss': 2.192, 'learning_rate': 0.00019874770026849693, 'epoch': 0.1}          \n",
      "{'loss': 2.1313, 'learning_rate': 0.0001987472088302327, 'epoch': 0.1}          \n",
      "{'loss': 2.1321, 'learning_rate': 0.0001987467172961679, 'epoch': 0.1}          \n",
      "{'loss': 2.2281, 'learning_rate': 0.00019874622566630316, 'epoch': 0.1}         \n",
      "{'loss': 2.417, 'learning_rate': 0.00019874573394063888, 'epoch': 0.1}          \n",
      "{'loss': 1.8426, 'learning_rate': 0.00019874524211917555, 'epoch': 0.1}         \n",
      "{'loss': 2.3717, 'learning_rate': 0.0001987447502019136, 'epoch': 0.1}          \n",
      "{'loss': 1.6638, 'learning_rate': 0.00019874425818885362, 'epoch': 0.1}         \n",
      "{'loss': 1.9159, 'learning_rate': 0.00019874376607999597, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105249/1061708 [15:45:34<141:19:56,  1.88it/s][2024-03-01 09:52:35,514] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.733, 'learning_rate': 0.00019874332310011755, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105250/1061708 [15:45:35<132:41:08,  2.00it/s][2024-03-01 09:52:35,938] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1453, 'learning_rate': 0.00019874288004264375, 'epoch': 0.1}         \n",
      "{'loss': 1.8064, 'learning_rate': 0.0001987423876655553, 'epoch': 0.1}          \n",
      "{'loss': 1.9191, 'learning_rate': 0.00019874189519267108, 'epoch': 0.1}         \n",
      "{'loss': 2.2348, 'learning_rate': 0.0001987414026239915, 'epoch': 0.1}          \n",
      "{'loss': 2.1206, 'learning_rate': 0.0001987409099595171, 'epoch': 0.1}          \n",
      "{'loss': 2.1564, 'learning_rate': 0.0001987404171992483, 'epoch': 0.1}          \n",
      "{'loss': 2.1453, 'learning_rate': 0.0001987399243431856, 'epoch': 0.1}          \n",
      "{'loss': 2.4433, 'learning_rate': 0.0001987394313913295, 'epoch': 0.1}          \n",
      "{'loss': 1.8875, 'learning_rate': 0.00019873893834368046, 'epoch': 0.1}         \n",
      "{'loss': 2.2467, 'learning_rate': 0.00019873844520023899, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105351/1061708 [15:46:29<140:52:31,  1.89it/s][2024-03-01 09:53:29,825] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 105352/1061708 [15:46:29<132:19:13,  2.01it/s][2024-03-01 09:53:30,250] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0091, 'learning_rate': 0.00019873805061651552, 'epoch': 0.1}         \n",
      "{'loss': 2.2194, 'learning_rate': 0.0001987375573006488, 'epoch': 0.1}          \n",
      "{'loss': 2.2912, 'learning_rate': 0.00019873706388899096, 'epoch': 0.1}         \n",
      "{'loss': 1.8687, 'learning_rate': 0.00019873657038154248, 'epoch': 0.1}         \n",
      "{'loss': 1.5958, 'learning_rate': 0.00019873607677830382, 'epoch': 0.1}         \n",
      "{'loss': 2.0843, 'learning_rate': 0.00019873558307927552, 'epoch': 0.1}         \n",
      "{'loss': 2.0512, 'learning_rate': 0.000198735089284458, 'epoch': 0.1}           \n",
      "{'loss': 2.2379, 'learning_rate': 0.0001987345953938517, 'epoch': 0.1}          \n",
      "{'loss': 2.4334, 'learning_rate': 0.00019873410140745722, 'epoch': 0.1}         \n",
      "{'loss': 2.0298, 'learning_rate': 0.00019873360732527493, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105452/1061708 [15:47:22<140:29:46,  1.89it/s][2024-03-01 09:54:23,480] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1701, 'learning_rate': 0.00019873316256941275, 'epoch': 0.1}         \n",
      "{'loss': 1.9578, 'learning_rate': 0.00019873266830523504, 'epoch': 0.1}         \n",
      "{'loss': 2.0846, 'learning_rate': 0.00019873217394527094, 'epoch': 0.1}         \n",
      "{'loss': 1.7418, 'learning_rate': 0.00019873167948952097, 'epoch': 0.1}         \n",
      "{'loss': 2.1775, 'learning_rate': 0.00019873118493798556, 'epoch': 0.1}         \n",
      "{'loss': 1.8486, 'learning_rate': 0.00019873069029066525, 'epoch': 0.1}         \n",
      "{'loss': 2.6091, 'learning_rate': 0.00019873019554756045, 'epoch': 0.1}         \n",
      "{'loss': 1.7698, 'learning_rate': 0.00019872970070867164, 'epoch': 0.1}         \n",
      "{'loss': 2.2303, 'learning_rate': 0.00019872920577399938, 'epoch': 0.1}         \n",
      "{'loss': 1.664, 'learning_rate': 0.00019872871074354408, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105557/1061708 [15:48:18<141:11:04,  1.88it/s][2024-03-01 09:55:19,391] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 105559/1061708 [15:48:19<135:13:05,  1.96it/s][2024-03-01 09:55:20,350] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1109, 'learning_rate': 0.00019872831465021643, 'epoch': 0.1}         \n",
      "{'loss': 2.3599, 'learning_rate': 0.0001987278194473529, 'epoch': 0.1}          \n",
      "{'loss': 1.8241, 'learning_rate': 0.0001987273241487077, 'epoch': 0.1}          \n",
      "{'loss': 2.1577, 'learning_rate': 0.00019872682875428135, 'epoch': 0.1}         \n",
      "{'loss': 2.1922, 'learning_rate': 0.00019872633326407425, 'epoch': 0.1}         \n",
      "{'loss': 2.2122, 'learning_rate': 0.00019872583767808694, 'epoch': 0.1}         \n",
      "{'loss': 2.4377, 'learning_rate': 0.0001987253419963199, 'epoch': 0.1}          \n",
      "{'loss': 2.249, 'learning_rate': 0.00019872484621877355, 'epoch': 0.1}          \n",
      "{'loss': 2.1536, 'learning_rate': 0.00019872435034544843, 'epoch': 0.1}         \n",
      "{'loss': 1.7123, 'learning_rate': 0.00019872385437634497, 'epoch': 0.1}         \n",
      "{'loss': 2.1259, 'learning_rate': 0.00019872335831146374, 'epoch': 0.1}         \n",
      "{'loss': 2.2608, 'learning_rate': 0.00019872286215080515, 'epoch': 0.1}         \n",
      "{'loss': 2.6021, 'learning_rate': 0.00019872236589436965, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105684/1061708 [15:49:26<142:43:30,  1.86it/s][2024-03-01 09:56:26,862] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1632, 'learning_rate': 0.00019872191918168892, 'epoch': 0.1}         \n",
      "{'loss': 1.9555, 'learning_rate': 0.0001987214227432787, 'epoch': 0.1}          \n",
      "{'loss': 2.0599, 'learning_rate': 0.00019872092620909309, 'epoch': 0.1}         \n",
      "{'loss': 2.1255, 'learning_rate': 0.00019872042957913246, 'epoch': 0.1}         \n",
      "{'loss': 1.8876, 'learning_rate': 0.00019871993285339732, 'epoch': 0.1}         \n",
      "{'loss': 1.6017, 'learning_rate': 0.00019871943603188817, 'epoch': 0.1}         \n",
      "{'loss': 2.2375, 'learning_rate': 0.00019871893911460546, 'epoch': 0.1}         \n",
      "{'loss': 1.9506, 'learning_rate': 0.00019871844210154972, 'epoch': 0.1}         \n",
      "{'loss': 2.3198, 'learning_rate': 0.0001987179449927214, 'epoch': 0.1}          \n",
      "{'loss': 1.728, 'learning_rate': 0.00019871744778812098, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105785/1061708 [15:50:20<142:05:18,  1.87it/s][2024-03-01 09:57:20,637] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 105786/1061708 [15:50:20<133:01:23,  2.00it/s][2024-03-01 09:57:21,059] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2763, 'learning_rate': 0.00019871704995548504, 'epoch': 0.1}         \n",
      "{'loss': 1.9584, 'learning_rate': 0.00019871655257849608, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 105803/1061708 [15:50:29<144:06:53,  1.84it/s][2024-03-01 09:57:30,077] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8542, 'learning_rate': 0.000198716104857322, 'epoch': 0.1}           \n",
      "{'loss': 2.1248, 'learning_rate': 0.00019871560729836903, 'epoch': 0.1}         \n",
      "{'loss': 2.1992, 'learning_rate': 0.00019871510964364624, 'epoch': 0.1}         \n",
      "{'loss': 2.1478, 'learning_rate': 0.0001987146118931541, 'epoch': 0.1}          \n",
      "{'loss': 2.1871, 'learning_rate': 0.0001987141140468931, 'epoch': 0.1}          \n",
      "{'loss': 2.0107, 'learning_rate': 0.0001987136161048637, 'epoch': 0.1}          \n",
      "{'loss': 2.0765, 'learning_rate': 0.00019871311806706645, 'epoch': 0.1}         \n",
      "{'loss': 1.9759, 'learning_rate': 0.00019871261993350178, 'epoch': 0.1}         \n",
      "{'loss': 2.3524, 'learning_rate': 0.00019871212170417018, 'epoch': 0.1}         \n",
      "{'loss': 2.2225, 'learning_rate': 0.00019871162337907215, 'epoch': 0.1}         \n",
      "{'loss': 2.0869, 'learning_rate': 0.00019871112495820813, 'epoch': 0.1}         \n",
      "{'loss': 1.6865, 'learning_rate': 0.00019871062644157865, 'epoch': 0.1}         \n",
      "{'loss': 2.1043, 'learning_rate': 0.00019871012782918417, 'epoch': 0.1}         \n",
      "{'loss': 2.2696, 'learning_rate': 0.00019870962912102517, 'epoch': 0.1}         \n",
      "{'loss': 2.025, 'learning_rate': 0.00019870913031710217, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105955/1061708 [15:51:50<142:11:36,  1.87it/s][2024-03-01 09:58:51,085] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7221, 'learning_rate': 0.00019870868131169364, 'epoch': 0.1}         \n",
      "{'loss': 1.909, 'learning_rate': 0.00019870818232582032, 'epoch': 0.1}          \n",
      "{'loss': 2.0781, 'learning_rate': 0.00019870768324418435, 'epoch': 0.1}         \n",
      "{'loss': 1.9475, 'learning_rate': 0.0001987071840667863, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 105999/1061708 [15:52:13<140:33:48,  1.89it/s][2024-03-01 09:59:14,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=106000, skipped=1223, lr=[0.00019870668479362655], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 09:59:14,583] [INFO] [timer.py:260:stop] epoch=0/micro_step=106000/global_step=106000, RunningAvgSamplesPerSec=1.8926122521160338, CurrSamplesPerSec=1.9125951725292079, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0697, 'learning_rate': 0.00019870668479362655, 'epoch': 0.1}         \n",
      "{'loss': 2.2011, 'learning_rate': 0.00019870618542470564, 'epoch': 0.1}         \n",
      "{'loss': 2.0357, 'learning_rate': 0.00019870568596002406, 'epoch': 0.1}         \n",
      "{'loss': 1.967, 'learning_rate': 0.00019870518639958227, 'epoch': 0.1}          \n",
      "{'loss': 1.9608, 'learning_rate': 0.00019870468674338076, 'epoch': 0.1}         \n",
      "{'loss': 1.8766, 'learning_rate': 0.00019870418699142003, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 106056/1061708 [15:52:44<141:31:45,  1.88it/s][2024-03-01 09:59:44,862] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 106057/1061708 [15:52:44<132:36:22,  2.00it/s][2024-03-01 09:59:45,285] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8667, 'learning_rate': 0.00019870378712090513, 'epoch': 0.1}         \n",
      "{'loss': 2.1584, 'learning_rate': 0.000198703287196579, 'epoch': 0.1}           \n",
      "{'loss': 2.0561, 'learning_rate': 0.000198702787176495, 'epoch': 0.1}           \n",
      "{'loss': 1.996, 'learning_rate': 0.0001987022870606536, 'epoch': 0.1}           \n",
      "{'loss': 1.8348, 'learning_rate': 0.0001987017868490553, 'epoch': 0.1}          \n",
      "{'loss': 2.2653, 'learning_rate': 0.00019870128654170063, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 106116/1061708 [15:53:16<141:30:02,  1.88it/s][2024-03-01 10:00:16,636] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3503, 'learning_rate': 0.00019870083618321005, 'epoch': 0.1}         \n",
      "{'loss': 1.9743, 'learning_rate': 0.00019870033569391947, 'epoch': 0.1}         \n",
      "{'loss': 2.1146, 'learning_rate': 0.00019869983510887394, 'epoch': 0.1}         \n",
      "{'loss': 2.1129, 'learning_rate': 0.00019869933442807382, 'epoch': 0.1}         \n",
      "{'loss': 1.8549, 'learning_rate': 0.0001986988336515197, 'epoch': 0.1}          \n",
      "{'loss': 2.1169, 'learning_rate': 0.000198698332779212, 'epoch': 0.1}           \n",
      "{'loss': 2.3814, 'learning_rate': 0.00019869783181115127, 'epoch': 0.1}         \n",
      "{'loss': 2.2533, 'learning_rate': 0.00019869733074733795, 'epoch': 0.1}         \n",
      "{'loss': 2.3448, 'learning_rate': 0.00019869682958777252, 'epoch': 0.1}         \n",
      "{'loss': 1.9177, 'learning_rate': 0.00019869632833245549, 'epoch': 0.1}         \n",
      "{'loss': 2.0416, 'learning_rate': 0.00019869582698138735, 'epoch': 0.1}         \n",
      "{'loss': 1.9535, 'learning_rate': 0.00019869532553456857, 'epoch': 0.1}         \n",
      "{'loss': 2.0316, 'learning_rate': 0.00019869482399199962, 'epoch': 0.1}         \n",
      "{'loss': 1.8998, 'learning_rate': 0.000198694322353681, 'epoch': 0.1}           \n",
      "{'loss': 1.5791, 'learning_rate': 0.00019869382061961325, 'epoch': 0.1}         \n",
      "{'loss': 1.8231, 'learning_rate': 0.0001986933187897968, 'epoch': 0.1}          \n",
      "{'loss': 2.4145, 'learning_rate': 0.00019869281686423214, 'epoch': 0.1}         \n",
      "{'loss': 2.1072, 'learning_rate': 0.00019869231484291976, 'epoch': 0.1}         \n",
      "{'loss': 2.0375, 'learning_rate': 0.00019869181272586016, 'epoch': 0.1}         \n",
      "{'loss': 2.196, 'learning_rate': 0.0001986913105130538, 'epoch': 0.1}           \n",
      " 10%|██▋                        | 106317/1061708 [15:55:03<140:57:43,  1.88it/s][2024-03-01 10:02:03,717] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 106318/1061708 [15:55:03<132:12:18,  2.01it/s][2024-03-01 10:02:04,139] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9253, 'learning_rate': 0.00019869090867387143, 'epoch': 0.1}         \n",
      "{'loss': 1.9147, 'learning_rate': 0.00019869040628872219, 'epoch': 0.1}         \n",
      "{'loss': 1.642, 'learning_rate': 0.00019868990380782754, 'epoch': 0.1}          \n",
      "{'loss': 1.9815, 'learning_rate': 0.00019868940123118805, 'epoch': 0.1}         \n",
      "{'loss': 2.2234, 'learning_rate': 0.00019868889855880414, 'epoch': 0.1}         \n",
      "{'loss': 1.9419, 'learning_rate': 0.0001986883957906763, 'epoch': 0.1}          \n",
      "{'loss': 2.0786, 'learning_rate': 0.00019868789292680505, 'epoch': 0.1}         \n",
      "{'loss': 2.047, 'learning_rate': 0.00019868738996719088, 'epoch': 0.1}          \n",
      "{'loss': 1.7986, 'learning_rate': 0.0001986868869118342, 'epoch': 0.1}          \n",
      "{'loss': 1.8095, 'learning_rate': 0.0001986863837607356, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 106419/1061708 [15:55:57<141:09:30,  1.88it/s][2024-03-01 10:02:57,874] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.498, 'learning_rate': 0.00019868593084288792, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 106420/1061708 [15:55:57<132:28:13,  2.00it/s][2024-03-01 10:02:58,297] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0293, 'learning_rate': 0.00019868547784748994, 'epoch': 0.1}         \n",
      "{'loss': 2.1818, 'learning_rate': 0.00019868497442831646, 'epoch': 0.1}         \n",
      "{'loss': 2.1275, 'learning_rate': 0.00019868447091340286, 'epoch': 0.1}         \n",
      "{'loss': 1.8027, 'learning_rate': 0.00019868396730274963, 'epoch': 0.1}         \n",
      "{'loss': 2.2044, 'learning_rate': 0.0001986834635963573, 'epoch': 0.1}          \n",
      "{'loss': 2.271, 'learning_rate': 0.00019868295979422632, 'epoch': 0.1}          \n",
      "{'loss': 2.3887, 'learning_rate': 0.00019868245589635714, 'epoch': 0.1}         \n",
      "{'loss': 1.9479, 'learning_rate': 0.00019868195190275037, 'epoch': 0.1}         \n",
      "{'loss': 1.8421, 'learning_rate': 0.0001986814478134064, 'epoch': 0.1}          \n",
      "{'loss': 2.3448, 'learning_rate': 0.00019868094362832572, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 106521/1061708 [15:56:51<140:23:50,  1.89it/s][2024-03-01 10:03:52,086] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 106522/1061708 [15:56:51<131:52:51,  2.01it/s][2024-03-01 10:03:52,509] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9269, 'learning_rate': 0.00019868054021133111, 'epoch': 0.1}         \n",
      "{'loss': 2.2205, 'learning_rate': 0.00019868003585392564, 'epoch': 0.1}         \n",
      "{'loss': 1.9576, 'learning_rate': 0.00019867953140078487, 'epoch': 0.1}         \n",
      "{'loss': 2.2376, 'learning_rate': 0.00019867902685190926, 'epoch': 0.1}         \n",
      "{'loss': 2.1794, 'learning_rate': 0.00019867852220729933, 'epoch': 0.1}         \n",
      "{'loss': 1.6437, 'learning_rate': 0.00019867801746695555, 'epoch': 0.1}         \n",
      "{'loss': 1.9187, 'learning_rate': 0.0001986775126308784, 'epoch': 0.1}          \n",
      "{'loss': 1.8979, 'learning_rate': 0.00019867700769906837, 'epoch': 0.1}         \n",
      "{'loss': 2.2691, 'learning_rate': 0.000198676502671526, 'epoch': 0.1}           \n",
      "{'loss': 1.8777, 'learning_rate': 0.00019867599754825173, 'epoch': 0.1}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|██▋                        | 106623/1061708 [15:57:45<141:00:49,  1.88it/s][2024-03-01 10:04:46,379] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 106624/1061708 [15:57:46<135:43:49,  1.95it/s][2024-03-01 10:04:46,804] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.5848, 'learning_rate': 0.00019867559338070567, 'epoch': 0.1}         \n",
      "{'loss': 1.9238, 'learning_rate': 0.00019867508808511523, 'epoch': 0.1}         \n",
      "{'loss': 1.9849, 'learning_rate': 0.0001986745826937943, 'epoch': 0.1}          \n",
      "{'loss': 1.8228, 'learning_rate': 0.0001986740772067433, 'epoch': 0.1}          \n",
      "{'loss': 2.0854, 'learning_rate': 0.0001986735716239628, 'epoch': 0.1}          \n",
      "{'loss': 1.9317, 'learning_rate': 0.00019867306594545324, 'epoch': 0.1}         \n",
      "{'loss': 1.7835, 'learning_rate': 0.00019867256017121516, 'epoch': 0.1}         \n",
      "{'loss': 2.0983, 'learning_rate': 0.000198672054301249, 'epoch': 0.1}           \n",
      "{'loss': 1.9113, 'learning_rate': 0.00019867154833555525, 'epoch': 0.1}         \n",
      "{'loss': 2.0768, 'learning_rate': 0.00019867104227413442, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 106725/1061708 [15:58:40<142:10:41,  1.87it/s][2024-03-01 10:05:40,703] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 106726/1061708 [15:58:40<133:14:33,  1.99it/s][2024-03-01 10:05:41,127] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.183, 'learning_rate': 0.0001986706373560746, 'epoch': 0.1}           \n",
      "{'loss': 2.0362, 'learning_rate': 0.00019867013112234626, 'epoch': 0.1}         \n",
      "{'loss': 2.0887, 'learning_rate': 0.0001986696247928922, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 106758/1061708 [15:58:57<140:53:45,  1.88it/s][2024-03-01 10:05:58,158] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9118, 'learning_rate': 0.00019866916901453853, 'epoch': 0.1}         \n",
      "{'loss': 1.9597, 'learning_rate': 0.000198668662503207, 'epoch': 0.1}           \n",
      "{'loss': 2.1481, 'learning_rate': 0.00019866815589615123, 'epoch': 0.1}         \n",
      "{'loss': 1.8102, 'learning_rate': 0.00019866764919337161, 'epoch': 0.1}         \n",
      "{'loss': 2.1433, 'learning_rate': 0.0001986671423948687, 'epoch': 0.1}          \n",
      "{'loss': 2.1275, 'learning_rate': 0.000198666635500643, 'epoch': 0.1}           \n",
      "{'loss': 2.088, 'learning_rate': 0.00019866612851069502, 'epoch': 0.1}          \n",
      "{'loss': 2.2963, 'learning_rate': 0.00019866562142502518, 'epoch': 0.1}         \n",
      "{'loss': 1.6694, 'learning_rate': 0.000198665114243634, 'epoch': 0.1}           \n",
      "{'loss': 1.8977, 'learning_rate': 0.000198664606966522, 'epoch': 0.1}           \n",
      "{'loss': 1.965, 'learning_rate': 0.00019866409959368967, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 106867/1061708 [15:59:55<141:59:59,  1.87it/s][2024-03-01 10:06:56,298] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9587, 'learning_rate': 0.00019866364287630006, 'epoch': 0.1}         \n",
      "{'loss': 1.5341, 'learning_rate': 0.0001986631353216004, 'epoch': 0.1}          \n",
      "{'loss': 1.6504, 'learning_rate': 0.00019866262767118182, 'epoch': 0.1}         \n",
      "{'loss': 2.2612, 'learning_rate': 0.00019866211992504483, 'epoch': 0.1}         \n",
      "{'loss': 2.19, 'learning_rate': 0.0001986616120831899, 'epoch': 0.1}            \n",
      "{'loss': 1.8423, 'learning_rate': 0.00019866110414561755, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 106924/1061708 [16:00:26<142:48:42,  1.86it/s][2024-03-01 10:07:26,657] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0773, 'learning_rate': 0.00019866064691996437, 'epoch': 0.1}         \n",
      "{'loss': 2.0917, 'learning_rate': 0.00019866013880053026, 'epoch': 0.1}         \n",
      "{'loss': 2.2418, 'learning_rate': 0.00019865963058538007, 'epoch': 0.1}         \n",
      "{'loss': 1.7961, 'learning_rate': 0.0001986591222745144, 'epoch': 0.1}          \n",
      "{'loss': 1.9397, 'learning_rate': 0.0001986586138679337, 'epoch': 0.1}          \n",
      "{'loss': 2.0911, 'learning_rate': 0.00019865810536563844, 'epoch': 0.1}         \n",
      "{'loss': 2.1825, 'learning_rate': 0.0001986575967676292, 'epoch': 0.1}          \n",
      "{'loss': 2.5015, 'learning_rate': 0.00019865708807390636, 'epoch': 0.1}         \n",
      "{'loss': 2.1166, 'learning_rate': 0.00019865657928447048, 'epoch': 0.1}         \n",
      "{'loss': 2.0835, 'learning_rate': 0.00019865607039932205, 'epoch': 0.1}         \n",
      "{'loss': 2.088, 'learning_rate': 0.00019865556141846153, 'epoch': 0.1}          \n",
      "{'loss': 1.9877, 'learning_rate': 0.00019865505234188945, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107042/1061708 [16:01:29<140:48:20,  1.88it/s][2024-03-01 10:08:29,631] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.198, 'learning_rate': 0.0001986545940911416, 'epoch': 0.1}           \n",
      "{'loss': 2.0563, 'learning_rate': 0.00019865408483271893, 'epoch': 0.1}         \n",
      "{'loss': 2.2076, 'learning_rate': 0.0001986535754785861, 'epoch': 0.1}          \n",
      "{'loss': 2.0421, 'learning_rate': 0.0001986530660287436, 'epoch': 0.1}          \n",
      "{'loss': 2.3528, 'learning_rate': 0.000198652556483192, 'epoch': 0.1}           \n",
      "{'loss': 1.8321, 'learning_rate': 0.0001986520468419317, 'epoch': 0.1}          \n",
      "{'loss': 2.646, 'learning_rate': 0.00019865153710496328, 'epoch': 0.1}          \n",
      "{'loss': 2.2893, 'learning_rate': 0.00019865102727228715, 'epoch': 0.1}         \n",
      "{'loss': 1.8637, 'learning_rate': 0.00019865051734390386, 'epoch': 0.1}         \n",
      "{'loss': 2.2913, 'learning_rate': 0.00019865000731981392, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107143/1061708 [16:02:23<144:02:20,  1.84it/s][2024-03-01 10:09:23,539] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 107144/1061708 [16:02:23<134:31:14,  1.97it/s][2024-03-01 10:09:23,962] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4418, 'learning_rate': 0.00019864959923163346, 'epoch': 0.1}         \n",
      "{'loss': 2.2467, 'learning_rate': 0.00019864908903527274, 'epoch': 0.1}         \n",
      "{'loss': 2.2047, 'learning_rate': 0.0001986485787432067, 'epoch': 0.1}          \n",
      "{'loss': 2.029, 'learning_rate': 0.00019864806835543587, 'epoch': 0.1}          \n",
      "{'loss': 2.0321, 'learning_rate': 0.00019864755787196077, 'epoch': 0.1}         \n",
      "{'loss': 2.1589, 'learning_rate': 0.00019864704729278182, 'epoch': 0.1}         \n",
      "{'loss': 2.0662, 'learning_rate': 0.0001986465366178996, 'epoch': 0.1}          \n",
      "{'loss': 1.9929, 'learning_rate': 0.00019864602584731455, 'epoch': 0.1}         \n",
      "{'loss': 1.9361, 'learning_rate': 0.00019864551498102718, 'epoch': 0.1}         \n",
      "{'loss': 2.0612, 'learning_rate': 0.00019864500401903796, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107245/1061708 [16:03:17<142:15:17,  1.86it/s][2024-03-01 10:10:17,835] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 107246/1061708 [16:03:17<133:18:45,  1.99it/s][2024-03-01 10:10:18,259] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1123, 'learning_rate': 0.00019864459518054162, 'epoch': 0.1}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1713, 'learning_rate': 0.0001986440840462904, 'epoch': 0.1}          \n",
      "{'loss': 2.1451, 'learning_rate': 0.00019864357281633872, 'epoch': 0.1}         \n",
      "{'loss': 2.1769, 'learning_rate': 0.0001986430614906871, 'epoch': 0.1}          \n",
      "{'loss': 1.9315, 'learning_rate': 0.00019864255006933607, 'epoch': 0.1}         \n",
      "{'loss': 2.0643, 'learning_rate': 0.00019864203855228608, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107304/1061708 [16:03:48<142:58:19,  1.85it/s][2024-03-01 10:10:49,196] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0931, 'learning_rate': 0.00019864157810511887, 'epoch': 0.1}         \n",
      "{'loss': 1.9854, 'learning_rate': 0.00019864106640624224, 'epoch': 0.1}         \n",
      "{'loss': 1.83, 'learning_rate': 0.0001986405546116681, 'epoch': 0.1}            \n",
      "{'loss': 2.3466, 'learning_rate': 0.00019864004272139693, 'epoch': 0.1}         \n",
      "{'loss': 2.3336, 'learning_rate': 0.00019863953073542926, 'epoch': 0.1}         \n",
      "{'loss': 2.0997, 'learning_rate': 0.00019863901865376556, 'epoch': 0.1}         \n",
      "{'loss': 1.9732, 'learning_rate': 0.0001986385064764064, 'epoch': 0.1}          \n",
      "{'loss': 1.7678, 'learning_rate': 0.00019863799420335215, 'epoch': 0.1}         \n",
      "{'loss': 2.3931, 'learning_rate': 0.00019863748183460337, 'epoch': 0.1}         \n",
      "{'loss': 2.1845, 'learning_rate': 0.00019863696937016059, 'epoch': 0.1}         \n",
      "{'loss': 2.1295, 'learning_rate': 0.00019863645681002424, 'epoch': 0.1}         \n",
      "{'loss': 1.8579, 'learning_rate': 0.00019863594415419491, 'epoch': 0.1}         \n",
      "{'loss': 2.0167, 'learning_rate': 0.000198635431402673, 'epoch': 0.1}           \n",
      " 10%|██▋                        | 107434/1061708 [16:04:58<142:38:21,  1.86it/s][2024-03-01 10:11:58,591] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2621, 'learning_rate': 0.00019863496984448656, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107443/1061708 [16:05:02<143:37:34,  1.85it/s][2024-03-01 10:12:03,345] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.757, 'learning_rate': 0.00019863450820878996, 'epoch': 0.1}          \n",
      "{'loss': 1.953, 'learning_rate': 0.00019863399518933158, 'epoch': 0.1}          \n",
      "{'loss': 2.3366, 'learning_rate': 0.00019863348207418258, 'epoch': 0.1}         \n",
      "{'loss': 2.346, 'learning_rate': 0.0001986329688633434, 'epoch': 0.1}           \n",
      "{'loss': 2.1667, 'learning_rate': 0.0001986324555568146, 'epoch': 0.1}          \n",
      "{'loss': 1.8864, 'learning_rate': 0.0001986319421545966, 'epoch': 0.1}          \n",
      "{'loss': 2.3177, 'learning_rate': 0.00019863142865669, 'epoch': 0.1}            \n",
      "{'loss': 1.9767, 'learning_rate': 0.00019863091506309522, 'epoch': 0.1}         \n",
      "{'loss': 2.3664, 'learning_rate': 0.00019863040137381278, 'epoch': 0.1}         \n",
      "{'loss': 1.7873, 'learning_rate': 0.00019862988758884316, 'epoch': 0.1}         \n",
      "{'loss': 2.4055, 'learning_rate': 0.00019862937370818687, 'epoch': 0.1}         \n",
      "{'loss': 2.1339, 'learning_rate': 0.00019862885973184446, 'epoch': 0.1}         \n",
      "{'loss': 1.7901, 'learning_rate': 0.00019862834565981637, 'epoch': 0.1}         \n",
      "{'loss': 2.2065, 'learning_rate': 0.00019862783149210308, 'epoch': 0.1}         \n",
      "{'loss': 2.1248, 'learning_rate': 0.00019862731722870516, 'epoch': 0.1}         \n",
      "{'loss': 1.7951, 'learning_rate': 0.00019862680286962303, 'epoch': 0.1}         \n",
      "{'loss': 2.2088, 'learning_rate': 0.00019862628841485727, 'epoch': 0.1}         \n",
      "{'loss': 1.6875, 'learning_rate': 0.0001986257738644083, 'epoch': 0.1}          \n",
      "{'loss': 2.2644, 'learning_rate': 0.0001986252592182767, 'epoch': 0.1}          \n",
      "{'loss': 1.9441, 'learning_rate': 0.0001986247444764629, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 107644/1061708 [16:06:50<142:57:10,  1.85it/s][2024-03-01 10:13:50,708] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 107645/1061708 [16:06:50<133:37:12,  1.98it/s][2024-03-01 10:13:51,131] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2059, 'learning_rate': 0.00019862433261412103, 'epoch': 0.1}         \n",
      "{'loss': 2.439, 'learning_rate': 0.0001986238177000806, 'epoch': 0.1}           \n",
      "{'loss': 2.1838, 'learning_rate': 0.00019862330269035936, 'epoch': 0.1}         \n",
      "{'loss': 1.8686, 'learning_rate': 0.00019862278758495786, 'epoch': 0.1}         \n",
      "{'loss': 1.5423, 'learning_rate': 0.0001986222723838766, 'epoch': 0.1}          \n",
      "{'loss': 1.9749, 'learning_rate': 0.00019862175708711604, 'epoch': 0.1}         \n",
      "{'loss': 1.8587, 'learning_rate': 0.00019862124169467676, 'epoch': 0.1}         \n",
      "{'loss': 2.2584, 'learning_rate': 0.00019862072620655913, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107722/1061708 [16:07:31<140:50:09,  1.88it/s][2024-03-01 10:14:32,232] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0575, 'learning_rate': 0.0001986202621854488, 'epoch': 0.1}          \n",
      "{'loss': 1.758, 'learning_rate': 0.00019861974651554386, 'epoch': 0.1}          \n",
      "{'loss': 1.8935, 'learning_rate': 0.00019861923074996208, 'epoch': 0.1}         \n",
      "{'loss': 2.0838, 'learning_rate': 0.000198618714888704, 'epoch': 0.1}           \n",
      "{'loss': 1.9392, 'learning_rate': 0.0001986181989317701, 'epoch': 0.1}          \n",
      "{'loss': 1.8881, 'learning_rate': 0.00019861768287916082, 'epoch': 0.1}         \n",
      "{'loss': 1.9377, 'learning_rate': 0.00019861716673087677, 'epoch': 0.1}         \n",
      "{'loss': 2.0061, 'learning_rate': 0.0001986166504869184, 'epoch': 0.1}          \n",
      "{'loss': 2.0319, 'learning_rate': 0.0001986161341472862, 'epoch': 0.1}          \n",
      "{'loss': 2.0597, 'learning_rate': 0.00019861561771198068, 'epoch': 0.1}         \n",
      "{'loss': 2.2975, 'learning_rate': 0.00019861510118100232, 'epoch': 0.1}         \n",
      "{'loss': 2.0413, 'learning_rate': 0.00019861458455435168, 'epoch': 0.1}         \n",
      "{'loss': 2.3896, 'learning_rate': 0.00019861406783202917, 'epoch': 0.1}         \n",
      "{'loss': 2.2058, 'learning_rate': 0.00019861355101403542, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107865/1061708 [16:08:48<141:52:58,  1.87it/s][2024-03-01 10:15:48,530] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3471, 'learning_rate': 0.00019861308579604243, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107876/1061708 [16:08:53<141:29:19,  1.87it/s][2024-03-01 10:15:54,341] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0089, 'learning_rate': 0.00019861262050055646, 'epoch': 0.1}         \n",
      "{'loss': 2.1498, 'learning_rate': 0.00019861210341468564, 'epoch': 0.1}         \n",
      "{'loss': 2.475, 'learning_rate': 0.00019861158623314543, 'epoch': 0.1}          \n",
      "{'loss': 1.8608, 'learning_rate': 0.0001986110689559363, 'epoch': 0.1}          \n",
      "{'loss': 2.2853, 'learning_rate': 0.00019861055158305877, 'epoch': 0.1}         \n",
      "{'loss': 2.1911, 'learning_rate': 0.00019861003411451332, 'epoch': 0.1}         \n",
      "{'loss': 2.0476, 'learning_rate': 0.00019860951655030048, 'epoch': 0.1}         \n",
      "{'loss': 2.3077, 'learning_rate': 0.00019860899889042076, 'epoch': 0.1}         \n",
      "{'loss': 1.9102, 'learning_rate': 0.0001986084811348746, 'epoch': 0.1}          \n",
      "{'loss': 1.9139, 'learning_rate': 0.00019860796328366258, 'epoch': 0.1}         \n",
      "{'loss': 2.4193, 'learning_rate': 0.00019860744533678514, 'epoch': 0.1}         \n",
      "{'loss': 1.9551, 'learning_rate': 0.00019860692729424285, 'epoch': 0.1}         \n",
      " 10%|██▋                        | 107999/1061708 [16:09:59<140:31:25,  1.89it/s][2024-03-01 10:16:59,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=108000, skipped=1252, lr=[0.00019860640915603613], mom=[(0.9, 0.999)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-01 10:16:59,911] [INFO] [timer.py:260:stop] epoch=0/micro_step=108000/global_step=108000, RunningAvgSamplesPerSec=1.8926163065458845, CurrSamplesPerSec=1.9006297362195073, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1269, 'learning_rate': 0.00019860640915603613, 'epoch': 0.1}         \n",
      "{'loss': 2.426, 'learning_rate': 0.00019860589092216552, 'epoch': 0.1}          \n",
      "{'loss': 1.9222, 'learning_rate': 0.00019860537259263158, 'epoch': 0.1}         \n",
      "{'loss': 2.1473, 'learning_rate': 0.0001986048541674347, 'epoch': 0.1}          \n",
      "{'loss': 2.1817, 'learning_rate': 0.00019860433564657549, 'epoch': 0.1}         \n",
      "{'loss': 2.2312, 'learning_rate': 0.00019860381703005437, 'epoch': 0.1}         \n",
      "{'loss': 2.3426, 'learning_rate': 0.00019860329831787192, 'epoch': 0.1}         \n",
      "{'loss': 1.669, 'learning_rate': 0.00019860277951002858, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 108077/1061708 [16:10:40<140:58:01,  1.88it/s][2024-03-01 10:17:41,392] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▋                        | 108078/1061708 [16:10:41<132:10:31,  2.00it/s][2024-03-01 10:17:41,814] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3177, 'learning_rate': 0.0001986023643948784, 'epoch': 0.1}          \n",
      " 10%|██▋                        | 108089/1061708 [16:10:47<140:15:42,  1.89it/s][2024-03-01 10:17:47,579] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8945, 'learning_rate': 0.00019860189731715463, 'epoch': 0.1}         \n",
      "{'loss': 1.9365, 'learning_rate': 0.00019860137825102947, 'epoch': 0.1}         \n",
      "{'loss': 2.5274, 'learning_rate': 0.00019860085908924527, 'epoch': 0.1}         \n",
      "{'loss': 2.112, 'learning_rate': 0.0001986003398318026, 'epoch': 0.1}           \n",
      "{'loss': 1.9934, 'learning_rate': 0.00019859982047870195, 'epoch': 0.1}         \n",
      "{'loss': 1.8944, 'learning_rate': 0.00019859930102994377, 'epoch': 0.1}         \n",
      "{'loss': 1.7444, 'learning_rate': 0.00019859878148552863, 'epoch': 0.1}         \n",
      "{'loss': 2.1149, 'learning_rate': 0.000198598261845457, 'epoch': 0.1}           \n",
      "{'loss': 2.0391, 'learning_rate': 0.00019859774210972936, 'epoch': 0.1}         \n",
      "{'loss': 1.8939, 'learning_rate': 0.00019859722227834626, 'epoch': 0.1}         \n",
      "{'loss': 1.9703, 'learning_rate': 0.00019859670235130818, 'epoch': 0.1}         \n",
      "{'loss': 1.8863, 'learning_rate': 0.00019859618232861566, 'epoch': 0.1}         \n",
      "{'loss': 2.0546, 'learning_rate': 0.00019859566221026916, 'epoch': 0.1}         \n",
      "{'loss': 1.9442, 'learning_rate': 0.00019859514199626919, 'epoch': 0.1}         \n",
      "{'loss': 2.2059, 'learning_rate': 0.00019859462168661627, 'epoch': 0.1}         \n",
      "{'loss': 2.1853, 'learning_rate': 0.0001985941012813109, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 108240/1061708 [16:12:07<140:37:00,  1.88it/s][2024-03-01 10:19:08,180] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9622, 'learning_rate': 0.00019859363283475365, 'epoch': 0.1}         \n",
      "{'loss': 1.6321, 'learning_rate': 0.00019859311224771002, 'epoch': 0.1}         \n",
      "{'loss': 1.6993, 'learning_rate': 0.00019859259156501538, 'epoch': 0.1}         \n",
      "{'loss': 1.8482, 'learning_rate': 0.00019859207078667032, 'epoch': 0.1}         \n",
      "{'loss': 2.3296, 'learning_rate': 0.00019859154991267523, 'epoch': 0.1}         \n",
      "{'loss': 1.7372, 'learning_rate': 0.0001985910289430307, 'epoch': 0.1}          \n",
      "{'loss': 2.4604, 'learning_rate': 0.00019859050787773722, 'epoch': 0.1}         \n",
      "{'loss': 2.0361, 'learning_rate': 0.00019858998671679526, 'epoch': 0.1}         \n",
      "{'loss': 1.8318, 'learning_rate': 0.0001985894654602053, 'epoch': 0.1}          \n",
      "{'loss': 2.2021, 'learning_rate': 0.00019858894410796797, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108341/1061708 [16:13:01<140:14:24,  1.89it/s][2024-03-01 10:20:02,016] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 108342/1061708 [16:13:01<131:40:01,  2.01it/s][2024-03-01 10:20:02,438] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0869, 'learning_rate': 0.00019858852695731226, 'epoch': 0.1}         \n",
      "{'loss': 2.058, 'learning_rate': 0.00019858800543291077, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 108360/1061708 [16:13:11<140:09:09,  1.89it/s][2024-03-01 10:20:11,942] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1962, 'learning_rate': 0.00019858753597917206, 'epoch': 0.1}         \n",
      "{'loss': 2.0389, 'learning_rate': 0.00019858701427304354, 'epoch': 0.1}         \n",
      "{'loss': 2.0069, 'learning_rate': 0.00019858649247126995, 'epoch': 0.1}         \n",
      "{'loss': 2.2071, 'learning_rate': 0.0001985859705738518, 'epoch': 0.1}          \n",
      "{'loss': 2.1026, 'learning_rate': 0.0001985854485807896, 'epoch': 0.1}          \n",
      "{'loss': 2.1287, 'learning_rate': 0.0001985849264920838, 'epoch': 0.1}          \n",
      "{'loss': 2.3168, 'learning_rate': 0.00019858440430773505, 'epoch': 0.1}         \n",
      "{'loss': 1.5989, 'learning_rate': 0.00019858388202774368, 'epoch': 0.1}         \n",
      "{'loss': 1.9899, 'learning_rate': 0.00019858335965211032, 'epoch': 0.1}         \n",
      "{'loss': 2.0429, 'learning_rate': 0.00019858283718083544, 'epoch': 0.1}         \n",
      "{'loss': 2.4086, 'learning_rate': 0.00019858231461391952, 'epoch': 0.1}         \n",
      "{'loss': 1.9809, 'learning_rate': 0.0001985817919513631, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 108486/1061708 [16:14:18<141:15:40,  1.87it/s][2024-03-01 10:21:19,071] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0707, 'learning_rate': 0.00019858132147329013, 'epoch': 0.1}         \n",
      "{'loss': 2.0571, 'learning_rate': 0.0001985807986290181, 'epoch': 0.1}          \n",
      "{'loss': 1.8669, 'learning_rate': 0.00019858027568910708, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108515/1061708 [16:14:33<141:53:06,  1.87it/s][2024-03-01 10:21:34,485] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0887, 'learning_rate': 0.0001985798049614162, 'epoch': 0.1}          \n",
      "{'loss': 2.1973, 'learning_rate': 0.0001985792818397924, 'epoch': 0.1}          \n",
      "{'loss': 2.2576, 'learning_rate': 0.00019857875862253104, 'epoch': 0.1}         \n",
      "{'loss': 1.9291, 'learning_rate': 0.0001985782353096326, 'epoch': 0.1}          \n",
      "{'loss': 2.0466, 'learning_rate': 0.0001985777119010976, 'epoch': 0.1}          \n",
      "{'loss': 2.2013, 'learning_rate': 0.00019857718839692655, 'epoch': 0.1}         \n",
      "{'loss': 1.6358, 'learning_rate': 0.00019857666479712, 'epoch': 0.1}            \n",
      "{'loss': 2.0243, 'learning_rate': 0.0001985761411016784, 'epoch': 0.1}          \n",
      "{'loss': 2.1814, 'learning_rate': 0.00019857561731060228, 'epoch': 0.1}         \n",
      "{'loss': 2.1739, 'learning_rate': 0.00019857509342389217, 'epoch': 0.1}         \n",
      "{'loss': 2.3879, 'learning_rate': 0.00019857456944154852, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108629/1061708 [16:15:34<140:36:13,  1.88it/s][2024-03-01 10:22:35,326] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.106, 'learning_rate': 0.00019857409777567303, 'epoch': 0.1}          \n",
      "{'loss': 2.0896, 'learning_rate': 0.00019857357361162715, 'epoch': 0.1}         \n",
      "{'loss': 2.1605, 'learning_rate': 0.00019857304935194921, 'epoch': 0.1}         \n",
      "{'loss': 2.1924, 'learning_rate': 0.0001985725249966398, 'epoch': 0.1}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2849, 'learning_rate': 0.00019857200054569933, 'epoch': 0.1}         \n",
      "{'loss': 2.1128, 'learning_rate': 0.00019857147599912838, 'epoch': 0.1}         \n",
      "{'loss': 1.879, 'learning_rate': 0.00019857095135692742, 'epoch': 0.1}          \n",
      "{'loss': 2.3082, 'learning_rate': 0.000198570426619097, 'epoch': 0.1}           \n",
      "{'loss': 1.9338, 'learning_rate': 0.00019856990178563757, 'epoch': 0.1}         \n",
      "{'loss': 1.8159, 'learning_rate': 0.00019856937685654969, 'epoch': 0.1}         \n",
      "{'loss': 2.4675, 'learning_rate': 0.00019856885183183384, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108730/1061708 [16:16:28<140:06:21,  1.89it/s][2024-03-01 10:23:29,105] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 108731/1061708 [16:16:29<131:36:34,  2.01it/s][2024-03-01 10:23:29,528] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8327, 'learning_rate': 0.0001985684317432094, 'epoch': 0.1}          \n",
      "{'loss': 1.874, 'learning_rate': 0.0001985679065463645, 'epoch': 0.1}           \n",
      "{'loss': 2.3965, 'learning_rate': 0.0001985673812538931, 'epoch': 0.1}          \n",
      "{'loss': 2.2276, 'learning_rate': 0.00019856685586579567, 'epoch': 0.1}         \n",
      "{'loss': 1.9426, 'learning_rate': 0.0001985663303820727, 'epoch': 0.1}          \n",
      "{'loss': 2.3297, 'learning_rate': 0.00019856580480272476, 'epoch': 0.1}         \n",
      "{'loss': 2.1285, 'learning_rate': 0.00019856527912775232, 'epoch': 0.1}         \n",
      "{'loss': 2.0812, 'learning_rate': 0.0001985647533571559, 'epoch': 0.1}          \n",
      "{'loss': 2.1281, 'learning_rate': 0.000198564227490936, 'epoch': 0.1}           \n",
      "{'loss': 2.1328, 'learning_rate': 0.00019856370152909313, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108832/1061708 [16:17:22<140:01:25,  1.89it/s][2024-03-01 10:24:23,297] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 108833/1061708 [16:17:23<135:15:24,  1.96it/s][2024-03-01 10:24:23,722] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2106, 'learning_rate': 0.00019856328069077066, 'epoch': 0.1}         \n",
      "{'loss': 2.1641, 'learning_rate': 0.00019856275455680774, 'epoch': 0.1}         \n",
      "{'loss': 2.0866, 'learning_rate': 0.0001985622283272233, 'epoch': 0.1}          \n",
      "{'loss': 1.8287, 'learning_rate': 0.00019856170200201783, 'epoch': 0.1}         \n",
      "{'loss': 2.0178, 'learning_rate': 0.00019856117558119188, 'epoch': 0.1}         \n",
      "{'loss': 2.2482, 'learning_rate': 0.0001985606490647459, 'epoch': 0.1}          \n",
      "{'loss': 1.9885, 'learning_rate': 0.00019856012245268042, 'epoch': 0.1}         \n",
      "{'loss': 2.1142, 'learning_rate': 0.00019855959574499599, 'epoch': 0.1}         \n",
      "{'loss': 2.0838, 'learning_rate': 0.00019855906894169307, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 108921/1061708 [16:18:10<139:59:57,  1.89it/s][2024-03-01 10:25:10,536] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9332, 'learning_rate': 0.00019855859473696708, 'epoch': 0.1}         \n",
      "{'loss': 2.112, 'learning_rate': 0.00019855806775199048, 'epoch': 0.1}          \n",
      "{'loss': 2.1515, 'learning_rate': 0.0001985575406713969, 'epoch': 0.1}          \n",
      "{'loss': 2.1352, 'learning_rate': 0.00019855701349518682, 'epoch': 0.1}         \n",
      "{'loss': 2.136, 'learning_rate': 0.0001985564862233608, 'epoch': 0.1}           \n",
      "{'loss': 2.0287, 'learning_rate': 0.00019855595885591933, 'epoch': 0.1}         \n",
      "{'loss': 2.3916, 'learning_rate': 0.0001985554313928629, 'epoch': 0.1}          \n",
      "{'loss': 2.0635, 'learning_rate': 0.00019855490383419207, 'epoch': 0.1}         \n",
      "{'loss': 2.053, 'learning_rate': 0.0001985543761799073, 'epoch': 0.1}           \n",
      "{'loss': 2.2668, 'learning_rate': 0.00019855384843000913, 'epoch': 0.1}         \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019855332058449804, 'epoch': 0.1}         \n",
      "{'loss': 1.673, 'learning_rate': 0.0001985527926433746, 'epoch': 0.1}           \n",
      "{'loss': 1.8498, 'learning_rate': 0.00019855226460663929, 'epoch': 0.1}         \n",
      "{'loss': 1.6432, 'learning_rate': 0.00019855173647429265, 'epoch': 0.1}         \n",
      "{'loss': 2.1018, 'learning_rate': 0.00019855120824633512, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109072/1061708 [16:19:30<139:54:08,  1.89it/s][2024-03-01 10:26:30,964] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9204, 'learning_rate': 0.00019855073275942652, 'epoch': 0.1}         \n",
      "{'loss': 1.837, 'learning_rate': 0.0001985502043498098, 'epoch': 0.1}           \n",
      "{'loss': 2.1373, 'learning_rate': 0.00019854967584458376, 'epoch': 0.1}         \n",
      "{'loss': 1.9097, 'learning_rate': 0.00019854914724374884, 'epoch': 0.1}         \n",
      "{'loss': 1.8541, 'learning_rate': 0.0001985486185473056, 'epoch': 0.1}          \n",
      "{'loss': 2.2052, 'learning_rate': 0.00019854808975525455, 'epoch': 0.1}         \n",
      "{'loss': 2.135, 'learning_rate': 0.0001985475608675962, 'epoch': 0.1}           \n",
      "{'loss': 1.8903, 'learning_rate': 0.00019854703188433104, 'epoch': 0.1}         \n",
      "{'loss': 1.7932, 'learning_rate': 0.00019854650280545956, 'epoch': 0.1}         \n",
      "{'loss': 2.1014, 'learning_rate': 0.00019854597363098237, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109173/1061708 [16:20:24<143:11:40,  1.85it/s][2024-03-01 10:27:24,757] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 109174/1061708 [16:20:24<134:02:11,  1.97it/s][2024-03-01 10:27:25,178] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2049, 'learning_rate': 0.0001985455502225648, 'epoch': 0.1}          \n",
      "{'loss': 1.9408, 'learning_rate': 0.0001985450208759985, 'epoch': 0.1}          \n",
      "{'loss': 1.8386, 'learning_rate': 0.0001985444914338279, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 109207/1061708 [16:20:42<140:48:00,  1.88it/s][2024-03-01 10:27:42,705] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0123, 'learning_rate': 0.00019854401485413305, 'epoch': 0.1}         \n",
      "{'loss': 1.9746, 'learning_rate': 0.00019854348523031562, 'epoch': 0.1}         \n",
      "{'loss': 2.0172, 'learning_rate': 0.00019854295551089535, 'epoch': 0.1}         \n",
      "{'loss': 2.328, 'learning_rate': 0.00019854242569587277, 'epoch': 0.1}          \n",
      "{'loss': 1.7774, 'learning_rate': 0.00019854189578524836, 'epoch': 0.1}         \n",
      "{'loss': 1.7674, 'learning_rate': 0.00019854136577902268, 'epoch': 0.1}         \n",
      "{'loss': 2.2603, 'learning_rate': 0.00019854083567719618, 'epoch': 0.1}         \n",
      "{'loss': 2.1709, 'learning_rate': 0.00019854030547976942, 'epoch': 0.1}         \n",
      "{'loss': 2.3741, 'learning_rate': 0.0001985397751867429, 'epoch': 0.1}          \n",
      "{'loss': 2.0185, 'learning_rate': 0.00019853924479811715, 'epoch': 0.1}         \n",
      "{'loss': 2.2635, 'learning_rate': 0.00019853871431389268, 'epoch': 0.1}         \n",
      "{'loss': 2.0517, 'learning_rate': 0.00019853818373406998, 'epoch': 0.1}         \n",
      "{'loss': 2.0593, 'learning_rate': 0.00019853765305864962, 'epoch': 0.1}         \n",
      "{'loss': 2.197, 'learning_rate': 0.00019853712228763205, 'epoch': 0.1}          \n",
      "{'loss': 1.8853, 'learning_rate': 0.00019853659142101782, 'epoch': 0.1}         \n",
      "{'loss': 1.8563, 'learning_rate': 0.00019853606045880745, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109366/1061708 [16:22:06<141:03:28,  1.88it/s][2024-03-01 10:29:07,465] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4575, 'learning_rate': 0.0001985355825110838, 'epoch': 0.1}          \n",
      "{'loss': 2.0468, 'learning_rate': 0.00019853505136724217, 'epoch': 0.1}         \n",
      "{'loss': 2.0689, 'learning_rate': 0.00019853452012780586, 'epoch': 0.1}         \n",
      "{'loss': 2.044, 'learning_rate': 0.00019853398879277542, 'epoch': 0.1}          \n",
      "{'loss': 2.107, 'learning_rate': 0.00019853345736215133, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 109414/1061708 [16:22:32<142:27:32,  1.86it/s][2024-03-01 10:29:32,981] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3366, 'learning_rate': 0.00019853297899285755, 'epoch': 0.1}         \n",
      "{'loss': 2.0534, 'learning_rate': 0.00019853244738060702, 'epoch': 0.1}         \n",
      "{'loss': 2.1776, 'learning_rate': 0.00019853191567276434, 'epoch': 0.1}         \n",
      "{'loss': 2.102, 'learning_rate': 0.00019853138386933007, 'epoch': 0.1}          \n",
      "{'loss': 2.1327, 'learning_rate': 0.00019853085197030467, 'epoch': 0.1}         \n",
      "{'loss': 2.1682, 'learning_rate': 0.00019853031997568872, 'epoch': 0.1}         \n",
      "{'loss': 2.3898, 'learning_rate': 0.00019852978788548268, 'epoch': 0.1}         \n",
      "{'loss': 1.8691, 'learning_rate': 0.0001985292556996871, 'epoch': 0.1}          \n",
      "{'loss': 1.7825, 'learning_rate': 0.00019852872341830246, 'epoch': 0.1}         \n",
      "{'loss': 1.6043, 'learning_rate': 0.00019852819104132933, 'epoch': 0.1}         \n",
      "{'loss': 2.1906, 'learning_rate': 0.00019852765856876817, 'epoch': 0.1}         \n",
      "{'loss': 2.0951, 'learning_rate': 0.00019852712600061957, 'epoch': 0.1}         \n",
      "{'loss': 2.4232, 'learning_rate': 0.00019852659333688397, 'epoch': 0.1}         \n",
      "{'loss': 2.4836, 'learning_rate': 0.00019852606057756193, 'epoch': 0.1}         \n",
      "{'loss': 1.8681, 'learning_rate': 0.00019852552772265395, 'epoch': 0.1}         \n",
      "{'loss': 1.5401, 'learning_rate': 0.00019852499477216056, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109571/1061708 [16:23:56<140:31:10,  1.88it/s][2024-03-01 10:30:56,638] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7292, 'learning_rate': 0.00019852451503499138, 'epoch': 0.1}         \n",
      "{'loss': 2.2653, 'learning_rate': 0.00019852398190288713, 'epoch': 0.1}         \n",
      "{'loss': 1.913, 'learning_rate': 0.00019852344867519897, 'epoch': 0.1}          \n",
      "{'loss': 2.2842, 'learning_rate': 0.0001985229153519274, 'epoch': 0.1}          \n",
      "{'loss': 2.0546, 'learning_rate': 0.00019852238193307292, 'epoch': 0.1}         \n",
      "{'loss': 2.185, 'learning_rate': 0.00019852184841863605, 'epoch': 0.1}          \n",
      "{'loss': 1.9844, 'learning_rate': 0.00019852131480861739, 'epoch': 0.1}         \n",
      "{'loss': 1.8542, 'learning_rate': 0.00019852078110301738, 'epoch': 0.1}         \n",
      "{'loss': 2.1162, 'learning_rate': 0.00019852024730183655, 'epoch': 0.1}         \n",
      "{'loss': 2.2706, 'learning_rate': 0.00019851971340507544, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109672/1061708 [16:24:49<140:32:09,  1.88it/s][2024-03-01 10:31:50,377] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 109673/1061708 [16:24:50<135:20:48,  1.95it/s][2024-03-01 10:31:50,801] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.638, 'learning_rate': 0.0001985192862188491, 'epoch': 0.1}           \n",
      "{'loss': 2.2622, 'learning_rate': 0.00019851875215004473, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109698/1061708 [16:25:03<140:06:40,  1.89it/s][2024-03-01 10:32:04,039] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9769, 'learning_rate': 0.0001985182714064009, 'epoch': 0.1}          \n",
      "{'loss': 2.2716, 'learning_rate': 0.00019851773715599718, 'epoch': 0.1}         \n",
      "{'loss': 1.7818, 'learning_rate': 0.00019851720281001566, 'epoch': 0.1}         \n",
      "{'loss': 2.2536, 'learning_rate': 0.00019851666836845673, 'epoch': 0.1}         \n",
      "{'loss': 1.8716, 'learning_rate': 0.000198516133831321, 'epoch': 0.1}           \n",
      "{'loss': 2.0666, 'learning_rate': 0.000198515599198609, 'epoch': 0.1}           \n",
      "{'loss': 1.9675, 'learning_rate': 0.00019851506447032116, 'epoch': 0.1}         \n",
      "{'loss': 2.245, 'learning_rate': 0.00019851452964645807, 'epoch': 0.1}          \n",
      "{'loss': 1.9247, 'learning_rate': 0.00019851399472702025, 'epoch': 0.1}         \n",
      "{'loss': 1.7621, 'learning_rate': 0.0001985134597120082, 'epoch': 0.1}          \n",
      "{'loss': 2.1316, 'learning_rate': 0.00019851292460142244, 'epoch': 0.1}         \n",
      "{'loss': 2.0861, 'learning_rate': 0.00019851238939526346, 'epoch': 0.1}         \n",
      "{'loss': 2.1965, 'learning_rate': 0.00019851185409353183, 'epoch': 0.1}         \n",
      "{'loss': 2.3261, 'learning_rate': 0.00019851131869622806, 'epoch': 0.1}         \n",
      "{'loss': 2.1275, 'learning_rate': 0.00019851078320335264, 'epoch': 0.1}         \n",
      "{'loss': 2.2113, 'learning_rate': 0.0001985102476149061, 'epoch': 0.1}          \n",
      "{'loss': 1.888, 'learning_rate': 0.000198509711930889, 'epoch': 0.1}            \n",
      "{'loss': 1.8845, 'learning_rate': 0.0001985091761513018, 'epoch': 0.1}          \n",
      "{'loss': 2.0254, 'learning_rate': 0.00019850864027614505, 'epoch': 0.1}         \n",
      "{'loss': 2.005, 'learning_rate': 0.00019850810430541927, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 109899/1061708 [16:26:50<140:27:16,  1.88it/s][2024-03-01 10:33:51,189] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3566, 'learning_rate': 0.00019850762185005496, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109900/1061708 [16:26:51<131:46:17,  2.01it/s][2024-03-01 10:33:51,614] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3238, 'learning_rate': 0.00019850713931728056, 'epoch': 0.1}         \n",
      "{'loss': 2.1906, 'learning_rate': 0.00019850660307896425, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109921/1061708 [16:27:02<140:12:29,  1.89it/s][2024-03-01 10:34:02,734] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3248, 'learning_rate': 0.0001985061203827697, 'epoch': 0.1}          \n",
      "{'loss': 2.1457, 'learning_rate': 0.00019850558396287645, 'epoch': 0.1}         \n",
      "{'loss': 1.7488, 'learning_rate': 0.0001985050474474171, 'epoch': 0.1}          \n",
      "{'loss': 2.0733, 'learning_rate': 0.00019850451083639218, 'epoch': 0.1}         \n",
      "{'loss': 2.0295, 'learning_rate': 0.0001985039741298023, 'epoch': 0.1}          \n",
      "{'loss': 2.1288, 'learning_rate': 0.00019850343732764786, 'epoch': 0.1}         \n",
      "{'loss': 2.4119, 'learning_rate': 0.00019850290042992943, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 109999/1061708 [16:27:43<140:11:31,  1.89it/s][2024-03-01 10:34:44,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=110000, skipped=1280, lr=[0.00019850236343664756], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 10:34:44,358] [INFO] [timer.py:260:stop] epoch=0/micro_step=110000/global_step=110000, RunningAvgSamplesPerSec=1.8926465424491437, CurrSamplesPerSec=1.8957460444875935, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.8056, 'learning_rate': 0.00019850236343664756, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110000/1061708 [16:27:44<140:19:33,  1.88it/s][INFO|trainer.py:2868] 2024-03-01 10:34:44,360 >> Saving model checkpoint to output_model/checkpoint-110000\n",
      "[INFO|trainer.py:2880] 2024-03-01 10:34:44,363 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 10:34:45,576 >> tokenizer config file saved in output_model/checkpoint-110000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 10:34:45,577 >> Special tokens file saved in output_model/checkpoint-110000/special_tokens_map.json\n",
      "[2024-03-01 10:34:45,578] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step110000 is about to be saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-01 10:34:50,815] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-110000/global_step110000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 10:34:50,816] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-110000/global_step110000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 10:35:04,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-110000/global_step110000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 10:35:05,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-110000/global_step110000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 10:35:12,605] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-110000/global_step110000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 10:35:12,605] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-110000/global_step110000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 10:35:12,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step110000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 10:35:12,702 >> Deleting older checkpoint [output_model/checkpoint-95000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 10:35:16,470 >> tokenizer config file saved in output_model/checkpoint-110000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 10:35:16,471 >> Special tokens file saved in output_model/checkpoint-110000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 1.8602, 'learning_rate': 0.00019850182634780273, 'epoch': 0.1}         \n",
      "{'loss': 2.2013, 'learning_rate': 0.0001985012891633955, 'epoch': 0.1}          \n",
      "{'loss': 2.3462, 'learning_rate': 0.00019850075188342634, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110035/1061708 [16:28:34<140:31:27,  1.88it/s][2024-03-01 10:35:35,389] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2105, 'learning_rate': 0.00019850026824974913, 'epoch': 0.1}         \n",
      "{'loss': 1.9311, 'learning_rate': 0.00019849973078821378, 'epoch': 0.1}         \n",
      "{'loss': 2.1588, 'learning_rate': 0.00019849919323111808, 'epoch': 0.1}         \n",
      "{'loss': 2.0294, 'learning_rate': 0.00019849865557846247, 'epoch': 0.1}         \n",
      "{'loss': 1.6648, 'learning_rate': 0.00019849811783024754, 'epoch': 0.1}         \n",
      "{'loss': 2.0165, 'learning_rate': 0.00019849757998647379, 'epoch': 0.1}         \n",
      "{'loss': 1.9836, 'learning_rate': 0.0001984970420471417, 'epoch': 0.1}          \n",
      "{'loss': 1.7549, 'learning_rate': 0.00019849650401225185, 'epoch': 0.1}         \n",
      "{'loss': 1.9628, 'learning_rate': 0.00019849596588180477, 'epoch': 0.1}         \n",
      "{'loss': 1.9564, 'learning_rate': 0.0001984954276558009, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 110136/1061708 [16:29:28<141:47:25,  1.86it/s][2024-03-01 10:36:29,211] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 110137/1061708 [16:29:29<133:01:33,  1.99it/s][2024-03-01 10:36:29,639] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1927, 'learning_rate': 0.00019849499700619734, 'epoch': 0.1}         \n",
      "{'loss': 1.9069, 'learning_rate': 0.0001984944586081927, 'epoch': 0.1}          \n",
      "{'loss': 2.229, 'learning_rate': 0.00019849392011463276, 'epoch': 0.1}          \n",
      "{'loss': 1.5794, 'learning_rate': 0.00019849338152551807, 'epoch': 0.1}         \n",
      "{'loss': 1.8413, 'learning_rate': 0.00019849284284084915, 'epoch': 0.1}         \n",
      "{'loss': 1.8833, 'learning_rate': 0.00019849230406062654, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110199/1061708 [16:30:02<140:30:01,  1.88it/s][2024-03-01 10:37:02,710] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0855, 'learning_rate': 0.00019849181907672818, 'epoch': 0.1}         \n",
      "{'loss': 1.9466, 'learning_rate': 0.00019849128011495493, 'epoch': 0.1}         \n",
      "{'loss': 1.7927, 'learning_rate': 0.00019849074105762955, 'epoch': 0.1}         \n",
      "{'loss': 1.9267, 'learning_rate': 0.00019849020190475244, 'epoch': 0.1}         \n",
      "{'loss': 2.0504, 'learning_rate': 0.0001984896626563242, 'epoch': 0.1}          \n",
      "{'loss': 1.7596, 'learning_rate': 0.00019848912331234535, 'epoch': 0.1}         \n",
      "{'loss': 2.0296, 'learning_rate': 0.0001984885838728164, 'epoch': 0.1}          \n",
      "{'loss': 1.8445, 'learning_rate': 0.00019848804433773786, 'epoch': 0.1}         \n",
      "{'loss': 1.9727, 'learning_rate': 0.0001984875047071103, 'epoch': 0.1}          \n",
      "{'loss': 2.363, 'learning_rate': 0.00019848696498093416, 'epoch': 0.1}          \n",
      "{'loss': 2.1218, 'learning_rate': 0.00019848642515921003, 'epoch': 0.1}         \n",
      "{'loss': 2.3217, 'learning_rate': 0.00019848588524193844, 'epoch': 0.1}         \n",
      "{'loss': 1.791, 'learning_rate': 0.00019848534522911985, 'epoch': 0.1}          \n",
      "{'loss': 2.4143, 'learning_rate': 0.0001984848051207549, 'epoch': 0.1}          \n",
      "{'loss': 2.355, 'learning_rate': 0.00019848426491684396, 'epoch': 0.1}          \n",
      "{'loss': 2.7607, 'learning_rate': 0.00019848372461738766, 'epoch': 0.1}         \n",
      "{'loss': 2.0668, 'learning_rate': 0.0001984831842223865, 'epoch': 0.1}          \n",
      "{'loss': 2.074, 'learning_rate': 0.00019848264373184104, 'epoch': 0.1}          \n",
      "{'loss': 1.9353, 'learning_rate': 0.00019848210314575172, 'epoch': 0.1}         \n",
      "{'loss': 1.8114, 'learning_rate': 0.00019848156246411914, 'epoch': 0.1}         \n",
      "{'loss': 2.1297, 'learning_rate': 0.00019848102168694377, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110400/1061708 [16:31:49<140:02:35,  1.89it/s][2024-03-01 10:38:49,969] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 110401/1061708 [16:31:49<131:37:12,  2.01it/s][2024-03-01 10:38:50,391] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2234, 'learning_rate': 0.00019848058899641307, 'epoch': 0.1}         \n",
      "{'loss': 2.1197, 'learning_rate': 0.00019848004804726203, 'epoch': 0.1}         \n",
      "{'loss': 2.0424, 'learning_rate': 0.00019847950700256975, 'epoch': 0.1}         \n",
      "{'loss': 1.8356, 'learning_rate': 0.00019847896586233668, 'epoch': 0.1}         \n",
      "{'loss': 2.4036, 'learning_rate': 0.00019847842462656336, 'epoch': 0.1}         \n",
      "{'loss': 1.9982, 'learning_rate': 0.0001984778832952503, 'epoch': 0.1}          \n",
      "{'loss': 2.0082, 'learning_rate': 0.00019847734186839806, 'epoch': 0.1}         \n",
      "{'loss': 2.0754, 'learning_rate': 0.00019847680034600716, 'epoch': 0.1}         \n",
      "{'loss': 1.9203, 'learning_rate': 0.0001984762587280781, 'epoch': 0.1}          \n",
      "{'loss': 2.2466, 'learning_rate': 0.00019847571701461143, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110502/1061708 [16:32:43<140:01:07,  1.89it/s][2024-03-01 10:39:44,130] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 110503/1061708 [16:32:44<134:36:27,  1.96it/s][2024-03-01 10:39:44,553] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8318, 'learning_rate': 0.00019847528357505137, 'epoch': 0.1}         \n",
      "{'loss': 1.757, 'learning_rate': 0.0001984747416896183, 'epoch': 0.1}           \n",
      "{'loss': 1.9498, 'learning_rate': 0.0001984741997086491, 'epoch': 0.1}          \n",
      "{'loss': 1.9385, 'learning_rate': 0.0001984736576321443, 'epoch': 0.1}          \n",
      "{'loss': 1.8279, 'learning_rate': 0.00019847311546010438, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 110558/1061708 [16:33:13<140:01:12,  1.89it/s][2024-03-01 10:40:13,746] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3028, 'learning_rate': 0.00019847262742358636, 'epoch': 0.1}         \n",
      "{'loss': 2.0033, 'learning_rate': 0.0001984720850700312, 'epoch': 0.1}          \n",
      "{'loss': 2.2908, 'learning_rate': 0.00019847154262094249, 'epoch': 0.1}         \n",
      "{'loss': 2.1442, 'learning_rate': 0.0001984710000763207, 'epoch': 0.1}          \n",
      "{'loss': 1.8051, 'learning_rate': 0.00019847045743616644, 'epoch': 0.1}         \n",
      "{'loss': 2.2073, 'learning_rate': 0.00019846991470048018, 'epoch': 0.1}         \n",
      "{'loss': 1.9806, 'learning_rate': 0.00019846937186926243, 'epoch': 0.1}         \n",
      "{'loss': 1.7123, 'learning_rate': 0.00019846882894251376, 'epoch': 0.1}         \n",
      "{'loss': 2.4667, 'learning_rate': 0.00019846828592023466, 'epoch': 0.1}         \n",
      "{'loss': 1.8904, 'learning_rate': 0.0001984677428024257, 'epoch': 0.1}          \n",
      "{'loss': 1.7672, 'learning_rate': 0.00019846719958908736, 'epoch': 0.1}         \n",
      "{'loss': 1.9918, 'learning_rate': 0.0001984666562802202, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 110670/1061708 [16:34:12<139:40:33,  1.89it/s][2024-03-01 10:41:13,272] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0941, 'learning_rate': 0.00019846616722056303, 'epoch': 0.1}         \n",
      "{'loss': 2.2817, 'learning_rate': 0.00019846562373019257, 'epoch': 0.1}         \n",
      "{'loss': 1.9573, 'learning_rate': 0.00019846508014429478, 'epoch': 0.1}         \n",
      "{'loss': 1.9572, 'learning_rate': 0.00019846453646287025, 'epoch': 0.1}         \n",
      "{'loss': 1.9046, 'learning_rate': 0.00019846399268591945, 'epoch': 0.1}         \n",
      "{'loss': 2.0235, 'learning_rate': 0.00019846344881344289, 'epoch': 0.1}         \n",
      "{'loss': 2.1813, 'learning_rate': 0.0001984629048454412, 'epoch': 0.1}          \n",
      "{'loss': 1.8562, 'learning_rate': 0.0001984623607819148, 'epoch': 0.1}          \n",
      "{'loss': 1.7837, 'learning_rate': 0.0001984618166228643, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 110767/1061708 [16:35:04<140:43:14,  1.88it/s][2024-03-01 10:42:04,984] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0982, 'learning_rate': 0.00019846132679804615, 'epoch': 0.1}         \n",
      "{'loss': 1.9029, 'learning_rate': 0.00019846078245750124, 'epoch': 0.1}         \n",
      "{'loss': 1.9554, 'learning_rate': 0.00019846023802143372, 'epoch': 0.1}         \n",
      "{'loss': 1.9952, 'learning_rate': 0.00019845969348984411, 'epoch': 0.1}         \n",
      "{'loss': 2.2691, 'learning_rate': 0.000198459148862733, 'epoch': 0.1}           \n",
      "{'loss': 2.0699, 'learning_rate': 0.0001984586041401008, 'epoch': 0.1}          \n",
      "{'loss': 2.0783, 'learning_rate': 0.00019845805932194814, 'epoch': 0.1}         \n",
      "{'loss': 2.0003, 'learning_rate': 0.00019845751440827553, 'epoch': 0.1}         \n",
      "{'loss': 2.3298, 'learning_rate': 0.00019845696939908348, 'epoch': 0.1}         \n",
      "{'loss': 1.8157, 'learning_rate': 0.00019845642429437252, 'epoch': 0.1}         \n",
      "{'loss': 2.1894, 'learning_rate': 0.00019845587909414318, 'epoch': 0.1}         \n",
      "{'loss': 1.9268, 'learning_rate': 0.000198455333798396, 'epoch': 0.1}           \n",
      "{'loss': 1.7904, 'learning_rate': 0.00019845478840713153, 'epoch': 0.1}         \n",
      "{'loss': 2.0518, 'learning_rate': 0.00019845424292035024, 'epoch': 0.1}         \n",
      "{'loss': 1.8589, 'learning_rate': 0.0001984536973380527, 'epoch': 0.1}          \n",
      "{'loss': 2.2066, 'learning_rate': 0.0001984531516602394, 'epoch': 0.1}          \n",
      "{'loss': 1.9749, 'learning_rate': 0.00019845260588691093, 'epoch': 0.1}         \n",
      "{'loss': 2.3877, 'learning_rate': 0.00019845206001806778, 'epoch': 0.1}         \n",
      "{'loss': 2.4742, 'learning_rate': 0.00019845151405371047, 'epoch': 0.1}         \n",
      "{'loss': 2.0129, 'learning_rate': 0.0001984509679938396, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 110968/1061708 [16:36:51<140:24:53,  1.88it/s][2024-03-01 10:43:52,262] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 110969/1061708 [16:36:52<131:45:46,  2.00it/s][2024-03-01 10:43:52,685] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9046, 'learning_rate': 0.00019845053107717342, 'epoch': 0.1}         \n",
      "{'loss': 2.0311, 'learning_rate': 0.00019844998484537936, 'epoch': 0.1}         \n",
      "{'loss': 1.8577, 'learning_rate': 0.00019844943851807313, 'epoch': 0.1}         \n",
      "{'loss': 1.7399, 'learning_rate': 0.00019844889209525535, 'epoch': 0.1}         \n",
      "{'loss': 2.0484, 'learning_rate': 0.00019844834557692647, 'epoch': 0.1}         \n",
      "{'loss': 2.154, 'learning_rate': 0.00019844779896308707, 'epoch': 0.1}          \n",
      "{'loss': 2.6916, 'learning_rate': 0.00019844725225373763, 'epoch': 0.1}         \n",
      "{'loss': 2.1642, 'learning_rate': 0.00019844670544887875, 'epoch': 0.1}         \n",
      "{'loss': 2.1724, 'learning_rate': 0.0001984461585485109, 'epoch': 0.1}          \n",
      "{'loss': 2.2754, 'learning_rate': 0.00019844561155263465, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 111062/1061708 [16:37:41<140:40:16,  1.88it/s][2024-03-01 10:44:42,270] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8885, 'learning_rate': 0.00019844511917468678, 'epoch': 0.1}         \n",
      "{'loss': 2.2519, 'learning_rate': 0.00019844457199734596, 'epoch': 0.1}         \n",
      "{'loss': 2.0273, 'learning_rate': 0.0001984440247244983, 'epoch': 0.1}          \n",
      "{'loss': 2.0022, 'learning_rate': 0.0001984434773561443, 'epoch': 0.1}          \n",
      "{'loss': 2.0629, 'learning_rate': 0.00019844292989228447, 'epoch': 0.1}         \n",
      "{'loss': 2.1956, 'learning_rate': 0.00019844238233291937, 'epoch': 0.1}         \n",
      "{'loss': 1.736, 'learning_rate': 0.0001984418346780495, 'epoch': 0.1}           \n",
      "{'loss': 1.7123, 'learning_rate': 0.0001984412869276754, 'epoch': 0.1}          \n",
      "{'loss': 2.2639, 'learning_rate': 0.00019844073908179764, 'epoch': 0.1}         \n",
      "{'loss': 1.8408, 'learning_rate': 0.0001984401911404167, 'epoch': 0.1}          \n",
      "{'loss': 2.295, 'learning_rate': 0.00019843964310353314, 'epoch': 0.1}          \n",
      "{'loss': 2.4788, 'learning_rate': 0.00019843909497114747, 'epoch': 0.1}         \n",
      "{'loss': 1.9286, 'learning_rate': 0.00019843854674326023, 'epoch': 0.1}         \n",
      "{'loss': 2.1286, 'learning_rate': 0.000198437998419872, 'epoch': 0.1}           \n",
      "{'loss': 2.2396, 'learning_rate': 0.0001984374500009832, 'epoch': 0.1}          \n",
      "{'loss': 1.8535, 'learning_rate': 0.00019843690148659452, 'epoch': 0.1}         \n",
      "{'loss': 2.3787, 'learning_rate': 0.0001984363528767063, 'epoch': 0.1}          \n",
      "{'loss': 2.2755, 'learning_rate': 0.00019843580417131928, 'epoch': 0.1}         \n",
      "{'loss': 2.1304, 'learning_rate': 0.00019843525537043384, 'epoch': 0.1}         \n",
      "{'loss': 2.2825, 'learning_rate': 0.00019843470647405056, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 111263/1061708 [16:39:28<142:55:08,  1.85it/s][2024-03-01 10:46:29,523] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 10%|██▊                        | 111264/1061708 [16:39:29<133:42:01,  1.97it/s][2024-03-01 10:46:29,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9591, 'learning_rate': 0.00019843426728818585, 'epoch': 0.1}         \n",
      "{'loss': 2.1061, 'learning_rate': 0.0001984337182199078, 'epoch': 0.1}          \n",
      "{'loss': 2.0919, 'learning_rate': 0.0001984331690561334, 'epoch': 0.1}          \n",
      "{'loss': 2.0383, 'learning_rate': 0.0001984326197968632, 'epoch': 0.1}          \n",
      "{'loss': 2.2854, 'learning_rate': 0.00019843207044209773, 'epoch': 0.1}         \n",
      "{'loss': 1.9874, 'learning_rate': 0.00019843152099183748, 'epoch': 0.1}         \n",
      "{'loss': 2.3224, 'learning_rate': 0.00019843097144608304, 'epoch': 0.1}         \n",
      "{'loss': 2.1587, 'learning_rate': 0.0001984304218048349, 'epoch': 0.1}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7551, 'learning_rate': 0.00019842987206809362, 'epoch': 0.1}         \n",
      " 10%|██▊                        | 111353/1061708 [16:40:16<143:25:59,  1.84it/s][2024-03-01 10:47:17,427] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0614, 'learning_rate': 0.00019842937722338024, 'epoch': 0.1}         \n",
      "{'loss': 1.5765, 'learning_rate': 0.00019842882730520343, 'epoch': 0.1}         \n",
      "{'loss': 2.0688, 'learning_rate': 0.00019842827729153503, 'epoch': 0.1}         \n",
      "{'loss': 2.2046, 'learning_rate': 0.00019842772718237556, 'epoch': 0.1}         \n",
      "{'loss': 1.9008, 'learning_rate': 0.00019842717697772554, 'epoch': 0.1}         \n",
      "{'loss': 1.8918, 'learning_rate': 0.00019842662667758556, 'epoch': 0.1}         \n",
      "{'loss': 1.8072, 'learning_rate': 0.0001984260762819561, 'epoch': 0.1}          \n",
      "{'loss': 1.8694, 'learning_rate': 0.00019842552579083767, 'epoch': 0.1}         \n",
      "{'loss': 1.7034, 'learning_rate': 0.00019842497520423084, 'epoch': 0.1}         \n",
      "{'loss': 2.1299, 'learning_rate': 0.0001984244245221362, 'epoch': 0.1}          \n",
      "{'loss': 2.337, 'learning_rate': 0.00019842387374455415, 'epoch': 0.1}          \n",
      " 10%|██▊                        | 111468/1061708 [16:41:18<140:16:03,  1.88it/s][2024-03-01 10:48:18,728] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2569, 'learning_rate': 0.0001984233779630891, 'epoch': 0.1}          \n",
      "{'loss': 2.3033, 'learning_rate': 0.00019842282700408267, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 111483/1061708 [16:41:26<143:02:45,  1.85it/s][2024-03-01 10:48:26,659] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1613, 'learning_rate': 0.00019842233105933648, 'epoch': 0.11}        \n",
      "{'loss': 2.287, 'learning_rate': 0.00019842177991890748, 'epoch': 0.11}         \n",
      "{'loss': 2.3762, 'learning_rate': 0.00019842122868299374, 'epoch': 0.11}        \n",
      "{'loss': 2.123, 'learning_rate': 0.00019842067735159575, 'epoch': 0.11}         \n",
      "{'loss': 1.9212, 'learning_rate': 0.00019842012592471403, 'epoch': 0.11}        \n",
      "{'loss': 1.8108, 'learning_rate': 0.0001984195744023492, 'epoch': 0.11}         \n",
      "{'loss': 2.0825, 'learning_rate': 0.00019841902278450174, 'epoch': 0.11}        \n",
      "{'loss': 2.3827, 'learning_rate': 0.00019841847107117215, 'epoch': 0.11}        \n",
      "{'loss': 1.9713, 'learning_rate': 0.00019841791926236105, 'epoch': 0.11}        \n",
      "{'loss': 2.002, 'learning_rate': 0.0001984173673580689, 'epoch': 0.11}          \n",
      "{'loss': 2.1917, 'learning_rate': 0.0001984168153582963, 'epoch': 0.11}         \n",
      "{'loss': 2.2459, 'learning_rate': 0.0001984162632630437, 'epoch': 0.11}         \n",
      "{'loss': 2.4492, 'learning_rate': 0.00019841571107231173, 'epoch': 0.11}        \n",
      "{'loss': 2.1449, 'learning_rate': 0.00019841515878610088, 'epoch': 0.11}        \n",
      "{'loss': 2.1339, 'learning_rate': 0.00019841460640441163, 'epoch': 0.11}        \n",
      "{'loss': 2.2121, 'learning_rate': 0.00019841405392724464, 'epoch': 0.11}        \n",
      "{'loss': 2.1998, 'learning_rate': 0.00019841350135460032, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 111659/1061708 [16:43:00<140:09:48,  1.88it/s][2024-03-01 10:50:00,558] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0812, 'learning_rate': 0.00019841300395758785, 'epoch': 0.11}        \n",
      "{'loss': 2.3526, 'learning_rate': 0.00019841245120353823, 'epoch': 0.11}        \n",
      "{'loss': 2.189, 'learning_rate': 0.0001984118983540129, 'epoch': 0.11}          \n",
      " 11%|██▊                        | 111684/1061708 [16:43:13<141:51:34,  1.86it/s][2024-03-01 10:50:13,829] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0066, 'learning_rate': 0.00019841140070780882, 'epoch': 0.11}        \n",
      "{'loss': 1.3964, 'learning_rate': 0.0001984108476768811, 'epoch': 0.11}         \n",
      "{'loss': 1.8683, 'learning_rate': 0.00019841029455047921, 'epoch': 0.11}        \n",
      "{'loss': 2.3656, 'learning_rate': 0.00019840974132860373, 'epoch': 0.11}        \n",
      "{'loss': 1.4206, 'learning_rate': 0.00019840918801125511, 'epoch': 0.11}        \n",
      "{'loss': 1.9177, 'learning_rate': 0.00019840863459843398, 'epoch': 0.11}        \n",
      "{'loss': 1.6674, 'learning_rate': 0.00019840808109014083, 'epoch': 0.11}        \n",
      "{'loss': 2.1866, 'learning_rate': 0.00019840752748637622, 'epoch': 0.11}        \n",
      "{'loss': 2.25, 'learning_rate': 0.00019840697378714067, 'epoch': 0.11}          \n",
      "{'loss': 2.2916, 'learning_rate': 0.0001984064199924347, 'epoch': 0.11}         \n",
      "{'loss': 2.0122, 'learning_rate': 0.00019840586610225886, 'epoch': 0.11}        \n",
      "{'loss': 2.0719, 'learning_rate': 0.00019840531211661372, 'epoch': 0.11}        \n",
      "{'loss': 1.9777, 'learning_rate': 0.00019840475803549975, 'epoch': 0.11}        \n",
      "{'loss': 2.0256, 'learning_rate': 0.00019840420385891755, 'epoch': 0.11}        \n",
      "{'loss': 2.0329, 'learning_rate': 0.00019840364958686766, 'epoch': 0.11}        \n",
      "{'loss': 1.7513, 'learning_rate': 0.00019840309521935057, 'epoch': 0.11}        \n",
      "{'loss': 2.1409, 'learning_rate': 0.00019840254075636685, 'epoch': 0.11}        \n",
      "{'loss': 2.3202, 'learning_rate': 0.00019840198619791702, 'epoch': 0.11}        \n",
      "{'loss': 2.1459, 'learning_rate': 0.0001984014315440016, 'epoch': 0.11}         \n",
      "{'loss': 1.7131, 'learning_rate': 0.0001984008767946212, 'epoch': 0.11}         \n",
      " 11%|██▊                        | 111885/1061708 [16:45:00<141:19:59,  1.87it/s][2024-03-01 10:52:01,031] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▊                        | 111886/1061708 [16:45:00<132:24:12,  1.99it/s][2024-03-01 10:52:01,456] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2558, 'learning_rate': 0.00019840043292638243, 'epoch': 0.11}        \n",
      "{'loss': 2.3874, 'learning_rate': 0.0001983998780051663, 'epoch': 0.11}         \n",
      "{'loss': 2.3516, 'learning_rate': 0.00019839932298848671, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 111911/1061708 [16:45:14<139:52:50,  1.89it/s][2024-03-01 10:52:14,713] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0932, 'learning_rate': 0.00019839882339185418, 'epoch': 0.11}        \n",
      "{'loss': 2.0908, 'learning_rate': 0.00019839826819379537, 'epoch': 0.11}        \n",
      "{'loss': 1.6531, 'learning_rate': 0.00019839771290027457, 'epoch': 0.11}        \n",
      "{'loss': 2.1479, 'learning_rate': 0.0001983971575112924, 'epoch': 0.11}         \n",
      "{'loss': 1.6667, 'learning_rate': 0.00019839660202684934, 'epoch': 0.11}        \n",
      "{'loss': 2.0823, 'learning_rate': 0.00019839604644694591, 'epoch': 0.11}        \n",
      "{'loss': 1.9907, 'learning_rate': 0.00019839549077158272, 'epoch': 0.11}        \n",
      "{'loss': 2.2248, 'learning_rate': 0.00019839493500076026, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 111999/1061708 [16:46:01<139:57:15,  1.88it/s][2024-03-01 10:53:01,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=112000, skipped=1304, lr=[0.00019839437913447906], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 10:53:01,718] [INFO] [timer.py:260:stop] epoch=0/micro_step=112000/global_step=112000, RunningAvgSamplesPerSec=1.892655390203848, CurrSamplesPerSec=1.9093580442379943, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3409, 'learning_rate': 0.00019839437913447906, 'epoch': 0.11}        \n",
      "{'loss': 1.8542, 'learning_rate': 0.00019839382317273966, 'epoch': 0.11}        \n",
      "{'loss': 2.0177, 'learning_rate': 0.00019839326711554268, 'epoch': 0.11}        \n",
      "{'loss': 2.1248, 'learning_rate': 0.00019839271096288856, 'epoch': 0.11}        \n",
      "{'loss': 2.2073, 'learning_rate': 0.00019839215471477786, 'epoch': 0.11}        \n",
      "{'loss': 1.8075, 'learning_rate': 0.00019839159837121116, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9553, 'learning_rate': 0.00019839104193218897, 'epoch': 0.11}        \n",
      "{'loss': 2.2595, 'learning_rate': 0.00019839048539771183, 'epoch': 0.11}        \n",
      "{'loss': 2.0319, 'learning_rate': 0.00019838992876778025, 'epoch': 0.11}        \n",
      "{'loss': 2.4964, 'learning_rate': 0.00019838937204239487, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112091/1061708 [16:46:50<139:42:28,  1.89it/s][2024-03-01 10:53:50,631] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1319, 'learning_rate': 0.0001983888709079354, 'epoch': 0.11}         \n",
      "{'loss': 2.2978, 'learning_rate': 0.00019838831400118912, 'epoch': 0.11}        \n",
      "{'loss': 2.2962, 'learning_rate': 0.00019838775699899057, 'epoch': 0.11}        \n",
      "{'loss': 1.8649, 'learning_rate': 0.00019838719990134026, 'epoch': 0.11}        \n",
      "{'loss': 2.1098, 'learning_rate': 0.0001983866427082387, 'epoch': 0.11}         \n",
      "{'loss': 2.1027, 'learning_rate': 0.00019838608541968648, 'epoch': 0.11}        \n",
      "{'loss': 2.3279, 'learning_rate': 0.00019838552803568414, 'epoch': 0.11}        \n",
      "{'loss': 1.9824, 'learning_rate': 0.0001983849705562322, 'epoch': 0.11}         \n",
      "{'loss': 2.3942, 'learning_rate': 0.0001983844129813312, 'epoch': 0.11}         \n",
      "{'loss': 2.1773, 'learning_rate': 0.00019838385531098168, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112192/1061708 [16:47:43<139:55:33,  1.88it/s][2024-03-01 10:54:44,382] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▊                        | 112193/1061708 [16:47:44<134:35:28,  1.96it/s][2024-03-01 10:54:44,806] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1293, 'learning_rate': 0.00019838340910597948, 'epoch': 0.11}        \n",
      "{'loss': 2.1342, 'learning_rate': 0.000198382851263824, 'epoch': 0.11}          \n",
      "{'loss': 1.9694, 'learning_rate': 0.00019838229332622153, 'epoch': 0.11}        \n",
      "{'loss': 2.187, 'learning_rate': 0.00019838173529317262, 'epoch': 0.11}         \n",
      "{'loss': 2.0963, 'learning_rate': 0.00019838117716467775, 'epoch': 0.11}        \n",
      "{'loss': 2.0477, 'learning_rate': 0.0001983806189407375, 'epoch': 0.11}         \n",
      "{'loss': 2.088, 'learning_rate': 0.00019838006062135248, 'epoch': 0.11}         \n",
      "{'loss': 1.5927, 'learning_rate': 0.00019837950220652313, 'epoch': 0.11}        \n",
      "{'loss': 2.084, 'learning_rate': 0.00019837894369625003, 'epoch': 0.11}         \n",
      "{'loss': 1.7143, 'learning_rate': 0.00019837838509053374, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112294/1061708 [16:48:38<142:00:14,  1.86it/s][2024-03-01 10:55:38,648] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▊                        | 112295/1061708 [16:48:38<132:48:04,  1.99it/s][2024-03-01 10:55:39,072] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0032, 'learning_rate': 0.00019837793813724192, 'epoch': 0.11}        \n",
      "{'loss': 2.3918, 'learning_rate': 0.00019837737935972922, 'epoch': 0.11}        \n",
      "{'loss': 2.183, 'learning_rate': 0.00019837682048677482, 'epoch': 0.11}         \n",
      "{'loss': 2.1124, 'learning_rate': 0.00019837626151837924, 'epoch': 0.11}        \n",
      "{'loss': 1.9333, 'learning_rate': 0.0001983757024545431, 'epoch': 0.11}         \n",
      "{'loss': 1.9356, 'learning_rate': 0.00019837514329526688, 'epoch': 0.11}        \n",
      "{'loss': 1.7662, 'learning_rate': 0.00019837458404055113, 'epoch': 0.11}        \n",
      "{'loss': 1.8214, 'learning_rate': 0.0001983740246903964, 'epoch': 0.11}         \n",
      "{'loss': 2.1594, 'learning_rate': 0.00019837346524480327, 'epoch': 0.11}        \n",
      "{'loss': 2.0855, 'learning_rate': 0.0001983729057037722, 'epoch': 0.11}         \n",
      " 11%|██▊                        | 112396/1061708 [16:49:32<140:58:41,  1.87it/s][2024-03-01 10:56:32,996] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▊                        | 112397/1061708 [16:49:32<132:07:05,  2.00it/s][2024-03-01 10:56:33,419] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1985, 'learning_rate': 0.00019837245800223246, 'epoch': 0.11}        \n",
      "{'loss': 1.6217, 'learning_rate': 0.0001983718982894146, 'epoch': 0.11}         \n",
      "{'loss': 2.1464, 'learning_rate': 0.0001983713384811603, 'epoch': 0.11}         \n",
      "{'loss': 1.8939, 'learning_rate': 0.00019837077857747022, 'epoch': 0.11}        \n",
      "{'loss': 1.8433, 'learning_rate': 0.00019837021857834486, 'epoch': 0.11}        \n",
      "{'loss': 2.1004, 'learning_rate': 0.00019836965848378472, 'epoch': 0.11}        \n",
      "{'loss': 2.4143, 'learning_rate': 0.00019836909829379042, 'epoch': 0.11}        \n",
      "{'loss': 1.8573, 'learning_rate': 0.00019836853800836245, 'epoch': 0.11}        \n",
      "{'loss': 1.8412, 'learning_rate': 0.00019836797762750138, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112488/1061708 [16:50:21<139:50:54,  1.89it/s][2024-03-01 10:57:21,868] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.088, 'learning_rate': 0.0001983674732031315, 'epoch': 0.11}          \n",
      "{'loss': 2.2782, 'learning_rate': 0.000198366912640949, 'epoch': 0.11}          \n",
      "{'loss': 2.1553, 'learning_rate': 0.00019836635198333498, 'epoch': 0.11}        \n",
      "{'loss': 1.8388, 'learning_rate': 0.00019836579123028996, 'epoch': 0.11}        \n",
      "{'loss': 1.9535, 'learning_rate': 0.00019836523038181447, 'epoch': 0.11}        \n",
      "{'loss': 2.0059, 'learning_rate': 0.0001983646694379091, 'epoch': 0.11}         \n",
      "{'loss': 1.7799, 'learning_rate': 0.00019836410839857432, 'epoch': 0.11}        \n",
      "{'loss': 2.5749, 'learning_rate': 0.00019836354726381078, 'epoch': 0.11}        \n",
      "{'loss': 1.8878, 'learning_rate': 0.00019836298603361895, 'epoch': 0.11}        \n",
      "{'loss': 2.2586, 'learning_rate': 0.00019836242470799942, 'epoch': 0.11}        \n",
      "{'loss': 1.9405, 'learning_rate': 0.00019836186328695267, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112594/1061708 [16:51:17<142:06:16,  1.86it/s][2024-03-01 10:58:18,359] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7381, 'learning_rate': 0.00019836135792642082, 'epoch': 0.11}        \n",
      "{'loss': 2.3251, 'learning_rate': 0.00019836079632406395, 'epoch': 0.11}        \n",
      "{'loss': 2.1275, 'learning_rate': 0.00019836023462628145, 'epoch': 0.11}        \n",
      "{'loss': 2.0101, 'learning_rate': 0.00019835967283307393, 'epoch': 0.11}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.00019835911094444187, 'epoch': 0.11}        \n",
      "{'loss': 1.813, 'learning_rate': 0.00019835854896038587, 'epoch': 0.11}         \n",
      " 11%|██▊                        | 112657/1061708 [16:51:51<140:26:15,  1.88it/s][2024-03-01 10:58:52,019] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2604, 'learning_rate': 0.0001983580430931484, 'epoch': 0.11}         \n",
      "{'loss': 2.3259, 'learning_rate': 0.00019835748092778834, 'epoch': 0.11}        \n",
      "{'loss': 2.0502, 'learning_rate': 0.00019835691866700592, 'epoch': 0.11}        \n",
      "{'loss': 2.0045, 'learning_rate': 0.00019835635631080165, 'epoch': 0.11}        \n",
      "{'loss': 2.2546, 'learning_rate': 0.00019835579385917606, 'epoch': 0.11}        \n",
      "{'loss': 1.9263, 'learning_rate': 0.00019835523131212974, 'epoch': 0.11}        \n",
      "{'loss': 1.8617, 'learning_rate': 0.0001983546686696632, 'epoch': 0.11}         \n",
      "{'loss': 1.9979, 'learning_rate': 0.00019835410593177703, 'epoch': 0.11}        \n",
      "{'loss': 2.1821, 'learning_rate': 0.00019835354309847176, 'epoch': 0.11}        \n",
      "{'loss': 2.026, 'learning_rate': 0.0001983529801697479, 'epoch': 0.11}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6144, 'learning_rate': 0.000198352417145606, 'epoch': 0.11}          \n",
      "{'loss': 1.8185, 'learning_rate': 0.00019835185402604667, 'epoch': 0.11}        \n",
      "{'loss': 2.2375, 'learning_rate': 0.0001983512908110704, 'epoch': 0.11}         \n",
      "{'loss': 2.0971, 'learning_rate': 0.00019835072750067772, 'epoch': 0.11}        \n",
      "{'loss': 2.1033, 'learning_rate': 0.00019835016409486923, 'epoch': 0.11}        \n",
      "{'loss': 2.0707, 'learning_rate': 0.00019834960059364546, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112812/1061708 [16:53:14<139:27:43,  1.89it/s][2024-03-01 11:00:14,669] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8768, 'learning_rate': 0.00019834909336096445, 'epoch': 0.11}        \n",
      "{'loss': 2.2839, 'learning_rate': 0.00019834852967845314, 'epoch': 0.11}        \n",
      "{'loss': 2.167, 'learning_rate': 0.0001983479659005281, 'epoch': 0.11}          \n",
      " 11%|██▊                        | 112848/1061708 [16:53:33<139:52:44,  1.88it/s][2024-03-01 11:00:33,786] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2477, 'learning_rate': 0.00019834745841881734, 'epoch': 0.11}        \n",
      "{'loss': 2.0345, 'learning_rate': 0.00019834689445960782, 'epoch': 0.11}        \n",
      "{'loss': 2.5665, 'learning_rate': 0.00019834633040498613, 'epoch': 0.11}        \n",
      "{'loss': 2.1509, 'learning_rate': 0.0001983457662549529, 'epoch': 0.11}         \n",
      "{'loss': 2.2447, 'learning_rate': 0.00019834520200950868, 'epoch': 0.11}        \n",
      "{'loss': 2.5025, 'learning_rate': 0.00019834463766865397, 'epoch': 0.11}        \n",
      "{'loss': 1.664, 'learning_rate': 0.0001983440732323893, 'epoch': 0.11}          \n",
      "{'loss': 2.3687, 'learning_rate': 0.0001983435087007153, 'epoch': 0.11}         \n",
      "{'loss': 2.0101, 'learning_rate': 0.00019834294407363245, 'epoch': 0.11}        \n",
      "{'loss': 1.8002, 'learning_rate': 0.00019834237935114133, 'epoch': 0.11}        \n",
      "{'loss': 2.3803, 'learning_rate': 0.00019834181453324244, 'epoch': 0.11}        \n",
      "{'loss': 2.0042, 'learning_rate': 0.0001983412496199364, 'epoch': 0.11}         \n",
      "{'loss': 2.2559, 'learning_rate': 0.0001983406846112237, 'epoch': 0.11}         \n",
      "{'loss': 1.9143, 'learning_rate': 0.00019834011950710492, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 112983/1061708 [16:54:45<142:58:13,  1.84it/s][2024-03-01 11:01:45,673] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.968, 'learning_rate': 0.00019833961083182627, 'epoch': 0.11}         \n",
      "{'loss': 2.0501, 'learning_rate': 0.00019833904554643743, 'epoch': 0.11}        \n",
      "{'loss': 2.0007, 'learning_rate': 0.00019833848016564406, 'epoch': 0.11}        \n",
      " 11%|██▊                        | 113010/1061708 [16:54:59<139:24:43,  1.89it/s][2024-03-01 11:01:59,976] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9438, 'learning_rate': 0.00019833797124135967, 'epoch': 0.11}        \n",
      "{'loss': 1.912, 'learning_rate': 0.00019833740567929927, 'epoch': 0.11}         \n",
      "{'loss': 1.8919, 'learning_rate': 0.000198336840021836, 'epoch': 0.11}          \n",
      "{'loss': 1.73, 'learning_rate': 0.00019833627426897032, 'epoch': 0.11}          \n",
      "{'loss': 2.2263, 'learning_rate': 0.00019833570842070288, 'epoch': 0.11}        \n",
      "{'loss': 2.2626, 'learning_rate': 0.00019833514247703416, 'epoch': 0.11}        \n",
      "{'loss': 2.0688, 'learning_rate': 0.0001983345764379647, 'epoch': 0.11}         \n",
      "{'loss': 1.8855, 'learning_rate': 0.0001983340103034951, 'epoch': 0.11}         \n",
      "{'loss': 2.0843, 'learning_rate': 0.00019833344407362588, 'epoch': 0.11}        \n",
      "{'loss': 2.1841, 'learning_rate': 0.00019833287774835757, 'epoch': 0.11}        \n",
      "{'loss': 1.8386, 'learning_rate': 0.0001983323113276908, 'epoch': 0.11}         \n",
      "{'loss': 2.2356, 'learning_rate': 0.00019833174481162603, 'epoch': 0.11}        \n",
      "{'loss': 2.0726, 'learning_rate': 0.00019833117820016382, 'epoch': 0.11}        \n",
      "{'loss': 1.9196, 'learning_rate': 0.00019833061149330478, 'epoch': 0.11}        \n",
      "{'loss': 1.8765, 'learning_rate': 0.0001983300446910494, 'epoch': 0.11}         \n",
      "{'loss': 2.4861, 'learning_rate': 0.00019832947779339826, 'epoch': 0.11}        \n",
      "{'loss': 1.8423, 'learning_rate': 0.0001983289108003519, 'epoch': 0.11}         \n",
      "{'loss': 2.3118, 'learning_rate': 0.00019832834371191088, 'epoch': 0.11}        \n",
      "{'loss': 2.1561, 'learning_rate': 0.00019832777652807571, 'epoch': 0.11}        \n",
      "{'loss': 2.3438, 'learning_rate': 0.00019832720924884702, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 113211/1061708 [16:56:46<139:29:15,  1.89it/s][2024-03-01 11:03:47,017] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 113212/1061708 [16:56:46<130:52:56,  2.01it/s][2024-03-01 11:03:47,438] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.122, 'learning_rate': 0.00019832675535678106, 'epoch': 0.11}         \n",
      "{'loss': 2.4099, 'learning_rate': 0.0001983261879058453, 'epoch': 0.11}         \n",
      "{'loss': 2.1818, 'learning_rate': 0.00019832562035951754, 'epoch': 0.11}        \n",
      "{'loss': 1.9646, 'learning_rate': 0.00019832505271779826, 'epoch': 0.11}        \n",
      "{'loss': 1.9269, 'learning_rate': 0.0001983244849806881, 'epoch': 0.11}         \n",
      "{'loss': 2.3372, 'learning_rate': 0.00019832391714818756, 'epoch': 0.11}        \n",
      "{'loss': 2.257, 'learning_rate': 0.0001983233492202972, 'epoch': 0.11}          \n",
      "{'loss': 2.0234, 'learning_rate': 0.00019832278119701755, 'epoch': 0.11}        \n",
      "{'loss': 2.0852, 'learning_rate': 0.00019832221307834917, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 113309/1061708 [16:57:38<139:32:04,  1.89it/s][2024-03-01 11:04:39,066] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3471, 'learning_rate': 0.00019832170168999075, 'epoch': 0.11}        \n",
      "{'loss': 2.3717, 'learning_rate': 0.00019832113339008533, 'epoch': 0.11}        \n",
      "{'loss': 2.3074, 'learning_rate': 0.0001983205649947928, 'epoch': 0.11}         \n",
      "{'loss': 1.6942, 'learning_rate': 0.00019831999650411368, 'epoch': 0.11}        \n",
      "{'loss': 2.1235, 'learning_rate': 0.00019831942791804857, 'epoch': 0.11}        \n",
      "{'loss': 2.1707, 'learning_rate': 0.00019831885923659798, 'epoch': 0.11}        \n",
      "{'loss': 2.1218, 'learning_rate': 0.00019831829045976245, 'epoch': 0.11}        \n",
      "{'loss': 2.2511, 'learning_rate': 0.00019831772158754256, 'epoch': 0.11}        \n",
      "{'loss': 2.0742, 'learning_rate': 0.0001983171526199389, 'epoch': 0.11}         \n",
      "{'loss': 2.0679, 'learning_rate': 0.00019831658355695192, 'epoch': 0.11}        \n",
      "{'loss': 1.9493, 'learning_rate': 0.00019831601439858226, 'epoch': 0.11}        \n",
      "{'loss': 2.3524, 'learning_rate': 0.00019831544514483044, 'epoch': 0.11}        \n",
      "{'loss': 2.1994, 'learning_rate': 0.00019831487579569703, 'epoch': 0.11}        \n",
      "{'loss': 1.9322, 'learning_rate': 0.00019831430635118257, 'epoch': 0.11}        \n",
      "{'loss': 1.9817, 'learning_rate': 0.0001983137368112876, 'epoch': 0.11}         \n",
      "{'loss': 2.4882, 'learning_rate': 0.00019831316717601268, 'epoch': 0.11}        \n",
      "{'loss': 1.9035, 'learning_rate': 0.00019831259744535834, 'epoch': 0.11}        \n",
      "{'loss': 2.4147, 'learning_rate': 0.0001983120276193252, 'epoch': 0.11}         \n",
      "{'loss': 1.7644, 'learning_rate': 0.00019831145769791376, 'epoch': 0.11}        \n",
      "{'loss': 2.1217, 'learning_rate': 0.00019831088768112461, 'epoch': 0.11}        \n",
      "{'loss': 2.2876, 'learning_rate': 0.00019831031756895824, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 113510/1061708 [16:59:25<139:19:58,  1.89it/s][2024-03-01 11:06:26,200] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|██▉                        | 113511/1061708 [16:59:26<130:58:17,  2.01it/s][2024-03-01 11:06:26,621] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2078, 'learning_rate': 0.00019830986141055395, 'epoch': 0.11}        \n",
      "{'loss': 2.2138, 'learning_rate': 0.00019830929112671006, 'epoch': 0.11}        \n",
      "{'loss': 2.1226, 'learning_rate': 0.0001983087207474905, 'epoch': 0.11}         \n",
      "{'loss': 2.1243, 'learning_rate': 0.00019830815027289592, 'epoch': 0.11}        \n",
      "{'loss': 2.2039, 'learning_rate': 0.00019830757970292677, 'epoch': 0.11}        \n",
      "{'loss': 2.0585, 'learning_rate': 0.00019830700903758367, 'epoch': 0.11}        \n",
      "{'loss': 1.9476, 'learning_rate': 0.00019830643827686712, 'epoch': 0.11}        \n",
      "{'loss': 2.0661, 'learning_rate': 0.00019830586742077775, 'epoch': 0.11}        \n",
      "{'loss': 1.9709, 'learning_rate': 0.00019830529646931605, 'epoch': 0.11}        \n",
      "{'loss': 2.1338, 'learning_rate': 0.00019830472542248259, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 113612/1061708 [17:00:19<139:45:18,  1.88it/s][2024-03-01 11:07:20,439] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 113613/1061708 [17:00:20<135:11:59,  1.95it/s][2024-03-01 11:07:20,864] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.156, 'learning_rate': 0.00019830426851634848, 'epoch': 0.11}         \n",
      "{'loss': 2.1935, 'learning_rate': 0.00019830369729784727, 'epoch': 0.11}        \n",
      "{'loss': 2.4632, 'learning_rate': 0.00019830312598397582, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 113646/1061708 [17:00:37<140:37:43,  1.87it/s][2024-03-01 11:07:38,432] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9021, 'learning_rate': 0.00019830261171995046, 'epoch': 0.11}        \n",
      "{'loss': 1.9998, 'learning_rate': 0.00019830204022487714, 'epoch': 0.11}        \n",
      "{'loss': 1.6477, 'learning_rate': 0.00019830146863443525, 'epoch': 0.11}        \n",
      "{'loss': 2.1272, 'learning_rate': 0.0001983008969486253, 'epoch': 0.11}         \n",
      "{'loss': 2.0137, 'learning_rate': 0.00019830032516744787, 'epoch': 0.11}        \n",
      "{'loss': 2.2115, 'learning_rate': 0.00019829975329090348, 'epoch': 0.11}        \n",
      "{'loss': 2.1768, 'learning_rate': 0.00019829918131899273, 'epoch': 0.11}        \n",
      "{'loss': 1.9271, 'learning_rate': 0.00019829860925171615, 'epoch': 0.11}        \n",
      "{'loss': 2.12, 'learning_rate': 0.00019829803708907432, 'epoch': 0.11}          \n",
      "{'loss': 2.4545, 'learning_rate': 0.00019829746483106774, 'epoch': 0.11}        \n",
      "{'loss': 2.3663, 'learning_rate': 0.000198296892477697, 'epoch': 0.11}          \n",
      "{'loss': 1.5114, 'learning_rate': 0.00019829632002896268, 'epoch': 0.11}        \n",
      "{'loss': 2.0494, 'learning_rate': 0.0001982957474848653, 'epoch': 0.11}         \n",
      "{'loss': 1.8134, 'learning_rate': 0.00019829517484540543, 'epoch': 0.11}        \n",
      "{'loss': 1.6718, 'learning_rate': 0.0001982946021105836, 'epoch': 0.11}         \n",
      "{'loss': 2.2128, 'learning_rate': 0.0001982940292804004, 'epoch': 0.11}         \n",
      "{'loss': 2.2592, 'learning_rate': 0.00019829345635485638, 'epoch': 0.11}        \n",
      "{'loss': 1.9431, 'learning_rate': 0.00019829288333395208, 'epoch': 0.11}        \n",
      "{'loss': 1.7076, 'learning_rate': 0.00019829231021768808, 'epoch': 0.11}        \n",
      "{'loss': 1.9362, 'learning_rate': 0.0001982917370060649, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 113847/1061708 [17:02:25<140:31:14,  1.87it/s][2024-03-01 11:09:25,853] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 113848/1061708 [17:02:25<131:50:23,  2.00it/s][2024-03-01 11:09:26,279] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8491, 'learning_rate': 0.00019829127836810812, 'epoch': 0.11}        \n",
      "{'loss': 2.2359, 'learning_rate': 0.00019829070498483987, 'epoch': 0.11}        \n",
      "{'loss': 2.267, 'learning_rate': 0.00019829013150621404, 'epoch': 0.11}         \n",
      "{'loss': 1.7344, 'learning_rate': 0.0001982895579322311, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 113886/1061708 [17:02:45<140:35:34,  1.87it/s][2024-03-01 11:09:46,485] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1432, 'learning_rate': 0.00019828904163411668, 'epoch': 0.11}        \n",
      "{'loss': 2.227, 'learning_rate': 0.00019828846787895693, 'epoch': 0.11}         \n",
      "{'loss': 2.2552, 'learning_rate': 0.00019828789402844175, 'epoch': 0.11}        \n",
      "{'loss': 2.0817, 'learning_rate': 0.00019828732008257167, 'epoch': 0.11}        \n",
      "{'loss': 2.2737, 'learning_rate': 0.0001982867460413473, 'epoch': 0.11}         \n",
      "{'loss': 2.1062, 'learning_rate': 0.00019828617190476914, 'epoch': 0.11}        \n",
      "{'loss': 1.774, 'learning_rate': 0.00019828559767283782, 'epoch': 0.11}         \n",
      "{'loss': 1.8487, 'learning_rate': 0.00019828502334555382, 'epoch': 0.11}        \n",
      "{'loss': 1.8172, 'learning_rate': 0.0001982844489229177, 'epoch': 0.11}         \n",
      "{'loss': 1.9802, 'learning_rate': 0.0001982838744049301, 'epoch': 0.11}         \n",
      "{'loss': 1.884, 'learning_rate': 0.0001982832997915915, 'epoch': 0.11}          \n",
      " 11%|██▉                        | 113999/1061708 [17:03:46<139:46:37,  1.88it/s][2024-03-01 11:10:46,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=114000, skipped=1329, lr=[0.00019828272508290248], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 11:10:46,934] [INFO] [timer.py:260:stop] epoch=0/micro_step=114000/global_step=114000, RunningAvgSamplesPerSec=1.892662477707178, CurrSamplesPerSec=1.9131482691915596, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0791, 'learning_rate': 0.00019828272508290248, 'epoch': 0.11}        \n",
      "{'loss': 1.877, 'learning_rate': 0.0001982821502788636, 'epoch': 0.11}          \n",
      " 11%|██▉                        | 114012/1061708 [17:03:53<140:27:38,  1.87it/s][2024-03-01 11:10:53,815] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0852, 'learning_rate': 0.00019828163287370493, 'epoch': 0.11}        \n",
      "{'loss': 2.1871, 'learning_rate': 0.00019828105788850285, 'epoch': 0.11}        \n",
      "{'loss': 2.3757, 'learning_rate': 0.00019828048280795256, 'epoch': 0.11}        \n",
      "{'loss': 2.2885, 'learning_rate': 0.00019827990763205455, 'epoch': 0.11}        \n",
      "{'loss': 2.0845, 'learning_rate': 0.0001982793323608094, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 114062/1061708 [17:04:19<139:28:46,  1.89it/s][2024-03-01 11:11:20,431] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9233, 'learning_rate': 0.0001982788145351675, 'epoch': 0.11}         \n",
      "{'loss': 1.7001, 'learning_rate': 0.00019827823908276436, 'epoch': 0.11}        \n",
      "{'loss': 2.1507, 'learning_rate': 0.0001982776635350157, 'epoch': 0.11}         \n",
      "{'loss': 2.1592, 'learning_rate': 0.00019827708789192213, 'epoch': 0.11}        \n",
      "{'loss': 2.4369, 'learning_rate': 0.00019827651215348414, 'epoch': 0.11}        \n",
      "{'loss': 1.775, 'learning_rate': 0.00019827593631970234, 'epoch': 0.11}         \n",
      "{'loss': 2.0271, 'learning_rate': 0.00019827536039057723, 'epoch': 0.11}        \n",
      "{'loss': 2.1513, 'learning_rate': 0.00019827478436610944, 'epoch': 0.11}        \n",
      "{'loss': 1.8712, 'learning_rate': 0.00019827420824629948, 'epoch': 0.11}        \n",
      "{'loss': 2.0278, 'learning_rate': 0.00019827363203114794, 'epoch': 0.11}        \n",
      "{'loss': 2.1487, 'learning_rate': 0.0001982730557206553, 'epoch': 0.11}         \n",
      "{'loss': 2.1443, 'learning_rate': 0.00019827247931482225, 'epoch': 0.11}        \n",
      "{'loss': 1.8857, 'learning_rate': 0.00019827190281364928, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9189, 'learning_rate': 0.0001982713262171369, 'epoch': 0.11}         \n",
      "{'loss': 1.8969, 'learning_rate': 0.00019827074952528576, 'epoch': 0.11}        \n",
      "{'loss': 2.1849, 'learning_rate': 0.00019827017273809633, 'epoch': 0.11}        \n",
      "{'loss': 2.2714, 'learning_rate': 0.00019826959585556923, 'epoch': 0.11}        \n",
      "{'loss': 2.237, 'learning_rate': 0.00019826901887770503, 'epoch': 0.11}         \n",
      "{'loss': 2.2885, 'learning_rate': 0.00019826844180450426, 'epoch': 0.11}        \n",
      "{'loss': 1.8135, 'learning_rate': 0.00019826786463596752, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114263/1061708 [17:06:07<142:13:08,  1.85it/s][2024-03-01 11:13:07,525] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 114264/1061708 [17:06:07<132:50:04,  1.98it/s][2024-03-01 11:13:07,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2437, 'learning_rate': 0.00019826740283249654, 'epoch': 0.11}        \n",
      "{'loss': 2.1342, 'learning_rate': 0.00019826682549235636, 'epoch': 0.11}        \n",
      "{'loss': 2.3859, 'learning_rate': 0.00019826624805688176, 'epoch': 0.11}        \n",
      "{'loss': 1.9593, 'learning_rate': 0.0001982656705260733, 'epoch': 0.11}         \n",
      "{'loss': 2.3413, 'learning_rate': 0.0001982650928999315, 'epoch': 0.11}         \n",
      "{'loss': 1.8198, 'learning_rate': 0.00019826451517845694, 'epoch': 0.11}        \n",
      "{'loss': 2.2727, 'learning_rate': 0.00019826393736165022, 'epoch': 0.11}        \n",
      "{'loss': 2.2499, 'learning_rate': 0.00019826335944951183, 'epoch': 0.11}        \n",
      "{'loss': 2.3601, 'learning_rate': 0.0001982627814420424, 'epoch': 0.11}         \n",
      "{'loss': 1.9479, 'learning_rate': 0.0001982622033392424, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 114365/1061708 [17:07:01<140:41:01,  1.87it/s][2024-03-01 11:14:01,736] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 114366/1061708 [17:07:01<131:39:45,  2.00it/s][2024-03-01 11:14:02,159] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8939, 'learning_rate': 0.00019826174078836485, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114373/1061708 [17:07:05<141:36:38,  1.86it/s][2024-03-01 11:14:05,840] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3112, 'learning_rate': 0.00019826122034570057, 'epoch': 0.11}        \n",
      "{'loss': 1.9114, 'learning_rate': 0.000198260641985511, 'epoch': 0.11}          \n",
      "{'loss': 2.1126, 'learning_rate': 0.00019826006352999302, 'epoch': 0.11}        \n",
      "{'loss': 2.0917, 'learning_rate': 0.00019825948497914717, 'epoch': 0.11}        \n",
      "{'loss': 2.4799, 'learning_rate': 0.00019825890633297401, 'epoch': 0.11}        \n",
      "{'loss': 2.1726, 'learning_rate': 0.0001982583275914741, 'epoch': 0.11}         \n",
      "{'loss': 1.9812, 'learning_rate': 0.000198257748754648, 'epoch': 0.11}          \n",
      "{'loss': 1.9811, 'learning_rate': 0.00019825716982249626, 'epoch': 0.11}        \n",
      "{'loss': 2.2087, 'learning_rate': 0.00019825659079501944, 'epoch': 0.11}        \n",
      "{'loss': 2.127, 'learning_rate': 0.00019825601167221813, 'epoch': 0.11}         \n",
      "{'loss': 1.7111, 'learning_rate': 0.00019825543245409287, 'epoch': 0.11}        \n",
      "{'loss': 2.3279, 'learning_rate': 0.00019825485314064426, 'epoch': 0.11}        \n",
      "{'loss': 2.1757, 'learning_rate': 0.0001982542737318728, 'epoch': 0.11}         \n",
      "{'loss': 2.4787, 'learning_rate': 0.00019825369422777906, 'epoch': 0.11}        \n",
      "{'loss': 1.8323, 'learning_rate': 0.00019825311462836367, 'epoch': 0.11}        \n",
      "{'loss': 2.2796, 'learning_rate': 0.0001982525349336271, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 114536/1061708 [17:08:32<140:07:44,  1.88it/s][2024-03-01 11:15:32,568] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2043, 'learning_rate': 0.00019825201312686513, 'epoch': 0.11}        \n",
      "{'loss': 2.1999, 'learning_rate': 0.00019825143325101995, 'epoch': 0.11}        \n",
      "{'loss': 2.1958, 'learning_rate': 0.0001982508532798553, 'epoch': 0.11}         \n",
      "{'loss': 2.2675, 'learning_rate': 0.0001982502732133717, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 114578/1061708 [17:08:54<139:56:03,  1.88it/s][2024-03-01 11:15:54,909] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0637, 'learning_rate': 0.00019824975107203926, 'epoch': 0.11}        \n",
      "{'loss': 1.7992, 'learning_rate': 0.00019824917082445122, 'epoch': 0.11}        \n",
      "{'loss': 1.909, 'learning_rate': 0.0001982485904815459, 'epoch': 0.11}          \n",
      "{'loss': 2.2804, 'learning_rate': 0.0001982480100433238, 'epoch': 0.11}         \n",
      "{'loss': 1.8021, 'learning_rate': 0.00019824742950978554, 'epoch': 0.11}        \n",
      "{'loss': 2.1694, 'learning_rate': 0.00019824684888093168, 'epoch': 0.11}        \n",
      "{'loss': 2.5299, 'learning_rate': 0.00019824626815676276, 'epoch': 0.11}        \n",
      "{'loss': 2.1388, 'learning_rate': 0.00019824568733727932, 'epoch': 0.11}        \n",
      "{'loss': 1.9191, 'learning_rate': 0.00019824510642248196, 'epoch': 0.11}        \n",
      "{'loss': 1.872, 'learning_rate': 0.00019824452541237125, 'epoch': 0.11}         \n",
      "{'loss': 2.0583, 'learning_rate': 0.00019824394430694776, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114684/1061708 [17:09:50<142:11:24,  1.85it/s][2024-03-01 11:16:51,452] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2789, 'learning_rate': 0.0001982434212305746, 'epoch': 0.11}         \n",
      "{'loss': 2.389, 'learning_rate': 0.0001982428399440583, 'epoch': 0.11}          \n",
      "{'loss': 2.1699, 'learning_rate': 0.00019824225856223087, 'epoch': 0.11}        \n",
      "{'loss': 2.0789, 'learning_rate': 0.00019824167708509281, 'epoch': 0.11}        \n",
      "{'loss': 2.2369, 'learning_rate': 0.0001982410955126447, 'epoch': 0.11}         \n",
      "{'loss': 1.843, 'learning_rate': 0.00019824051384488716, 'epoch': 0.11}         \n",
      "{'loss': 2.1868, 'learning_rate': 0.00019823993208182067, 'epoch': 0.11}        \n",
      "{'loss': 2.2024, 'learning_rate': 0.00019823935022344588, 'epoch': 0.11}        \n",
      "{'loss': 1.8409, 'learning_rate': 0.00019823876826976326, 'epoch': 0.11}        \n",
      "{'loss': 2.1969, 'learning_rate': 0.00019823818622077347, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114785/1061708 [17:10:44<141:15:14,  1.86it/s][2024-03-01 11:17:45,395] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 114786/1061708 [17:10:45<132:08:43,  1.99it/s][2024-03-01 11:17:45,818] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3144, 'learning_rate': 0.0001982377205129608, 'epoch': 0.11}         \n",
      "{'loss': 1.8901, 'learning_rate': 0.0001982371382924194, 'epoch': 0.11}         \n",
      "{'loss': 1.989, 'learning_rate': 0.00019823655597657242, 'epoch': 0.11}         \n",
      "{'loss': 1.9911, 'learning_rate': 0.0001982359735654203, 'epoch': 0.11}         \n",
      "{'loss': 1.8672, 'learning_rate': 0.00019823539105896375, 'epoch': 0.11}        \n",
      "{'loss': 2.139, 'learning_rate': 0.00019823480845720321, 'epoch': 0.11}         \n",
      "{'loss': 1.8045, 'learning_rate': 0.00019823422576013933, 'epoch': 0.11}        \n",
      "{'loss': 1.9237, 'learning_rate': 0.00019823364296777264, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114868/1061708 [17:11:28<139:51:21,  1.88it/s][2024-03-01 11:18:29,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9756, 'learning_rate': 0.00019823311837315917, 'epoch': 0.11}        \n",
      "{'loss': 2.2889, 'learning_rate': 0.00019823253539971865, 'epoch': 0.11}        \n",
      "{'loss': 2.2682, 'learning_rate': 0.000198231952330977, 'epoch': 0.11}          \n",
      "{'loss': 2.0303, 'learning_rate': 0.00019823136916693475, 'epoch': 0.11}        \n",
      "{'loss': 1.9381, 'learning_rate': 0.00019823078590759248, 'epoch': 0.11}        \n",
      "{'loss': 2.3811, 'learning_rate': 0.00019823020255295073, 'epoch': 0.11}        \n",
      "{'loss': 1.807, 'learning_rate': 0.00019822961910301006, 'epoch': 0.11}         \n",
      "{'loss': 2.2584, 'learning_rate': 0.00019822903555777104, 'epoch': 0.11}        \n",
      "{'loss': 1.9084, 'learning_rate': 0.00019822845191723427, 'epoch': 0.11}        \n",
      "{'loss': 1.8432, 'learning_rate': 0.0001982278681814003, 'epoch': 0.11}         \n",
      "{'loss': 2.3903, 'learning_rate': 0.00019822728435026966, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 114974/1061708 [17:12:25<141:40:12,  1.86it/s][2024-03-01 11:19:26,070] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1364, 'learning_rate': 0.00019822675882077393, 'epoch': 0.11}        \n",
      "{'loss': 2.212, 'learning_rate': 0.00019822617480858126, 'epoch': 0.11}         \n",
      "{'loss': 2.1617, 'learning_rate': 0.00019822559070109357, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 115000/1061708 [17:12:39<139:28:22,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 11:19:39,446 >> Saving model checkpoint to output_model/checkpoint-115000\n",
      "[INFO|trainer.py:2880] 2024-03-01 11:19:39,449 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 11:19:40,662 >> tokenizer config file saved in output_model/checkpoint-115000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 11:19:40,662 >> Special tokens file saved in output_model/checkpoint-115000/special_tokens_map.json\n",
      "[2024-03-01 11:19:40,664] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step115000 is about to be saved!\n",
      "[2024-03-01 11:19:45,883] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-115000/global_step115000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 11:19:45,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-115000/global_step115000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 11:19:59,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-115000/global_step115000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 11:20:00,473] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-115000/global_step115000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 11:20:07,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-115000/global_step115000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 11:20:07,583] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-115000/global_step115000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 11:20:07,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step115000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 11:20:07,683 >> Deleting older checkpoint [output_model/checkpoint-100000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 11:20:11,477 >> tokenizer config file saved in output_model/checkpoint-115000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 11:20:11,477 >> Special tokens file saved in output_model/checkpoint-115000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.1184, 'learning_rate': 0.00019822500649831147, 'epoch': 0.11}        \n",
      "{'loss': 1.7773, 'learning_rate': 0.00019822442220023548, 'epoch': 0.11}        \n",
      "{'loss': 2.2607, 'learning_rate': 0.00019822383780686623, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 115030/1061708 [17:13:27<138:28:45,  1.90it/s][2024-03-01 11:20:27,733] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1417, 'learning_rate': 0.00019822331177135857, 'epoch': 0.11}        \n",
      "{'loss': 1.9459, 'learning_rate': 0.0001982227271969336, 'epoch': 0.11}         \n",
      "{'loss': 1.8725, 'learning_rate': 0.00019822214252721696, 'epoch': 0.11}        \n",
      "{'loss': 1.9564, 'learning_rate': 0.00019822155776220925, 'epoch': 0.11}        \n",
      "{'loss': 2.2784, 'learning_rate': 0.000198220972901911, 'epoch': 0.11}          \n",
      "{'loss': 2.1773, 'learning_rate': 0.00019822038794632284, 'epoch': 0.11}        \n",
      "{'loss': 1.9271, 'learning_rate': 0.00019821980289544527, 'epoch': 0.11}        \n",
      "{'loss': 2.2061, 'learning_rate': 0.0001982192177492789, 'epoch': 0.11}         \n",
      "{'loss': 2.0062, 'learning_rate': 0.00019821863250782425, 'epoch': 0.11}        \n",
      "{'loss': 2.3326, 'learning_rate': 0.00019821804717108193, 'epoch': 0.11}        \n",
      "{'loss': 2.1082, 'learning_rate': 0.00019821746173905253, 'epoch': 0.11}        \n",
      "{'loss': 1.9493, 'learning_rate': 0.00019821687621173655, 'epoch': 0.11}        \n",
      "{'loss': 2.265, 'learning_rate': 0.00019821629058913462, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 115169/1061708 [17:14:41<139:33:17,  1.88it/s][2024-03-01 11:21:41,760] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9525, 'learning_rate': 0.00019821576344732382, 'epoch': 0.11}        \n",
      "{'loss': 1.7775, 'learning_rate': 0.0001982151776436801, 'epoch': 0.11}         \n",
      "{'loss': 2.0661, 'learning_rate': 0.00019821459174475205, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 115198/1061708 [17:14:56<140:00:24,  1.88it/s][2024-03-01 11:21:57,166] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.307, 'learning_rate': 0.00019821406435424914, 'epoch': 0.11}         \n",
      "{'loss': 2.2986, 'learning_rate': 0.00019821347827428244, 'epoch': 0.11}        \n",
      "{'loss': 2.2082, 'learning_rate': 0.000198212892099033, 'epoch': 0.11}          \n",
      "{'loss': 1.9427, 'learning_rate': 0.00019821230582850149, 'epoch': 0.11}        \n",
      "{'loss': 2.1453, 'learning_rate': 0.00019821171946268846, 'epoch': 0.11}        \n",
      "{'loss': 2.4221, 'learning_rate': 0.00019821113300159441, 'epoch': 0.11}        \n",
      "{'loss': 2.0174, 'learning_rate': 0.00019821054644521998, 'epoch': 0.11}        \n",
      "{'loss': 2.2236, 'learning_rate': 0.00019820995979356572, 'epoch': 0.11}        \n",
      "{'loss': 2.0611, 'learning_rate': 0.00019820937304663216, 'epoch': 0.11}        \n",
      "{'loss': 2.1481, 'learning_rate': 0.00019820878620441996, 'epoch': 0.11}        \n",
      "{'loss': 2.1929, 'learning_rate': 0.00019820819926692957, 'epoch': 0.11}        \n",
      "{'loss': 1.9881, 'learning_rate': 0.00019820761223416166, 'epoch': 0.11}        \n",
      "{'loss': 1.7155, 'learning_rate': 0.00019820702510611675, 'epoch': 0.11}        \n",
      "{'loss': 2.32, 'learning_rate': 0.00019820643788279543, 'epoch': 0.11}          \n",
      " 11%|██▉                        | 115336/1061708 [17:16:10<140:11:11,  1.88it/s][2024-03-01 11:23:10,769] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1457, 'learning_rate': 0.00019820590930034535, 'epoch': 0.11}        \n",
      "{'loss': 2.1384, 'learning_rate': 0.0001982053218960004, 'epoch': 0.11}         \n",
      "{'loss': 1.996, 'learning_rate': 0.0001982047343963807, 'epoch': 0.11}          \n",
      "{'loss': 1.6038, 'learning_rate': 0.00019820414680148677, 'epoch': 0.11}        \n",
      "{'loss': 2.2378, 'learning_rate': 0.0001982035591113192, 'epoch': 0.11}         \n",
      "{'loss': 1.9798, 'learning_rate': 0.0001982029713258786, 'epoch': 0.11}         \n",
      "{'loss': 2.2321, 'learning_rate': 0.00019820238344516554, 'epoch': 0.11}        \n",
      "{'loss': 1.8582, 'learning_rate': 0.00019820179546918052, 'epoch': 0.11}        \n",
      "{'loss': 2.5214, 'learning_rate': 0.00019820120739792415, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2332, 'learning_rate': 0.00019820061923139703, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 115437/1061708 [17:17:04<139:55:58,  1.88it/s][2024-03-01 11:24:04,601] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 115438/1061708 [17:17:04<131:13:53,  2.00it/s][2024-03-01 11:24:05,025] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8777, 'learning_rate': 0.00019820014862958075, 'epoch': 0.11}        \n",
      "{'loss': 2.107, 'learning_rate': 0.00019819956029156768, 'epoch': 0.11}         \n",
      "{'loss': 1.7818, 'learning_rate': 0.00019819897185828543, 'epoch': 0.11}        \n",
      "{'loss': 2.2066, 'learning_rate': 0.00019819838332973456, 'epoch': 0.11}        \n",
      "{'loss': 1.9853, 'learning_rate': 0.00019819779470591563, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 115487/1061708 [17:17:30<140:01:07,  1.88it/s][2024-03-01 11:24:31,099] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3266, 'learning_rate': 0.00019819726486302496, 'epoch': 0.11}        \n",
      "{'loss': 1.8195, 'learning_rate': 0.00019819667605819834, 'epoch': 0.11}        \n",
      "{'loss': 2.2419, 'learning_rate': 0.00019819608715810537, 'epoch': 0.11}        \n",
      "{'loss': 1.5506, 'learning_rate': 0.00019819549816274656, 'epoch': 0.11}        \n",
      "{'loss': 2.0428, 'learning_rate': 0.00019819490907212254, 'epoch': 0.11}        \n",
      "{'loss': 2.1811, 'learning_rate': 0.00019819431988623386, 'epoch': 0.11}        \n",
      "{'loss': 1.7479, 'learning_rate': 0.0001981937306050811, 'epoch': 0.11}         \n",
      "{'loss': 2.1242, 'learning_rate': 0.0001981931412286648, 'epoch': 0.11}         \n",
      "{'loss': 1.9878, 'learning_rate': 0.00019819255175698554, 'epoch': 0.11}        \n",
      "{'loss': 2.1455, 'learning_rate': 0.0001981919621900439, 'epoch': 0.11}         \n",
      "{'loss': 2.1952, 'learning_rate': 0.00019819137252784046, 'epoch': 0.11}        \n",
      "{'loss': 1.8708, 'learning_rate': 0.0001981907827703758, 'epoch': 0.11}         \n",
      "{'loss': 1.7467, 'learning_rate': 0.00019819019291765047, 'epoch': 0.11}        \n",
      "{'loss': 1.8768, 'learning_rate': 0.00019818960296966504, 'epoch': 0.11}        \n",
      "{'loss': 2.2746, 'learning_rate': 0.00019818901292642012, 'epoch': 0.11}        \n",
      "{'loss': 2.3434, 'learning_rate': 0.00019818842278791622, 'epoch': 0.11}        \n",
      "{'loss': 1.7027, 'learning_rate': 0.00019818783255415396, 'epoch': 0.11}        \n",
      "{'loss': 1.9126, 'learning_rate': 0.00019818724222513392, 'epoch': 0.11}        \n",
      "{'loss': 1.8618, 'learning_rate': 0.00019818665180085666, 'epoch': 0.11}        \n",
      "{'loss': 1.9523, 'learning_rate': 0.0001981860612813227, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 115688/1061708 [17:19:17<139:38:52,  1.88it/s][2024-03-01 11:26:18,319] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 115689/1061708 [17:19:18<131:05:58,  2.00it/s][2024-03-01 11:26:18,744] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3106, 'learning_rate': 0.00019818558879711117, 'epoch': 0.11}        \n",
      "{'loss': 2.0444, 'learning_rate': 0.00019818499810611668, 'epoch': 0.11}        \n",
      "{'loss': 1.9619, 'learning_rate': 0.00019818440731986716, 'epoch': 0.11}        \n",
      "{'loss': 1.911, 'learning_rate': 0.00019818381643836317, 'epoch': 0.11}         \n",
      "{'loss': 2.243, 'learning_rate': 0.00019818322546160525, 'epoch': 0.11}         \n",
      "{'loss': 2.2218, 'learning_rate': 0.00019818263438959406, 'epoch': 0.11}        \n",
      "{'loss': 1.8626, 'learning_rate': 0.00019818204322233007, 'epoch': 0.11}        \n",
      "{'loss': 1.9656, 'learning_rate': 0.00019818145195981389, 'epoch': 0.11}        \n",
      "{'loss': 2.0616, 'learning_rate': 0.0001981808606020461, 'epoch': 0.11}         \n",
      "{'loss': 2.1614, 'learning_rate': 0.0001981802691490273, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 115788/1061708 [17:20:11<139:49:27,  1.88it/s][2024-03-01 11:27:11,550] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8547, 'learning_rate': 0.0001981797367598712, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 115796/1061708 [17:20:15<139:44:22,  1.88it/s][2024-03-01 11:27:15,732] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9732, 'learning_rate': 0.00019817920429356266, 'epoch': 0.11}        \n",
      "{'loss': 1.6745, 'learning_rate': 0.000198178612573844, 'epoch': 0.11}          \n",
      "{'loss': 2.1525, 'learning_rate': 0.00019817802075887647, 'epoch': 0.11}        \n",
      "{'loss': 2.3135, 'learning_rate': 0.0001981774288486607, 'epoch': 0.11}         \n",
      "{'loss': 2.1542, 'learning_rate': 0.0001981768368431972, 'epoch': 0.11}         \n",
      "{'loss': 2.0395, 'learning_rate': 0.00019817624474248655, 'epoch': 0.11}        \n",
      "{'loss': 2.3066, 'learning_rate': 0.00019817565254652938, 'epoch': 0.11}        \n",
      "{'loss': 2.2725, 'learning_rate': 0.0001981750602553262, 'epoch': 0.11}         \n",
      "{'loss': 1.8491, 'learning_rate': 0.0001981744678688776, 'epoch': 0.11}         \n",
      "{'loss': 2.0451, 'learning_rate': 0.00019817387538718418, 'epoch': 0.11}        \n",
      "{'loss': 2.2858, 'learning_rate': 0.00019817328281024652, 'epoch': 0.11}        \n",
      "{'loss': 2.3722, 'learning_rate': 0.00019817269013806517, 'epoch': 0.11}        \n",
      "{'loss': 2.1809, 'learning_rate': 0.0001981720973706407, 'epoch': 0.11}         \n",
      "{'loss': 2.1131, 'learning_rate': 0.00019817150450797367, 'epoch': 0.11}        \n",
      "{'loss': 1.9257, 'learning_rate': 0.00019817091155006474, 'epoch': 0.11}        \n",
      "{'loss': 2.113, 'learning_rate': 0.0001981703184969144, 'epoch': 0.11}          \n",
      "{'loss': 1.9592, 'learning_rate': 0.00019816972534852326, 'epoch': 0.11}        \n",
      "{'loss': 2.1295, 'learning_rate': 0.00019816913210489185, 'epoch': 0.11}        \n",
      "{'loss': 1.8943, 'learning_rate': 0.00019816853876602081, 'epoch': 0.11}        \n",
      "{'loss': 1.7787, 'learning_rate': 0.0001981679453319107, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 115999/1061708 [17:22:03<140:56:38,  1.86it/s][2024-03-01 11:29:04,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=116000, skipped=1354, lr=[0.00019816735180256206], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 11:29:04,181] [INFO] [timer.py:260:stop] epoch=0/micro_step=116000/global_step=116000, RunningAvgSamplesPerSec=1.8926684103627254, CurrSamplesPerSec=1.9028949672665902, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.8731, 'learning_rate': 0.00019816735180256206, 'epoch': 0.11}        \n",
      "{'loss': 1.8927, 'learning_rate': 0.0001981667581779755, 'epoch': 0.11}         \n",
      "{'loss': 2.3747, 'learning_rate': 0.00019816616445815158, 'epoch': 0.11}        \n",
      "{'loss': 2.1228, 'learning_rate': 0.0001981655706430909, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 116034/1061708 [17:22:22<141:20:23,  1.86it/s][2024-03-01 11:29:22,749] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0691, 'learning_rate': 0.0001981650361281093, 'epoch': 0.11}         \n",
      "{'loss': 1.9106, 'learning_rate': 0.00019816444213210032, 'epoch': 0.11}        \n",
      "{'loss': 2.0848, 'learning_rate': 0.00019816384804085621, 'epoch': 0.11}        \n",
      "{'loss': 2.0762, 'learning_rate': 0.00019816325385437763, 'epoch': 0.11}        \n",
      "{'loss': 1.909, 'learning_rate': 0.00019816265957266503, 'epoch': 0.11}         \n",
      "{'loss': 2.257, 'learning_rate': 0.0001981620651957191, 'epoch': 0.11}          \n",
      "{'loss': 2.092, 'learning_rate': 0.0001981614707235403, 'epoch': 0.11}          \n",
      "{'loss': 1.9491, 'learning_rate': 0.00019816087615612935, 'epoch': 0.11}        \n",
      "{'loss': 2.0743, 'learning_rate': 0.00019816028149348674, 'epoch': 0.11}        \n",
      "{'loss': 1.7104, 'learning_rate': 0.00019815968673561304, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|██▉                        | 116135/1061708 [17:23:15<141:04:56,  1.86it/s][2024-03-01 11:30:16,471] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 116136/1061708 [17:23:16<131:56:14,  1.99it/s][2024-03-01 11:30:16,893] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4348, 'learning_rate': 0.00019815921086074808, 'epoch': 0.11}        \n",
      "{'loss': 1.9191, 'learning_rate': 0.00019815861593145992, 'epoch': 0.11}        \n",
      "{'loss': 2.0211, 'learning_rate': 0.00019815802090694228, 'epoch': 0.11}        \n",
      "{'loss': 2.0656, 'learning_rate': 0.00019815742578719578, 'epoch': 0.11}        \n",
      "{'loss': 2.0204, 'learning_rate': 0.00019815683057222098, 'epoch': 0.11}        \n",
      "{'loss': 2.6153, 'learning_rate': 0.00019815623526201845, 'epoch': 0.11}        \n",
      "{'loss': 2.0413, 'learning_rate': 0.0001981556398565888, 'epoch': 0.11}         \n",
      "{'loss': 2.2547, 'learning_rate': 0.00019815504435593257, 'epoch': 0.11}        \n",
      "{'loss': 1.9234, 'learning_rate': 0.00019815444876005033, 'epoch': 0.11}        \n",
      "{'loss': 1.4496, 'learning_rate': 0.0001981538530689427, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 116237/1061708 [17:24:10<139:28:17,  1.88it/s][2024-03-01 11:31:10,636] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 116238/1061708 [17:24:10<130:48:00,  2.01it/s][2024-03-01 11:31:11,058] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2626, 'learning_rate': 0.0001981533764474947, 'epoch': 0.11}         \n",
      "{'loss': 2.2143, 'learning_rate': 0.00019815278058498277, 'epoch': 0.11}        \n",
      "{'loss': 1.915, 'learning_rate': 0.00019815218462724703, 'epoch': 0.11}         \n",
      "{'loss': 1.7182, 'learning_rate': 0.00019815158857428808, 'epoch': 0.11}        \n",
      "{'loss': 1.8762, 'learning_rate': 0.00019815099242610654, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116283/1061708 [17:24:34<142:13:36,  1.85it/s][2024-03-01 11:31:34,957] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8859, 'learning_rate': 0.00019815045581132826, 'epoch': 0.11}        \n",
      "{'loss': 2.3296, 'learning_rate': 0.00019814985948222528, 'epoch': 0.11}        \n",
      "{'loss': 1.9344, 'learning_rate': 0.00019814926305790128, 'epoch': 0.11}        \n",
      "{'loss': 2.3889, 'learning_rate': 0.00019814866653835697, 'epoch': 0.11}        \n",
      "{'loss': 1.9954, 'learning_rate': 0.00019814806992359284, 'epoch': 0.11}        \n",
      "{'loss': 2.0539, 'learning_rate': 0.0001981474732136095, 'epoch': 0.11}         \n",
      "{'loss': 1.8945, 'learning_rate': 0.00019814687640840747, 'epoch': 0.11}        \n",
      "{'loss': 2.1672, 'learning_rate': 0.00019814627950798743, 'epoch': 0.11}        \n",
      "{'loss': 1.8179, 'learning_rate': 0.00019814568251234985, 'epoch': 0.11}        \n",
      "{'loss': 2.5324, 'learning_rate': 0.00019814508542149542, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116387/1061708 [17:25:29<139:35:43,  1.88it/s][2024-03-01 11:32:30,287] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2584, 'learning_rate': 0.00019814454795831642, 'epoch': 0.11}        \n",
      "{'loss': 2.359, 'learning_rate': 0.00019814395068655144, 'epoch': 0.11}         \n",
      "{'loss': 2.1142, 'learning_rate': 0.00019814335331957124, 'epoch': 0.11}        \n",
      "{'loss': 2.0375, 'learning_rate': 0.0001981427558573764, 'epoch': 0.11}         \n",
      "{'loss': 2.2871, 'learning_rate': 0.00019814215829996746, 'epoch': 0.11}        \n",
      "{'loss': 2.355, 'learning_rate': 0.00019814156064734505, 'epoch': 0.11}         \n",
      "{'loss': 1.9921, 'learning_rate': 0.0001981409628995097, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 116456/1061708 [17:26:06<139:54:23,  1.88it/s][2024-03-01 11:33:06,988] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8887, 'learning_rate': 0.00019814042484505136, 'epoch': 0.11}        \n",
      "{'loss': 1.8442, 'learning_rate': 0.0001981398269163131, 'epoch': 0.11}         \n",
      "{'loss': 1.8597, 'learning_rate': 0.00019813922889236362, 'epoch': 0.11}        \n",
      "{'loss': 2.4768, 'learning_rate': 0.00019813863077320346, 'epoch': 0.11}        \n",
      "{'loss': 2.1847, 'learning_rate': 0.0001981380325588333, 'epoch': 0.11}         \n",
      "{'loss': 1.9394, 'learning_rate': 0.0001981374342492536, 'epoch': 0.11}         \n",
      "{'loss': 1.9176, 'learning_rate': 0.00019813683584446503, 'epoch': 0.11}        \n",
      "{'loss': 1.9023, 'learning_rate': 0.00019813623734446812, 'epoch': 0.11}        \n",
      "{'loss': 2.3512, 'learning_rate': 0.0001981356387492635, 'epoch': 0.11}         \n",
      "{'loss': 2.3961, 'learning_rate': 0.00019813504005885166, 'epoch': 0.11}        \n",
      "{'loss': 2.1225, 'learning_rate': 0.00019813444127323327, 'epoch': 0.11}        \n",
      "{'loss': 2.3076, 'learning_rate': 0.00019813384239240886, 'epoch': 0.11}        \n",
      "{'loss': 2.2938, 'learning_rate': 0.00019813324341637904, 'epoch': 0.11}        \n",
      "{'loss': 2.4555, 'learning_rate': 0.00019813264434514439, 'epoch': 0.11}        \n",
      "{'loss': 1.7567, 'learning_rate': 0.00019813204517870548, 'epoch': 0.11}        \n",
      "{'loss': 2.2465, 'learning_rate': 0.00019813144591706284, 'epoch': 0.11}        \n",
      "{'loss': 2.2297, 'learning_rate': 0.0001981308465602172, 'epoch': 0.11}         \n",
      "{'loss': 2.1953, 'learning_rate': 0.00019813024710816897, 'epoch': 0.11}        \n",
      "{'loss': 1.752, 'learning_rate': 0.00019812964756091883, 'epoch': 0.11}         \n",
      "{'loss': 2.2439, 'learning_rate': 0.00019812904791846733, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116657/1061708 [17:27:53<140:30:02,  1.87it/s][2024-03-01 11:34:54,007] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 116658/1061708 [17:27:53<131:32:41,  2.00it/s][2024-03-01 11:34:54,430] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1166, 'learning_rate': 0.00019812856813596156, 'epoch': 0.11}        \n",
      "{'loss': 1.9307, 'learning_rate': 0.00019812796832214912, 'epoch': 0.11}        \n",
      "{'loss': 2.2162, 'learning_rate': 0.00019812736841313695, 'epoch': 0.11}        \n",
      "{'loss': 2.1231, 'learning_rate': 0.00019812676840892562, 'epoch': 0.11}        \n",
      "{'loss': 2.1543, 'learning_rate': 0.00019812616830951576, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116704/1061708 [17:28:18<141:25:25,  1.86it/s][2024-03-01 11:35:18,876] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0196, 'learning_rate': 0.0001981256281386526, 'epoch': 0.11}         \n",
      "{'loss': 2.0832, 'learning_rate': 0.00019812502785836707, 'epoch': 0.11}        \n",
      "{'loss': 2.1511, 'learning_rate': 0.00019812442748288466, 'epoch': 0.11}        \n",
      "{'loss': 2.2343, 'learning_rate': 0.00019812382701220598, 'epoch': 0.11}        \n",
      "{'loss': 1.967, 'learning_rate': 0.00019812322644633158, 'epoch': 0.11}         \n",
      "{'loss': 1.46, 'learning_rate': 0.00019812262578526208, 'epoch': 0.11}          \n",
      "{'loss': 1.8853, 'learning_rate': 0.00019812202502899804, 'epoch': 0.11}        \n",
      "{'loss': 2.3065, 'learning_rate': 0.00019812142417754004, 'epoch': 0.11}        \n",
      "{'loss': 2.0372, 'learning_rate': 0.00019812082323088866, 'epoch': 0.11}        \n",
      "{'loss': 1.997, 'learning_rate': 0.0001981202221890445, 'epoch': 0.11}          \n",
      "{'loss': 2.0309, 'learning_rate': 0.00019811962105200812, 'epoch': 0.11}        \n",
      "{'loss': 2.0306, 'learning_rate': 0.0001981190198197801, 'epoch': 0.11}         \n",
      "{'loss': 2.1527, 'learning_rate': 0.00019811841849236107, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9679, 'learning_rate': 0.00019811781706975158, 'epoch': 0.11}        \n",
      "{'loss': 2.0118, 'learning_rate': 0.00019811721555195222, 'epoch': 0.11}        \n",
      "{'loss': 2.2931, 'learning_rate': 0.00019811661393896358, 'epoch': 0.11}        \n",
      "{'loss': 1.8831, 'learning_rate': 0.0001981160122307862, 'epoch': 0.11}         \n",
      "{'loss': 2.2156, 'learning_rate': 0.0001981154104274207, 'epoch': 0.11}         \n",
      "{'loss': 2.0022, 'learning_rate': 0.00019811480852886768, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116894/1061708 [17:29:59<140:59:12,  1.86it/s][2024-03-01 11:37:00,097] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0005, 'learning_rate': 0.0001981142667387851, 'epoch': 0.11}         \n",
      "{'loss': 2.3451, 'learning_rate': 0.00019811366465937737, 'epoch': 0.11}        \n",
      "{'loss': 2.1549, 'learning_rate': 0.0001981130624847838, 'epoch': 0.11}         \n",
      "{'loss': 2.05, 'learning_rate': 0.00019811246021500493, 'epoch': 0.11}          \n",
      "{'loss': 2.2897, 'learning_rate': 0.00019811185785004144, 'epoch': 0.11}        \n",
      "{'loss': 2.0928, 'learning_rate': 0.00019811125538989378, 'epoch': 0.11}        \n",
      "{'loss': 2.3434, 'learning_rate': 0.00019811065283456268, 'epoch': 0.11}        \n",
      "{'loss': 1.9492, 'learning_rate': 0.00019811005018404863, 'epoch': 0.11}        \n",
      "{'loss': 1.9964, 'learning_rate': 0.00019810944743835223, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 116988/1061708 [17:30:49<139:18:07,  1.88it/s][2024-03-01 11:37:50,117] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1223, 'learning_rate': 0.0001981089048858451, 'epoch': 0.11}         \n",
      "{'loss': 2.1249, 'learning_rate': 0.00019810830195930387, 'epoch': 0.11}        \n",
      "{'loss': 1.8926, 'learning_rate': 0.00019810769893758204, 'epoch': 0.11}        \n",
      "{'loss': 2.2018, 'learning_rate': 0.00019810709582068013, 'epoch': 0.11}        \n",
      "{'loss': 2.3258, 'learning_rate': 0.00019810649260859875, 'epoch': 0.11}        \n",
      "{'loss': 2.1328, 'learning_rate': 0.00019810588930133848, 'epoch': 0.11}        \n",
      "{'loss': 2.2384, 'learning_rate': 0.0001981052858988999, 'epoch': 0.11}         \n",
      "{'loss': 2.2377, 'learning_rate': 0.0001981046824012836, 'epoch': 0.11}         \n",
      "{'loss': 2.2338, 'learning_rate': 0.00019810407880849017, 'epoch': 0.11}        \n",
      "{'loss': 2.1579, 'learning_rate': 0.00019810347512052022, 'epoch': 0.11}        \n",
      "{'loss': 2.3392, 'learning_rate': 0.0001981028713373743, 'epoch': 0.11}         \n",
      "{'loss': 2.4972, 'learning_rate': 0.000198102267459053, 'epoch': 0.11}          \n",
      "{'loss': 2.4449, 'learning_rate': 0.0001981016634855569, 'epoch': 0.11}         \n",
      "{'loss': 2.2742, 'learning_rate': 0.00019810105941688662, 'epoch': 0.11}        \n",
      "{'loss': 1.8022, 'learning_rate': 0.00019810045525304272, 'epoch': 0.11}        \n",
      "{'loss': 2.5084, 'learning_rate': 0.00019809985099402576, 'epoch': 0.11}        \n",
      "{'loss': 2.1324, 'learning_rate': 0.0001980992466398364, 'epoch': 0.11}         \n",
      "{'loss': 2.0891, 'learning_rate': 0.00019809864219047518, 'epoch': 0.11}        \n",
      "{'loss': 1.8546, 'learning_rate': 0.00019809803764594266, 'epoch': 0.11}        \n",
      "{'loss': 1.8047, 'learning_rate': 0.00019809743300623945, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117189/1061708 [17:32:36<138:56:06,  1.89it/s][2024-03-01 11:39:37,120] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3429, 'learning_rate': 0.00019809688874913614, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117190/1061708 [17:32:37<130:25:02,  2.01it/s][2024-03-01 11:39:37,542] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.986, 'learning_rate': 0.00019809634441494547, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 117206/1061708 [17:32:45<139:54:39,  1.88it/s][2024-03-01 11:39:45,987] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0273, 'learning_rate': 0.0001980958000036679, 'epoch': 0.11}         \n",
      "{'loss': 2.6051, 'learning_rate': 0.00019809519501183827, 'epoch': 0.11}        \n",
      "{'loss': 2.1297, 'learning_rate': 0.00019809458992484073, 'epoch': 0.11}        \n",
      "{'loss': 1.8639, 'learning_rate': 0.00019809398474267585, 'epoch': 0.11}        \n",
      "{'loss': 2.0436, 'learning_rate': 0.0001980933794653442, 'epoch': 0.11}         \n",
      "{'loss': 2.2454, 'learning_rate': 0.00019809277409284638, 'epoch': 0.11}        \n",
      "{'loss': 2.2943, 'learning_rate': 0.00019809216862518302, 'epoch': 0.11}        \n",
      "{'loss': 2.3585, 'learning_rate': 0.00019809156306235465, 'epoch': 0.11}        \n",
      "{'loss': 1.9785, 'learning_rate': 0.00019809095740436187, 'epoch': 0.11}        \n",
      "{'loss': 1.932, 'learning_rate': 0.00019809035165120528, 'epoch': 0.11}         \n",
      "{'loss': 2.0277, 'learning_rate': 0.00019808974580288547, 'epoch': 0.11}        \n",
      "{'loss': 2.1261, 'learning_rate': 0.000198089139859403, 'epoch': 0.11}          \n",
      "{'loss': 2.4066, 'learning_rate': 0.0001980885338207585, 'epoch': 0.11}         \n",
      "{'loss': 2.2372, 'learning_rate': 0.00019808792768695251, 'epoch': 0.11}        \n",
      "{'loss': 2.0014, 'learning_rate': 0.00019808732145798564, 'epoch': 0.11}        \n",
      "{'loss': 2.0325, 'learning_rate': 0.00019808671513385853, 'epoch': 0.11}        \n",
      "{'loss': 2.2763, 'learning_rate': 0.0001980861087145717, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 117374/1061708 [17:34:14<141:24:06,  1.86it/s][2024-03-01 11:41:15,498] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8783, 'learning_rate': 0.0001980855628558525, 'epoch': 0.11}         \n",
      "{'loss': 1.9337, 'learning_rate': 0.00019808495625576385, 'epoch': 0.11}        \n",
      "{'loss': 1.9686, 'learning_rate': 0.0001980843495605172, 'epoch': 0.11}         \n",
      "{'loss': 2.0197, 'learning_rate': 0.00019808374277011318, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117415/1061708 [17:34:36<140:10:28,  1.87it/s][2024-03-01 11:41:37,287] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0087, 'learning_rate': 0.00019808319657739043, 'epoch': 0.11}        \n",
      "{'loss': 1.9804, 'learning_rate': 0.00019808258960618896, 'epoch': 0.11}        \n",
      "{'loss': 2.2016, 'learning_rate': 0.00019808198253983177, 'epoch': 0.11}        \n",
      "{'loss': 1.7476, 'learning_rate': 0.00019808137537831946, 'epoch': 0.11}        \n",
      "{'loss': 2.0317, 'learning_rate': 0.00019808076812165263, 'epoch': 0.11}        \n",
      "{'loss': 1.4966, 'learning_rate': 0.00019808016076983188, 'epoch': 0.11}        \n",
      "{'loss': 2.0604, 'learning_rate': 0.00019807955332285781, 'epoch': 0.11}        \n",
      "{'loss': 1.9896, 'learning_rate': 0.00019807894578073096, 'epoch': 0.11}        \n",
      "{'loss': 2.1749, 'learning_rate': 0.00019807833814345198, 'epoch': 0.11}        \n",
      "{'loss': 2.2936, 'learning_rate': 0.00019807773041102141, 'epoch': 0.11}        \n",
      "{'loss': 2.4414, 'learning_rate': 0.00019807712258343988, 'epoch': 0.11}        \n",
      "{'loss': 2.0438, 'learning_rate': 0.00019807651466070792, 'epoch': 0.11}        \n",
      "{'loss': 1.7691, 'learning_rate': 0.00019807590664282617, 'epoch': 0.11}        \n",
      "{'loss': 1.9507, 'learning_rate': 0.0001980752985297952, 'epoch': 0.11}         \n",
      "{'loss': 1.9388, 'learning_rate': 0.00019807469032161561, 'epoch': 0.11}        \n",
      "{'loss': 1.8276, 'learning_rate': 0.00019807408201828798, 'epoch': 0.11}        \n",
      "{'loss': 2.0986, 'learning_rate': 0.0001980734736198129, 'epoch': 0.11}         \n",
      "{'loss': 1.7548, 'learning_rate': 0.000198072865126191, 'epoch': 0.11}          \n",
      "{'loss': 2.3137, 'learning_rate': 0.0001980722565374228, 'epoch': 0.11}         \n",
      "{'loss': 1.9142, 'learning_rate': 0.00019807164785350895, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117616/1061708 [17:36:23<139:53:48,  1.87it/s][2024-03-01 11:43:24,360] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|██▉                        | 117617/1061708 [17:36:24<131:05:47,  2.00it/s][2024-03-01 11:43:24,782] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0205, 'learning_rate': 0.0001980711608378734, 'epoch': 0.11}         \n",
      "{'loss': 2.1021, 'learning_rate': 0.0001980705519826988, 'epoch': 0.11}         \n",
      "{'loss': 2.4347, 'learning_rate': 0.00019806994303238023, 'epoch': 0.11}        \n",
      "{'loss': 2.1346, 'learning_rate': 0.0001980693339869182, 'epoch': 0.11}         \n",
      "{'loss': 2.2935, 'learning_rate': 0.00019806872484631334, 'epoch': 0.11}        \n",
      "{'loss': 2.0996, 'learning_rate': 0.00019806811561056623, 'epoch': 0.11}        \n",
      "{'loss': 2.1124, 'learning_rate': 0.00019806750627967748, 'epoch': 0.11}        \n",
      "{'loss': 1.9999, 'learning_rate': 0.00019806689685364766, 'epoch': 0.11}        \n",
      "{'loss': 2.1255, 'learning_rate': 0.00019806628733247736, 'epoch': 0.11}        \n",
      "{'loss': 2.1412, 'learning_rate': 0.00019806567771616718, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117716/1061708 [17:37:16<139:44:35,  1.88it/s][2024-03-01 11:44:17,515] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2549, 'learning_rate': 0.00019806512898014388, 'epoch': 0.11}        \n",
      "{'loss': 2.1422, 'learning_rate': 0.00019806451918306958, 'epoch': 0.11}        \n",
      "{'loss': 1.7964, 'learning_rate': 0.00019806390929085708, 'epoch': 0.11}        \n",
      "{'loss': 1.9775, 'learning_rate': 0.00019806329930350703, 'epoch': 0.11}        \n",
      "{'loss': 2.014, 'learning_rate': 0.00019806268922101995, 'epoch': 0.11}         \n",
      "{'loss': 2.1632, 'learning_rate': 0.00019806207904339652, 'epoch': 0.11}        \n",
      "{'loss': 2.0875, 'learning_rate': 0.0001980614687706373, 'epoch': 0.11}         \n",
      "{'loss': 2.2246, 'learning_rate': 0.00019806085840274283, 'epoch': 0.11}        \n",
      "{'loss': 2.1786, 'learning_rate': 0.00019806024793971377, 'epoch': 0.11}        \n",
      "{'loss': 2.0121, 'learning_rate': 0.00019805963738155072, 'epoch': 0.11}        \n",
      "{'loss': 2.0418, 'learning_rate': 0.0001980590267282542, 'epoch': 0.11}         \n",
      "{'loss': 2.1298, 'learning_rate': 0.00019805841597982484, 'epoch': 0.11}        \n",
      "{'loss': 2.1046, 'learning_rate': 0.00019805780513626326, 'epoch': 0.11}        \n",
      "{'loss': 2.2512, 'learning_rate': 0.00019805719419757002, 'epoch': 0.11}        \n",
      "{'loss': 2.0913, 'learning_rate': 0.0001980565831637457, 'epoch': 0.11}         \n",
      " 11%|██▉                        | 117860/1061708 [17:38:33<139:10:16,  1.88it/s][2024-03-01 11:45:34,350] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2492, 'learning_rate': 0.00019805603315196728, 'epoch': 0.11}        \n",
      "{'loss': 2.0149, 'learning_rate': 0.0001980554219373956, 'epoch': 0.11}         \n",
      "{'loss': 2.217, 'learning_rate': 0.00019805481062769458, 'epoch': 0.11}         \n",
      "{'loss': 2.2884, 'learning_rate': 0.0001980541992228648, 'epoch': 0.11}         \n",
      "{'loss': 2.5354, 'learning_rate': 0.00019805358772290686, 'epoch': 0.11}        \n",
      "{'loss': 2.1435, 'learning_rate': 0.00019805297612782136, 'epoch': 0.11}        \n",
      "{'loss': 1.9064, 'learning_rate': 0.00019805236443760894, 'epoch': 0.11}        \n",
      "{'loss': 2.3263, 'learning_rate': 0.0001980517526522701, 'epoch': 0.11}         \n",
      "{'loss': 2.375, 'learning_rate': 0.00019805114077180549, 'epoch': 0.11}         \n",
      "{'loss': 2.3071, 'learning_rate': 0.00019805052879621568, 'epoch': 0.11}        \n",
      " 11%|██▉                        | 117961/1061708 [17:39:27<138:58:20,  1.89it/s][2024-03-01 11:46:28,194] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|██▉                        | 117962/1061708 [17:39:28<130:28:07,  2.01it/s][2024-03-01 11:46:28,618] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.111, 'learning_rate': 0.00019805003914725414, 'epoch': 0.11}         \n",
      "{'loss': 2.2164, 'learning_rate': 0.00019804942700044049, 'epoch': 0.11}        \n",
      "{'loss': 2.0683, 'learning_rate': 0.00019804881475850335, 'epoch': 0.11}        \n",
      " 11%|███                        | 117999/1061708 [17:39:47<139:02:57,  1.89it/s][2024-03-01 11:46:48,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=118000, skipped=1378, lr=[0.00019804820242144325], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 11:46:48,377] [INFO] [timer.py:260:stop] epoch=0/micro_step=118000/global_step=118000, RunningAvgSamplesPerSec=1.8927010322027629, CurrSamplesPerSec=1.8957631814693314, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.714, 'learning_rate': 0.00019804820242144325, 'epoch': 0.11}         \n",
      "{'loss': 2.067, 'learning_rate': 0.00019804758998926084, 'epoch': 0.11}         \n",
      "{'loss': 2.4537, 'learning_rate': 0.00019804697746195665, 'epoch': 0.11}        \n",
      "{'loss': 2.4153, 'learning_rate': 0.00019804636483953134, 'epoch': 0.11}        \n",
      "{'loss': 2.373, 'learning_rate': 0.00019804575212198545, 'epoch': 0.11}         \n",
      "{'loss': 2.0176, 'learning_rate': 0.00019804513930931963, 'epoch': 0.11}        \n",
      " 11%|███                        | 118051/1061708 [17:40:15<139:01:53,  1.89it/s][2024-03-01 11:47:16,062] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1718, 'learning_rate': 0.00019804458769659333, 'epoch': 0.11}        \n",
      "{'loss': 1.6424, 'learning_rate': 0.0001980439747032012, 'epoch': 0.11}         \n",
      "{'loss': 1.9303, 'learning_rate': 0.00019804336161469087, 'epoch': 0.11}        \n",
      "{'loss': 2.0944, 'learning_rate': 0.00019804274843106286, 'epoch': 0.11}        \n",
      "{'loss': 1.9679, 'learning_rate': 0.00019804213515231783, 'epoch': 0.11}        \n",
      "{'loss': 2.0922, 'learning_rate': 0.00019804152177845636, 'epoch': 0.11}        \n",
      "{'loss': 2.2362, 'learning_rate': 0.000198040908309479, 'epoch': 0.11}          \n",
      "{'loss': 1.9659, 'learning_rate': 0.0001980402947453864, 'epoch': 0.11}         \n",
      "{'loss': 2.0726, 'learning_rate': 0.00019803968108617916, 'epoch': 0.11}        \n",
      "{'loss': 2.0086, 'learning_rate': 0.00019803906733185784, 'epoch': 0.11}        \n",
      "{'loss': 2.0717, 'learning_rate': 0.00019803845348242302, 'epoch': 0.11}        \n",
      "{'loss': 2.0713, 'learning_rate': 0.00019803783953787536, 'epoch': 0.11}        \n",
      "{'loss': 1.7841, 'learning_rate': 0.0001980372254982154, 'epoch': 0.11}         \n",
      "{'loss': 1.9229, 'learning_rate': 0.00019803661136344374, 'epoch': 0.11}        \n",
      "{'loss': 1.8897, 'learning_rate': 0.000198035997133561, 'epoch': 0.11}          \n",
      " 11%|███                        | 118200/1061708 [17:41:34<138:59:29,  1.89it/s][2024-03-01 11:48:35,497] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9334, 'learning_rate': 0.00019803544424534705, 'epoch': 0.11}        \n",
      "{'loss': 1.8164, 'learning_rate': 0.00019803482983475488, 'epoch': 0.11}        \n",
      "{'loss': 1.7926, 'learning_rate': 0.00019803421532905334, 'epoch': 0.11}        \n",
      "{'loss': 2.1982, 'learning_rate': 0.00019803360072824302, 'epoch': 0.11}        \n",
      "{'loss': 2.1382, 'learning_rate': 0.00019803298603232456, 'epoch': 0.11}        \n",
      "{'loss': 1.6374, 'learning_rate': 0.00019803237124129853, 'epoch': 0.11}        \n",
      "{'loss': 2.0438, 'learning_rate': 0.0001980317563551655, 'epoch': 0.11}         \n",
      " 11%|███                        | 118274/1061708 [17:42:14<141:17:25,  1.85it/s][2024-03-01 11:49:14,946] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2089, 'learning_rate': 0.00019803120287632982, 'epoch': 0.11}        \n",
      "{'loss': 2.0699, 'learning_rate': 0.00019803058780949514, 'epoch': 0.11}        \n",
      "{'loss': 1.9192, 'learning_rate': 0.00019802997264755528, 'epoch': 0.11}        \n",
      "{'loss': 1.8833, 'learning_rate': 0.00019802935739051075, 'epoch': 0.11}        \n",
      "{'loss': 1.8737, 'learning_rate': 0.00019802874203836215, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|███                        | 118325/1061708 [17:42:41<141:19:50,  1.85it/s][2024-03-01 11:49:42,104] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9323, 'learning_rate': 0.00019802818814011497, 'epoch': 0.11}        \n",
      "{'loss': 2.2552, 'learning_rate': 0.0001980275726072703, 'epoch': 0.11}         \n",
      "{'loss': 2.0729, 'learning_rate': 0.00019802695697932333, 'epoch': 0.11}        \n",
      "{'loss': 2.2256, 'learning_rate': 0.00019802634125627467, 'epoch': 0.11}        \n",
      "{'loss': 2.2012, 'learning_rate': 0.00019802572543812483, 'epoch': 0.11}        \n",
      "{'loss': 1.8943, 'learning_rate': 0.0001980251095248745, 'epoch': 0.11}         \n",
      "{'loss': 2.0638, 'learning_rate': 0.0001980244935165242, 'epoch': 0.11}         \n",
      "{'loss': 2.2349, 'learning_rate': 0.00019802387741307457, 'epoch': 0.11}        \n",
      "{'loss': 2.1079, 'learning_rate': 0.00019802326121452624, 'epoch': 0.11}        \n",
      "{'loss': 1.8339, 'learning_rate': 0.00019802264492087973, 'epoch': 0.11}        \n",
      "{'loss': 1.8753, 'learning_rate': 0.00019802202853213573, 'epoch': 0.11}        \n",
      "{'loss': 2.1718, 'learning_rate': 0.00019802141204829473, 'epoch': 0.11}        \n",
      "{'loss': 1.7855, 'learning_rate': 0.0001980207954693574, 'epoch': 0.11}         \n",
      "{'loss': 2.2233, 'learning_rate': 0.00019802017879532436, 'epoch': 0.11}        \n",
      "{'loss': 2.0496, 'learning_rate': 0.00019801956202619613, 'epoch': 0.11}        \n",
      "{'loss': 2.1599, 'learning_rate': 0.00019801894516197337, 'epoch': 0.11}        \n",
      "{'loss': 2.1215, 'learning_rate': 0.00019801832820265665, 'epoch': 0.11}        \n",
      "{'loss': 2.1068, 'learning_rate': 0.00019801771114824658, 'epoch': 0.11}        \n",
      "{'loss': 2.0977, 'learning_rate': 0.00019801709399874375, 'epoch': 0.11}        \n",
      "{'loss': 2.0938, 'learning_rate': 0.00019801647675414877, 'epoch': 0.11}        \n",
      "{'loss': 1.864, 'learning_rate': 0.00019801585941446224, 'epoch': 0.11}         \n",
      "{'loss': 1.6921, 'learning_rate': 0.0001980152419796847, 'epoch': 0.11}         \n",
      "{'loss': 2.3327, 'learning_rate': 0.00019801462444981685, 'epoch': 0.11}        \n",
      "{'loss': 2.0784, 'learning_rate': 0.00019801400682485925, 'epoch': 0.11}        \n",
      " 11%|███                        | 118561/1061708 [17:44:47<139:00:31,  1.88it/s][2024-03-01 11:51:47,847] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.3534, 'learning_rate': 0.00019801345088109613, 'epoch': 0.11}        \n",
      "{'loss': 2.1069, 'learning_rate': 0.00019801283307546963, 'epoch': 0.11}        \n",
      "{'loss': 1.9602, 'learning_rate': 0.0001980122151747551, 'epoch': 0.11}         \n",
      "{'loss': 1.9611, 'learning_rate': 0.00019801159717895312, 'epoch': 0.11}        \n",
      "{'loss': 1.8637, 'learning_rate': 0.00019801097908806435, 'epoch': 0.11}        \n",
      "{'loss': 2.0117, 'learning_rate': 0.00019801036090208937, 'epoch': 0.11}        \n",
      "{'loss': 2.0224, 'learning_rate': 0.0001980097426210287, 'epoch': 0.11}         \n",
      "{'loss': 2.1004, 'learning_rate': 0.00019800912424488307, 'epoch': 0.11}        \n",
      "{'loss': 2.0008, 'learning_rate': 0.00019800850577365297, 'epoch': 0.11}        \n",
      "{'loss': 1.8682, 'learning_rate': 0.0001980078872073391, 'epoch': 0.11}         \n",
      " 11%|███                        | 118662/1061708 [17:45:41<138:58:07,  1.89it/s][2024-03-01 11:52:41,621] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 118663/1061708 [17:45:41<133:43:25,  1.96it/s][2024-03-01 11:52:42,045] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2677, 'learning_rate': 0.000198007392285828, 'epoch': 0.11}          \n",
      "{'loss': 2.0395, 'learning_rate': 0.00019800677354836474, 'epoch': 0.11}        \n",
      "{'loss': 1.9548, 'learning_rate': 0.00019800615471581934, 'epoch': 0.11}        \n",
      "{'loss': 2.3003, 'learning_rate': 0.0001980055357881924, 'epoch': 0.11}         \n",
      "{'loss': 1.9024, 'learning_rate': 0.00019800491676548446, 'epoch': 0.11}        \n",
      "{'loss': 2.4322, 'learning_rate': 0.0001980042976476962, 'epoch': 0.11}         \n",
      " 11%|███                        | 118722/1061708 [17:46:12<138:56:36,  1.89it/s][2024-03-01 11:53:13,507] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1306, 'learning_rate': 0.00019800374036039363, 'epoch': 0.11}        \n",
      "{'loss': 2.2562, 'learning_rate': 0.0001980031210619544, 'epoch': 0.11}         \n",
      "{'loss': 1.6598, 'learning_rate': 0.00019800250166843655, 'epoch': 0.11}        \n",
      "{'loss': 1.9121, 'learning_rate': 0.0001980018821798407, 'epoch': 0.11}         \n",
      "{'loss': 2.2303, 'learning_rate': 0.00019800126259616744, 'epoch': 0.11}        \n",
      "{'loss': 2.2811, 'learning_rate': 0.00019800064291741745, 'epoch': 0.11}        \n",
      "{'loss': 2.4453, 'learning_rate': 0.0001980000231435912, 'epoch': 0.11}         \n",
      "{'loss': 2.0477, 'learning_rate': 0.00019799940327468938, 'epoch': 0.11}        \n",
      " 11%|███                        | 118807/1061708 [17:46:58<139:19:52,  1.88it/s][2024-03-01 11:53:58,742] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.9765, 'learning_rate': 0.0001979988453113886, 'epoch': 0.11}         \n",
      "{'loss': 1.6403, 'learning_rate': 0.0001979982252618448, 'epoch': 0.11}         \n",
      "{'loss': 1.8415, 'learning_rate': 0.00019799760511722715, 'epoch': 0.11}        \n",
      "{'loss': 2.2319, 'learning_rate': 0.00019799698487753628, 'epoch': 0.11}        \n",
      "{'loss': 2.0528, 'learning_rate': 0.0001979963645427727, 'epoch': 0.11}         \n",
      "{'loss': 2.1942, 'learning_rate': 0.00019799574411293714, 'epoch': 0.11}        \n",
      "{'loss': 2.1198, 'learning_rate': 0.0001979951235880301, 'epoch': 0.11}         \n",
      "{'loss': 2.148, 'learning_rate': 0.00019799450296805225, 'epoch': 0.11}         \n",
      "{'loss': 2.272, 'learning_rate': 0.00019799388225300412, 'epoch': 0.11}         \n",
      "{'loss': 1.5558, 'learning_rate': 0.00019799326144288637, 'epoch': 0.11}        \n",
      "{'loss': 2.2846, 'learning_rate': 0.0001979926405376996, 'epoch': 0.11}         \n",
      "{'loss': 2.134, 'learning_rate': 0.00019799201953744436, 'epoch': 0.11}         \n",
      "{'loss': 1.7659, 'learning_rate': 0.00019799139844212135, 'epoch': 0.11}        \n",
      "{'loss': 1.8206, 'learning_rate': 0.00019799077725173104, 'epoch': 0.11}        \n",
      "{'loss': 2.039, 'learning_rate': 0.00019799015596627415, 'epoch': 0.11}         \n",
      "{'loss': 2.1193, 'learning_rate': 0.00019798953458575124, 'epoch': 0.11}        \n",
      "{'loss': 2.1103, 'learning_rate': 0.0001979889131101629, 'epoch': 0.11}         \n",
      "{'loss': 1.7356, 'learning_rate': 0.00019798829153950975, 'epoch': 0.11}        \n",
      "{'loss': 2.0454, 'learning_rate': 0.00019798766987379237, 'epoch': 0.11}        \n",
      "{'loss': 2.1047, 'learning_rate': 0.0001979870481130114, 'epoch': 0.11}         \n",
      "{'loss': 2.3026, 'learning_rate': 0.00019798642625716743, 'epoch': 0.11}        \n",
      "{'loss': 1.9199, 'learning_rate': 0.00019798580430626105, 'epoch': 0.11}        \n",
      "{'loss': 2.8168, 'learning_rate': 0.00019798518226029285, 'epoch': 0.11}        \n",
      "{'loss': 1.9127, 'learning_rate': 0.00019798456011926345, 'epoch': 0.11}        \n",
      "{'loss': 1.9261, 'learning_rate': 0.00019798393788317349, 'epoch': 0.11}        \n",
      "{'loss': 1.9373, 'learning_rate': 0.00019798331555202352, 'epoch': 0.11}        \n",
      "{'loss': 1.8964, 'learning_rate': 0.00019798269312581418, 'epoch': 0.11}        \n",
      "{'loss': 2.0884, 'learning_rate': 0.00019798207060454603, 'epoch': 0.11}        \n",
      "{'loss': 2.4346, 'learning_rate': 0.00019798144798821975, 'epoch': 0.11}        \n",
      "{'loss': 2.2813, 'learning_rate': 0.00019798082527683583, 'epoch': 0.11}        \n",
      " 11%|███                        | 119108/1061708 [17:49:38<139:18:45,  1.88it/s][2024-03-01 11:56:39,274] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 119109/1061708 [17:49:39<130:45:01,  2.00it/s][2024-03-01 11:56:39,697] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2442, 'learning_rate': 0.00019798032703928768, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.184, 'learning_rate': 0.0001979797041568017, 'epoch': 0.11}          \n",
      "{'loss': 2.0268, 'learning_rate': 0.0001979790811792598, 'epoch': 0.11}         \n",
      "{'loss': 2.0852, 'learning_rate': 0.00019797845810666265, 'epoch': 0.11}        \n",
      "{'loss': 2.0903, 'learning_rate': 0.00019797783493901084, 'epoch': 0.11}        \n",
      "{'loss': 2.2249, 'learning_rate': 0.0001979772116763049, 'epoch': 0.11}         \n",
      "{'loss': 2.5638, 'learning_rate': 0.00019797658831854556, 'epoch': 0.11}        \n",
      "{'loss': 2.3452, 'learning_rate': 0.00019797596486573335, 'epoch': 0.11}        \n",
      "{'loss': 1.9565, 'learning_rate': 0.00019797534131786888, 'epoch': 0.11}        \n",
      "{'loss': 1.9071, 'learning_rate': 0.00019797471767495276, 'epoch': 0.11}        \n",
      "{'loss': 2.1778, 'learning_rate': 0.0001979740939369856, 'epoch': 0.11}         \n",
      " 11%|███                        | 119210/1061708 [17:50:33<138:53:34,  1.88it/s][2024-03-01 11:57:33,541] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 119211/1061708 [17:50:33<130:30:11,  2.01it/s][2024-03-01 11:57:33,970] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      " 11%|███                        | 119214/1061708 [17:50:34<132:06:11,  1.98it/s][2024-03-01 11:57:35,504] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2681, 'learning_rate': 0.00019797365726385353, 'epoch': 0.11}        \n",
      "{'loss': 1.5862, 'learning_rate': 0.00019797303336430098, 'epoch': 0.11}        \n",
      "{'loss': 2.1024, 'learning_rate': 0.000197972409369699, 'epoch': 0.11}          \n",
      "{'loss': 2.0857, 'learning_rate': 0.00019797178528004823, 'epoch': 0.11}        \n",
      "{'loss': 2.0046, 'learning_rate': 0.00019797116109534927, 'epoch': 0.11}        \n",
      "{'loss': 1.6971, 'learning_rate': 0.00019797053681560272, 'epoch': 0.11}        \n",
      "{'loss': 1.6953, 'learning_rate': 0.00019796991244080915, 'epoch': 0.11}        \n",
      "{'loss': 2.197, 'learning_rate': 0.00019796928797096924, 'epoch': 0.11}         \n",
      "{'loss': 1.7176, 'learning_rate': 0.0001979686634060835, 'epoch': 0.11}         \n",
      "{'loss': 2.0175, 'learning_rate': 0.00019796803874615263, 'epoch': 0.11}        \n",
      "{'loss': 1.9975, 'learning_rate': 0.0001979674139911772, 'epoch': 0.11}         \n",
      "{'loss': 1.8654, 'learning_rate': 0.0001979667891411578, 'epoch': 0.11}         \n",
      "{'loss': 1.7682, 'learning_rate': 0.00019796616419609503, 'epoch': 0.11}        \n",
      "{'loss': 1.5861, 'learning_rate': 0.00019796553915598955, 'epoch': 0.11}        \n",
      " 11%|███                        | 119356/1061708 [17:51:50<139:27:13,  1.88it/s][2024-03-01 11:58:51,117] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.146, 'learning_rate': 0.00019796497653863356, 'epoch': 0.11}         \n",
      "{'loss': 2.0669, 'learning_rate': 0.0001979643513179485, 'epoch': 0.11}         \n",
      "{'loss': 2.0302, 'learning_rate': 0.00019796372600222245, 'epoch': 0.11}        \n",
      "{'loss': 2.0777, 'learning_rate': 0.00019796310059145604, 'epoch': 0.11}        \n",
      "{'loss': 2.213, 'learning_rate': 0.00019796247508564984, 'epoch': 0.11}         \n",
      "{'loss': 2.1653, 'learning_rate': 0.0001979618494848045, 'epoch': 0.11}         \n",
      "{'loss': 2.286, 'learning_rate': 0.00019796122378892058, 'epoch': 0.11}         \n",
      "{'loss': 2.1928, 'learning_rate': 0.0001979605979979987, 'epoch': 0.11}         \n",
      "{'loss': 2.2545, 'learning_rate': 0.00019795997211203949, 'epoch': 0.11}        \n",
      "{'loss': 1.9792, 'learning_rate': 0.00019795934613104352, 'epoch': 0.11}        \n",
      " 11%|███                        | 119457/1061708 [17:52:44<138:55:22,  1.88it/s][2024-03-01 11:59:44,906] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 119458/1061708 [17:52:44<130:19:00,  2.01it/s][2024-03-01 11:59:45,328] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.6727, 'learning_rate': 0.00019795884527782072, 'epoch': 0.11}        \n",
      "{'loss': 2.1646, 'learning_rate': 0.00019795821912576016, 'epoch': 0.11}        \n",
      "{'loss': 2.0471, 'learning_rate': 0.00019795759287866458, 'epoch': 0.11}        \n",
      "{'loss': 2.1951, 'learning_rate': 0.00019795696653653456, 'epoch': 0.11}        \n",
      "{'loss': 2.0644, 'learning_rate': 0.00019795634009937073, 'epoch': 0.11}        \n",
      "{'loss': 2.1577, 'learning_rate': 0.00019795571356717365, 'epoch': 0.11}        \n",
      "{'loss': 1.8405, 'learning_rate': 0.00019795508693994397, 'epoch': 0.11}        \n",
      "{'loss': 1.9385, 'learning_rate': 0.00019795446021768235, 'epoch': 0.11}        \n",
      "{'loss': 1.9273, 'learning_rate': 0.00019795383340038926, 'epoch': 0.11}        \n",
      "{'loss': 2.6293, 'learning_rate': 0.00019795320648806542, 'epoch': 0.11}        \n",
      " 11%|███                        | 119559/1061708 [17:53:38<138:47:54,  1.89it/s][2024-03-01 12:00:39,003] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8145, 'learning_rate': 0.00019795264218572313, 'epoch': 0.11}        \n",
      " 11%|███                        | 119560/1061708 [17:53:38<130:14:01,  2.01it/s][2024-03-01 12:00:39,426] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7769, 'learning_rate': 0.00019795207780640687, 'epoch': 0.11}        \n",
      "{'loss': 2.514, 'learning_rate': 0.00019795145062800006, 'epoch': 0.11}         \n",
      "{'loss': 1.895, 'learning_rate': 0.0001979508233545648, 'epoch': 0.11}          \n",
      "{'loss': 2.0085, 'learning_rate': 0.00019795019598610168, 'epoch': 0.11}        \n",
      "{'loss': 2.2341, 'learning_rate': 0.00019794956852261128, 'epoch': 0.11}        \n",
      "{'loss': 2.0243, 'learning_rate': 0.00019794894096409428, 'epoch': 0.11}        \n",
      "{'loss': 1.9755, 'learning_rate': 0.00019794831331055118, 'epoch': 0.11}        \n",
      " 11%|███                        | 119634/1061708 [17:54:18<140:48:00,  1.86it/s][2024-03-01 12:01:18,774] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7821, 'learning_rate': 0.00019794774834111566, 'epoch': 0.11}        \n",
      "{'loss': 1.7627, 'learning_rate': 0.0001979471205070248, 'epoch': 0.11}         \n",
      "{'loss': 2.0809, 'learning_rate': 0.00019794649257790967, 'epoch': 0.11}        \n",
      "{'loss': 2.2064, 'learning_rate': 0.00019794586455377088, 'epoch': 0.11}        \n",
      "{'loss': 1.9467, 'learning_rate': 0.00019794523643460902, 'epoch': 0.11}        \n",
      "{'loss': 2.1324, 'learning_rate': 0.00019794460822042474, 'epoch': 0.11}        \n",
      "{'loss': 2.2415, 'learning_rate': 0.00019794397991121861, 'epoch': 0.11}        \n",
      "{'loss': 2.3994, 'learning_rate': 0.00019794335150699128, 'epoch': 0.11}        \n",
      "{'loss': 1.9774, 'learning_rate': 0.0001979427230077433, 'epoch': 0.11}         \n",
      "{'loss': 1.9563, 'learning_rate': 0.00019794209441347533, 'epoch': 0.11}        \n",
      " 11%|███                        | 119738/1061708 [17:55:13<138:43:06,  1.89it/s][2024-03-01 12:02:14,042] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.832, 'learning_rate': 0.0001979415285973926, 'epoch': 0.11}          \n",
      "{'loss': 2.5053, 'learning_rate': 0.00019794089982258829, 'epoch': 0.11}        \n",
      "{'loss': 1.8767, 'learning_rate': 0.00019794027095276575, 'epoch': 0.11}        \n",
      "{'loss': 2.202, 'learning_rate': 0.0001979396419879256, 'epoch': 0.11}          \n",
      "{'loss': 2.0474, 'learning_rate': 0.00019793901292806845, 'epoch': 0.11}        \n",
      "{'loss': 2.0736, 'learning_rate': 0.00019793838377319488, 'epoch': 0.11}        \n",
      "{'loss': 1.9883, 'learning_rate': 0.0001979377545233055, 'epoch': 0.11}         \n",
      "{'loss': 2.0972, 'learning_rate': 0.00019793712517840093, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2032, 'learning_rate': 0.0001979364957384818, 'epoch': 0.11}         \n",
      " 11%|███                        | 119821/1061708 [17:55:57<139:05:57,  1.88it/s][2024-03-01 12:02:58,189] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8, 'learning_rate': 0.00019793592916131765, 'epoch': 0.11}           \n",
      "{'loss': 2.2446, 'learning_rate': 0.00019793529954087252, 'epoch': 0.11}        \n",
      "{'loss': 2.4465, 'learning_rate': 0.00019793466982541457, 'epoch': 0.11}        \n",
      "{'loss': 2.5633, 'learning_rate': 0.00019793404001494443, 'epoch': 0.11}        \n",
      "{'loss': 2.3792, 'learning_rate': 0.00019793341010946274, 'epoch': 0.11}        \n",
      "{'loss': 2.3389, 'learning_rate': 0.00019793278010897005, 'epoch': 0.11}        \n",
      "{'loss': 1.7433, 'learning_rate': 0.00019793215001346704, 'epoch': 0.11}        \n",
      " 11%|███                        | 119896/1061708 [17:56:37<139:48:41,  1.87it/s][2024-03-01 12:03:38,169] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0079, 'learning_rate': 0.00019793158284628094, 'epoch': 0.11}        \n",
      "{'loss': 2.3157, 'learning_rate': 0.00019793095257025992, 'epoch': 0.11}        \n",
      "{'loss': 2.1195, 'learning_rate': 0.0001979303221992303, 'epoch': 0.11}         \n",
      "{'loss': 1.9928, 'learning_rate': 0.0001979296917331927, 'epoch': 0.11}         \n",
      "{'loss': 2.1207, 'learning_rate': 0.00019792906117214773, 'epoch': 0.11}        \n",
      "{'loss': 2.124, 'learning_rate': 0.00019792843051609605, 'epoch': 0.11}         \n",
      "{'loss': 2.1725, 'learning_rate': 0.00019792779976503822, 'epoch': 0.11}        \n",
      "{'loss': 2.2403, 'learning_rate': 0.0001979271689189748, 'epoch': 0.11}         \n",
      "{'loss': 2.1668, 'learning_rate': 0.00019792653797790654, 'epoch': 0.11}        \n",
      "{'loss': 2.0272, 'learning_rate': 0.00019792590694183397, 'epoch': 0.11}        \n",
      " 11%|███                        | 119999/1061708 [17:57:32<138:49:09,  1.88it/s][2024-03-01 12:04:33,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=120000, skipped=1401, lr=[0.0001979252758107577], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 12:04:33,225] [INFO] [timer.py:260:stop] epoch=0/micro_step=120000/global_step=120000, RunningAvgSamplesPerSec=1.8927193815470584, CurrSamplesPerSec=1.8957666089028582, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.9503, 'learning_rate': 0.0001979252758107577, 'epoch': 0.11}         \n",
      " 11%|███                        | 120000/1061708 [17:57:33<138:54:58,  1.88it/s][INFO|trainer.py:2868] 2024-03-01 12:04:33,227 >> Saving model checkpoint to output_model/checkpoint-120000\n",
      "[INFO|trainer.py:2880] 2024-03-01 12:04:33,230 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 12:04:34,436 >> tokenizer config file saved in output_model/checkpoint-120000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 12:04:34,436 >> Special tokens file saved in output_model/checkpoint-120000/special_tokens_map.json\n",
      "[2024-03-01 12:04:34,437] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step120000 is about to be saved!\n",
      "[2024-03-01 12:04:39,655] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-120000/global_step120000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 12:04:39,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-120000/global_step120000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 12:04:53,498] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-120000/global_step120000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 12:04:54,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-120000/global_step120000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 12:05:01,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-120000/global_step120000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 12:05:01,335] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-120000/global_step120000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 12:05:01,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step120000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 12:05:01,441 >> Deleting older checkpoint [output_model/checkpoint-105000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 12:05:05,187 >> tokenizer config file saved in output_model/checkpoint-120000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 12:05:05,187 >> Special tokens file saved in output_model/checkpoint-120000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.2901, 'learning_rate': 0.00019792464458467834, 'epoch': 0.11}        \n",
      "{'loss': 1.9672, 'learning_rate': 0.00019792401326359653, 'epoch': 0.11}        \n",
      "{'loss': 2.1907, 'learning_rate': 0.00019792338184751283, 'epoch': 0.11}        \n",
      "{'loss': 1.5679, 'learning_rate': 0.00019792275033642795, 'epoch': 0.11}        \n",
      "{'loss': 2.2615, 'learning_rate': 0.0001979221187303424, 'epoch': 0.11}         \n",
      "{'loss': 1.6907, 'learning_rate': 0.00019792148702925687, 'epoch': 0.11}        \n",
      "{'loss': 1.9309, 'learning_rate': 0.0001979208552331719, 'epoch': 0.11}         \n",
      "{'loss': 2.1038, 'learning_rate': 0.00019792022334208815, 'epoch': 0.11}        \n",
      "{'loss': 1.8199, 'learning_rate': 0.00019791959135600622, 'epoch': 0.11}        \n",
      "{'loss': 2.0297, 'learning_rate': 0.00019791895927492674, 'epoch': 0.11}        \n",
      "{'loss': 1.9589, 'learning_rate': 0.0001979183270988503, 'epoch': 0.11}         \n",
      "{'loss': 1.9319, 'learning_rate': 0.00019791769482777754, 'epoch': 0.11}        \n",
      "{'loss': 2.0506, 'learning_rate': 0.00019791706246170905, 'epoch': 0.11}        \n",
      "{'loss': 2.2432, 'learning_rate': 0.00019791643000064544, 'epoch': 0.11}        \n",
      "{'loss': 2.1797, 'learning_rate': 0.00019791579744458734, 'epoch': 0.11}        \n",
      "{'loss': 2.1483, 'learning_rate': 0.00019791516479353538, 'epoch': 0.11}        \n",
      "{'loss': 2.2297, 'learning_rate': 0.00019791453204749012, 'epoch': 0.11}        \n",
      "{'loss': 1.9884, 'learning_rate': 0.0001979138992064522, 'epoch': 0.11}         \n",
      "{'loss': 2.2594, 'learning_rate': 0.00019791326627042227, 'epoch': 0.11}        \n",
      " 11%|███                        | 120197/1061708 [17:59:50<139:39:49,  1.87it/s][2024-03-01 12:06:50,710] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 120198/1061708 [17:59:50<131:11:00,  1.99it/s][2024-03-01 12:06:51,138] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4394, 'learning_rate': 0.00019791275985320446, 'epoch': 0.11}        \n",
      " 11%|███                        | 120201/1061708 [17:59:52<132:17:54,  1.98it/s][2024-03-01 12:06:52,629] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8279, 'learning_rate': 0.00019791219006116636, 'epoch': 0.11}        \n",
      "{'loss': 1.9011, 'learning_rate': 0.00019791155686866096, 'epoch': 0.11}        \n",
      "{'loss': 2.0458, 'learning_rate': 0.00019791092358116582, 'epoch': 0.11}        \n",
      "{'loss': 2.1371, 'learning_rate': 0.00019791029019868153, 'epoch': 0.11}        \n",
      "{'loss': 2.2529, 'learning_rate': 0.00019790965672120868, 'epoch': 0.11}        \n",
      "{'loss': 1.8215, 'learning_rate': 0.00019790902314874788, 'epoch': 0.11}        \n",
      "{'loss': 2.251, 'learning_rate': 0.0001979083894812998, 'epoch': 0.11}          \n",
      "{'loss': 1.8211, 'learning_rate': 0.00019790775571886502, 'epoch': 0.11}        \n",
      "{'loss': 1.8921, 'learning_rate': 0.00019790712186144415, 'epoch': 0.11}        \n",
      "{'loss': 1.9331, 'learning_rate': 0.00019790648790903785, 'epoch': 0.11}        \n",
      "{'loss': 2.4041, 'learning_rate': 0.00019790585386164668, 'epoch': 0.11}        \n",
      "{'loss': 2.0019, 'learning_rate': 0.00019790521971927124, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8913, 'learning_rate': 0.00019790458548191221, 'epoch': 0.11}        \n",
      "{'loss': 1.6085, 'learning_rate': 0.0001979039511495702, 'epoch': 0.11}         \n",
      "{'loss': 2.0855, 'learning_rate': 0.00019790331672224577, 'epoch': 0.11}        \n",
      "{'loss': 1.6539, 'learning_rate': 0.00019790268219993954, 'epoch': 0.11}        \n",
      "{'loss': 1.813, 'learning_rate': 0.00019790204758265222, 'epoch': 0.11}         \n",
      "{'loss': 1.6943, 'learning_rate': 0.0001979014128703843, 'epoch': 0.11}         \n",
      "{'loss': 1.6996, 'learning_rate': 0.00019790077806313647, 'epoch': 0.11}        \n",
      "{'loss': 2.3252, 'learning_rate': 0.00019790014316090933, 'epoch': 0.11}        \n",
      " 11%|███                        | 120402/1061708 [18:01:39<138:56:54,  1.88it/s][2024-03-01 12:08:40,070] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 120403/1061708 [18:01:39<133:33:34,  1.96it/s][2024-03-01 12:08:40,502] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0725, 'learning_rate': 0.00019789963517074296, 'epoch': 0.11}        \n",
      "{'loss': 2.3433, 'learning_rate': 0.0001978990000975546, 'epoch': 0.11}         \n",
      "{'loss': 1.7341, 'learning_rate': 0.00019789836492938866, 'epoch': 0.11}        \n",
      "{'loss': 2.0112, 'learning_rate': 0.00019789772966624577, 'epoch': 0.11}        \n",
      " 11%|███                        | 120445/1061708 [18:02:02<140:18:51,  1.86it/s][2024-03-01 12:09:02,901] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0272, 'learning_rate': 0.00019789715784821233, 'epoch': 0.11}        \n",
      "{'loss': 2.4548, 'learning_rate': 0.0001978965224046149, 'epoch': 0.11}         \n",
      "{'loss': 1.7476, 'learning_rate': 0.00019789588686604226, 'epoch': 0.11}        \n",
      "{'loss': 2.2529, 'learning_rate': 0.00019789525123249506, 'epoch': 0.11}        \n",
      "{'loss': 1.9849, 'learning_rate': 0.00019789461550397393, 'epoch': 0.11}        \n",
      "{'loss': 1.9236, 'learning_rate': 0.00019789397968047944, 'epoch': 0.11}        \n",
      "{'loss': 2.1237, 'learning_rate': 0.00019789334376201225, 'epoch': 0.11}        \n",
      "{'loss': 2.0745, 'learning_rate': 0.00019789270774857296, 'epoch': 0.11}        \n",
      " 11%|███                        | 120524/1061708 [18:02:44<141:12:31,  1.85it/s][2024-03-01 12:09:45,067] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.2566, 'learning_rate': 0.00019789213525527694, 'epoch': 0.11}        \n",
      "{'loss': 2.1666, 'learning_rate': 0.0001978914990613924, 'epoch': 0.11}         \n",
      "{'loss': 2.2624, 'learning_rate': 0.0001978908627725375, 'epoch': 0.11}         \n",
      "{'loss': 2.0436, 'learning_rate': 0.00019789022638871292, 'epoch': 0.11}        \n",
      "{'loss': 2.326, 'learning_rate': 0.00019788958990991926, 'epoch': 0.11}         \n",
      "{'loss': 2.3047, 'learning_rate': 0.00019788895333615715, 'epoch': 0.11}        \n",
      "{'loss': 1.9711, 'learning_rate': 0.0001978883166674272, 'epoch': 0.11}         \n",
      "{'loss': 1.8822, 'learning_rate': 0.00019788767990373, 'epoch': 0.11}           \n",
      "{'loss': 2.0013, 'learning_rate': 0.00019788704304506623, 'epoch': 0.11}        \n",
      "{'loss': 2.213, 'learning_rate': 0.0001978864060914365, 'epoch': 0.11}          \n",
      "{'loss': 2.063, 'learning_rate': 0.00019788576904284134, 'epoch': 0.11}         \n",
      "{'loss': 1.8165, 'learning_rate': 0.00019788513189928148, 'epoch': 0.11}        \n",
      "{'loss': 2.1376, 'learning_rate': 0.00019788449466075748, 'epoch': 0.11}        \n",
      "{'loss': 1.7496, 'learning_rate': 0.00019788385732726993, 'epoch': 0.11}        \n",
      "{'loss': 1.8984, 'learning_rate': 0.0001978832198988195, 'epoch': 0.11}         \n",
      "{'loss': 2.0447, 'learning_rate': 0.00019788258237540682, 'epoch': 0.11}        \n",
      "{'loss': 2.0205, 'learning_rate': 0.00019788194475703246, 'epoch': 0.11}        \n",
      "{'loss': 2.3794, 'learning_rate': 0.00019788130704369707, 'epoch': 0.11}        \n",
      "{'loss': 2.1729, 'learning_rate': 0.00019788066923540125, 'epoch': 0.11}        \n",
      "{'loss': 1.9285, 'learning_rate': 0.00019788003133214564, 'epoch': 0.11}        \n",
      "{'loss': 2.4349, 'learning_rate': 0.0001978793933339308, 'epoch': 0.11}         \n",
      "{'loss': 1.6357, 'learning_rate': 0.00019787875524075746, 'epoch': 0.11}        \n",
      "{'loss': 1.7249, 'learning_rate': 0.00019787811705262613, 'epoch': 0.11}        \n",
      "{'loss': 2.3385, 'learning_rate': 0.0001978774787695375, 'epoch': 0.11}         \n",
      "{'loss': 2.1646, 'learning_rate': 0.00019787684039149217, 'epoch': 0.11}        \n",
      "{'loss': 2.251, 'learning_rate': 0.00019787620191849073, 'epoch': 0.11}         \n",
      "{'loss': 2.114, 'learning_rate': 0.00019787556335053384, 'epoch': 0.11}         \n",
      "{'loss': 2.4833, 'learning_rate': 0.00019787492468762208, 'epoch': 0.11}        \n",
      "{'loss': 1.978, 'learning_rate': 0.0001978742859297561, 'epoch': 0.11}          \n",
      "{'loss': 2.466, 'learning_rate': 0.0001978736470769365, 'epoch': 0.11}          \n",
      " 11%|███                        | 120825/1061708 [18:05:25<140:03:15,  1.87it/s][2024-03-01 12:12:25,826] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 120826/1061708 [18:05:25<131:12:17,  1.99it/s][2024-03-01 12:12:26,249] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      " 11%|███                        | 120828/1061708 [18:05:26<129:12:42,  2.02it/s][2024-03-01 12:12:27,210] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1957, 'learning_rate': 0.00019787319982346574, 'epoch': 0.11}        \n",
      "{'loss': 1.6735, 'learning_rate': 0.00019787256080922643, 'epoch': 0.11}        \n",
      "{'loss': 2.0356, 'learning_rate': 0.00019787192170003519, 'epoch': 0.11}        \n",
      "{'loss': 2.4432, 'learning_rate': 0.00019787128249589264, 'epoch': 0.11}        \n",
      "{'loss': 2.3195, 'learning_rate': 0.00019787064319679937, 'epoch': 0.11}        \n",
      "{'loss': 1.8599, 'learning_rate': 0.00019787000380275607, 'epoch': 0.11}        \n",
      "{'loss': 1.8601, 'learning_rate': 0.00019786936431376328, 'epoch': 0.11}        \n",
      "{'loss': 2.2551, 'learning_rate': 0.00019786872472982164, 'epoch': 0.11}        \n",
      "{'loss': 1.8535, 'learning_rate': 0.00019786808505093184, 'epoch': 0.11}        \n",
      "{'loss': 2.1929, 'learning_rate': 0.0001978674452770944, 'epoch': 0.11}         \n",
      "{'loss': 1.8239, 'learning_rate': 0.00019786680540831002, 'epoch': 0.11}        \n",
      "{'loss': 2.0762, 'learning_rate': 0.00019786616544457925, 'epoch': 0.11}        \n",
      "{'loss': 1.821, 'learning_rate': 0.00019786552538590278, 'epoch': 0.11}         \n",
      "{'loss': 2.1771, 'learning_rate': 0.00019786488523228118, 'epoch': 0.11}        \n",
      "{'loss': 1.6944, 'learning_rate': 0.0001978642449837151, 'epoch': 0.11}         \n",
      "{'loss': 2.2161, 'learning_rate': 0.00019786360464020515, 'epoch': 0.11}        \n",
      "{'loss': 1.7753, 'learning_rate': 0.00019786296420175195, 'epoch': 0.11}        \n",
      "{'loss': 1.9924, 'learning_rate': 0.00019786232366835613, 'epoch': 0.11}        \n",
      "{'loss': 2.0796, 'learning_rate': 0.0001978616830400183, 'epoch': 0.11}         \n",
      "{'loss': 1.9566, 'learning_rate': 0.0001978610423167391, 'epoch': 0.11}         \n",
      " 11%|███                        | 121029/1061708 [18:07:13<138:39:45,  1.88it/s][2024-03-01 12:14:14,474] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7226, 'learning_rate': 0.00019786046558461343, 'epoch': 0.11}        \n",
      " 11%|███                        | 121030/1061708 [18:07:14<130:08:28,  2.01it/s][2024-03-01 12:14:14,898] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0701, 'learning_rate': 0.0001978598887755862, 'epoch': 0.11}         \n",
      "{'loss': 2.0853, 'learning_rate': 0.00019785924778647446, 'epoch': 0.11}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7749, 'learning_rate': 0.00019785860670242365, 'epoch': 0.11}        \n",
      "{'loss': 2.1518, 'learning_rate': 0.00019785796552343447, 'epoch': 0.11}        \n",
      "{'loss': 2.1685, 'learning_rate': 0.00019785732424950752, 'epoch': 0.11}        \n",
      "{'loss': 2.1253, 'learning_rate': 0.0001978566828806434, 'epoch': 0.11}         \n",
      "{'loss': 1.8732, 'learning_rate': 0.00019785604141684272, 'epoch': 0.11}        \n",
      "{'loss': 1.87, 'learning_rate': 0.00019785539985810616, 'epoch': 0.11}          \n",
      "{'loss': 1.979, 'learning_rate': 0.00019785475820443434, 'epoch': 0.11}         \n",
      "{'loss': 1.9117, 'learning_rate': 0.0001978541164558278, 'epoch': 0.11}         \n",
      " 11%|███                        | 121131/1061708 [18:08:08<138:40:05,  1.88it/s][2024-03-01 12:15:08,838] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 121132/1061708 [18:08:08<130:39:31,  2.00it/s][2024-03-01 12:15:09,261] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9856, 'learning_rate': 0.00019785360298859005, 'epoch': 0.11}        \n",
      "{'loss': 1.7517, 'learning_rate': 0.0001978529610691027, 'epoch': 0.11}         \n",
      " 11%|███                        | 121153/1061708 [18:08:19<142:02:45,  1.84it/s][2024-03-01 12:15:20,420] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8552, 'learning_rate': 0.00019785238326039644, 'epoch': 0.11}        \n",
      "{'loss': 1.8814, 'learning_rate': 0.00019785174116053708, 'epoch': 0.11}        \n",
      "{'loss': 2.6889, 'learning_rate': 0.00019785109896574597, 'epoch': 0.11}        \n",
      "{'loss': 1.6136, 'learning_rate': 0.00019785045667602378, 'epoch': 0.11}        \n",
      "{'loss': 2.1038, 'learning_rate': 0.0001978498142913711, 'epoch': 0.11}         \n",
      "{'loss': 2.1022, 'learning_rate': 0.0001978491718117885, 'epoch': 0.11}         \n",
      "{'loss': 2.0119, 'learning_rate': 0.0001978485292372767, 'epoch': 0.11}         \n",
      "{'loss': 2.0399, 'learning_rate': 0.00019784788656783623, 'epoch': 0.11}        \n",
      "{'loss': 2.1129, 'learning_rate': 0.00019784724380346783, 'epoch': 0.11}        \n",
      "{'loss': 2.111, 'learning_rate': 0.000197846600944172, 'epoch': 0.11}           \n",
      "{'loss': 2.1658, 'learning_rate': 0.00019784595798994945, 'epoch': 0.11}        \n",
      "{'loss': 2.031, 'learning_rate': 0.00019784531494080077, 'epoch': 0.11}         \n",
      "{'loss': 1.9848, 'learning_rate': 0.00019784467179672656, 'epoch': 0.11}        \n",
      "{'loss': 1.8771, 'learning_rate': 0.0001978440285577275, 'epoch': 0.11}         \n",
      "{'loss': 2.2368, 'learning_rate': 0.0001978433852238042, 'epoch': 0.11}         \n",
      "{'loss': 1.9566, 'learning_rate': 0.00019784274179495724, 'epoch': 0.11}        \n",
      "{'loss': 1.9097, 'learning_rate': 0.00019784209827118727, 'epoch': 0.11}        \n",
      "{'loss': 1.7691, 'learning_rate': 0.00019784145465249494, 'epoch': 0.11}        \n",
      "{'loss': 2.2613, 'learning_rate': 0.00019784081093888081, 'epoch': 0.11}        \n",
      "{'loss': 2.3432, 'learning_rate': 0.00019784016713034558, 'epoch': 0.11}        \n",
      " 11%|███                        | 121354/1061708 [18:10:07<140:39:31,  1.86it/s][2024-03-01 12:17:07,790] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 121355/1061708 [18:10:07<131:29:34,  1.99it/s][2024-03-01 12:17:08,212] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2439, 'learning_rate': 0.0001978396520151746, 'epoch': 0.11}         \n",
      "{'loss': 2.368, 'learning_rate': 0.0001978390080357829, 'epoch': 0.11}          \n",
      "{'loss': 2.0696, 'learning_rate': 0.0001978383639614718, 'epoch': 0.11}         \n",
      "{'loss': 2.1297, 'learning_rate': 0.00019783771979224197, 'epoch': 0.11}        \n",
      "{'loss': 2.3067, 'learning_rate': 0.000197837075528094, 'epoch': 0.11}          \n",
      " 11%|███                        | 121400/1061708 [18:10:31<138:13:58,  1.89it/s][2024-03-01 12:17:32,138] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5128, 'learning_rate': 0.00019783649560920635, 'epoch': 0.11}        \n",
      "{'loss': 2.0498, 'learning_rate': 0.0001978358511647156, 'epoch': 0.11}         \n",
      "{'loss': 2.0905, 'learning_rate': 0.0001978352066253086, 'epoch': 0.11}         \n",
      "{'loss': 1.9338, 'learning_rate': 0.00019783456199098584, 'epoch': 0.11}        \n",
      "{'loss': 1.6569, 'learning_rate': 0.00019783391726174804, 'epoch': 0.11}        \n",
      "{'loss': 2.1826, 'learning_rate': 0.0001978332724375958, 'epoch': 0.11}         \n",
      "{'loss': 2.0915, 'learning_rate': 0.00019783262751852973, 'epoch': 0.11}        \n",
      "{'loss': 1.8882, 'learning_rate': 0.00019783198250455047, 'epoch': 0.11}        \n",
      "{'loss': 2.2138, 'learning_rate': 0.00019783133739565862, 'epoch': 0.11}        \n",
      "{'loss': 1.7557, 'learning_rate': 0.00019783069219185485, 'epoch': 0.11}        \n",
      "{'loss': 1.8523, 'learning_rate': 0.00019783004689313978, 'epoch': 0.11}        \n",
      "{'loss': 2.1001, 'learning_rate': 0.00019782940149951398, 'epoch': 0.11}        \n",
      "{'loss': 1.5975, 'learning_rate': 0.00019782875601097815, 'epoch': 0.11}        \n",
      "{'loss': 2.0533, 'learning_rate': 0.00019782811042753286, 'epoch': 0.11}        \n",
      "{'loss': 1.9751, 'learning_rate': 0.00019782746474917876, 'epoch': 0.11}        \n",
      "{'loss': 1.8112, 'learning_rate': 0.00019782681897591647, 'epoch': 0.11}        \n",
      "{'loss': 2.1197, 'learning_rate': 0.00019782617310774666, 'epoch': 0.11}        \n",
      "{'loss': 2.0294, 'learning_rate': 0.0001978255271446699, 'epoch': 0.11}         \n",
      "{'loss': 2.0185, 'learning_rate': 0.0001978248810866868, 'epoch': 0.11}         \n",
      "{'loss': 2.0715, 'learning_rate': 0.00019782423493379804, 'epoch': 0.11}        \n",
      " 11%|███                        | 121601/1061708 [18:12:19<138:39:09,  1.88it/s][2024-03-01 12:19:19,661] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 121602/1061708 [18:12:19<130:06:27,  2.01it/s][2024-03-01 12:19:20,083] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1119, 'learning_rate': 0.00019782371794315539, 'epoch': 0.11}        \n",
      "{'loss': 1.9784, 'learning_rate': 0.000197823071619438, 'epoch': 0.11}          \n",
      "{'loss': 1.7673, 'learning_rate': 0.00019782242520081664, 'epoch': 0.11}        \n",
      "{'loss': 1.9458, 'learning_rate': 0.00019782177868729204, 'epoch': 0.11}        \n",
      "{'loss': 2.2067, 'learning_rate': 0.00019782113207886473, 'epoch': 0.11}        \n",
      " 11%|███                        | 121659/1061708 [18:12:49<138:12:29,  1.89it/s][2024-03-01 12:19:50,359] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0418, 'learning_rate': 0.00019782055005013893, 'epoch': 0.11}        \n",
      "{'loss': 2.2901, 'learning_rate': 0.00019781990326139828, 'epoch': 0.11}        \n",
      "{'loss': 2.0891, 'learning_rate': 0.00019781925637775682, 'epoch': 0.11}        \n",
      "{'loss': 2.0494, 'learning_rate': 0.0001978186093992151, 'epoch': 0.11}         \n",
      "{'loss': 1.6695, 'learning_rate': 0.00019781796232577382, 'epoch': 0.11}        \n",
      "{'loss': 2.0243, 'learning_rate': 0.00019781731515743352, 'epoch': 0.11}        \n",
      "{'loss': 2.102, 'learning_rate': 0.00019781666789419494, 'epoch': 0.11}         \n",
      "{'loss': 1.7842, 'learning_rate': 0.00019781602053605863, 'epoch': 0.11}        \n",
      "{'loss': 2.3095, 'learning_rate': 0.00019781537308302522, 'epoch': 0.11}        \n",
      "{'loss': 2.2222, 'learning_rate': 0.00019781472553509537, 'epoch': 0.11}        \n",
      "{'loss': 2.3354, 'learning_rate': 0.00019781407789226972, 'epoch': 0.11}        \n",
      " 11%|███                        | 121765/1061708 [18:13:46<139:50:13,  1.87it/s][2024-03-01 12:20:46,827] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9465, 'learning_rate': 0.0001978134949325912, 'epoch': 0.11}         \n",
      "{'loss': 1.9137, 'learning_rate': 0.00019781284710946518, 'epoch': 0.11}        \n",
      "{'loss': 1.6901, 'learning_rate': 0.00019781219919144514, 'epoch': 0.11}        \n",
      "{'loss': 2.0824, 'learning_rate': 0.00019781155117853176, 'epoch': 0.11}        \n",
      "{'loss': 1.8294, 'learning_rate': 0.0001978109030707256, 'epoch': 0.11}         \n",
      "{'loss': 1.9614, 'learning_rate': 0.00019781025486802735, 'epoch': 0.11}        \n",
      "{'loss': 2.032, 'learning_rate': 0.00019780960657043758, 'epoch': 0.11}         \n",
      "{'loss': 1.9738, 'learning_rate': 0.00019780895817795698, 'epoch': 0.11}        \n",
      "{'loss': 2.0883, 'learning_rate': 0.00019780830969058614, 'epoch': 0.11}        \n",
      "{'loss': 1.732, 'learning_rate': 0.0001978076611083257, 'epoch': 0.11}          \n",
      " 11%|███                        | 121866/1061708 [18:14:40<139:39:55,  1.87it/s][2024-03-01 12:21:40,728] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 11%|███                        | 121867/1061708 [18:14:40<130:48:32,  2.00it/s][2024-03-01 12:21:41,151] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8482, 'learning_rate': 0.00019780714217419726, 'epoch': 0.11}        \n",
      "{'loss': 2.0711, 'learning_rate': 0.00019780649342113712, 'epoch': 0.11}        \n",
      "{'loss': 1.7694, 'learning_rate': 0.00019780584457318914, 'epoch': 0.11}        \n",
      " 11%|███                        | 121896/1061708 [18:14:56<139:23:42,  1.87it/s][2024-03-01 12:21:56,547] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8671, 'learning_rate': 0.00019780526052890735, 'epoch': 0.11}        \n",
      "{'loss': 2.0137, 'learning_rate': 0.00019780461150067423, 'epoch': 0.11}        \n",
      "{'loss': 2.3336, 'learning_rate': 0.00019780396237755506, 'epoch': 0.11}        \n",
      "{'loss': 1.8996, 'learning_rate': 0.00019780331315955051, 'epoch': 0.11}        \n",
      "{'loss': 2.1111, 'learning_rate': 0.00019780266384666122, 'epoch': 0.11}        \n",
      "{'loss': 1.9447, 'learning_rate': 0.0001978020144388878, 'epoch': 0.11}         \n",
      "{'loss': 1.8188, 'learning_rate': 0.00019780136493623086, 'epoch': 0.11}        \n",
      "{'loss': 2.1579, 'learning_rate': 0.00019780071533869112, 'epoch': 0.11}        \n",
      "{'loss': 2.5242, 'learning_rate': 0.00019780006564626908, 'epoch': 0.11}        \n",
      "{'loss': 2.1485, 'learning_rate': 0.00019779941585896548, 'epoch': 0.11}        \n",
      " 11%|███                        | 121999/1061708 [18:15:50<138:56:56,  1.88it/s][2024-03-01 12:22:51,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=122000, skipped=1426, lr=[0.00019779876597678087], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 12:22:51,610] [INFO] [timer.py:260:stop] epoch=0/micro_step=122000/global_step=122000, RunningAvgSamplesPerSec=1.8926890506222462, CurrSamplesPerSec=1.8973900337696472, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.011, 'learning_rate': 0.00019779876597678087, 'epoch': 0.11}         \n",
      "{'loss': 2.1265, 'learning_rate': 0.00019779811599971594, 'epoch': 0.11}        \n",
      "{'loss': 2.1458, 'learning_rate': 0.0001977974659277713, 'epoch': 0.11}         \n",
      "{'loss': 1.7289, 'learning_rate': 0.00019779681576094756, 'epoch': 0.11}        \n",
      "{'loss': 2.1519, 'learning_rate': 0.00019779616549924539, 'epoch': 0.11}        \n",
      "{'loss': 2.0869, 'learning_rate': 0.00019779551514266535, 'epoch': 0.11}        \n",
      "{'loss': 2.5418, 'learning_rate': 0.00019779486469120817, 'epoch': 0.11}        \n",
      "{'loss': 1.9932, 'learning_rate': 0.0001977942141448744, 'epoch': 0.11}         \n",
      "{'loss': 1.8351, 'learning_rate': 0.00019779356350366475, 'epoch': 0.11}        \n",
      "{'loss': 1.8124, 'learning_rate': 0.00019779291276757977, 'epoch': 0.11}        \n",
      " 12%|███                        | 122097/1061708 [18:16:43<139:40:47,  1.87it/s][2024-03-01 12:23:43,899] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███                        | 122098/1061708 [18:16:43<130:58:37,  1.99it/s][2024-03-01 12:23:44,323] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0148, 'learning_rate': 0.000197792392110402, 'epoch': 0.12}          \n",
      "{'loss': 1.8632, 'learning_rate': 0.0001977917412035431, 'epoch': 0.12}         \n",
      "{'loss': 2.149, 'learning_rate': 0.00019779109020181065, 'epoch': 0.12}         \n",
      "{'loss': 2.1197, 'learning_rate': 0.0001977904391052053, 'epoch': 0.12}         \n",
      "{'loss': 2.1921, 'learning_rate': 0.0001977897879137277, 'epoch': 0.12}         \n",
      " 12%|███                        | 122148/1061708 [18:17:10<138:53:13,  1.88it/s][2024-03-01 12:24:10,951] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1143, 'learning_rate': 0.0001977892017602826, 'epoch': 0.12}         \n",
      "{'loss': 1.9532, 'learning_rate': 0.00019778855038854944, 'epoch': 0.12}        \n",
      "{'loss': 1.8987, 'learning_rate': 0.00019778789892194585, 'epoch': 0.12}        \n",
      "{'loss': 1.9354, 'learning_rate': 0.00019778724736047246, 'epoch': 0.12}        \n",
      "{'loss': 2.1888, 'learning_rate': 0.00019778659570412988, 'epoch': 0.12}        \n",
      "{'loss': 2.0165, 'learning_rate': 0.00019778594395291875, 'epoch': 0.12}        \n",
      "{'loss': 2.2775, 'learning_rate': 0.00019778529210683977, 'epoch': 0.12}        \n",
      "{'loss': 2.3176, 'learning_rate': 0.00019778464016589347, 'epoch': 0.12}        \n",
      "{'loss': 2.1004, 'learning_rate': 0.00019778398813008057, 'epoch': 0.12}        \n",
      "{'loss': 2.1242, 'learning_rate': 0.00019778333599940166, 'epoch': 0.12}        \n",
      "{'loss': 1.7928, 'learning_rate': 0.00019778268377385733, 'epoch': 0.12}        \n",
      "{'loss': 1.9174, 'learning_rate': 0.0001977820314534483, 'epoch': 0.12}         \n",
      "{'loss': 1.9955, 'learning_rate': 0.00019778137903817515, 'epoch': 0.12}        \n",
      "{'loss': 2.0842, 'learning_rate': 0.00019778072652803852, 'epoch': 0.12}        \n",
      "{'loss': 2.0045, 'learning_rate': 0.00019778007392303904, 'epoch': 0.12}        \n",
      "{'loss': 1.8441, 'learning_rate': 0.00019777942122317738, 'epoch': 0.12}        \n",
      "{'loss': 2.0546, 'learning_rate': 0.00019777876842845413, 'epoch': 0.12}        \n",
      "{'loss': 1.9596, 'learning_rate': 0.00019777811553886996, 'epoch': 0.12}        \n",
      "{'loss': 2.1185, 'learning_rate': 0.00019777746255442545, 'epoch': 0.12}        \n",
      "{'loss': 2.327, 'learning_rate': 0.0001977768094751213, 'epoch': 0.12}          \n",
      " 12%|███                        | 122349/1061708 [18:18:57<138:32:46,  1.88it/s][2024-03-01 12:25:58,199] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.865, 'learning_rate': 0.00019777622162264302, 'epoch': 0.12}         \n",
      " 12%|███                        | 122350/1061708 [18:18:58<130:00:05,  2.01it/s][2024-03-01 12:25:58,624] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0403, 'learning_rate': 0.00019777563369332941, 'epoch': 0.12}        \n",
      "{'loss': 1.9346, 'learning_rate': 0.0001977749803484215, 'epoch': 0.12}         \n",
      "{'loss': 1.8984, 'learning_rate': 0.00019777432690865635, 'epoch': 0.12}        \n",
      "{'loss': 2.0001, 'learning_rate': 0.00019777367337403456, 'epoch': 0.12}        \n",
      "{'loss': 2.2996, 'learning_rate': 0.00019777301974455676, 'epoch': 0.12}        \n",
      "{'loss': 2.2771, 'learning_rate': 0.00019777236602022357, 'epoch': 0.12}        \n",
      " 12%|███                        | 122414/1061708 [18:19:32<140:27:25,  1.86it/s][2024-03-01 12:26:32,634] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1156, 'learning_rate': 0.00019777177758722295, 'epoch': 0.12}        \n",
      "{'loss': 1.9978, 'learning_rate': 0.00019777112368266634, 'epoch': 0.12}        \n",
      "{'loss': 2.006, 'learning_rate': 0.0001977704696832562, 'epoch': 0.12}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1301, 'learning_rate': 0.00019776981558899316, 'epoch': 0.12}        \n",
      "{'loss': 1.8985, 'learning_rate': 0.0001977691613998779, 'epoch': 0.12}         \n",
      "{'loss': 2.0955, 'learning_rate': 0.000197768507115911, 'epoch': 0.12}          \n",
      "{'loss': 2.0445, 'learning_rate': 0.0001977678527370931, 'epoch': 0.12}         \n",
      "{'loss': 1.8928, 'learning_rate': 0.00019776719826342483, 'epoch': 0.12}        \n",
      "{'loss': 2.1542, 'learning_rate': 0.0001977665436949069, 'epoch': 0.12}         \n",
      "{'loss': 1.9342, 'learning_rate': 0.00019776588903153985, 'epoch': 0.12}        \n",
      "{'loss': 1.6959, 'learning_rate': 0.00019776523427332437, 'epoch': 0.12}        \n",
      "{'loss': 1.9637, 'learning_rate': 0.00019776457942026105, 'epoch': 0.12}        \n",
      "{'loss': 2.3198, 'learning_rate': 0.00019776392447235058, 'epoch': 0.12}        \n",
      "{'loss': 2.0672, 'learning_rate': 0.00019776326942959357, 'epoch': 0.12}        \n",
      "{'loss': 2.1873, 'learning_rate': 0.00019776261429199063, 'epoch': 0.12}        \n",
      "{'loss': 1.9276, 'learning_rate': 0.00019776195905954244, 'epoch': 0.12}        \n",
      "{'loss': 2.0215, 'learning_rate': 0.0001977613037322496, 'epoch': 0.12}         \n",
      "{'loss': 2.0278, 'learning_rate': 0.0001977606483101128, 'epoch': 0.12}         \n",
      "{'loss': 2.2422, 'learning_rate': 0.00019775999279313258, 'epoch': 0.12}        \n",
      "{'loss': 2.1523, 'learning_rate': 0.00019775933718130966, 'epoch': 0.12}        \n",
      " 12%|███                        | 122615/1061708 [18:21:19<139:40:57,  1.87it/s][2024-03-01 12:28:19,696] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███                        | 122616/1061708 [18:21:19<130:48:37,  1.99it/s][2024-03-01 12:28:20,118] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.819, 'learning_rate': 0.00019775881262356498, 'epoch': 0.12}         \n",
      "{'loss': 1.7017, 'learning_rate': 0.00019775815684102676, 'epoch': 0.12}        \n",
      "{'loss': 2.1719, 'learning_rate': 0.0001977575009636476, 'epoch': 0.12}         \n",
      "{'loss': 1.9792, 'learning_rate': 0.00019775684499142811, 'epoch': 0.12}        \n",
      "{'loss': 2.0442, 'learning_rate': 0.00019775618892436896, 'epoch': 0.12}        \n",
      "{'loss': 2.18, 'learning_rate': 0.00019775553276247076, 'epoch': 0.12}          \n",
      "{'loss': 2.0027, 'learning_rate': 0.00019775487650573417, 'epoch': 0.12}        \n",
      "{'loss': 2.564, 'learning_rate': 0.00019775422015415984, 'epoch': 0.12}         \n",
      "{'loss': 1.9231, 'learning_rate': 0.00019775356370774834, 'epoch': 0.12}        \n",
      "{'loss': 2.0217, 'learning_rate': 0.00019775290716650038, 'epoch': 0.12}        \n",
      " 12%|███                        | 122717/1061708 [18:22:13<138:34:09,  1.88it/s][2024-03-01 12:29:13,925] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███                        | 122718/1061708 [18:22:13<129:57:29,  2.01it/s][2024-03-01 12:29:14,347] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9276, 'learning_rate': 0.00019775238186522015, 'epoch': 0.12}        \n",
      "{'loss': 1.8252, 'learning_rate': 0.00019775172515326813, 'epoch': 0.12}        \n",
      " 12%|███                        | 122731/1061708 [18:22:20<138:01:47,  1.89it/s][2024-03-01 12:29:21,168] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0647, 'learning_rate': 0.00019775113403142758, 'epoch': 0.12}        \n",
      "{'loss': 2.2752, 'learning_rate': 0.00019775047713929014, 'epoch': 0.12}        \n",
      "{'loss': 2.1803, 'learning_rate': 0.00019774982015231918, 'epoch': 0.12}        \n",
      "{'loss': 2.1462, 'learning_rate': 0.0001977491630705154, 'epoch': 0.12}         \n",
      "{'loss': 2.037, 'learning_rate': 0.00019774850589387936, 'epoch': 0.12}         \n",
      "{'loss': 1.9758, 'learning_rate': 0.00019774784862241175, 'epoch': 0.12}        \n",
      "{'loss': 2.2637, 'learning_rate': 0.00019774719125611323, 'epoch': 0.12}        \n",
      " 12%|███                        | 122800/1061708 [18:22:57<138:05:12,  1.89it/s][2024-03-01 12:29:57,926] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.0054, 'learning_rate': 0.0001977465995453646, 'epoch': 0.12}         \n",
      "{'loss': 1.8124, 'learning_rate': 0.000197745941998889, 'epoch': 0.12}          \n",
      "{'loss': 2.3304, 'learning_rate': 0.00019774528435758434, 'epoch': 0.12}        \n",
      "{'loss': 1.8538, 'learning_rate': 0.0001977446266214512, 'epoch': 0.12}         \n",
      "{'loss': 1.9229, 'learning_rate': 0.00019774396879049025, 'epoch': 0.12}        \n",
      "{'loss': 1.6913, 'learning_rate': 0.0001977433108647021, 'epoch': 0.12}         \n",
      "{'loss': 2.2061, 'learning_rate': 0.00019774265284408743, 'epoch': 0.12}        \n",
      "{'loss': 2.1315, 'learning_rate': 0.00019774199472864684, 'epoch': 0.12}        \n",
      "{'loss': 2.026, 'learning_rate': 0.00019774133651838098, 'epoch': 0.12}         \n",
      "{'loss': 2.2095, 'learning_rate': 0.0001977406782132905, 'epoch': 0.12}         \n",
      "{'loss': 2.1032, 'learning_rate': 0.00019774001981337605, 'epoch': 0.12}        \n",
      "{'loss': 1.6578, 'learning_rate': 0.00019773936131863823, 'epoch': 0.12}        \n",
      "{'loss': 2.0933, 'learning_rate': 0.0001977387027290777, 'epoch': 0.12}         \n",
      "{'loss': 1.8424, 'learning_rate': 0.0001977380440446951, 'epoch': 0.12}         \n",
      "{'loss': 2.0064, 'learning_rate': 0.00019773738526549105, 'epoch': 0.12}        \n",
      "{'loss': 1.8721, 'learning_rate': 0.00019773672639146624, 'epoch': 0.12}        \n",
      "{'loss': 2.0924, 'learning_rate': 0.00019773606742262123, 'epoch': 0.12}        \n",
      "{'loss': 2.3762, 'learning_rate': 0.00019773540835895675, 'epoch': 0.12}        \n",
      "{'loss': 1.8391, 'learning_rate': 0.00019773474920047336, 'epoch': 0.12}        \n",
      "{'loss': 2.3956, 'learning_rate': 0.00019773408994717176, 'epoch': 0.12}        \n",
      "{'loss': 2.2915, 'learning_rate': 0.00019773343059905255, 'epoch': 0.12}        \n",
      "{'loss': 1.9653, 'learning_rate': 0.00019773277115611636, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123020/1061708 [18:24:54<138:03:52,  1.89it/s][2024-03-01 12:31:55,055] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1757, 'learning_rate': 0.0001977321775764059, 'epoch': 0.12}         \n",
      "{'loss': 2.0129, 'learning_rate': 0.00019773151795331924, 'epoch': 0.12}        \n",
      "{'loss': 2.0716, 'learning_rate': 0.0001977308582354175, 'epoch': 0.12}         \n",
      "{'loss': 2.0219, 'learning_rate': 0.00019773019842270133, 'epoch': 0.12}        \n",
      "{'loss': 2.1217, 'learning_rate': 0.00019772953851517132, 'epoch': 0.12}        \n",
      "{'loss': 1.6762, 'learning_rate': 0.00019772887851282812, 'epoch': 0.12}        \n",
      "{'loss': 1.7141, 'learning_rate': 0.0001977282184156724, 'epoch': 0.12}         \n",
      "{'loss': 2.1318, 'learning_rate': 0.00019772755822370475, 'epoch': 0.12}        \n",
      "{'loss': 1.883, 'learning_rate': 0.00019772689793692586, 'epoch': 0.12}         \n",
      "{'loss': 2.5116, 'learning_rate': 0.00019772623755533636, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123121/1061708 [18:25:48<138:02:18,  1.89it/s][2024-03-01 12:32:48,755] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 123122/1061708 [18:25:48<129:33:35,  2.01it/s][2024-03-01 12:32:49,177] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1669, 'learning_rate': 0.00019772570918180152, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123134/1061708 [18:25:55<142:05:10,  1.83it/s][2024-03-01 12:32:55,548] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.008, 'learning_rate': 0.0001977251146890456, 'epoch': 0.12}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0324, 'learning_rate': 0.00019772445405147064, 'epoch': 0.12}        \n",
      "{'loss': 2.3561, 'learning_rate': 0.00019772379331908741, 'epoch': 0.12}        \n",
      "{'loss': 2.0381, 'learning_rate': 0.00019772313249189657, 'epoch': 0.12}        \n",
      "{'loss': 2.3119, 'learning_rate': 0.00019772247156989875, 'epoch': 0.12}        \n",
      "{'loss': 2.2633, 'learning_rate': 0.00019772181055309463, 'epoch': 0.12}        \n",
      "{'loss': 2.1803, 'learning_rate': 0.00019772114944148478, 'epoch': 0.12}        \n",
      "{'loss': 2.3877, 'learning_rate': 0.00019772048823506994, 'epoch': 0.12}        \n",
      "{'loss': 1.9057, 'learning_rate': 0.00019771982693385067, 'epoch': 0.12}        \n",
      "{'loss': 1.9782, 'learning_rate': 0.00019771916553782765, 'epoch': 0.12}        \n",
      "{'loss': 2.1891, 'learning_rate': 0.00019771850404700153, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123247/1061708 [18:26:55<138:32:10,  1.88it/s][2024-03-01 12:33:55,670] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9306, 'learning_rate': 0.00019771790862420184, 'epoch': 0.12}        \n",
      "{'loss': 2.0353, 'learning_rate': 0.00019771724695325155, 'epoch': 0.12}        \n",
      "{'loss': 1.9206, 'learning_rate': 0.0001977165851875, 'epoch': 0.12}            \n",
      "{'loss': 2.0131, 'learning_rate': 0.00019771592332694783, 'epoch': 0.12}        \n",
      "{'loss': 2.1105, 'learning_rate': 0.0001977152613715957, 'epoch': 0.12}         \n",
      "{'loss': 2.1139, 'learning_rate': 0.00019771459932144424, 'epoch': 0.12}        \n",
      "{'loss': 1.8814, 'learning_rate': 0.00019771393717649407, 'epoch': 0.12}        \n",
      "{'loss': 2.0907, 'learning_rate': 0.00019771327493674586, 'epoch': 0.12}        \n",
      "{'loss': 1.5205, 'learning_rate': 0.00019771261260220023, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123331/1061708 [18:27:39<137:56:14,  1.89it/s][2024-03-01 12:34:40,326] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8953, 'learning_rate': 0.00019771201642005797, 'epoch': 0.12}        \n",
      "{'loss': 2.1453, 'learning_rate': 0.00019771135390539906, 'epoch': 0.12}        \n",
      "{'loss': 1.4769, 'learning_rate': 0.0001977106912959446, 'epoch': 0.12}         \n",
      "{'loss': 1.7607, 'learning_rate': 0.00019771002859169528, 'epoch': 0.12}        \n",
      "{'loss': 2.0723, 'learning_rate': 0.0001977093657926517, 'epoch': 0.12}         \n",
      "{'loss': 1.6738, 'learning_rate': 0.00019770870289881446, 'epoch': 0.12}        \n",
      "{'loss': 1.9145, 'learning_rate': 0.0001977080399101843, 'epoch': 0.12}         \n",
      "{'loss': 1.822, 'learning_rate': 0.00019770737682676178, 'epoch': 0.12}         \n",
      "{'loss': 2.3519, 'learning_rate': 0.0001977067136485476, 'epoch': 0.12}         \n",
      "{'loss': 1.7342, 'learning_rate': 0.0001977060503755424, 'epoch': 0.12}         \n",
      "{'loss': 1.8462, 'learning_rate': 0.0001977053870077468, 'epoch': 0.12}         \n",
      "{'loss': 1.9271, 'learning_rate': 0.00019770472354516144, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123459/1061708 [18:28:47<138:05:20,  1.89it/s][2024-03-01 12:35:48,461] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9495, 'learning_rate': 0.00019770412634778989, 'epoch': 0.12}        \n",
      "{'loss': 2.11, 'learning_rate': 0.0001977034627051058, 'epoch': 0.12}           \n",
      "{'loss': 1.828, 'learning_rate': 0.0001977027989676338, 'epoch': 0.12}          \n",
      "{'loss': 1.7854, 'learning_rate': 0.00019770213513537456, 'epoch': 0.12}        \n",
      "{'loss': 2.005, 'learning_rate': 0.00019770147120832873, 'epoch': 0.12}         \n",
      "{'loss': 2.0939, 'learning_rate': 0.00019770080718649693, 'epoch': 0.12}        \n",
      "{'loss': 2.191, 'learning_rate': 0.00019770014306987984, 'epoch': 0.12}         \n",
      "{'loss': 2.1912, 'learning_rate': 0.0001976994788584781, 'epoch': 0.12}         \n",
      "{'loss': 2.1185, 'learning_rate': 0.00019769881455229232, 'epoch': 0.12}        \n",
      "{'loss': 2.0383, 'learning_rate': 0.00019769815015132315, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123550/1061708 [18:29:36<138:06:24,  1.89it/s][2024-03-01 12:36:36,867] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0769, 'learning_rate': 0.00019769755210941167, 'epoch': 0.12}        \n",
      "{'loss': 1.8589, 'learning_rate': 0.0001976968875283559, 'epoch': 0.12}         \n",
      "{'loss': 2.2041, 'learning_rate': 0.00019769622285251858, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123582/1061708 [18:29:53<138:59:24,  1.87it/s][2024-03-01 12:36:53,867] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.8636, 'learning_rate': 0.00019769562456322737, 'epoch': 0.12}        \n",
      "{'loss': 2.1442, 'learning_rate': 0.000197694959707307, 'epoch': 0.12}          \n",
      "{'loss': 1.8552, 'learning_rate': 0.000197694294756607, 'epoch': 0.12}          \n",
      "{'loss': 1.9822, 'learning_rate': 0.000197693629711128, 'epoch': 0.12}          \n",
      "{'loss': 1.9298, 'learning_rate': 0.00019769296457087068, 'epoch': 0.12}        \n",
      "{'loss': 1.8766, 'learning_rate': 0.00019769229933583562, 'epoch': 0.12}        \n",
      "{'loss': 2.0069, 'learning_rate': 0.00019769163400602352, 'epoch': 0.12}        \n",
      "{'loss': 1.9169, 'learning_rate': 0.00019769096858143504, 'epoch': 0.12}        \n",
      "{'loss': 2.2539, 'learning_rate': 0.00019769030306207076, 'epoch': 0.12}        \n",
      "{'loss': 2.0785, 'learning_rate': 0.00019768963744793138, 'epoch': 0.12}        \n",
      "{'loss': 1.9332, 'learning_rate': 0.00019768897173901752, 'epoch': 0.12}        \n",
      "{'loss': 1.7852, 'learning_rate': 0.00019768830593532987, 'epoch': 0.12}        \n",
      "{'loss': 1.5329, 'learning_rate': 0.00019768764003686898, 'epoch': 0.12}        \n",
      "{'loss': 1.5957, 'learning_rate': 0.0001976869740436356, 'epoch': 0.12}         \n",
      "{'loss': 1.8245, 'learning_rate': 0.00019768630795563032, 'epoch': 0.12}        \n",
      "{'loss': 2.354, 'learning_rate': 0.00019768564177285382, 'epoch': 0.12}         \n",
      "{'loss': 1.8014, 'learning_rate': 0.00019768497549530668, 'epoch': 0.12}        \n",
      "{'loss': 2.0316, 'learning_rate': 0.00019768430912298963, 'epoch': 0.12}        \n",
      "{'loss': 1.8483, 'learning_rate': 0.00019768364265590328, 'epoch': 0.12}        \n",
      "{'loss': 2.2395, 'learning_rate': 0.00019768297609404825, 'epoch': 0.12}        \n",
      "{'loss': 1.8899, 'learning_rate': 0.0001976823094374252, 'epoch': 0.12}         \n",
      "{'loss': 2.242, 'learning_rate': 0.00019768164268603485, 'epoch': 0.12}         \n",
      "{'loss': 1.9789, 'learning_rate': 0.00019768097583987773, 'epoch': 0.12}        \n",
      "{'loss': 2.1296, 'learning_rate': 0.00019768030889895456, 'epoch': 0.12}        \n",
      "{'loss': 2.0193, 'learning_rate': 0.00019767964186326596, 'epoch': 0.12}        \n",
      "{'loss': 1.854, 'learning_rate': 0.00019767897473281257, 'epoch': 0.12}         \n",
      "{'loss': 1.8877, 'learning_rate': 0.0001976783075075951, 'epoch': 0.12}         \n",
      "{'loss': 2.132, 'learning_rate': 0.00019767764018761415, 'epoch': 0.12}         \n",
      "{'loss': 1.9514, 'learning_rate': 0.00019767697277287032, 'epoch': 0.12}        \n",
      "{'loss': 1.938, 'learning_rate': 0.00019767630526336435, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 123883/1061708 [18:32:33<141:17:55,  1.84it/s][2024-03-01 12:39:34,152] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 123884/1061708 [18:32:34<131:53:50,  1.98it/s][2024-03-01 12:39:34,575] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2068, 'learning_rate': 0.00019767577118753123, 'epoch': 0.12}        \n",
      "{'loss': 2.0609, 'learning_rate': 0.00019767510350745493, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123906/1061708 [18:32:45<138:59:13,  1.87it/s][2024-03-01 12:39:46,243] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9394, 'learning_rate': 0.00019767450251436612, 'epoch': 0.12}        \n",
      "{'loss': 2.2904, 'learning_rate': 0.00019767383465424566, 'epoch': 0.12}        \n",
      "{'loss': 2.024, 'learning_rate': 0.00019767316669936608, 'epoch': 0.12}         \n",
      "{'loss': 2.0771, 'learning_rate': 0.00019767249864972804, 'epoch': 0.12}        \n",
      "{'loss': 2.5561, 'learning_rate': 0.0001976718305053321, 'epoch': 0.12}         \n",
      "{'loss': 1.8888, 'learning_rate': 0.00019767116226617902, 'epoch': 0.12}        \n",
      "{'loss': 2.0979, 'learning_rate': 0.00019767049393226932, 'epoch': 0.12}        \n",
      "{'loss': 1.6359, 'learning_rate': 0.0001976698255036038, 'epoch': 0.12}         \n",
      "{'loss': 1.9724, 'learning_rate': 0.00019766915698018298, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 123999/1061708 [18:33:35<137:56:04,  1.89it/s][2024-03-01 12:40:35,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=124000, skipped=1450, lr=[0.00019766848836200756], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 12:40:35,739] [INFO] [timer.py:260:stop] epoch=0/micro_step=124000/global_step=124000, RunningAvgSamplesPerSec=1.8927219560496789, CurrSamplesPerSec=1.9066101483038809, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.3788, 'learning_rate': 0.00019766848836200756, 'epoch': 0.12}        \n",
      "{'loss': 2.3104, 'learning_rate': 0.00019766781964907821, 'epoch': 0.12}        \n",
      "{'loss': 1.8655, 'learning_rate': 0.00019766715084139553, 'epoch': 0.12}        \n",
      "{'loss': 2.2247, 'learning_rate': 0.0001976664819389602, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 124033/1061708 [18:33:53<141:02:04,  1.85it/s][2024-03-01 12:40:53,771] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7995, 'learning_rate': 0.00019766587984575544, 'epoch': 0.12}        \n",
      "{'loss': 1.6715, 'learning_rate': 0.00019766521076329188, 'epoch': 0.12}        \n",
      "{'loss': 2.4103, 'learning_rate': 0.0001976645415860775, 'epoch': 0.12}         \n",
      "{'loss': 2.2785, 'learning_rate': 0.00019766387231411303, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 124076/1061708 [18:34:16<138:38:21,  1.88it/s][2024-03-01 12:41:16,610] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.113, 'learning_rate': 0.0001976632698883342, 'epoch': 0.12}          \n",
      "{'loss': 2.2637, 'learning_rate': 0.00019766260043634627, 'epoch': 0.12}        \n",
      "{'loss': 2.2385, 'learning_rate': 0.00019766193088961007, 'epoch': 0.12}        \n",
      "{'loss': 2.0954, 'learning_rate': 0.00019766126124812634, 'epoch': 0.12}        \n",
      "{'loss': 1.6959, 'learning_rate': 0.00019766059151189565, 'epoch': 0.12}        \n",
      "{'loss': 2.3769, 'learning_rate': 0.00019765992168091864, 'epoch': 0.12}        \n",
      "{'loss': 1.894, 'learning_rate': 0.00019765925175519602, 'epoch': 0.12}         \n",
      "{'loss': 2.1146, 'learning_rate': 0.00019765858173472837, 'epoch': 0.12}        \n",
      "{'loss': 2.2883, 'learning_rate': 0.0001976579116195164, 'epoch': 0.12}         \n",
      "{'loss': 2.4587, 'learning_rate': 0.00019765724140956075, 'epoch': 0.12}        \n",
      "{'loss': 2.5591, 'learning_rate': 0.00019765657110486204, 'epoch': 0.12}        \n",
      "{'loss': 1.9278, 'learning_rate': 0.00019765590070542098, 'epoch': 0.12}        \n",
      "{'loss': 2.0749, 'learning_rate': 0.00019765523021123812, 'epoch': 0.12}        \n",
      "{'loss': 2.2294, 'learning_rate': 0.0001976545596223142, 'epoch': 0.12}         \n",
      "{'loss': 2.1755, 'learning_rate': 0.00019765388893864985, 'epoch': 0.12}        \n",
      "{'loss': 2.5648, 'learning_rate': 0.0001976532181602457, 'epoch': 0.12}         \n",
      "{'loss': 2.0155, 'learning_rate': 0.0001976525472871024, 'epoch': 0.12}         \n",
      "{'loss': 1.9764, 'learning_rate': 0.0001976518763192206, 'epoch': 0.12}         \n",
      "{'loss': 2.0075, 'learning_rate': 0.000197651205256601, 'epoch': 0.12}          \n",
      "{'loss': 1.8375, 'learning_rate': 0.0001976505340992442, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 124277/1061708 [18:36:03<138:18:56,  1.88it/s][2024-03-01 12:43:03,652] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 124278/1061708 [18:36:03<129:44:25,  2.01it/s][2024-03-01 12:43:04,076] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1899, 'learning_rate': 0.0001976499971051484, 'epoch': 0.12}         \n",
      "{'loss': 2.2033, 'learning_rate': 0.0001976493257772663, 'epoch': 0.12}         \n",
      "{'loss': 2.1605, 'learning_rate': 0.00019764865435464884, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 124300/1061708 [18:36:15<137:52:57,  1.89it/s][2024-03-01 12:43:15,691] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4127, 'learning_rate': 0.00019764804999329493, 'epoch': 0.12}        \n",
      "{'loss': 1.888, 'learning_rate': 0.00019764737839068208, 'epoch': 0.12}         \n",
      "{'loss': 1.5373, 'learning_rate': 0.00019764670669333577, 'epoch': 0.12}        \n",
      "{'loss': 2.1063, 'learning_rate': 0.0001976460349012566, 'epoch': 0.12}         \n",
      "{'loss': 2.4346, 'learning_rate': 0.00019764536301444528, 'epoch': 0.12}        \n",
      "{'loss': 2.0982, 'learning_rate': 0.00019764469103290244, 'epoch': 0.12}        \n",
      "{'loss': 2.0108, 'learning_rate': 0.00019764401895662874, 'epoch': 0.12}        \n",
      "{'loss': 2.038, 'learning_rate': 0.00019764334678562483, 'epoch': 0.12}         \n",
      "{'loss': 1.5913, 'learning_rate': 0.00019764267451989135, 'epoch': 0.12}        \n",
      "{'loss': 1.862, 'learning_rate': 0.00019764200215942896, 'epoch': 0.12}         \n",
      "{'loss': 2.4658, 'learning_rate': 0.00019764132970423833, 'epoch': 0.12}        \n",
      "{'loss': 1.9714, 'learning_rate': 0.00019764065715432007, 'epoch': 0.12}        \n",
      "{'loss': 2.2827, 'learning_rate': 0.00019763998450967487, 'epoch': 0.12}        \n",
      "{'loss': 2.0794, 'learning_rate': 0.00019763931177030336, 'epoch': 0.12}        \n",
      "{'loss': 2.4561, 'learning_rate': 0.00019763863893620622, 'epoch': 0.12}        \n",
      "{'loss': 1.952, 'learning_rate': 0.00019763796600738408, 'epoch': 0.12}         \n",
      "{'loss': 2.2319, 'learning_rate': 0.00019763729298383757, 'epoch': 0.12}        \n",
      "{'loss': 2.3481, 'learning_rate': 0.00019763661986556743, 'epoch': 0.12}        \n",
      "{'loss': 2.076, 'learning_rate': 0.00019763594665257422, 'epoch': 0.12}         \n",
      "{'loss': 1.6598, 'learning_rate': 0.00019763527334485862, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 124501/1061708 [18:38:02<137:46:13,  1.89it/s][2024-03-01 12:45:02,678] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 124502/1061708 [18:38:02<129:17:34,  2.01it/s][2024-03-01 12:45:03,100] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1498, 'learning_rate': 0.00019763473463048646, 'epoch': 0.12}        \n",
      "{'loss': 2.3787, 'learning_rate': 0.00019763406115227224, 'epoch': 0.12}        \n",
      "{'loss': 2.8248, 'learning_rate': 0.00019763338757933742, 'epoch': 0.12}        \n",
      "{'loss': 1.9856, 'learning_rate': 0.00019763271391168275, 'epoch': 0.12}        \n",
      "{'loss': 2.2355, 'learning_rate': 0.0001976320401493088, 'epoch': 0.12}         \n",
      "{'loss': 2.1387, 'learning_rate': 0.0001976313662922163, 'epoch': 0.12}         \n",
      "{'loss': 2.0214, 'learning_rate': 0.00019763069234040583, 'epoch': 0.12}        \n",
      "{'loss': 2.0638, 'learning_rate': 0.0001976300182938781, 'epoch': 0.12}         \n",
      "{'loss': 2.0148, 'learning_rate': 0.0001976293441526337, 'epoch': 0.12}         \n",
      "{'loss': 2.1807, 'learning_rate': 0.00019762866991667334, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 124603/1061708 [18:38:56<140:44:32,  1.85it/s][2024-03-01 12:45:56,830] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|███▏                       | 124604/1061708 [18:38:56<131:26:44,  1.98it/s][2024-03-01 12:45:57,252] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9152, 'learning_rate': 0.00019762813045970997, 'epoch': 0.12}        \n",
      "{'loss': 1.8919, 'learning_rate': 0.0001976274560532625, 'epoch': 0.12}         \n",
      "{'loss': 1.6325, 'learning_rate': 0.0001976267815521009, 'epoch': 0.12}         \n",
      "{'loss': 2.1106, 'learning_rate': 0.00019762610695622579, 'epoch': 0.12}        \n",
      "{'loss': 1.8062, 'learning_rate': 0.00019762543226563785, 'epoch': 0.12}        \n",
      "{'loss': 2.0401, 'learning_rate': 0.00019762475748033774, 'epoch': 0.12}        \n",
      "{'loss': 2.2322, 'learning_rate': 0.00019762408260032608, 'epoch': 0.12}        \n",
      "{'loss': 2.0527, 'learning_rate': 0.00019762340762560356, 'epoch': 0.12}        \n",
      "{'loss': 2.1771, 'learning_rate': 0.0001976227325561708, 'epoch': 0.12}         \n",
      "{'loss': 2.1218, 'learning_rate': 0.0001976220573920285, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 124705/1061708 [18:39:50<139:09:45,  1.87it/s][2024-03-01 12:46:50,974] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 124706/1061708 [18:39:50<130:19:28,  2.00it/s][2024-03-01 12:46:51,396] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8495, 'learning_rate': 0.00019762151719252422, 'epoch': 0.12}        \n",
      "{'loss': 1.9353, 'learning_rate': 0.00019762084185790635, 'epoch': 0.12}        \n",
      "{'loss': 1.8301, 'learning_rate': 0.00019762016642858074, 'epoch': 0.12}        \n",
      "{'loss': 2.2604, 'learning_rate': 0.00019761949090454808, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 124749/1061708 [18:40:13<137:40:23,  1.89it/s][2024-03-01 12:47:14,235] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0854, 'learning_rate': 0.00019761888285194467, 'epoch': 0.12}        \n",
      "{'loss': 1.8816, 'learning_rate': 0.00019761820714797037, 'epoch': 0.12}        \n",
      "{'loss': 1.7172, 'learning_rate': 0.0001976175313492909, 'epoch': 0.12}         \n",
      "{'loss': 2.1216, 'learning_rate': 0.00019761685545590692, 'epoch': 0.12}        \n",
      "{'loss': 2.0973, 'learning_rate': 0.00019761617946781906, 'epoch': 0.12}        \n",
      "{'loss': 1.8822, 'learning_rate': 0.000197615503385028, 'epoch': 0.12}          \n",
      " 12%|███▏                       | 124800/1061708 [18:40:40<137:49:36,  1.89it/s][2024-03-01 12:47:41,296] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.7575, 'learning_rate': 0.00019761489482954535, 'epoch': 0.12}        \n",
      "{'loss': 1.987, 'learning_rate': 0.00019761421856682, 'epoch': 0.12}            \n",
      "{'loss': 2.0684, 'learning_rate': 0.00019761354220939337, 'epoch': 0.12}        \n",
      "{'loss': 2.401, 'learning_rate': 0.00019761286575726607, 'epoch': 0.12}         \n",
      "{'loss': 2.0511, 'learning_rate': 0.00019761218921043877, 'epoch': 0.12}        \n",
      "{'loss': 1.8423, 'learning_rate': 0.0001976115125689122, 'epoch': 0.12}         \n",
      "{'loss': 1.9853, 'learning_rate': 0.0001976108358326869, 'epoch': 0.12}         \n",
      "{'loss': 2.5164, 'learning_rate': 0.00019761015900176355, 'epoch': 0.12}        \n",
      "{'loss': 2.1937, 'learning_rate': 0.0001976094820761429, 'epoch': 0.12}         \n",
      "{'loss': 1.8265, 'learning_rate': 0.00019760880505582552, 'epoch': 0.12}        \n",
      "{'loss': 2.2741, 'learning_rate': 0.0001976081279408121, 'epoch': 0.12}         \n",
      "{'loss': 1.9189, 'learning_rate': 0.00019760745073110325, 'epoch': 0.12}        \n",
      "{'loss': 2.4046, 'learning_rate': 0.00019760677342669968, 'epoch': 0.12}        \n",
      "{'loss': 2.2088, 'learning_rate': 0.00019760609602760204, 'epoch': 0.12}        \n",
      "{'loss': 2.0229, 'learning_rate': 0.000197605418533811, 'epoch': 0.12}          \n",
      "{'loss': 1.7861, 'learning_rate': 0.00019760474094532713, 'epoch': 0.12}        \n",
      "{'loss': 1.7054, 'learning_rate': 0.0001976040632621512, 'epoch': 0.12}         \n",
      "{'loss': 1.9931, 'learning_rate': 0.00019760338548428381, 'epoch': 0.12}        \n",
      "{'loss': 2.0676, 'learning_rate': 0.00019760270761172563, 'epoch': 0.12}        \n",
      "{'loss': 1.9256, 'learning_rate': 0.00019760202964447729, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125000/1061708 [18:42:27<137:48:48,  1.89it/s][INFO|trainer.py:2868] 2024-03-01 12:49:27,342 >> Saving model checkpoint to output_model/checkpoint-125000\n",
      "[INFO|trainer.py:2880] 2024-03-01 12:49:27,345 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 12:49:28,597 >> tokenizer config file saved in output_model/checkpoint-125000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 12:49:28,597 >> Special tokens file saved in output_model/checkpoint-125000/special_tokens_map.json\n",
      "[2024-03-01 12:49:28,599] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step125000 is about to be saved!\n",
      "[2024-03-01 12:49:33,852] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-125000/global_step125000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 12:49:33,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-125000/global_step125000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 12:49:47,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-125000/global_step125000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 12:49:48,452] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-125000/global_step125000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 12:49:55,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-125000/global_step125000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 12:49:55,669] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-125000/global_step125000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 12:49:55,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step125000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 12:49:55,778 >> Deleting older checkpoint [output_model/checkpoint-110000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 12:49:59,566 >> tokenizer config file saved in output_model/checkpoint-125000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 12:49:59,566 >> Special tokens file saved in output_model/checkpoint-125000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 2.377, 'learning_rate': 0.0001976013515825395, 'epoch': 0.12}          \n",
      "{'loss': 2.2227, 'learning_rate': 0.00019760067342591286, 'epoch': 0.12}        \n",
      "{'loss': 2.3364, 'learning_rate': 0.00019759999517459807, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125039/1061708 [18:43:20<137:20:16,  1.89it/s][2024-03-01 12:50:20,584] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.5128, 'learning_rate': 0.00019759938466745695, 'epoch': 0.12}        \n",
      "{'loss': 1.8503, 'learning_rate': 0.00019759870623623646, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125054/1061708 [18:43:27<139:20:54,  1.87it/s][2024-03-01 12:50:28,460] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9317, 'learning_rate': 0.00019759809556718129, 'epoch': 0.12}        \n",
      "{'loss': 2.4443, 'learning_rate': 0.0001975974169560575, 'epoch': 0.12}         \n",
      "{'loss': 1.9778, 'learning_rate': 0.0001975967382502487, 'epoch': 0.12}         \n",
      "{'loss': 1.7124, 'learning_rate': 0.00019759605944975555, 'epoch': 0.12}        \n",
      "{'loss': 1.6896, 'learning_rate': 0.00019759538055457872, 'epoch': 0.12}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9088, 'learning_rate': 0.00019759470156471885, 'epoch': 0.12}        \n",
      "{'loss': 2.0649, 'learning_rate': 0.00019759402248017665, 'epoch': 0.12}        \n",
      "{'loss': 2.4066, 'learning_rate': 0.0001975933433009527, 'epoch': 0.12}         \n",
      "{'loss': 1.902, 'learning_rate': 0.00019759266402704772, 'epoch': 0.12}         \n",
      "{'loss': 1.9391, 'learning_rate': 0.00019759198465846235, 'epoch': 0.12}        \n",
      "{'loss': 1.7856, 'learning_rate': 0.00019759130519519724, 'epoch': 0.12}        \n",
      "{'loss': 2.2208, 'learning_rate': 0.00019759062563725306, 'epoch': 0.12}        \n",
      "{'loss': 2.1995, 'learning_rate': 0.00019758994598463047, 'epoch': 0.12}        \n",
      "{'loss': 2.0166, 'learning_rate': 0.0001975892662373301, 'epoch': 0.12}         \n",
      "{'loss': 1.9589, 'learning_rate': 0.0001975885863953527, 'epoch': 0.12}         \n",
      "{'loss': 2.0535, 'learning_rate': 0.0001975879064586988, 'epoch': 0.12}         \n",
      "{'loss': 2.4008, 'learning_rate': 0.00019758722642736914, 'epoch': 0.12}        \n",
      "{'loss': 2.0146, 'learning_rate': 0.00019758654630136436, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125230/1061708 [18:45:01<138:01:27,  1.88it/s][2024-03-01 12:52:02,471] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9675, 'learning_rate': 0.0001975859341070134, 'epoch': 0.12}         \n",
      "{'loss': 1.7911, 'learning_rate': 0.00019758525380112772, 'epoch': 0.12}        \n",
      "{'loss': 2.056, 'learning_rate': 0.00019758457340056883, 'epoch': 0.12}         \n",
      "{'loss': 2.1946, 'learning_rate': 0.00019758389290533743, 'epoch': 0.12}        \n",
      "{'loss': 2.0991, 'learning_rate': 0.00019758321231543412, 'epoch': 0.12}        \n",
      "{'loss': 2.0424, 'learning_rate': 0.00019758253163085963, 'epoch': 0.12}        \n",
      "{'loss': 2.2999, 'learning_rate': 0.0001975818508516145, 'epoch': 0.12}         \n",
      "{'loss': 2.2687, 'learning_rate': 0.00019758116997769954, 'epoch': 0.12}        \n",
      "{'loss': 2.3173, 'learning_rate': 0.0001975804890091153, 'epoch': 0.12}         \n",
      "{'loss': 1.6501, 'learning_rate': 0.0001975798079458625, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 125331/1061708 [18:45:55<139:04:57,  1.87it/s][2024-03-01 12:52:56,335] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 125332/1061708 [18:45:56<130:25:32,  1.99it/s][2024-03-01 12:52:56,760] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9208, 'learning_rate': 0.00019757926302709934, 'epoch': 0.12}        \n",
      "{'loss': 2.6664, 'learning_rate': 0.00019757858179344476, 'epoch': 0.12}        \n",
      "{'loss': 1.6481, 'learning_rate': 0.00019757790046512344, 'epoch': 0.12}        \n",
      "{'loss': 2.1609, 'learning_rate': 0.00019757721904213607, 'epoch': 0.12}        \n",
      "{'loss': 2.2596, 'learning_rate': 0.00019757653752448326, 'epoch': 0.12}        \n",
      "{'loss': 2.2171, 'learning_rate': 0.00019757585591216573, 'epoch': 0.12}        \n",
      "{'loss': 1.9203, 'learning_rate': 0.00019757517420518412, 'epoch': 0.12}        \n",
      "{'loss': 2.0424, 'learning_rate': 0.00019757449240353906, 'epoch': 0.12}        \n",
      "{'loss': 2.5225, 'learning_rate': 0.00019757381050723125, 'epoch': 0.12}        \n",
      "{'loss': 2.2413, 'learning_rate': 0.00019757312851626134, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125433/1061708 [18:46:50<141:11:50,  1.84it/s][2024-03-01 12:53:50,647] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 125434/1061708 [18:46:50<131:51:58,  1.97it/s][2024-03-01 12:53:51,070] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9921, 'learning_rate': 0.00019757258285532916, 'epoch': 0.12}        \n",
      "{'loss': 2.1371, 'learning_rate': 0.00019757190069396912, 'epoch': 0.12}        \n",
      "{'loss': 2.0264, 'learning_rate': 0.0001975712184379489, 'epoch': 0.12}         \n",
      "{'loss': 2.1917, 'learning_rate': 0.000197570536087269, 'epoch': 0.12}          \n",
      "{'loss': 1.9897, 'learning_rate': 0.00019756985364193023, 'epoch': 0.12}        \n",
      "{'loss': 2.2889, 'learning_rate': 0.0001975691711019332, 'epoch': 0.12}         \n",
      "{'loss': 1.8801, 'learning_rate': 0.00019756848846727855, 'epoch': 0.12}        \n",
      "{'loss': 2.0664, 'learning_rate': 0.000197567805737967, 'epoch': 0.12}          \n",
      "{'loss': 2.0935, 'learning_rate': 0.00019756712291399913, 'epoch': 0.12}        \n",
      "{'loss': 2.2662, 'learning_rate': 0.00019756643999537565, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125535/1061708 [18:47:44<139:12:12,  1.87it/s][2024-03-01 12:54:44,961] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 125536/1061708 [18:47:44<130:20:47,  2.00it/s][2024-03-01 12:54:45,385] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7579, 'learning_rate': 0.0001975658935923253, 'epoch': 0.12}         \n",
      "{'loss': 2.3812, 'learning_rate': 0.0001975652105033234, 'epoch': 0.12}         \n",
      "{'loss': 2.42, 'learning_rate': 0.00019756452731966776, 'epoch': 0.12}          \n",
      "{'loss': 1.9784, 'learning_rate': 0.000197563844041359, 'epoch': 0.12}          \n",
      "{'loss': 2.1541, 'learning_rate': 0.00019756316066839782, 'epoch': 0.12}        \n",
      "{'loss': 1.8503, 'learning_rate': 0.00019756247720078486, 'epoch': 0.12}        \n",
      "{'loss': 1.9376, 'learning_rate': 0.00019756179363852077, 'epoch': 0.12}        \n",
      "{'loss': 2.0377, 'learning_rate': 0.0001975611099816063, 'epoch': 0.12}         \n",
      "{'loss': 2.4846, 'learning_rate': 0.00019756042623004204, 'epoch': 0.12}        \n",
      "{'loss': 2.1673, 'learning_rate': 0.00019755974238382863, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125637/1061708 [18:48:38<138:31:37,  1.88it/s][2024-03-01 12:55:39,266] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 125638/1061708 [18:48:39<129:56:55,  2.00it/s][2024-03-01 12:55:39,692] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.6709, 'learning_rate': 0.000197559195238711, 'epoch': 0.12}          \n",
      "{'loss': 2.1851, 'learning_rate': 0.00019755851122213087, 'epoch': 0.12}        \n",
      "{'loss': 1.8707, 'learning_rate': 0.00019755782711090348, 'epoch': 0.12}        \n",
      "{'loss': 2.5865, 'learning_rate': 0.0001975571429050295, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 125678/1061708 [18:49:00<138:15:21,  1.88it/s][2024-03-01 12:56:00,968] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7764, 'learning_rate': 0.00019755652703882064, 'epoch': 0.12}        \n",
      "{'loss': 1.661, 'learning_rate': 0.00019755584265311994, 'epoch': 0.12}         \n",
      "{'loss': 1.5521, 'learning_rate': 0.0001975551581727746, 'epoch': 0.12}         \n",
      "{'loss': 2.0532, 'learning_rate': 0.00019755447359778524, 'epoch': 0.12}        \n",
      "{'loss': 1.9047, 'learning_rate': 0.00019755378892815254, 'epoch': 0.12}        \n",
      "{'loss': 2.3994, 'learning_rate': 0.00019755310416387717, 'epoch': 0.12}        \n",
      "{'loss': 2.4959, 'learning_rate': 0.00019755241930495977, 'epoch': 0.12}        \n",
      "{'loss': 1.9722, 'learning_rate': 0.00019755173435140105, 'epoch': 0.12}        \n",
      "{'loss': 2.1436, 'learning_rate': 0.00019755104930320165, 'epoch': 0.12}        \n",
      "{'loss': 1.9548, 'learning_rate': 0.00019755036416036222, 'epoch': 0.12}        \n",
      "{'loss': 2.0572, 'learning_rate': 0.00019754967892288342, 'epoch': 0.12}        \n",
      "{'loss': 1.7829, 'learning_rate': 0.00019754899359076595, 'epoch': 0.12}        \n",
      "{'loss': 2.1169, 'learning_rate': 0.00019754830816401048, 'epoch': 0.12}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9673, 'learning_rate': 0.0001975476226426176, 'epoch': 0.12}         \n",
      "{'loss': 2.1723, 'learning_rate': 0.00019754693702658802, 'epoch': 0.12}        \n",
      "{'loss': 2.1732, 'learning_rate': 0.00019754625131592244, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 125833/1061708 [18:50:23<141:12:09,  1.84it/s][2024-03-01 12:57:23,717] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2049, 'learning_rate': 0.0001975456340954102, 'epoch': 0.12}         \n",
      "{'loss': 2.0594, 'learning_rate': 0.00019754494820493795, 'epoch': 0.12}        \n",
      "{'loss': 2.0777, 'learning_rate': 0.00019754426221983167, 'epoch': 0.12}        \n",
      "{'loss': 1.8675, 'learning_rate': 0.0001975435761400919, 'epoch': 0.12}         \n",
      "{'loss': 1.9573, 'learning_rate': 0.0001975428899657194, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 125886/1061708 [18:50:51<138:50:45,  1.87it/s][2024-03-01 12:57:51,965] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0789, 'learning_rate': 0.00019754227232787367, 'epoch': 0.12}        \n",
      "{'loss': 1.8233, 'learning_rate': 0.00019754158597370074, 'epoch': 0.12}        \n",
      "{'loss': 1.9642, 'learning_rate': 0.000197540899524897, 'epoch': 0.12}          \n",
      "{'loss': 1.7647, 'learning_rate': 0.00019754021298146304, 'epoch': 0.12}        \n",
      "{'loss': 2.1833, 'learning_rate': 0.00019753952634339958, 'epoch': 0.12}        \n",
      "{'loss': 1.9785, 'learning_rate': 0.0001975388396107073, 'epoch': 0.12}         \n",
      "{'loss': 2.2911, 'learning_rate': 0.00019753815278338683, 'epoch': 0.12}        \n",
      "{'loss': 2.0853, 'learning_rate': 0.00019753746586143885, 'epoch': 0.12}        \n",
      "{'loss': 1.7919, 'learning_rate': 0.00019753677884486406, 'epoch': 0.12}        \n",
      "{'loss': 2.3272, 'learning_rate': 0.00019753609173366307, 'epoch': 0.12}        \n",
      "{'loss': 2.4234, 'learning_rate': 0.0001975354045278366, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 125999/1061708 [18:51:51<137:56:50,  1.88it/s][2024-03-01 12:58:52,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=126000, skipped=1477, lr=[0.00019753471722738523], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 12:58:52,335] [INFO] [timer.py:260:stop] epoch=0/micro_step=126000/global_step=126000, RunningAvgSamplesPerSec=1.8927491344055642, CurrSamplesPerSec=1.9004841941380157, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.5065, 'learning_rate': 0.00019753471722738523, 'epoch': 0.12}        \n",
      "{'loss': 1.9628, 'learning_rate': 0.00019753402983230974, 'epoch': 0.12}        \n",
      "{'loss': 1.9927, 'learning_rate': 0.00019753334234261073, 'epoch': 0.12}        \n",
      "{'loss': 1.9718, 'learning_rate': 0.0001975326547582889, 'epoch': 0.12}         \n",
      "{'loss': 1.9833, 'learning_rate': 0.00019753196707934486, 'epoch': 0.12}        \n",
      "{'loss': 1.7708, 'learning_rate': 0.0001975312793057793, 'epoch': 0.12}         \n",
      "{'loss': 1.9503, 'learning_rate': 0.00019753059143759293, 'epoch': 0.12}        \n",
      "{'loss': 2.3224, 'learning_rate': 0.00019752990347478638, 'epoch': 0.12}        \n",
      "{'loss': 2.0657, 'learning_rate': 0.00019752921541736035, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 126087/1061708 [18:52:38<138:30:06,  1.88it/s][2024-03-01 12:59:39,261] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126088/1061708 [18:52:39<129:55:34,  2.00it/s][2024-03-01 12:59:39,686] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8962, 'learning_rate': 0.00019752866490329392, 'epoch': 0.12}        \n",
      "{'loss': 2.1451, 'learning_rate': 0.00019752797667555445, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 126101/1061708 [18:52:45<137:51:27,  1.89it/s][2024-03-01 12:59:46,522] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7519, 'learning_rate': 0.00019752735718969085, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 126113/1061708 [18:52:52<141:14:17,  1.84it/s][2024-03-01 12:59:52,865] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 2.087, 'learning_rate': 0.00019752673762718742, 'epoch': 0.12}         \n",
      "{'loss': 1.7984, 'learning_rate': 0.00019752604913452024, 'epoch': 0.12}        \n",
      "{'loss': 1.8861, 'learning_rate': 0.00019752536054723732, 'epoch': 0.12}        \n",
      "{'loss': 2.1068, 'learning_rate': 0.00019752467186533933, 'epoch': 0.12}        \n",
      "{'loss': 1.8263, 'learning_rate': 0.00019752398308882686, 'epoch': 0.12}        \n",
      "{'loss': 2.0935, 'learning_rate': 0.00019752329421770064, 'epoch': 0.12}        \n",
      "{'loss': 2.1578, 'learning_rate': 0.00019752260525196134, 'epoch': 0.12}        \n",
      "{'loss': 1.7244, 'learning_rate': 0.0001975219161916096, 'epoch': 0.12}         \n",
      "{'loss': 1.828, 'learning_rate': 0.00019752122703664614, 'epoch': 0.12}         \n",
      "{'loss': 2.2038, 'learning_rate': 0.00019752053778707157, 'epoch': 0.12}        \n",
      "{'loss': 1.8523, 'learning_rate': 0.0001975198484428866, 'epoch': 0.12}         \n",
      "{'loss': 2.0776, 'learning_rate': 0.00019751915900409183, 'epoch': 0.12}        \n",
      "{'loss': 2.3089, 'learning_rate': 0.000197518469470688, 'epoch': 0.12}          \n",
      "{'loss': 2.1637, 'learning_rate': 0.0001975177798426758, 'epoch': 0.12}         \n",
      "{'loss': 2.267, 'learning_rate': 0.0001975170901200558, 'epoch': 0.12}          \n",
      "{'loss': 2.332, 'learning_rate': 0.0001975164003028288, 'epoch': 0.12}          \n",
      "{'loss': 1.7348, 'learning_rate': 0.00019751571039099532, 'epoch': 0.12}        \n",
      "{'loss': 2.2303, 'learning_rate': 0.00019751502038455615, 'epoch': 0.12}        \n",
      "{'loss': 2.1696, 'learning_rate': 0.0001975143302835119, 'epoch': 0.12}         \n",
      "{'loss': 2.1548, 'learning_rate': 0.00019751364008786326, 'epoch': 0.12}        \n",
      "{'loss': 2.2864, 'learning_rate': 0.0001975129497976109, 'epoch': 0.12}         \n",
      "{'loss': 2.3283, 'learning_rate': 0.00019751225941275547, 'epoch': 0.12}        \n",
      "{'loss': 1.7881, 'learning_rate': 0.00019751156893329768, 'epoch': 0.12}        \n",
      "{'loss': 1.678, 'learning_rate': 0.00019751087835923815, 'epoch': 0.12}         \n",
      "{'loss': 2.2833, 'learning_rate': 0.00019751018769057757, 'epoch': 0.12}        \n",
      "{'loss': 1.9764, 'learning_rate': 0.0001975094969273166, 'epoch': 0.12}         \n",
      "{'loss': 2.0662, 'learning_rate': 0.00019750880606945597, 'epoch': 0.12}        \n",
      "{'loss': 1.9753, 'learning_rate': 0.00019750811511699628, 'epoch': 0.12}        \n",
      "{'loss': 2.2589, 'learning_rate': 0.00019750742406993824, 'epoch': 0.12}        \n",
      "{'loss': 2.2277, 'learning_rate': 0.00019750673292828248, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 126414/1061708 [18:55:33<139:50:51,  1.86it/s][2024-03-01 13:02:33,528] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126415/1061708 [18:55:33<130:50:03,  1.99it/s][2024-03-01 13:02:33,951] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1149, 'learning_rate': 0.000197506179946848, 'epoch': 0.12}          \n",
      "{'loss': 1.9933, 'learning_rate': 0.0001975054886349181, 'epoch': 0.12}         \n",
      "{'loss': 1.7497, 'learning_rate': 0.00019750479722839234, 'epoch': 0.12}        \n",
      "{'loss': 2.0794, 'learning_rate': 0.00019750410572727146, 'epoch': 0.12}        \n",
      "{'loss': 1.9205, 'learning_rate': 0.0001975034141315561, 'epoch': 0.12}         \n",
      "{'loss': 2.0057, 'learning_rate': 0.00019750272244124694, 'epoch': 0.12}        \n",
      "{'loss': 2.3644, 'learning_rate': 0.00019750203065634468, 'epoch': 0.12}        \n",
      "{'loss': 1.9285, 'learning_rate': 0.00019750133877684993, 'epoch': 0.12}        \n",
      "{'loss': 2.1197, 'learning_rate': 0.00019750064680276337, 'epoch': 0.12}        \n",
      "{'loss': 2.2058, 'learning_rate': 0.00019749995473408572, 'epoch': 0.12}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|███▏                       | 126516/1061708 [18:56:27<139:03:49,  1.87it/s][2024-03-01 13:03:27,803] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126517/1061708 [18:56:27<130:18:51,  1.99it/s][2024-03-01 13:03:28,226] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8043, 'learning_rate': 0.00019749940101103844, 'epoch': 0.12}        \n",
      "{'loss': 2.0544, 'learning_rate': 0.00019749870877209844, 'epoch': 0.12}        \n",
      "{'loss': 2.1509, 'learning_rate': 0.0001974980164385692, 'epoch': 0.12}         \n",
      "{'loss': 2.0478, 'learning_rate': 0.0001974973240104514, 'epoch': 0.12}         \n",
      "{'loss': 1.9648, 'learning_rate': 0.00019749663148774572, 'epoch': 0.12}        \n",
      "{'loss': 1.8253, 'learning_rate': 0.0001974959388704528, 'epoch': 0.12}         \n",
      "{'loss': 1.8916, 'learning_rate': 0.00019749524615857332, 'epoch': 0.12}        \n",
      "{'loss': 2.5231, 'learning_rate': 0.00019749455335210797, 'epoch': 0.12}        \n",
      "{'loss': 2.1567, 'learning_rate': 0.0001974938604510574, 'epoch': 0.12}         \n",
      "{'loss': 2.0921, 'learning_rate': 0.0001974931674554223, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 126618/1061708 [18:57:21<138:04:17,  1.88it/s][2024-03-01 13:04:22,004] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126619/1061708 [18:57:21<129:33:12,  2.00it/s][2024-03-01 13:04:22,428] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.6092, 'learning_rate': 0.0001974926129908138, 'epoch': 0.12}         \n",
      "{'loss': 1.641, 'learning_rate': 0.00019749191982492822, 'epoch': 0.12}         \n",
      "{'loss': 1.61, 'learning_rate': 0.00019749122656446, 'epoch': 0.12}             \n",
      "{'loss': 2.3358, 'learning_rate': 0.00019749053320940982, 'epoch': 0.12}        \n",
      "{'loss': 2.2446, 'learning_rate': 0.0001974898397597783, 'epoch': 0.12}         \n",
      "{'loss': 1.826, 'learning_rate': 0.00019748914621556615, 'epoch': 0.12}         \n",
      "{'loss': 1.8981, 'learning_rate': 0.00019748845257677405, 'epoch': 0.12}        \n",
      "{'loss': 1.8753, 'learning_rate': 0.00019748775884340263, 'epoch': 0.12}        \n",
      "{'loss': 2.0358, 'learning_rate': 0.00019748706501545262, 'epoch': 0.12}        \n",
      "{'loss': 2.1035, 'learning_rate': 0.00019748637109292467, 'epoch': 0.12}        \n",
      "{'loss': 2.324, 'learning_rate': 0.0001974856770758194, 'epoch': 0.12}          \n",
      " 12%|███▏                       | 126720/1061708 [18:58:15<137:51:27,  1.88it/s][2024-03-01 13:05:16,280] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126721/1061708 [18:58:16<129:37:36,  2.00it/s][2024-03-01 13:05:16,703] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9472, 'learning_rate': 0.00019748512179404006, 'epoch': 0.12}        \n",
      "{'loss': 2.0693, 'learning_rate': 0.00019748442760669744, 'epoch': 0.12}        \n",
      "{'loss': 2.2104, 'learning_rate': 0.00019748373332477943, 'epoch': 0.12}        \n",
      "{'loss': 2.2702, 'learning_rate': 0.00019748303894828671, 'epoch': 0.12}        \n",
      "{'loss': 1.8642, 'learning_rate': 0.00019748234447721996, 'epoch': 0.12}        \n",
      "{'loss': 1.9746, 'learning_rate': 0.00019748164991157985, 'epoch': 0.12}        \n",
      "{'loss': 1.9177, 'learning_rate': 0.00019748095525136706, 'epoch': 0.12}        \n",
      "{'loss': 2.2663, 'learning_rate': 0.0001974802604965822, 'epoch': 0.12}         \n",
      "{'loss': 1.9644, 'learning_rate': 0.00019747956564722605, 'epoch': 0.12}        \n",
      "{'loss': 1.9688, 'learning_rate': 0.0001974788707032992, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 126822/1061708 [18:59:10<137:45:24,  1.89it/s][2024-03-01 13:06:10,614] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 126823/1061708 [18:59:10<133:05:36,  1.95it/s][2024-03-01 13:06:11,039] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.3164, 'learning_rate': 0.0001974783146800673, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 126830/1061708 [18:59:14<137:30:17,  1.89it/s][2024-03-01 13:06:14,697] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1854, 'learning_rate': 0.00019747768908158578, 'epoch': 0.12}        \n",
      "{'loss': 2.1387, 'learning_rate': 0.00019747699388232155, 'epoch': 0.12}        \n",
      "{'loss': 2.1856, 'learning_rate': 0.0001974762985884891, 'epoch': 0.12}         \n",
      "{'loss': 2.3493, 'learning_rate': 0.00019747560320008915, 'epoch': 0.12}        \n",
      "{'loss': 2.0021, 'learning_rate': 0.0001974749077171224, 'epoch': 0.12}         \n",
      "{'loss': 2.0356, 'learning_rate': 0.00019747421213958945, 'epoch': 0.12}        \n",
      "{'loss': 2.1066, 'learning_rate': 0.00019747351646749107, 'epoch': 0.12}        \n",
      "{'loss': 1.6587, 'learning_rate': 0.00019747282070082788, 'epoch': 0.12}        \n",
      "{'loss': 2.2014, 'learning_rate': 0.00019747212483960057, 'epoch': 0.12}        \n",
      "{'loss': 2.1462, 'learning_rate': 0.0001974714288838098, 'epoch': 0.12}         \n",
      "{'loss': 2.2735, 'learning_rate': 0.00019747073283345626, 'epoch': 0.12}        \n",
      "{'loss': 2.2219, 'learning_rate': 0.0001974700366885406, 'epoch': 0.12}         \n",
      "{'loss': 2.0713, 'learning_rate': 0.00019746934044906356, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 126963/1061708 [19:00:25<141:14:08,  1.84it/s][2024-03-01 13:07:25,694] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1203, 'learning_rate': 0.00019746871375268474, 'epoch': 0.12}        \n",
      "{'loss': 1.724, 'learning_rate': 0.00019746801733354282, 'epoch': 0.12}         \n",
      "{'loss': 2.0029, 'learning_rate': 0.00019746732081984148, 'epoch': 0.12}        \n",
      "{'loss': 2.0934, 'learning_rate': 0.0001974666242115813, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 127000/1061708 [19:00:44<137:48:23,  1.88it/s][2024-03-01 13:07:45,390] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2416, 'learning_rate': 0.00019746599718329996, 'epoch': 0.12}        \n",
      "{'loss': 1.8463, 'learning_rate': 0.00019746530039537995, 'epoch': 0.12}        \n",
      "{'loss': 2.3633, 'learning_rate': 0.00019746460351290315, 'epoch': 0.12}        \n",
      "{'loss': 2.1012, 'learning_rate': 0.00019746390653587018, 'epoch': 0.12}        \n",
      "{'loss': 2.034, 'learning_rate': 0.0001974632094642817, 'epoch': 0.12}          \n",
      "{'loss': 1.7622, 'learning_rate': 0.00019746251229813846, 'epoch': 0.12}        \n",
      "{'loss': 2.2198, 'learning_rate': 0.0001974618150374411, 'epoch': 0.12}         \n",
      "{'loss': 2.2566, 'learning_rate': 0.00019746111768219026, 'epoch': 0.12}        \n",
      "{'loss': 2.0159, 'learning_rate': 0.00019746042023238667, 'epoch': 0.12}        \n",
      "{'loss': 2.0085, 'learning_rate': 0.00019745972268803098, 'epoch': 0.12}        \n",
      "{'loss': 2.0503, 'learning_rate': 0.00019745902504912386, 'epoch': 0.12}        \n",
      "{'loss': 1.8394, 'learning_rate': 0.00019745832731566602, 'epoch': 0.12}        \n",
      "{'loss': 2.075, 'learning_rate': 0.0001974576294876581, 'epoch': 0.12}          \n",
      "{'loss': 2.3422, 'learning_rate': 0.00019745693156510084, 'epoch': 0.12}        \n",
      "{'loss': 2.2048, 'learning_rate': 0.00019745623354799481, 'epoch': 0.12}        \n",
      "{'loss': 1.8861, 'learning_rate': 0.00019745553543634077, 'epoch': 0.12}        \n",
      "{'loss': 1.8479, 'learning_rate': 0.00019745483723013937, 'epoch': 0.12}        \n",
      "{'loss': 2.0266, 'learning_rate': 0.0001974541389293913, 'epoch': 0.12}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0449, 'learning_rate': 0.00019745344053409723, 'epoch': 0.12}        \n",
      "{'loss': 2.2031, 'learning_rate': 0.00019745274204425786, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 127201/1061708 [19:02:32<137:50:22,  1.88it/s][2024-03-01 13:09:32,677] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 127202/1061708 [19:02:32<129:24:25,  2.01it/s][2024-03-01 13:09:33,100] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8204, 'learning_rate': 0.00019745218318431419, 'epoch': 0.12}        \n",
      "{'loss': 1.8627, 'learning_rate': 0.0001974514845242949, 'epoch': 0.12}         \n",
      "{'loss': 2.0642, 'learning_rate': 0.00019745078576973221, 'epoch': 0.12}        \n",
      "{'loss': 2.1686, 'learning_rate': 0.00019745008692062676, 'epoch': 0.12}        \n",
      "{'loss': 2.0912, 'learning_rate': 0.00019744938797697925, 'epoch': 0.12}        \n",
      "{'loss': 2.2538, 'learning_rate': 0.00019744868893879037, 'epoch': 0.12}        \n",
      "{'loss': 2.2686, 'learning_rate': 0.00019744798980606074, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 127276/1061708 [19:03:12<138:48:22,  1.87it/s][2024-03-01 13:10:12,572] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2455, 'learning_rate': 0.00019744736050577235, 'epoch': 0.12}        \n",
      "{'loss': 1.7809, 'learning_rate': 0.00019744666119341726, 'epoch': 0.12}        \n",
      "{'loss': 1.8113, 'learning_rate': 0.00019744596178652342, 'epoch': 0.12}        \n",
      "{'loss': 2.0238, 'learning_rate': 0.0001974452622850915, 'epoch': 0.12}         \n",
      "{'loss': 2.3829, 'learning_rate': 0.0001974445626891222, 'epoch': 0.12}         \n",
      "{'loss': 2.3912, 'learning_rate': 0.00019744386299861623, 'epoch': 0.12}        \n",
      "{'loss': 2.3119, 'learning_rate': 0.00019744316321357417, 'epoch': 0.12}        \n",
      "{'loss': 1.9214, 'learning_rate': 0.00019744246333399675, 'epoch': 0.12}        \n",
      "{'loss': 1.9392, 'learning_rate': 0.0001974417633598847, 'epoch': 0.12}         \n",
      "{'loss': 1.9697, 'learning_rate': 0.00019744106329123865, 'epoch': 0.12}        \n",
      "{'loss': 2.09, 'learning_rate': 0.00019744036312805929, 'epoch': 0.12}          \n",
      "{'loss': 2.0059, 'learning_rate': 0.00019743966287034728, 'epoch': 0.12}        \n",
      "{'loss': 1.8938, 'learning_rate': 0.0001974389625181033, 'epoch': 0.12}         \n",
      "{'loss': 2.0592, 'learning_rate': 0.00019743826207132807, 'epoch': 0.12}        \n",
      "{'loss': 2.3815, 'learning_rate': 0.00019743756153002223, 'epoch': 0.12}        \n",
      "{'loss': 1.8454, 'learning_rate': 0.00019743686089418648, 'epoch': 0.12}        \n",
      "{'loss': 2.541, 'learning_rate': 0.0001974361601638215, 'epoch': 0.12}          \n",
      "{'loss': 2.26, 'learning_rate': 0.00019743545933892793, 'epoch': 0.12}          \n",
      "{'loss': 2.1104, 'learning_rate': 0.00019743475841950653, 'epoch': 0.12}        \n",
      "{'loss': 2.0166, 'learning_rate': 0.0001974340574055579, 'epoch': 0.12}         \n",
      " 12%|███▏                       | 127477/1061708 [19:04:59<137:48:17,  1.88it/s][2024-03-01 13:11:59,668] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 127478/1061708 [19:04:59<129:13:24,  2.01it/s][2024-03-01 13:12:00,092] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0098, 'learning_rate': 0.00019743349652633985, 'epoch': 0.12}        \n",
      "{'loss': 2.1714, 'learning_rate': 0.00019743279534224399, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 127499/1061708 [19:05:10<137:30:52,  1.89it/s][2024-03-01 13:12:11,184] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.3726, 'learning_rate': 0.00019743216419573857, 'epoch': 0.12}        \n",
      "{'loss': 1.9638, 'learning_rate': 0.0001974314628320452, 'epoch': 0.12}         \n",
      "{'loss': 2.0173, 'learning_rate': 0.00019743076137382785, 'epoch': 0.12}        \n",
      "{'loss': 2.0518, 'learning_rate': 0.0001974300598210872, 'epoch': 0.12}         \n",
      "{'loss': 1.8763, 'learning_rate': 0.00019742935817382388, 'epoch': 0.12}        \n",
      "{'loss': 2.097, 'learning_rate': 0.00019742865643203862, 'epoch': 0.12}         \n",
      "{'loss': 1.9431, 'learning_rate': 0.0001974279545957321, 'epoch': 0.12}         \n",
      "{'loss': 2.0736, 'learning_rate': 0.00019742725266490494, 'epoch': 0.12}        \n",
      "{'loss': 1.8103, 'learning_rate': 0.00019742655063955788, 'epoch': 0.12}        \n",
      "{'loss': 1.8631, 'learning_rate': 0.0001974258485196916, 'epoch': 0.12}         \n",
      "{'loss': 1.91, 'learning_rate': 0.00019742514630530678, 'epoch': 0.12}          \n",
      "{'loss': 2.488, 'learning_rate': 0.00019742444399640404, 'epoch': 0.12}         \n",
      "{'loss': 2.1097, 'learning_rate': 0.00019742374159298413, 'epoch': 0.12}        \n",
      "{'loss': 1.9705, 'learning_rate': 0.0001974230390950477, 'epoch': 0.12}         \n",
      "{'loss': 1.9086, 'learning_rate': 0.00019742233650259548, 'epoch': 0.12}        \n",
      "{'loss': 2.4785, 'learning_rate': 0.00019742163381562805, 'epoch': 0.12}        \n",
      "{'loss': 2.2346, 'learning_rate': 0.0001974209310341462, 'epoch': 0.12}         \n",
      "{'loss': 2.3307, 'learning_rate': 0.00019742022815815057, 'epoch': 0.12}        \n",
      "{'loss': 2.3587, 'learning_rate': 0.00019741952518764183, 'epoch': 0.12}        \n",
      "{'loss': 2.0878, 'learning_rate': 0.00019741882212262067, 'epoch': 0.12}        \n",
      "{'loss': 2.2896, 'learning_rate': 0.00019741811896308777, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 127700/1061708 [19:06:57<137:23:37,  1.89it/s][2024-03-01 13:13:58,223] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▏                       | 127701/1061708 [19:06:58<129:04:10,  2.01it/s][2024-03-01 13:13:58,645] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0332, 'learning_rate': 0.00019741755636741345, 'epoch': 0.12}        \n",
      "{'loss': 2.4632, 'learning_rate': 0.00019741685303776115, 'epoch': 0.12}        \n",
      "{'loss': 2.8613, 'learning_rate': 0.00019741614961359902, 'epoch': 0.12}        \n",
      "{'loss': 2.0598, 'learning_rate': 0.00019741544609492775, 'epoch': 0.12}        \n",
      " 12%|███▏                       | 127745/1061708 [19:07:21<139:30:55,  1.86it/s][2024-03-01 13:14:22,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2667, 'learning_rate': 0.0001974148128473188, 'epoch': 0.12}         \n",
      "{'loss': 2.0038, 'learning_rate': 0.00019741410914908206, 'epoch': 0.12}        \n",
      "{'loss': 2.0865, 'learning_rate': 0.0001974134053563381, 'epoch': 0.12}         \n",
      "{'loss': 1.6535, 'learning_rate': 0.00019741270146908768, 'epoch': 0.12}        \n",
      "{'loss': 1.8894, 'learning_rate': 0.0001974119974873315, 'epoch': 0.12}         \n",
      "{'loss': 2.4366, 'learning_rate': 0.00019741129341107014, 'epoch': 0.12}        \n",
      "{'loss': 2.2175, 'learning_rate': 0.00019741058924030439, 'epoch': 0.12}        \n",
      "{'loss': 1.6455, 'learning_rate': 0.00019740988497503486, 'epoch': 0.12}        \n",
      "{'loss': 1.9175, 'learning_rate': 0.00019740918061526226, 'epoch': 0.12}        \n",
      "{'loss': 1.9624, 'learning_rate': 0.0001974084761609873, 'epoch': 0.12}         \n",
      "{'loss': 1.9567, 'learning_rate': 0.00019740777161221062, 'epoch': 0.12}        \n",
      "{'loss': 1.9868, 'learning_rate': 0.00019740706696893295, 'epoch': 0.12}        \n",
      "{'loss': 2.2269, 'learning_rate': 0.00019740636223115492, 'epoch': 0.12}        \n",
      "{'loss': 1.7321, 'learning_rate': 0.00019740565739887728, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 127887/1061708 [19:08:37<138:19:55,  1.88it/s][2024-03-01 13:15:38,082] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7326, 'learning_rate': 0.00019740502296903073, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 127893/1061708 [19:08:40<139:43:11,  1.86it/s][2024-03-01 13:15:41,208] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1189, 'learning_rate': 0.0001974043884626405, 'epoch': 0.12}         \n",
      "{'loss': 2.0588, 'learning_rate': 0.00019740368336576748, 'epoch': 0.12}        \n",
      "{'loss': 1.972, 'learning_rate': 0.00019740297817439736, 'epoch': 0.12}         \n",
      "{'loss': 2.2897, 'learning_rate': 0.0001974022728885309, 'epoch': 0.12}         \n",
      "{'loss': 2.1492, 'learning_rate': 0.00019740156750816876, 'epoch': 0.12}        \n",
      "{'loss': 1.78, 'learning_rate': 0.0001974008620333116, 'epoch': 0.12}           \n",
      "{'loss': 2.5074, 'learning_rate': 0.00019740015646396017, 'epoch': 0.12}        \n",
      "{'loss': 2.029, 'learning_rate': 0.00019739945080011508, 'epoch': 0.12}         \n",
      "{'loss': 1.9043, 'learning_rate': 0.00019739874504177705, 'epoch': 0.12}        \n",
      "{'loss': 2.14, 'learning_rate': 0.00019739803918894676, 'epoch': 0.12}          \n",
      " 12%|███▎                       | 127999/1061708 [19:09:37<137:42:24,  1.88it/s][2024-03-01 13:16:37,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=128000, skipped=1505, lr=[0.0001973973332416249], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 13:16:37,868] [INFO] [timer.py:260:stop] epoch=0/micro_step=128000/global_step=128000, RunningAvgSamplesPerSec=1.8927504777268356, CurrSamplesPerSec=1.901594839135558, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.1827, 'learning_rate': 0.0001973973332416249, 'epoch': 0.12}         \n",
      "{'loss': 2.329, 'learning_rate': 0.00019739662719981213, 'epoch': 0.12}         \n",
      "{'loss': 2.0679, 'learning_rate': 0.00019739592106350918, 'epoch': 0.12}        \n",
      "{'loss': 2.0714, 'learning_rate': 0.0001973952148327167, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 128037/1061708 [19:09:57<138:28:50,  1.87it/s][2024-03-01 13:16:58,115] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.041, 'learning_rate': 0.0001973945791442155, 'epoch': 0.12}          \n",
      "{'loss': 2.0833, 'learning_rate': 0.00019739387273389484, 'epoch': 0.12}        \n",
      "{'loss': 2.4916, 'learning_rate': 0.00019739316622908662, 'epoch': 0.12}        \n",
      "{'loss': 2.0094, 'learning_rate': 0.00019739245962979157, 'epoch': 0.12}        \n",
      "{'loss': 2.2199, 'learning_rate': 0.00019739175293601035, 'epoch': 0.12}        \n",
      "{'loss': 2.2585, 'learning_rate': 0.00019739104614774363, 'epoch': 0.12}        \n",
      "{'loss': 1.8013, 'learning_rate': 0.00019739033926499216, 'epoch': 0.12}        \n",
      "{'loss': 1.633, 'learning_rate': 0.00019738963228775657, 'epoch': 0.12}         \n",
      "{'loss': 2.2802, 'learning_rate': 0.00019738892521603757, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128128/1061708 [19:10:46<137:54:40,  1.88it/s][2024-03-01 13:17:46,615] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.9146, 'learning_rate': 0.0001973882887707077, 'epoch': 0.12}         \n",
      "{'loss': 2.1542, 'learning_rate': 0.0001973875815194721, 'epoch': 0.12}         \n",
      "{'loss': 2.2032, 'learning_rate': 0.00019738687417375505, 'epoch': 0.12}        \n",
      "{'loss': 2.0148, 'learning_rate': 0.00019738616673355727, 'epoch': 0.12}        \n",
      "{'loss': 1.7484, 'learning_rate': 0.00019738545919887942, 'epoch': 0.12}        \n",
      "{'loss': 1.7031, 'learning_rate': 0.00019738475156972222, 'epoch': 0.12}        \n",
      "{'loss': 1.9947, 'learning_rate': 0.00019738404384608634, 'epoch': 0.12}        \n",
      "{'loss': 1.8398, 'learning_rate': 0.00019738333602797246, 'epoch': 0.12}        \n",
      "{'loss': 2.3513, 'learning_rate': 0.00019738262811538126, 'epoch': 0.12}        \n",
      "{'loss': 2.2636, 'learning_rate': 0.00019738192010831343, 'epoch': 0.12}        \n",
      "{'loss': 1.638, 'learning_rate': 0.00019738121200676967, 'epoch': 0.12}         \n",
      "{'loss': 1.8398, 'learning_rate': 0.00019738050381075067, 'epoch': 0.12}        \n",
      "{'loss': 2.1398, 'learning_rate': 0.00019737979552025709, 'epoch': 0.12}        \n",
      "{'loss': 2.1603, 'learning_rate': 0.00019737908713528962, 'epoch': 0.12}        \n",
      "{'loss': 2.0216, 'learning_rate': 0.000197378378655849, 'epoch': 0.12}          \n",
      "{'loss': 2.0195, 'learning_rate': 0.00019737767008193587, 'epoch': 0.12}        \n",
      "{'loss': 1.6616, 'learning_rate': 0.0001973769614135509, 'epoch': 0.12}         \n",
      "{'loss': 1.6038, 'learning_rate': 0.00019737625265069486, 'epoch': 0.12}        \n",
      "{'loss': 1.7767, 'learning_rate': 0.00019737554379336834, 'epoch': 0.12}        \n",
      "{'loss': 2.2983, 'learning_rate': 0.0001973748348415721, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 128329/1061708 [19:12:33<137:18:29,  1.89it/s][2024-03-01 13:19:33,822] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.7989, 'learning_rate': 0.00019737419670418435, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128330/1061708 [19:12:33<128:53:46,  2.01it/s][2024-03-01 13:19:34,246] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2042, 'learning_rate': 0.00019737355849027723, 'epoch': 0.12}        \n",
      "{'loss': 2.2686, 'learning_rate': 0.0001973728492739693, 'epoch': 0.12}         \n",
      "{'loss': 1.9679, 'learning_rate': 0.00019737213996319427, 'epoch': 0.12}        \n",
      "{'loss': 2.0468, 'learning_rate': 0.0001973714305579528, 'epoch': 0.12}         \n",
      "{'loss': 2.0846, 'learning_rate': 0.00019737072105824554, 'epoch': 0.12}        \n",
      "{'loss': 2.0387, 'learning_rate': 0.00019737001146407322, 'epoch': 0.12}        \n",
      "{'loss': 2.0766, 'learning_rate': 0.0001973693017754365, 'epoch': 0.12}         \n",
      "{'loss': 2.0644, 'learning_rate': 0.00019736859199233613, 'epoch': 0.12}        \n",
      "{'loss': 2.0492, 'learning_rate': 0.00019736788211477272, 'epoch': 0.12}        \n",
      "{'loss': 1.7676, 'learning_rate': 0.000197367172142747, 'epoch': 0.12}          \n",
      " 12%|███▎                       | 128431/1061708 [19:13:27<137:11:17,  1.89it/s][2024-03-01 13:20:27,947] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 128432/1061708 [19:13:27<128:43:42,  2.01it/s][2024-03-01 13:20:28,370] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8866, 'learning_rate': 0.000197366604097114, 'epoch': 0.12}          \n",
      "{'loss': 1.9118, 'learning_rate': 0.00019736589395505787, 'epoch': 0.12}        \n",
      "{'loss': 1.9592, 'learning_rate': 0.00019736518371854132, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128464/1061708 [19:13:44<139:25:35,  1.86it/s][2024-03-01 13:20:45,354] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.315, 'learning_rate': 0.00019736454442491336, 'epoch': 0.12}         \n",
      "{'loss': 2.1943, 'learning_rate': 0.00019736383400892398, 'epoch': 0.12}        \n",
      "{'loss': 2.0545, 'learning_rate': 0.00019736312349847622, 'epoch': 0.12}        \n",
      "{'loss': 1.8149, 'learning_rate': 0.0001973624128935707, 'epoch': 0.12}         \n",
      "{'loss': 1.9517, 'learning_rate': 0.00019736170219420825, 'epoch': 0.12}        \n",
      "{'loss': 1.783, 'learning_rate': 0.00019736099140038943, 'epoch': 0.12}         \n",
      "{'loss': 2.1697, 'learning_rate': 0.000197360280512115, 'epoch': 0.12}          \n",
      "{'loss': 2.1815, 'learning_rate': 0.0001973595695293856, 'epoch': 0.12}         \n",
      "{'loss': 2.2286, 'learning_rate': 0.00019735885845220197, 'epoch': 0.12}        \n",
      "{'loss': 1.8867, 'learning_rate': 0.00019735814728056477, 'epoch': 0.12}        \n",
      "{'loss': 2.1163, 'learning_rate': 0.0001973574360144747, 'epoch': 0.12}         \n",
      "{'loss': 2.0654, 'learning_rate': 0.0001973567246539324, 'epoch': 0.12}         \n",
      "{'loss': 2.0439, 'learning_rate': 0.0001973560131989387, 'epoch': 0.12}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8387, 'learning_rate': 0.00019735530164949414, 'epoch': 0.12}        \n",
      "{'loss': 2.0122, 'learning_rate': 0.00019735459000559948, 'epoch': 0.12}        \n",
      "{'loss': 2.1136, 'learning_rate': 0.0001973538782672554, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 128625/1061708 [19:15:10<138:40:00,  1.87it/s][2024-03-01 13:22:11,109] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1813, 'learning_rate': 0.00019735323762199204, 'epoch': 0.12}        \n",
      "{'loss': 2.1812, 'learning_rate': 0.00019735252570419594, 'epoch': 0.12}        \n",
      "{'loss': 1.8802, 'learning_rate': 0.00019735181369195243, 'epoch': 0.12}        \n",
      "{'loss': 1.9292, 'learning_rate': 0.00019735110158526222, 'epoch': 0.12}        \n",
      "{'loss': 2.1327, 'learning_rate': 0.00019735038938412594, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128673/1061708 [19:15:36<140:08:37,  1.85it/s][2024-03-01 13:22:36,626] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.245, 'learning_rate': 0.0001973497483223525, 'epoch': 0.12}          \n",
      "{'loss': 2.0347, 'learning_rate': 0.00019734903594177068, 'epoch': 0.12}        \n",
      "{'loss': 2.1575, 'learning_rate': 0.0001973483234667448, 'epoch': 0.12}         \n",
      "{'loss': 1.9248, 'learning_rate': 0.0001973476108972756, 'epoch': 0.12}         \n",
      "{'loss': 1.9508, 'learning_rate': 0.0001973468982333637, 'epoch': 0.12}         \n",
      "{'loss': 2.3698, 'learning_rate': 0.00019734618547500988, 'epoch': 0.12}        \n",
      "{'loss': 2.1046, 'learning_rate': 0.0001973454726222148, 'epoch': 0.12}         \n",
      "{'loss': 2.1929, 'learning_rate': 0.00019734475967497913, 'epoch': 0.12}        \n",
      "{'loss': 2.2232, 'learning_rate': 0.0001973440466333036, 'epoch': 0.12}         \n",
      "{'loss': 1.7532, 'learning_rate': 0.00019734333349718882, 'epoch': 0.12}        \n",
      "{'loss': 2.1688, 'learning_rate': 0.00019734262026663556, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128781/1061708 [19:16:33<137:17:41,  1.89it/s][2024-03-01 13:23:34,091] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.9315, 'learning_rate': 0.00019734197827839332, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128794/1061708 [19:16:40<139:00:55,  1.86it/s][2024-03-01 13:23:40,939] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7754, 'learning_rate': 0.00019734133621365692, 'epoch': 0.12}        \n",
      "{'loss': 1.83, 'learning_rate': 0.00019734062271867955, 'epoch': 0.12}          \n",
      "{'loss': 1.8484, 'learning_rate': 0.0001973399091292663, 'epoch': 0.12}         \n",
      "{'loss': 1.9771, 'learning_rate': 0.00019733919544541787, 'epoch': 0.12}        \n",
      "{'loss': 1.9431, 'learning_rate': 0.00019733848166713497, 'epoch': 0.12}        \n",
      "{'loss': 2.1124, 'learning_rate': 0.00019733776779441828, 'epoch': 0.12}        \n",
      "{'loss': 2.0977, 'learning_rate': 0.00019733705382726847, 'epoch': 0.12}        \n",
      "{'loss': 1.894, 'learning_rate': 0.00019733633976568626, 'epoch': 0.12}         \n",
      "{'loss': 2.3414, 'learning_rate': 0.00019733562560967234, 'epoch': 0.12}        \n",
      "{'loss': 1.8994, 'learning_rate': 0.00019733491135922736, 'epoch': 0.12}        \n",
      "{'loss': 2.2092, 'learning_rate': 0.0001973341970143521, 'epoch': 0.12}         \n",
      "{'loss': 2.2626, 'learning_rate': 0.00019733348257504717, 'epoch': 0.12}        \n",
      "{'loss': 2.4812, 'learning_rate': 0.00019733276804131332, 'epoch': 0.12}        \n",
      "{'loss': 2.2023, 'learning_rate': 0.0001973320534131512, 'epoch': 0.12}         \n",
      "{'loss': 2.175, 'learning_rate': 0.00019733133869056152, 'epoch': 0.12}         \n",
      "{'loss': 2.0735, 'learning_rate': 0.00019733062387354498, 'epoch': 0.12}        \n",
      "{'loss': 1.818, 'learning_rate': 0.00019732990896210228, 'epoch': 0.12}         \n",
      "{'loss': 2.1353, 'learning_rate': 0.0001973291939562341, 'epoch': 0.12}         \n",
      "{'loss': 1.9058, 'learning_rate': 0.00019732847885594112, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 128985/1061708 [19:18:22<139:42:18,  1.85it/s][2024-03-01 13:25:22,798] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0319, 'learning_rate': 0.00019732783518494485, 'epoch': 0.12}        \n",
      "{'loss': 1.847, 'learning_rate': 0.0001973271199052467, 'epoch': 0.12}          \n",
      "{'loss': 1.975, 'learning_rate': 0.00019732640453112577, 'epoch': 0.12}         \n",
      "{'loss': 2.1525, 'learning_rate': 0.00019732568906258278, 'epoch': 0.12}        \n",
      "{'loss': 1.8029, 'learning_rate': 0.0001973249734996184, 'epoch': 0.12}         \n",
      "{'loss': 2.1929, 'learning_rate': 0.00019732425784223332, 'epoch': 0.12}        \n",
      "{'loss': 1.8164, 'learning_rate': 0.00019732354209042825, 'epoch': 0.12}        \n",
      "{'loss': 1.9474, 'learning_rate': 0.0001973228262442039, 'epoch': 0.12}         \n",
      "{'loss': 1.91, 'learning_rate': 0.0001973221103035609, 'epoch': 0.12}           \n",
      "{'loss': 2.2925, 'learning_rate': 0.0001973213942685, 'epoch': 0.12}            \n",
      " 12%|███▎                       | 129086/1061708 [19:19:16<139:05:05,  1.86it/s][2024-03-01 13:26:16,939] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 129087/1061708 [19:19:16<130:16:53,  1.99it/s][2024-03-01 13:26:17,365] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0649, 'learning_rate': 0.00019732082137247092, 'epoch': 0.12}        \n",
      "{'loss': 1.9212, 'learning_rate': 0.00019732010516745953, 'epoch': 0.12}        \n",
      "{'loss': 2.2928, 'learning_rate': 0.0001973193888680322, 'epoch': 0.12}         \n",
      "{'loss': 1.8627, 'learning_rate': 0.0001973186724741896, 'epoch': 0.12}         \n",
      "{'loss': 2.0067, 'learning_rate': 0.0001973179559859324, 'epoch': 0.12}         \n",
      "{'loss': 2.1241, 'learning_rate': 0.00019731723940326132, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129149/1061708 [19:19:49<137:37:28,  1.88it/s][2024-03-01 13:26:50,378] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.4216, 'learning_rate': 0.00019731659439813407, 'epoch': 0.12}        \n",
      "{'loss': 2.1052, 'learning_rate': 0.00019731587763607857, 'epoch': 0.12}        \n",
      "{'loss': 2.2332, 'learning_rate': 0.00019731516077961116, 'epoch': 0.12}        \n",
      "{'loss': 2.148, 'learning_rate': 0.0001973144438287326, 'epoch': 0.12}          \n",
      "{'loss': 1.7985, 'learning_rate': 0.00019731372678344358, 'epoch': 0.12}        \n",
      "{'loss': 2.3458, 'learning_rate': 0.00019731300964374477, 'epoch': 0.12}        \n",
      "{'loss': 1.8805, 'learning_rate': 0.00019731229240963687, 'epoch': 0.12}        \n",
      "{'loss': 1.8467, 'learning_rate': 0.0001973115750811206, 'epoch': 0.12}         \n",
      "{'loss': 2.2798, 'learning_rate': 0.00019731085765819663, 'epoch': 0.12}        \n",
      "{'loss': 2.2854, 'learning_rate': 0.00019731014014086565, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129241/1061708 [19:20:38<137:12:20,  1.89it/s][2024-03-01 13:27:39,430] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "{'loss': 1.7557, 'learning_rate': 0.00019730949429455036, 'epoch': 0.12}        \n",
      "{'loss': 1.8106, 'learning_rate': 0.00019730877659784803, 'epoch': 0.12}        \n",
      "{'loss': 2.111, 'learning_rate': 0.00019730805880674072, 'epoch': 0.12}         \n",
      "{'loss': 1.4771, 'learning_rate': 0.00019730734092122915, 'epoch': 0.12}        \n",
      "{'loss': 1.8828, 'learning_rate': 0.00019730662294131397, 'epoch': 0.12}        \n",
      "{'loss': 2.2694, 'learning_rate': 0.0001973059048669959, 'epoch': 0.12}         \n",
      "{'loss': 2.1235, 'learning_rate': 0.00019730518669827568, 'epoch': 0.12}        \n",
      "{'loss': 2.311, 'learning_rate': 0.0001973044684351539, 'epoch': 0.12}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9552, 'learning_rate': 0.00019730375007763137, 'epoch': 0.12}        \n",
      "{'loss': 2.1223, 'learning_rate': 0.00019730303162570874, 'epoch': 0.12}        \n",
      "{'loss': 2.4116, 'learning_rate': 0.0001973023130793867, 'epoch': 0.12}         \n",
      "{'loss': 2.194, 'learning_rate': 0.00019730159443866595, 'epoch': 0.12}         \n",
      "{'loss': 2.0382, 'learning_rate': 0.0001973008757035472, 'epoch': 0.12}         \n",
      "{'loss': 1.8096, 'learning_rate': 0.00019730015687403115, 'epoch': 0.12}        \n",
      "{'loss': 1.9744, 'learning_rate': 0.00019729943795011848, 'epoch': 0.12}        \n",
      "{'loss': 2.0971, 'learning_rate': 0.00019729871893180986, 'epoch': 0.12}        \n",
      "{'loss': 2.204, 'learning_rate': 0.00019729799981910607, 'epoch': 0.12}         \n",
      "{'loss': 1.8348, 'learning_rate': 0.00019729728061200773, 'epoch': 0.12}        \n",
      "{'loss': 2.1425, 'learning_rate': 0.00019729656131051558, 'epoch': 0.12}        \n",
      "{'loss': 2.105, 'learning_rate': 0.0001972958419146303, 'epoch': 0.12}          \n",
      "{'loss': 2.0509, 'learning_rate': 0.00019729512242435258, 'epoch': 0.12}        \n",
      "{'loss': 2.255, 'learning_rate': 0.00019729440283968317, 'epoch': 0.12}         \n",
      "{'loss': 2.1061, 'learning_rate': 0.0001972936831606227, 'epoch': 0.12}         \n",
      "{'loss': 1.9378, 'learning_rate': 0.00019729296338717188, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129484/1061708 [19:22:48<139:05:53,  1.86it/s][2024-03-01 13:29:49,003] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1362, 'learning_rate': 0.00019729231551036302, 'epoch': 0.12}        \n",
      "{'loss': 1.8804, 'learning_rate': 0.00019729159555757252, 'epoch': 0.12}        \n",
      "{'loss': 2.1492, 'learning_rate': 0.0001972908755103937, 'epoch': 0.12}         \n",
      "{'loss': 1.9696, 'learning_rate': 0.0001972901553688273, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 129526/1061708 [19:23:10<138:08:01,  1.87it/s][2024-03-01 13:30:11,317] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0353, 'learning_rate': 0.00019728950716071668, 'epoch': 0.12}        \n",
      "{'loss': 2.3169, 'learning_rate': 0.00019728878683981571, 'epoch': 0.12}        \n",
      "{'loss': 2.0049, 'learning_rate': 0.00019728806642452918, 'epoch': 0.12}        \n",
      "{'loss': 2.0486, 'learning_rate': 0.00019728734591485775, 'epoch': 0.12}        \n",
      "{'loss': 1.8135, 'learning_rate': 0.00019728662531080216, 'epoch': 0.12}        \n",
      "{'loss': 1.9825, 'learning_rate': 0.00019728590461236308, 'epoch': 0.12}        \n",
      "{'loss': 2.0767, 'learning_rate': 0.0001972851838195412, 'epoch': 0.12}         \n",
      "{'loss': 1.8277, 'learning_rate': 0.00019728446293233726, 'epoch': 0.12}        \n",
      "{'loss': 1.9273, 'learning_rate': 0.00019728374195075187, 'epoch': 0.12}        \n",
      "{'loss': 2.1826, 'learning_rate': 0.00019728302087478584, 'epoch': 0.12}        \n",
      "{'loss': 1.8184, 'learning_rate': 0.0001972822997044398, 'epoch': 0.12}         \n",
      "{'loss': 1.9828, 'learning_rate': 0.0001972815784397145, 'epoch': 0.12}         \n",
      "{'loss': 1.4708, 'learning_rate': 0.00019728085708061062, 'epoch': 0.12}        \n",
      "{'loss': 2.2629, 'learning_rate': 0.0001972801356271288, 'epoch': 0.12}         \n",
      "{'loss': 2.1321, 'learning_rate': 0.00019727941407926983, 'epoch': 0.12}        \n",
      "{'loss': 2.1471, 'learning_rate': 0.00019727869243703436, 'epoch': 0.12}        \n",
      "{'loss': 1.7329, 'learning_rate': 0.00019727797070042308, 'epoch': 0.12}        \n",
      "{'loss': 2.0896, 'learning_rate': 0.00019727724886943674, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129709/1061708 [19:24:48<137:14:17,  1.89it/s][2024-03-01 13:31:48,833] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0752, 'learning_rate': 0.0001972765991408589, 'epoch': 0.12}         \n",
      "{'loss': 2.4726, 'learning_rate': 0.00019727587713056182, 'epoch': 0.12}        \n",
      "{'loss': 2.0812, 'learning_rate': 0.00019727515502589168, 'epoch': 0.12}        \n",
      "{'loss': 2.1256, 'learning_rate': 0.00019727443282684917, 'epoch': 0.12}        \n",
      "{'loss': 1.6316, 'learning_rate': 0.00019727371053343502, 'epoch': 0.12}        \n",
      "{'loss': 1.9296, 'learning_rate': 0.0001972729881456499, 'epoch': 0.12}         \n",
      "{'loss': 1.8544, 'learning_rate': 0.00019727226566349453, 'epoch': 0.12}        \n",
      "{'loss': 2.0893, 'learning_rate': 0.0001972715430869696, 'epoch': 0.12}         \n",
      "{'loss': 1.9547, 'learning_rate': 0.0001972708204160758, 'epoch': 0.12}         \n",
      "{'loss': 2.1322, 'learning_rate': 0.00019727009765081388, 'epoch': 0.12}        \n",
      "{'loss': 2.267, 'learning_rate': 0.0001972693747911845, 'epoch': 0.12}          \n",
      " 12%|███▎                       | 129810/1061708 [19:25:42<138:57:09,  1.86it/s][2024-03-01 13:32:43,460] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 129811/1061708 [19:25:43<130:07:57,  1.99it/s][2024-03-01 13:32:43,885] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.7897, 'learning_rate': 0.00019726879643553691, 'epoch': 0.12}        \n",
      "{'loss': 2.2296, 'learning_rate': 0.00019726807340604788, 'epoch': 0.12}        \n",
      "{'loss': 1.9147, 'learning_rate': 0.00019726735028219335, 'epoch': 0.12}        \n",
      "{'loss': 1.7259, 'learning_rate': 0.00019726662706397407, 'epoch': 0.12}        \n",
      "{'loss': 1.999, 'learning_rate': 0.0001972659037513907, 'epoch': 0.12}          \n",
      "{'loss': 2.1174, 'learning_rate': 0.0001972651803444439, 'epoch': 0.12}         \n",
      "{'loss': 2.1206, 'learning_rate': 0.00019726445684313445, 'epoch': 0.12}        \n",
      "{'loss': 1.7678, 'learning_rate': 0.00019726373324746303, 'epoch': 0.12}        \n",
      "{'loss': 2.3472, 'learning_rate': 0.00019726300955743033, 'epoch': 0.12}        \n",
      "{'loss': 1.9703, 'learning_rate': 0.00019726228577303707, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129912/1061708 [19:26:37<137:15:08,  1.89it/s][2024-03-01 13:33:37,820] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 129913/1061708 [19:26:37<132:28:01,  1.95it/s][2024-03-01 13:33:38,245] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      " 12%|███▎                       | 129919/1061708 [19:26:40<136:55:35,  1.89it/s][2024-03-01 13:33:41,369] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1383, 'learning_rate': 0.0001972617790678176, 'epoch': 0.12}         \n",
      "{'loss': 1.5591, 'learning_rate': 0.00019726105512301296, 'epoch': 0.12}        \n",
      "{'loss': 1.7087, 'learning_rate': 0.00019726033108384965, 'epoch': 0.12}        \n",
      "{'loss': 2.2175, 'learning_rate': 0.00019725960695032835, 'epoch': 0.12}        \n",
      "{'loss': 1.7375, 'learning_rate': 0.00019725888272244978, 'epoch': 0.12}        \n",
      "{'loss': 1.8695, 'learning_rate': 0.00019725815840021467, 'epoch': 0.12}        \n",
      "{'loss': 1.971, 'learning_rate': 0.00019725743398362364, 'epoch': 0.12}         \n",
      "{'loss': 2.0041, 'learning_rate': 0.00019725670947267748, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 129999/1061708 [19:27:23<137:33:34,  1.88it/s][2024-03-01 13:34:24,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=130000, skipped=1529, lr=[0.00019725598486737686], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 13:34:24,126] [INFO] [timer.py:260:stop] epoch=0/micro_step=130000/global_step=130000, RunningAvgSamplesPerSec=1.8927330884199964, CurrSamplesPerSec=1.9070375223585534, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 2.0713, 'learning_rate': 0.00019725598486737686, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130000/1061708 [19:27:24<137:21:21,  1.88it/s][INFO|trainer.py:2868] 2024-03-01 13:34:24,128 >> Saving model checkpoint to output_model/checkpoint-130000\n",
      "[INFO|trainer.py:2880] 2024-03-01 13:34:24,131 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 13:34:25,368 >> tokenizer config file saved in output_model/checkpoint-130000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 13:34:25,369 >> Special tokens file saved in output_model/checkpoint-130000/special_tokens_map.json\n",
      "[2024-03-01 13:34:25,370] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step130000 is about to be saved!\n",
      "[2024-03-01 13:34:30,617] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_model/checkpoint-130000/global_step130000/mp_rank_00_model_states.pt\n",
      "[2024-03-01 13:34:30,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-130000/global_step130000/mp_rank_00_model_states.pt...\n",
      "[2024-03-01 13:34:44,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-130000/global_step130000/mp_rank_00_model_states.pt.\n",
      "[2024-03-01 13:34:45,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_model/checkpoint-130000/global_step130000/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-03-01 13:34:52,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_model/checkpoint-130000/global_step130000/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-03-01 13:34:52,537] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_model/checkpoint-130000/global_step130000/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-03-01 13:34:52,537] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step130000 is ready now!\n",
      "[INFO|trainer.py:2953] 2024-03-01 13:34:52,653 >> Deleting older checkpoint [output_model/checkpoint-115000] due to args.save_total_limit\n",
      "[INFO|tokenization_utils_base.py:2171] 2024-03-01 13:34:56,456 >> tokenizer config file saved in output_model/checkpoint-130000/pt_lora_model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2178] 2024-03-01 13:34:56,456 >> Special tokens file saved in output_model/checkpoint-130000/pt_lora_model/special_tokens_map.json\n",
      "{'loss': 1.8523, 'learning_rate': 0.00019725526016772246, 'epoch': 0.12}        \n",
      "{'loss': 2.1571, 'learning_rate': 0.000197254535373715, 'epoch': 0.12}          \n",
      "{'loss': 2.1413, 'learning_rate': 0.0001972538104853552, 'epoch': 0.12}         \n",
      "{'loss': 1.9942, 'learning_rate': 0.00019725308550264377, 'epoch': 0.12}        \n",
      "{'loss': 2.1508, 'learning_rate': 0.00019725236042558136, 'epoch': 0.12}        \n",
      "{'loss': 2.0436, 'learning_rate': 0.00019725163525416873, 'epoch': 0.12}        \n",
      "{'loss': 2.3276, 'learning_rate': 0.0001972509099884065, 'epoch': 0.12}         \n",
      "{'loss': 1.9142, 'learning_rate': 0.00019725018462829553, 'epoch': 0.12}        \n",
      "{'loss': 1.6988, 'learning_rate': 0.00019724945917383636, 'epoch': 0.12}        \n",
      "{'loss': 1.8668, 'learning_rate': 0.0001972487336250298, 'epoch': 0.12}         \n",
      "{'loss': 2.0209, 'learning_rate': 0.0001972480079818765, 'epoch': 0.12}         \n",
      "{'loss': 2.271, 'learning_rate': 0.00019724728224437715, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 130120/1061708 [19:29:01<137:44:03,  1.88it/s][2024-03-01 13:36:01,797] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 130121/1061708 [19:29:01<129:34:40,  2.00it/s][2024-03-01 13:36:02,226] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1716, 'learning_rate': 0.00019724670158644905, 'epoch': 0.12}        \n",
      "{'loss': 1.8958, 'learning_rate': 0.0001972459756791287, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 130142/1061708 [19:29:12<137:49:37,  1.88it/s][2024-03-01 13:36:13,336] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1542, 'learning_rate': 0.00019724532228187615, 'epoch': 0.12}        \n",
      "{'loss': 2.2183, 'learning_rate': 0.00019724459619530266, 'epoch': 0.12}        \n",
      "{'loss': 2.0125, 'learning_rate': 0.00019724387001438648, 'epoch': 0.12}        \n",
      "{'loss': 1.9241, 'learning_rate': 0.00019724314373912832, 'epoch': 0.12}        \n",
      "{'loss': 2.2033, 'learning_rate': 0.0001972424173695288, 'epoch': 0.12}         \n",
      "{'loss': 2.3515, 'learning_rate': 0.0001972416909055887, 'epoch': 0.12}         \n",
      "{'loss': 1.795, 'learning_rate': 0.00019724096434730874, 'epoch': 0.12}         \n",
      "{'loss': 2.2877, 'learning_rate': 0.0001972402376946896, 'epoch': 0.12}         \n",
      "{'loss': 1.9851, 'learning_rate': 0.00019723951094773195, 'epoch': 0.12}        \n",
      "{'loss': 1.9323, 'learning_rate': 0.00019723878410643652, 'epoch': 0.12}        \n",
      "{'loss': 2.0548, 'learning_rate': 0.000197238057170804, 'epoch': 0.12}          \n",
      "{'loss': 2.0101, 'learning_rate': 0.00019723733014083514, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130269/1061708 [19:30:20<137:17:33,  1.88it/s][2024-03-01 13:37:20,850] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.8965, 'learning_rate': 0.00019723667573320617, 'epoch': 0.12}        \n",
      "{'loss': 1.8855, 'learning_rate': 0.00019723594852400013, 'epoch': 0.12}        \n",
      "{'loss': 2.3677, 'learning_rate': 0.0001972352212204598, 'epoch': 0.12}         \n",
      "{'loss': 1.9633, 'learning_rate': 0.00019723449382258587, 'epoch': 0.12}        \n",
      "{'loss': 1.8595, 'learning_rate': 0.00019723376633037898, 'epoch': 0.12}        \n",
      "{'loss': 2.178, 'learning_rate': 0.00019723303874383992, 'epoch': 0.12}         \n",
      "{'loss': 1.7158, 'learning_rate': 0.00019723231106296937, 'epoch': 0.12}        \n",
      "{'loss': 2.2428, 'learning_rate': 0.000197231583287768, 'epoch': 0.12}          \n",
      "{'loss': 2.0474, 'learning_rate': 0.00019723085541823655, 'epoch': 0.12}        \n",
      "{'loss': 2.2409, 'learning_rate': 0.00019723012745437576, 'epoch': 0.12}        \n",
      "{'loss': 2.1076, 'learning_rate': 0.00019722939939618623, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130370/1061708 [19:31:14<137:28:41,  1.88it/s][2024-03-01 13:38:15,048] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 130371/1061708 [19:31:14<129:14:50,  2.00it/s][2024-03-01 13:38:15,474] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.749, 'learning_rate': 0.00019722881688171847, 'epoch': 0.12}         \n",
      "{'loss': 2.2007, 'learning_rate': 0.00019722808865373916, 'epoch': 0.12}        \n",
      "{'loss': 2.4149, 'learning_rate': 0.0001972273603314331, 'epoch': 0.12}         \n",
      "{'loss': 2.2738, 'learning_rate': 0.0001972266319148011, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 130410/1061708 [19:31:35<137:34:01,  1.88it/s][2024-03-01 13:38:36,251] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.7992, 'learning_rate': 0.00019722597625918415, 'epoch': 0.12}        \n",
      "{'loss': 2.1919, 'learning_rate': 0.00019722524766333473, 'epoch': 0.12}        \n",
      "{'loss': 1.9934, 'learning_rate': 0.00019722451897316134, 'epoch': 0.12}        \n",
      "{'loss': 1.7523, 'learning_rate': 0.00019722379018866472, 'epoch': 0.12}        \n",
      "{'loss': 1.8567, 'learning_rate': 0.00019722306130984558, 'epoch': 0.12}        \n",
      "{'loss': 1.948, 'learning_rate': 0.00019722233233670461, 'epoch': 0.12}         \n",
      "{'loss': 1.9194, 'learning_rate': 0.00019722160326924256, 'epoch': 0.12}        \n",
      "{'loss': 1.7932, 'learning_rate': 0.0001972208741074601, 'epoch': 0.12}         \n",
      "{'loss': 1.805, 'learning_rate': 0.00019722014485135792, 'epoch': 0.12}         \n",
      "{'loss': 2.0818, 'learning_rate': 0.00019721941550093677, 'epoch': 0.12}        \n",
      "{'loss': 2.0439, 'learning_rate': 0.00019721868605619732, 'epoch': 0.12}        \n",
      "{'loss': 2.1497, 'learning_rate': 0.0001972179565171403, 'epoch': 0.12}         \n",
      "{'loss': 2.275, 'learning_rate': 0.00019721722688376643, 'epoch': 0.12}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8208, 'learning_rate': 0.0001972164971560764, 'epoch': 0.12}         \n",
      "{'loss': 2.3737, 'learning_rate': 0.00019721576733407092, 'epoch': 0.12}        \n",
      "{'loss': 2.2035, 'learning_rate': 0.0001972150374177507, 'epoch': 0.12}         \n",
      "{'loss': 2.0677, 'learning_rate': 0.00019721430740711642, 'epoch': 0.12}        \n",
      "{'loss': 1.7507, 'learning_rate': 0.00019721357730216884, 'epoch': 0.12}        \n",
      "{'loss': 2.0157, 'learning_rate': 0.00019721284710290862, 'epoch': 0.12}        \n",
      "{'loss': 2.1182, 'learning_rate': 0.0001972121168093365, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 130611/1061708 [19:33:23<137:26:30,  1.88it/s][2024-03-01 13:40:23,716] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 130612/1061708 [19:33:23<129:05:44,  2.00it/s][2024-03-01 13:40:24,140] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0416, 'learning_rate': 0.00019721153250657473, 'epoch': 0.12}        \n",
      "{'loss': 1.9617, 'learning_rate': 0.00019721080204324295, 'epoch': 0.12}        \n",
      "{'loss': 2.1114, 'learning_rate': 0.00019721007148560128, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130644/1061708 [19:33:40<139:33:18,  1.85it/s][2024-03-01 13:40:41,198] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.0471, 'learning_rate': 0.00019720941390308935, 'epoch': 0.12}        \n",
      "{'loss': 1.7677, 'learning_rate': 0.00019720868316626077, 'epoch': 0.12}        \n",
      "{'loss': 1.8271, 'learning_rate': 0.0001972079523351243, 'epoch': 0.12}         \n",
      "{'loss': 2.1209, 'learning_rate': 0.00019720722140968068, 'epoch': 0.12}        \n",
      "{'loss': 2.1017, 'learning_rate': 0.00019720649038993063, 'epoch': 0.12}        \n",
      "{'loss': 2.0646, 'learning_rate': 0.00019720575927587478, 'epoch': 0.12}        \n",
      "{'loss': 2.3964, 'learning_rate': 0.00019720502806751396, 'epoch': 0.12}        \n",
      "{'loss': 2.026, 'learning_rate': 0.00019720429676484878, 'epoch': 0.12}         \n",
      "{'loss': 2.2709, 'learning_rate': 0.00019720356536788, 'epoch': 0.12}           \n",
      "{'loss': 2.1178, 'learning_rate': 0.0001972028338766083, 'epoch': 0.12}         \n",
      "{'loss': 1.4244, 'learning_rate': 0.00019720210229103442, 'epoch': 0.12}        \n",
      "{'loss': 2.1465, 'learning_rate': 0.00019720137061115907, 'epoch': 0.12}        \n",
      "{'loss': 1.6427, 'learning_rate': 0.00019720063883698293, 'epoch': 0.12}        \n",
      "{'loss': 1.6826, 'learning_rate': 0.00019719990696850673, 'epoch': 0.12}        \n",
      "{'loss': 2.3169, 'learning_rate': 0.00019719917500573116, 'epoch': 0.12}        \n",
      "{'loss': 2.2497, 'learning_rate': 0.00019719844294865694, 'epoch': 0.12}        \n",
      "{'loss': 1.9672, 'learning_rate': 0.0001971977107972848, 'epoch': 0.12}         \n",
      "{'loss': 1.9475, 'learning_rate': 0.00019719697855161546, 'epoch': 0.12}        \n",
      "{'loss': 2.2845, 'learning_rate': 0.00019719624621164958, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130830/1061708 [19:35:20<137:49:22,  1.88it/s][2024-03-01 13:42:20,756] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1157, 'learning_rate': 0.00019719558702505733, 'epoch': 0.12}        \n",
      "{'loss': 1.8015, 'learning_rate': 0.00019719485450593008, 'epoch': 0.12}        \n",
      "{'loss': 2.0937, 'learning_rate': 0.00019719412189250833, 'epoch': 0.12}        \n",
      "{'loss': 2.2282, 'learning_rate': 0.00019719338918479279, 'epoch': 0.12}        \n",
      "{'loss': 2.1082, 'learning_rate': 0.0001971926563827843, 'epoch': 0.12}         \n",
      "{'loss': 1.9664, 'learning_rate': 0.00019719192348648345, 'epoch': 0.12}        \n",
      "{'loss': 2.1985, 'learning_rate': 0.000197191190495891, 'epoch': 0.12}          \n",
      "{'loss': 2.1722, 'learning_rate': 0.00019719045741100764, 'epoch': 0.12}        \n",
      "{'loss': 2.2382, 'learning_rate': 0.0001971897242318341, 'epoch': 0.12}         \n",
      "{'loss': 2.2232, 'learning_rate': 0.00019718899095837108, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 130931/1061708 [19:36:14<136:51:57,  1.89it/s][2024-03-01 13:43:15,215] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 130932/1061708 [19:36:15<128:33:11,  2.01it/s][2024-03-01 13:43:15,637] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1031, 'learning_rate': 0.00019718840427171276, 'epoch': 0.12}        \n",
      "{'loss': 2.1938, 'learning_rate': 0.00019718767082853048, 'epoch': 0.12}        \n",
      "{'loss': 1.795, 'learning_rate': 0.00019718693729106068, 'epoch': 0.12}         \n",
      "{'loss': 1.8473, 'learning_rate': 0.00019718620365930415, 'epoch': 0.12}        \n",
      "{'loss': 1.801, 'learning_rate': 0.00019718546993326158, 'epoch': 0.12}         \n",
      "{'loss': 2.3568, 'learning_rate': 0.00019718473611293362, 'epoch': 0.12}        \n",
      "{'loss': 1.9284, 'learning_rate': 0.00019718400219832105, 'epoch': 0.12}        \n",
      "{'loss': 1.6226, 'learning_rate': 0.00019718326818942455, 'epoch': 0.12}        \n",
      "{'loss': 2.052, 'learning_rate': 0.0001971825340862448, 'epoch': 0.12}          \n",
      "{'loss': 2.0837, 'learning_rate': 0.0001971817998887826, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131033/1061708 [19:37:08<136:49:04,  1.89it/s][2024-03-01 13:44:08,976] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 131034/1061708 [19:37:08<128:29:58,  2.01it/s][2024-03-01 13:44:09,403] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1443, 'learning_rate': 0.00019718121246292992, 'epoch': 0.12}        \n",
      "{'loss': 1.7453, 'learning_rate': 0.00019718047809576098, 'epoch': 0.12}        \n",
      "{'loss': 1.7914, 'learning_rate': 0.00019717974363431156, 'epoch': 0.12}        \n",
      "{'loss': 1.945, 'learning_rate': 0.00019717900907858233, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131078/1061708 [19:37:32<136:35:51,  1.89it/s][2024-03-01 13:44:32,572] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 1.8133, 'learning_rate': 0.0001971783478978174, 'epoch': 0.12}         \n",
      "{'loss': 1.8686, 'learning_rate': 0.00019717761316295852, 'epoch': 0.12}        \n",
      "{'loss': 2.3828, 'learning_rate': 0.00019717687833382196, 'epoch': 0.12}        \n",
      "{'loss': 1.9742, 'learning_rate': 0.00019717614341040833, 'epoch': 0.12}        \n",
      "{'loss': 2.0889, 'learning_rate': 0.00019717540839271844, 'epoch': 0.12}        \n",
      "{'loss': 1.8534, 'learning_rate': 0.00019717467328075295, 'epoch': 0.12}        \n",
      "{'loss': 2.5034, 'learning_rate': 0.00019717393807451258, 'epoch': 0.12}        \n",
      "{'loss': 2.2047, 'learning_rate': 0.00019717320277399804, 'epoch': 0.12}        \n",
      "{'loss': 2.3681, 'learning_rate': 0.00019717246737921008, 'epoch': 0.12}        \n",
      "{'loss': 1.9048, 'learning_rate': 0.00019717173189014937, 'epoch': 0.12}        \n",
      "{'loss': 2.2172, 'learning_rate': 0.00019717099630681663, 'epoch': 0.12}        \n",
      "{'loss': 1.7577, 'learning_rate': 0.0001971702606292126, 'epoch': 0.12}         \n",
      "{'loss': 2.202, 'learning_rate': 0.00019716952485733798, 'epoch': 0.12}         \n",
      "{'loss': 1.6673, 'learning_rate': 0.00019716878899119346, 'epoch': 0.12}        \n",
      "{'loss': 1.9133, 'learning_rate': 0.00019716805303077978, 'epoch': 0.12}        \n",
      "{'loss': 2.1383, 'learning_rate': 0.00019716731697609768, 'epoch': 0.12}        \n",
      "{'loss': 2.1806, 'learning_rate': 0.0001971665808271478, 'epoch': 0.12}         \n",
      "{'loss': 1.9176, 'learning_rate': 0.00019716584458393094, 'epoch': 0.12}        \n",
      "{'loss': 2.4721, 'learning_rate': 0.00019716510824644773, 'epoch': 0.12}        \n",
      "{'loss': 2.0589, 'learning_rate': 0.00019716437181469893, 'epoch': 0.12}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|███▎                       | 131279/1061708 [19:39:19<136:14:20,  1.90it/s][2024-03-01 13:46:20,268] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 1.746, 'learning_rate': 0.00019716370894552854, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131280/1061708 [19:39:20<127:58:05,  2.02it/s][2024-03-01 13:46:20,690] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.0997, 'learning_rate': 0.0001971630460000041, 'epoch': 0.12}         \n",
      "{'loss': 1.4996, 'learning_rate': 0.00019716230930431544, 'epoch': 0.12}        \n",
      "{'loss': 1.893, 'learning_rate': 0.00019716157251436394, 'epoch': 0.12}         \n",
      "{'loss': 1.8531, 'learning_rate': 0.00019716083563015024, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 131322/1061708 [19:39:42<136:17:27,  1.90it/s][2024-03-01 13:46:43,205] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.5119, 'learning_rate': 0.00019716017235376433, 'epoch': 0.12}        \n",
      "{'loss': 1.6376, 'learning_rate': 0.0001971594352904545, 'epoch': 0.12}         \n",
      "{'loss': 2.2692, 'learning_rate': 0.00019715869813288458, 'epoch': 0.12}        \n",
      "{'loss': 2.1958, 'learning_rate': 0.00019715796088105527, 'epoch': 0.12}        \n",
      "{'loss': 1.9917, 'learning_rate': 0.0001971572235349673, 'epoch': 0.12}         \n",
      "{'loss': 2.2842, 'learning_rate': 0.00019715648609462138, 'epoch': 0.12}        \n",
      "{'loss': 1.663, 'learning_rate': 0.00019715574856001827, 'epoch': 0.12}         \n",
      "{'loss': 1.8325, 'learning_rate': 0.0001971550109311586, 'epoch': 0.12}         \n",
      "{'loss': 1.7178, 'learning_rate': 0.00019715427320804313, 'epoch': 0.12}        \n",
      "{'loss': 1.9258, 'learning_rate': 0.0001971535353906726, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131425/1061708 [19:40:37<136:19:41,  1.90it/s][2024-03-01 13:47:38,249] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0789, 'learning_rate': 0.00019715287127445165, 'epoch': 0.12}        \n",
      "{'loss': 2.2413, 'learning_rate': 0.00019715213327799842, 'epoch': 0.12}        \n",
      "{'loss': 2.1557, 'learning_rate': 0.0001971513951872922, 'epoch': 0.12}         \n",
      "{'loss': 2.2989, 'learning_rate': 0.0001971506570023337, 'epoch': 0.12}         \n",
      "{'loss': 1.717, 'learning_rate': 0.00019714991872312357, 'epoch': 0.12}         \n",
      "{'loss': 1.917, 'learning_rate': 0.00019714918034966262, 'epoch': 0.12}         \n",
      "{'loss': 2.0237, 'learning_rate': 0.00019714844188195154, 'epoch': 0.12}        \n",
      "{'loss': 2.1725, 'learning_rate': 0.000197147703319991, 'epoch': 0.12}          \n",
      "{'loss': 2.5977, 'learning_rate': 0.00019714696466378178, 'epoch': 0.12}        \n",
      "{'loss': 2.0668, 'learning_rate': 0.0001971462259133246, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131526/1061708 [19:41:30<136:05:45,  1.90it/s][2024-03-01 13:48:31,353] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 131527/1061708 [19:41:31<127:52:21,  2.02it/s][2024-03-01 13:48:31,774] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.4165, 'learning_rate': 0.00019714563484510072, 'epoch': 0.12}        \n",
      "{'loss': 1.6985, 'learning_rate': 0.00019714489592499894, 'epoch': 0.12}        \n",
      "{'loss': 2.3835, 'learning_rate': 0.00019714415691065115, 'epoch': 0.12}        \n",
      "{'loss': 2.1728, 'learning_rate': 0.0001971434178020581, 'epoch': 0.12}         \n",
      "{'loss': 1.9445, 'learning_rate': 0.0001971426785992205, 'epoch': 0.12}         \n",
      "{'loss': 2.0433, 'learning_rate': 0.00019714193930213907, 'epoch': 0.12}        \n",
      "{'loss': 1.9372, 'learning_rate': 0.0001971411999108145, 'epoch': 0.12}         \n",
      "{'loss': 2.1624, 'learning_rate': 0.00019714046042524754, 'epoch': 0.12}        \n",
      "{'loss': 1.8884, 'learning_rate': 0.0001971397208454389, 'epoch': 0.12}         \n",
      "{'loss': 1.9183, 'learning_rate': 0.0001971389811713893, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131622/1061708 [19:42:21<136:06:18,  1.90it/s][2024-03-01 13:49:21,733] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.11, 'learning_rate': 0.00019713831538416922, 'epoch': 0.12}          \n",
      "{'loss': 2.2586, 'learning_rate': 0.00019713757553106379, 'epoch': 0.12}        \n",
      "{'loss': 1.8637, 'learning_rate': 0.00019713683558371945, 'epoch': 0.12}        \n",
      "{'loss': 2.3141, 'learning_rate': 0.00019713609554213694, 'epoch': 0.12}        \n",
      "{'loss': 2.106, 'learning_rate': 0.00019713535540631698, 'epoch': 0.12}         \n",
      "{'loss': 1.7373, 'learning_rate': 0.00019713461517626031, 'epoch': 0.12}        \n",
      "{'loss': 2.3131, 'learning_rate': 0.0001971338748519676, 'epoch': 0.12}         \n",
      "{'loss': 2.1216, 'learning_rate': 0.0001971331344334396, 'epoch': 0.12}         \n",
      "{'loss': 1.967, 'learning_rate': 0.00019713239392067707, 'epoch': 0.12}         \n",
      "{'loss': 1.9858, 'learning_rate': 0.00019713165331368063, 'epoch': 0.12}        \n",
      "{'loss': 2.1582, 'learning_rate': 0.00019713091261245108, 'epoch': 0.12}        \n",
      "{'loss': 2.158, 'learning_rate': 0.0001971301718169891, 'epoch': 0.12}          \n",
      " 12%|███▎                       | 131749/1061708 [19:43:28<142:14:36,  1.82it/s][2024-03-01 13:50:29,329] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2615, 'learning_rate': 0.0001971295050205052, 'epoch': 0.12}         \n",
      "{'loss': 1.8973, 'learning_rate': 0.0001971287640460036, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131768/1061708 [19:43:38<138:19:05,  1.87it/s][2024-03-01 13:50:39,520] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1781, 'learning_rate': 0.0001971280970883852, 'epoch': 0.12}         \n",
      "{'loss': 1.9809, 'learning_rate': 0.0001971273559348466, 'epoch': 0.12}         \n",
      "{'loss': 1.8067, 'learning_rate': 0.00019712661468707904, 'epoch': 0.12}        \n",
      "{'loss': 2.3402, 'learning_rate': 0.0001971258733450832, 'epoch': 0.12}         \n",
      "{'loss': 1.9525, 'learning_rate': 0.00019712513190885985, 'epoch': 0.12}        \n",
      "{'loss': 1.903, 'learning_rate': 0.00019712439037840963, 'epoch': 0.12}         \n",
      "{'loss': 2.0327, 'learning_rate': 0.00019712364875373338, 'epoch': 0.12}        \n",
      "{'loss': 1.8361, 'learning_rate': 0.00019712290703483175, 'epoch': 0.12}        \n",
      "{'loss': 2.2433, 'learning_rate': 0.00019712216522170545, 'epoch': 0.12}        \n",
      "{'loss': 2.6037, 'learning_rate': 0.00019712142331435525, 'epoch': 0.12}        \n",
      "{'loss': 2.1588, 'learning_rate': 0.0001971206813127818, 'epoch': 0.12}         \n",
      "{'loss': 1.6975, 'learning_rate': 0.0001971199392169859, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 131884/1061708 [19:44:40<136:43:52,  1.89it/s][2024-03-01 13:51:40,870] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.1177, 'learning_rate': 0.0001971192712502099, 'epoch': 0.12}         \n",
      "{'loss': 1.7649, 'learning_rate': 0.00019711852897539322, 'epoch': 0.12}        \n",
      "{'loss': 1.8264, 'learning_rate': 0.00019711778660635613, 'epoch': 0.12}        \n",
      "{'loss': 1.9163, 'learning_rate': 0.00019711704414309935, 'epoch': 0.12}        \n",
      "{'loss': 1.8998, 'learning_rate': 0.0001971163015856236, 'epoch': 0.12}         \n",
      "{'loss': 1.9949, 'learning_rate': 0.00019711555893392964, 'epoch': 0.12}        \n",
      "{'loss': 2.0187, 'learning_rate': 0.00019711481618801813, 'epoch': 0.12}        \n",
      "{'loss': 1.9133, 'learning_rate': 0.00019711407334788982, 'epoch': 0.12}        \n",
      "{'loss': 1.69, 'learning_rate': 0.00019711333041354544, 'epoch': 0.12}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7587, 'learning_rate': 0.00019711258738498567, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 131985/1061708 [19:45:33<136:22:16,  1.89it/s][2024-03-01 13:52:34,475] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 131986/1061708 [19:45:34<128:02:29,  2.02it/s][2024-03-01 13:52:34,896] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.2115, 'learning_rate': 0.00019711199289430332, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 131999/1061708 [19:45:41<139:12:03,  1.86it/s][2024-03-01 13:52:41,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=132000, skipped=1557, lr=[0.00019711124969615772], mom=[(0.9, 0.999)]\n",
      "[2024-03-01 13:52:41,902] [INFO] [timer.py:260:stop] epoch=0/micro_step=132000/global_step=132000, RunningAvgSamplesPerSec=1.8927329761387548, CurrSamplesPerSec=1.9074546784962125, MemAllocated=18.31GB, MaxMemAllocated=21.41GB\n",
      "{'loss': 1.7328, 'learning_rate': 0.00019711124969615772, 'epoch': 0.12}        \n",
      "{'loss': 1.7668, 'learning_rate': 0.00019711050640379877, 'epoch': 0.12}        \n",
      "{'loss': 1.9995, 'learning_rate': 0.0001971097630172272, 'epoch': 0.12}         \n",
      "{'loss': 2.4071, 'learning_rate': 0.00019710901953644378, 'epoch': 0.12}        \n",
      "{'loss': 1.6806, 'learning_rate': 0.00019710827596144915, 'epoch': 0.12}        \n",
      "{'loss': 1.9671, 'learning_rate': 0.00019710753229224402, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 132052/1061708 [19:46:09<136:21:01,  1.89it/s][2024-03-01 13:53:09,789] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1754, 'learning_rate': 0.0001971068629094101, 'epoch': 0.12}         \n",
      "{'loss': 2.3844, 'learning_rate': 0.00019710611906120713, 'epoch': 0.12}        \n",
      "{'loss': 1.8572, 'learning_rate': 0.0001971053751187958, 'epoch': 0.12}         \n",
      "{'loss': 2.0511, 'learning_rate': 0.00019710463108217683, 'epoch': 0.12}        \n",
      "{'loss': 1.7344, 'learning_rate': 0.00019710388695135092, 'epoch': 0.12}        \n",
      "{'loss': 1.8655, 'learning_rate': 0.00019710314272631881, 'epoch': 0.12}        \n",
      "{'loss': 1.9492, 'learning_rate': 0.00019710239840708122, 'epoch': 0.12}        \n",
      "{'loss': 1.8841, 'learning_rate': 0.0001971016539936389, 'epoch': 0.12}         \n",
      "{'loss': 1.9311, 'learning_rate': 0.0001971009094859925, 'epoch': 0.12}         \n",
      "{'loss': 2.3758, 'learning_rate': 0.00019710016488414282, 'epoch': 0.12}        \n",
      "{'loss': 2.1632, 'learning_rate': 0.00019709942018809054, 'epoch': 0.12}        \n",
      "{'loss': 2.1925, 'learning_rate': 0.00019709867539783642, 'epoch': 0.12}        \n",
      "{'loss': 2.0792, 'learning_rate': 0.0001970979305133811, 'epoch': 0.12}         \n",
      "{'loss': 1.7175, 'learning_rate': 0.00019709718553472541, 'epoch': 0.12}        \n",
      "{'loss': 2.0045, 'learning_rate': 0.00019709644046187, 'epoch': 0.12}           \n",
      "{'loss': 1.9157, 'learning_rate': 0.00019709569529481564, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 132217/1061708 [19:47:34<136:16:28,  1.89it/s][2024-03-01 13:54:35,469] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.2365, 'learning_rate': 0.00019709502456392717, 'epoch': 0.12}        \n",
      "{'loss': 2.1599, 'learning_rate': 0.00019709427921789675, 'epoch': 0.12}        \n",
      "{'loss': 2.417, 'learning_rate': 0.0001970935337776694, 'epoch': 0.12}          \n",
      "{'loss': 1.6881, 'learning_rate': 0.00019709278824324593, 'epoch': 0.12}        \n",
      "{'loss': 1.6135, 'learning_rate': 0.00019709204261462703, 'epoch': 0.12}        \n",
      "{'loss': 2.1598, 'learning_rate': 0.0001970912968918134, 'epoch': 0.12}         \n",
      "{'loss': 1.9577, 'learning_rate': 0.0001970905510748058, 'epoch': 0.12}         \n",
      "{'loss': 2.159, 'learning_rate': 0.00019708980516360493, 'epoch': 0.12}         \n",
      "{'loss': 2.1385, 'learning_rate': 0.00019708905915821152, 'epoch': 0.12}        \n",
      "{'loss': 2.0958, 'learning_rate': 0.0001970883130586263, 'epoch': 0.12}         \n",
      " 12%|███▎                       | 132318/1061708 [19:48:28<139:49:31,  1.85it/s][2024-03-01 13:55:29,342] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 132319/1061708 [19:48:29<132:36:33,  1.95it/s][2024-03-01 13:55:29,766] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.8351, 'learning_rate': 0.0001970877161111405, 'epoch': 0.12}         \n",
      "{'loss': 2.5806, 'learning_rate': 0.00019708696984201184, 'epoch': 0.12}        \n",
      "{'loss': 1.8877, 'learning_rate': 0.00019708622347869339, 'epoch': 0.12}        \n",
      "{'loss': 2.1298, 'learning_rate': 0.00019708547702118587, 'epoch': 0.12}        \n",
      "{'loss': 2.0426, 'learning_rate': 0.00019708473046949, 'epoch': 0.12}           \n",
      "{'loss': 1.9893, 'learning_rate': 0.00019708398382360654, 'epoch': 0.12}        \n",
      "{'loss': 2.0921, 'learning_rate': 0.00019708323708353618, 'epoch': 0.12}        \n",
      "{'loss': 2.1842, 'learning_rate': 0.00019708249024927966, 'epoch': 0.12}        \n",
      "{'loss': 2.3756, 'learning_rate': 0.00019708174332083768, 'epoch': 0.12}        \n",
      "{'loss': 2.0622, 'learning_rate': 0.000197080996298211, 'epoch': 0.12}          \n",
      "{'loss': 2.2105, 'learning_rate': 0.00019708024918140034, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 132420/1061708 [19:49:23<136:48:19,  1.89it/s][2024-03-01 13:56:24,235] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 12%|███▎                       | 132421/1061708 [19:49:24<128:45:38,  2.00it/s][2024-03-01 13:56:24,656] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9013, 'learning_rate': 0.0001970796514201398, 'epoch': 0.12}         \n",
      "{'loss': 2.1323, 'learning_rate': 0.00019707890413379976, 'epoch': 0.12}        \n",
      "{'loss': 2.0204, 'learning_rate': 0.00019707815675327776, 'epoch': 0.12}        \n",
      "{'loss': 1.8793, 'learning_rate': 0.0001970774092785745, 'epoch': 0.12}         \n",
      "{'loss': 2.3551, 'learning_rate': 0.00019707666170969078, 'epoch': 0.12}        \n",
      "{'loss': 1.8499, 'learning_rate': 0.0001970759140466272, 'epoch': 0.12}         \n",
      "{'loss': 1.888, 'learning_rate': 0.0001970751662893846, 'epoch': 0.12}          \n",
      " 12%|███▎                       | 132495/1061708 [19:50:03<146:20:12,  1.76it/s][2024-03-01 13:57:04,501] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.2133, 'learning_rate': 0.0001970744932273438, 'epoch': 0.12}         \n",
      "{'loss': 2.5143, 'learning_rate': 0.00019707374529116296, 'epoch': 0.12}        \n",
      "{'loss': 1.8116, 'learning_rate': 0.0001970729972608052, 'epoch': 0.12}         \n",
      "{'loss': 2.5636, 'learning_rate': 0.00019707224913627117, 'epoch': 0.12}        \n",
      "{'loss': 1.8973, 'learning_rate': 0.00019707150091756166, 'epoch': 0.12}        \n",
      "{'loss': 2.1354, 'learning_rate': 0.00019707075260467735, 'epoch': 0.12}        \n",
      "{'loss': 2.1055, 'learning_rate': 0.00019707000419761901, 'epoch': 0.12}        \n",
      "{'loss': 2.2148, 'learning_rate': 0.00019706925569638732, 'epoch': 0.12}        \n",
      "{'loss': 2.1692, 'learning_rate': 0.00019706850710098306, 'epoch': 0.12}        \n",
      "{'loss': 2.0292, 'learning_rate': 0.0001970677584114069, 'epoch': 0.12}         \n",
      "{'loss': 1.997, 'learning_rate': 0.00019706700962765958, 'epoch': 0.12}         \n",
      "{'loss': 1.8053, 'learning_rate': 0.00019706626074974187, 'epoch': 0.12}        \n",
      "{'loss': 1.8687, 'learning_rate': 0.00019706551177765442, 'epoch': 0.12}        \n",
      "{'loss': 2.1463, 'learning_rate': 0.00019706476271139802, 'epoch': 0.12}        \n",
      " 12%|███▎                       | 132630/1061708 [19:51:17<140:07:16,  1.84it/s][2024-03-01 13:58:17,987] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.114, 'learning_rate': 0.0001970640884712534, 'epoch': 0.12}          \n",
      "{'loss': 1.9331, 'learning_rate': 0.00019706333922607795, 'epoch': 0.12}        \n",
      "{'loss': 1.9755, 'learning_rate': 0.00019706258988673567, 'epoch': 0.12}        \n",
      "{'loss': 2.1303, 'learning_rate': 0.00019706184045322723, 'epoch': 0.12}        \n",
      "{'loss': 2.3333, 'learning_rate': 0.00019706109092555338, 'epoch': 0.12}        \n",
      "{'loss': 2.38, 'learning_rate': 0.00019706034130371485, 'epoch': 0.12}          \n",
      "{'loss': 2.2438, 'learning_rate': 0.00019705959158771236, 'epoch': 0.12}        \n",
      "{'loss': 1.9248, 'learning_rate': 0.00019705884177754666, 'epoch': 0.12}        \n",
      "{'loss': 2.2528, 'learning_rate': 0.00019705809187321846, 'epoch': 0.13}        \n",
      "{'loss': 1.8079, 'learning_rate': 0.00019705734187472847, 'epoch': 0.13}        \n",
      " 13%|███▍                       | 132731/1061708 [19:52:12<142:44:32,  1.81it/s][2024-03-01 13:59:12,707] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 13%|███▍                       | 132732/1061708 [19:52:12<133:14:26,  1.94it/s][2024-03-01 13:59:13,139] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 1.9572, 'learning_rate': 0.0001970567418081405, 'epoch': 0.13}         \n",
      " 13%|███▍                       | 132744/1061708 [19:52:19<141:49:11,  1.82it/s][2024-03-01 13:59:19,615] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1111, 'learning_rate': 0.00019705606666119632, 'epoch': 0.13}        \n",
      "{'loss': 1.8407, 'learning_rate': 0.0001970553164084732, 'epoch': 0.13}         \n",
      "{'loss': 2.1452, 'learning_rate': 0.000197054566061591, 'epoch': 0.13}          \n",
      "{'loss': 1.9403, 'learning_rate': 0.0001970538156205504, 'epoch': 0.13}         \n",
      "{'loss': 1.8047, 'learning_rate': 0.00019705306508535224, 'epoch': 0.13}        \n",
      "{'loss': 1.9639, 'learning_rate': 0.00019705231445599715, 'epoch': 0.13}        \n",
      "{'loss': 1.9833, 'learning_rate': 0.00019705156373248594, 'epoch': 0.13}        \n",
      "{'loss': 2.0411, 'learning_rate': 0.00019705081291481925, 'epoch': 0.13}        \n",
      "{'loss': 2.1675, 'learning_rate': 0.00019705006200299786, 'epoch': 0.13}        \n",
      "{'loss': 2.0837, 'learning_rate': 0.0001970493109970225, 'epoch': 0.13}         \n",
      "{'loss': 2.3929, 'learning_rate': 0.00019704855989689386, 'epoch': 0.13}        \n",
      "{'loss': 1.7713, 'learning_rate': 0.00019704780870261271, 'epoch': 0.13}        \n",
      "{'loss': 1.5954, 'learning_rate': 0.0001970470574141798, 'epoch': 0.13}         \n",
      "{'loss': 2.1834, 'learning_rate': 0.00019704630603159577, 'epoch': 0.13}        \n",
      "{'loss': 1.9594, 'learning_rate': 0.00019704555455486142, 'epoch': 0.13}        \n",
      "{'loss': 2.0297, 'learning_rate': 0.00019704480298397747, 'epoch': 0.13}        \n",
      "{'loss': 2.0262, 'learning_rate': 0.00019704405131894465, 'epoch': 0.13}        \n",
      "{'loss': 2.0286, 'learning_rate': 0.00019704329955976365, 'epoch': 0.13}        \n",
      "{'loss': 1.9773, 'learning_rate': 0.00019704254770643527, 'epoch': 0.13}        \n",
      "{'loss': 1.9736, 'learning_rate': 0.00019704179575896015, 'epoch': 0.13}        \n",
      " 13%|███▍                       | 132945/1061708 [19:54:11<138:47:45,  1.86it/s][2024-03-01 14:01:11,740] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      " 13%|███▍                       | 132946/1061708 [19:54:11<129:46:28,  1.99it/s][2024-03-01 14:01:12,162] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "{'loss': 2.1863, 'learning_rate': 0.00019704119413319497, 'epoch': 0.13}        \n",
      "{'loss': 1.8853, 'learning_rate': 0.00019704044201625766, 'epoch': 0.13}        \n",
      "{'loss': 1.9027, 'learning_rate': 0.00019703968980517572, 'epoch': 0.13}        \n",
      "{'loss': 1.9976, 'learning_rate': 0.00019703893749994984, 'epoch': 0.13}        \n",
      "{'loss': 1.8992, 'learning_rate': 0.0001970381851005808, 'epoch': 0.13}         \n",
      "{'loss': 2.1099, 'learning_rate': 0.00019703743260706929, 'epoch': 0.13}        \n",
      "{'loss': 1.8055, 'learning_rate': 0.000197036680019416, 'epoch': 0.13}          \n",
      " 13%|███▍                       | 133011/1061708 [19:54:46<145:53:01,  1.77it/s][2024-03-01 14:01:47,075] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "{'loss': 2.1857, 'learning_rate': 0.00019703600261003753, 'epoch': 0.13}        \n",
      "{'loss': 2.1348, 'learning_rate': 0.00019703524984351702, 'epoch': 0.13}        \n",
      "{'loss': 1.9063, 'learning_rate': 0.00019703449698285687, 'epoch': 0.13}        \n",
      "{'loss': 1.8464, 'learning_rate': 0.00019703374402805788, 'epoch': 0.13}        \n",
      "{'loss': 2.1639, 'learning_rate': 0.0001970329909791207, 'epoch': 0.13}         \n",
      "{'loss': 1.8764, 'learning_rate': 0.0001970322378360461, 'epoch': 0.13}         \n",
      "{'loss': 2.0758, 'learning_rate': 0.00019703148459883485, 'epoch': 0.13}        \n",
      "{'loss': 2.2198, 'learning_rate': 0.00019703073126748765, 'epoch': 0.13}        \n",
      "{'loss': 1.9598, 'learning_rate': 0.0001970299778420052, 'epoch': 0.13}         \n",
      "{'loss': 2.4167, 'learning_rate': 0.0001970292243223882, 'epoch': 0.13}         \n",
      "{'loss': 2.2653, 'learning_rate': 0.0001970284707086375, 'epoch': 0.13}         \n",
      " 13%|███▍                       | 133120/1061708 [19:55:44<140:08:25,  1.84it/s][2024-03-01 14:02:45,378] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, but hysteresis is 2. Reducing hysteresis to 1\n",
      "{'loss': 2.0347, 'learning_rate': 0.0001970277923757781, 'epoch': 0.13}         \n",
      "{'loss': 1.485, 'learning_rate': 0.00019702703858317527, 'epoch': 0.13}         \n",
      " 13%|███▍                       | 133144/1061708 [19:55:57<139:05:07,  1.85it/s]"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \\\n",
    "    --deepspeed ds_zero2_no_offload.json \\\n",
    "    --model_name_or_path \"llama_output\"\\\n",
    "    --tokenizer_name_or_path \"merged_tokenizer_hf\" \\\n",
    "    --dataset_dir data \\\n",
    "    --data_cache_dir data_cache \\\n",
    "    --validation_split_percentage 0.001 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --fp16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --seed $RANDOM \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --save_steps 5000 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 8 \\\n",
    "    --block_size 512 \\\n",
    "    --output_dir \"output_model\" \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32\\\n",
    "    --trainable q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj \\\n",
    "    --modules_to_save embed_tokens,lm_head \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --ddp_find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f82130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir output_model/peft_model\n",
    "# !mv \"output_model/pytorch_model.bin\" output_model/peft_model/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a031987-0d77-470b-8b01-6d0225885bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show transformers  # This will confirm the package is installed and show its details\n",
    "# import transformers  # Corrected import statement\n",
    "# print(transformers.__version__)  # This will print the version of the transformers library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "695dfa82-6a79-4cf1-b341-86f140834079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0473494e-ef98-402e-b789-3d4568217961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(f\"Using Python interpreter: {sys.executable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36b73a-33e3-4cbe-a8e6-f1edf82eea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
