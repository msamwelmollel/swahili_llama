{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56395669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/15/db/7f731524fe0e56c6b2eb57d05b55d3badd80ef7d1f1ed59db191b2fdd8ab/protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m261.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m252.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-4.25.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ec3771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/sw.txt\n",
      "  input_format: \n",
      "  model_prefix: swahili_sp\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/sw.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4615 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (11533741), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 11533741 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 2 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(524) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1763831405\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=3010\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 11533741 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1047077555\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1003010 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 11533741\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 4130108\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 4130108 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=669203 obj=11.7742 num_tokens=11002592 num_tokens/piece=16.4413\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=523355 obj=9.22407 num_tokens=10996407 num_tokens/piece=21.0114\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=392275 obj=9.17524 num_tokens=11034730 num_tokens/piece=28.1301\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=390160 obj=9.16939 num_tokens=11075163 num_tokens/piece=28.3862\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=292619 obj=9.17213 num_tokens=11214775 num_tokens/piece=38.3255\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=292615 obj=9.17063 num_tokens=11215383 num_tokens/piece=38.3281\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=219461 obj=9.18296 num_tokens=11494873 num_tokens/piece=52.3777\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=219461 obj=9.17993 num_tokens=11494674 num_tokens/piece=52.3768\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=164594 obj=9.21243 num_tokens=11891239 num_tokens/piece=72.2459\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=164593 obj=9.20298 num_tokens=11890915 num_tokens/piece=72.2444\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=123444 obj=9.25606 num_tokens=12290652 num_tokens/piece=99.5646\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=123444 obj=9.24305 num_tokens=12290684 num_tokens/piece=99.5649\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=92583 obj=9.31453 num_tokens=12744358 num_tokens/piece=137.653\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=92583 obj=9.29859 num_tokens=12744280 num_tokens/piece=137.652\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=69437 obj=9.38835 num_tokens=13223892 num_tokens/piece=190.444\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=69437 obj=9.36999 num_tokens=13224130 num_tokens/piece=190.448\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=52077 obj=9.48232 num_tokens=13758008 num_tokens/piece=264.186\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=52077 obj=9.45997 num_tokens=13758888 num_tokens/piece=264.203\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=39057 obj=9.59953 num_tokens=14367504 num_tokens/piece=367.86\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=39057 obj=9.57194 num_tokens=14368313 num_tokens/piece=367.881\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29292 obj=9.74765 num_tokens=15053673 num_tokens/piece=513.918\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29292 obj=9.7133 num_tokens=15055003 num_tokens/piece=513.963\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=9.93331 num_tokens=15841251 num_tokens/piece=720.057\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22000 obj=9.8886 num_tokens=15843963 num_tokens/piece=720.18\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: swahili_sp.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: swahili_sp.vocab\n"
     ]
    }
   ],
   "source": [
    "# !python train_token.py --input-file \"data/sw.txt\" \\\n",
    "#     --output-dir \"swahili_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748be4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁baba', '▁yangu', '▁ni', '▁mkubwa', '▁wa', '▁familia', '▁yetu', '▁hii']\r\n"
     ]
    }
   ],
   "source": [
    "!python test_token.py --model-path \"swahili_token/swahili_sp.model\" \\\n",
    "    --input-text \"baba yangu ni mkubwa wa familia yetu hii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933f70bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "32000\n",
      "Before:32000\n",
      "New model pieces: 48314\n",
      "Swahili-LLaMA tokenizer has been saved to merged_tokenizer_hf\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " Je Tanzania inaweza kuchukua kombe la mataifa ya Africa?\n",
      "Can Tanzania secure the CAF Cup trophy?\n",
      "Tokenized by LLaMA tokenizer:['▁Je', '▁T', 'anz', 'ania', '▁in', 'a', 'we', 'za', '▁k', 'uch', 'u', 'ku', 'a', '▁kom', 'be', '▁la', '▁m', 'ata', 'ifa', '▁ya', '▁Africa', '?', '<0x0A>', 'Can', '▁T', 'anz', 'ania', '▁secure', '▁the', '▁C', 'AF', '▁Cup', '▁tro', 'phy', '?']\n",
      "LLaMA tokenizer n_tokens=35\n",
      "Tokenized by Swahili-LLaMA tokenizer:['▁Je', '▁Ta', 'nza', 'nia', '▁inaweza', '▁kuchukua', '▁kombe', '▁la', '▁mataifa', '▁ya', '▁Africa', '?', '<0x0A>', 'Can', '▁Ta', 'nza', 'nia', '▁secure', '▁the', '▁CAF', '▁Cup', '▁tro', 'phy', '?']\n",
      "Swahili LLaMA tokenizer n_tokens=24\n"
     ]
    }
   ],
   "source": [
    "!python merge_tokenizer.py --llama_tokenizer_dir \"llama_output\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
